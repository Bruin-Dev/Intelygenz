{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CONCEPTS Monorepo Semantic Release Infrastructure as code Datalake Kafka PIPELINES BASIC CONFIGURATIONS Semantic release AIVEN AWS Snowflake PIPELINES RULES Add new section Add new template DOCUMENTATION Organization Rules Tools MetTel Decisions Metrics definitions Diagrams Logging DEVELOPMENT RULES Branch name convention Semantic release WORK IN A LOCAL ENVIRONMENT Launch docker compose DATALAKE Create private key for a user/service Key rotation policy Add a new provider Rules MANUAL PROCEDURES Vendor access to the API Switch Automation Engine region MANUAL CONFIGURATIONS Init Automation Engine project AWS SSO Okta identity provider Revoke Session token AWS SSO Okta JWT token AUDIT EVENTS Automation Communication between services Messages Bus Use Cases Service outage BYOB IPA Queue HNOC forwarding SA forwarding to ASR TNBA Monitor Ticket severity Ticket creation outcomes Service affecting InterMapper Outage Monitoring Ixia Outage Monitoring Bridges Bruin Bridge VeloCloud Bridge Infrastructure Lambda Parameter-Replicator","title":"Development rules"},{"location":"CONCEPTS/","text":"","title":"Concepts"},{"location":"CREATE_NEW_MICROSERVICE/","text":"CREATE NEW MICROSERVICE This process describes step by step how to create a new microservice, from the ECR repository to the helm chart templates that define the microservice. All of this is created from the Gitlab repository in the pipeline; no need for more tools or actions by the developer. Introduction The process requires that the steps be carried out in order. Basically, it is necessary to create the ECR repository first so that we can then start developing and testing our new microservice in ephemeral environments or in production. 1. Create ECR repository We can't create the ECR repository in the branch where we are developing because the creation or update of the ECR repositories is only in the Master branch. This means that the first thing that we need to do is make a little merge to master to create our new microservice repo, by that way we can deploy our microservice later in dev branches. create a new branch from master create a new terraform ECR repo file in the folder: infra-as-code/basic-infra/3-registry you can copy any of the other repos to have an example. this is an example: resource \"aws_ecr_repository\" \"new-bridge-repository\" { name = \"new-bridge\" tags = { Project = var.common_info.project Provisioning = var.common_info.provisioning Module = \"new-bridge\" } } merge the new repository to Master branch. The pipeline will run and create the new ECR repo new-bridge 2. Create our new microservice folder We can start working on our new microservice based on an existing one. It depends on if is a capability (bridges) or a use case . Select one or other depends on what are you developing. For example let's copy a capability \"bruing-bridge\" and paste in the root of the repo to change his name to \"new-bridge\". from this moment you can start to develop and do your tests locally. Every microservice must have the following directory structure: new-bridge \u251c\u2500\u2500 .gitlab-ci.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 package.json \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u251c\u2500\u2500 app.py \u251c\u2500\u2500 application \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests \u2514\u2500\u2500 ... 3. Update CI-CD gitlab files with the proper values It's important to have the .gitlab-ci.yaml files correctly defined to enable pipelines: * new-bridge/.gitlab-ci.yml Change any reference of the template microservice to the new one: example find and replace \"bruin-bridge\" for \"new-bridge\" * .gitlab-ci.yml (the file in the root of the repository) Here we need to specify to gitlab-ci that we define other jobs in a different directories (the .gitlab-ci.yml of our new repo). So locate the root gitlab file and add a new line with the path of the new micro jobs (do it respecting the alphabetical order) ... - local: 'services/links-metrics-api/.gitlab-ci.yml' - local: 'services/links-metrics-collector/.gitlab-ci.yml' - local: 'services/lumin-billing-report/.gitlab-ci.yml' - local: 'services/new-bridge/.gitlab-ci.yml' <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! - local: 'services/notifier/.gitlab-ci.yml' - local: 'services/service-affecting-monitor/.gitlab-ci.yml' - local: 'services/service-outage-monitor/.gitlab-ci.yml' ... Now in the same file, let's define the \"desired_tasks\" variable for this new micro (do it respecting the alphabetical order): ... NATS_SERVER_DESIRED_TASKS: \"1\" NATS_SERVER_1_DESIRED_TASKS: \"1\" NATS_SERVER_2_DESIRED_TASKS: \"1\" NEW_BRIDGE_DESIRED_TASKS: \"1\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! NOTIFIER_DESIRED_TASKS: \"1\" SERVICE_AFFECTING_MONITOR_DESIRED_TASKS: \"1\" SERVICE_OUTAGE_MONITOR_1_DESIRED_TASKS: \"1\" ... 3. Configure semantic-release We need to edit two files, one in the new micro path and other in the root of the repo: * new-bridge/package.json update the name of the micro with our new working name (do it respecting the alphabetical order): { \"name\": \"new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"version\": \"0.0.1\", \"dependencies\": {}, \"devDependencies\": {} } * package.json (the file in the root of the repository) add to the semantic-release global config our new path to analyze version changes (do it respecting the alphabetical order): ... \"./services/links-metrics-api\", \"./services/links-metrics-collector\", \"./services/lumin-billing-report\", \"./services/new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"./services/notifier\", \"./services/service-affecting-monitor\", \"./services/service-outage-monitor\", ... 3. Configure logs We use 2 systems to storage logs, papertrail for 3 days and cloudwath for 1 month. Let's add those config in: * ci-utils/papertrail-provisioning/config.py just copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... { \"query\": f\"lumin-billing-report AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[lumin-billing-report] - logs\", \"repository\": \"lumin-billing-report\", }, { \u00af\u2502 \"query\": f\"new-bridge AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \u2502 \"search_name\": f\"[new-bridge] - logs\", \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! \"repository\": \"new-bridge\", \u2502 }, _\u2502 { \"query\": f\"notifier AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[notifier] - logs\", \"repository\": \"notifier\", }, ... helm/charts/fluent-bit-custom/templates/configmap.yaml The same; copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... [OUTPUT] Name cloudwatch Match kube.var.log.containers.lumin-billing-report* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name lumin-billing-report auto_create_group true [OUTPUT] \u00af\u2502 Name cloudwatch \u2502 Match kube.var.log.containers.new-bridge* \u2502 region {{ .Values.config.region }} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! log_group_name {{ .Values.config.logGroupName }} \u2502 log_stream_name new-bridge \u2502 auto_create_group true _\u2502 [OUTPUT] Name cloudwatch Match kube.var.log.containers.notifier* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name notifier auto_create_group true ... 3. Update docker-compose to enable local deployments docker-compose.yml here we define our container along with the rest of the microservices. Just add the definition of our container respecting the alphabetical order: ... new-bridge: \u00af\u2502 build: \u2502 # Context must be the root of the monorepo \u2502 context: . \u2502 dockerfile: new-bridge/Dockerfile \u2502 args: \u2502 REPOSITORY_URL: 374050862540.dkr.ecr.us-east-1.amazonaws.com \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! env_file: \u2502 - new-bridge/src/config/env \u2502 depends_on: \u2502 - \"nats-server\" \u2502 - redis \u2502 ports: \u2502 - 5006:5000 _\u2502 notifier: build: ... 4. Add option to enable or disable our microservice helm/charts/automation-engine/Chart.yaml in this file we define our Automation-engine chart version and his dependencies. Let's add a condition for our microservice to have the possibility of disable or enable in our future deployments. ... - name: lumin-billing-report version: '*.*.*' condition: lumin-billing-report.enabled - name: new-bridge \u00af\u2502 version: '*.*.*' \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! condition: new-bridge.enabled _\u2502 - name: notifier version: '*.*.*' condition: notifier.enabled ... 5. Helm templates and variables Here we will define the infrastructure part of our microservice with a helm chart. Is very important to know that this \"how to\" is only to copy an existing microservice, therefore, we take the following statements for granted: 1. The microservice will not have a public endpoint (except email-tagger-monitor) 2. The microservice port will always be 5000 3. Depends on the base chart you use to copy & paste, you will have more or less kubernetes resources. although most microservices have: configmap, deployment, secret and service. Perfect, now let's copy and paste another chart to use as template, if we will develop a use-case, we must copy the most similar use-case. For this example we are creating a \"new-bridge\", so let's copy a \"bruin-bridge\" as a template: * BASE-FOLDER: copy this folder helm/charts/automation-engine/charts/bruin-bridge and paste here helm/charts/automation-engine/charts/ we will have something like bruin-bridge copy change the name to new-bridge . PREPARE BASE_FOLDER: now let's do a find and replace in our new folder new-bridge . find bruin-bridge and replace for new-bridge . many substitutions should appear (at the moment of write this, i can see 46 substitutions in 10 files but over time, this can change). Just remember to do this in the new-bridge folder context to evoid modify other resources. DEPENDENCIES and CHECKS: Now we have to customize our new microservice, first we must ask ourselves, what dependency does my new microservice have on other services? for example, bruin-bridge have a dependency with Nats and Redis, so it have a few checks to see if those services are available and if they are, it can be deployed. We can find this checks in the deployment.yaml file. Specifically in this part: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} ... We can add or remove all the init containers we want. Even, it is very possible that all the dependencies that we need already have the microservice that we use as a base or some other microservice already developed. So we can navigate through the folders of the rest of the microservices and copy any other dependency check and use it in ours. I will add a new dependency for notifier copied from other microservice, and my file will look like the following: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} {{- if .Values.config.capabilities_enabled.notifier }} \u00af\u2502 - name: wait-notifier \u2502 image: busybox:1.28 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! command: ['sh', '-c', 'until wget --spider -S http://notifier:5000/_health; do echo waiting for notifier; sleep 2; done'] \u2502 {{- end }} _\u2502 ... Note that we have a if condition. You will see this in some check, we use this because if we deploy only some microservices, we must contemplate this. If the notifier not exist, the check will not be created. Nats and Redis are always required, that's wy don't have the conditional. VARIABLES: time to update the variables that will use our microservice, this involves various files: helm/charts/automation-engine/charts/new-bridge/templates/configmap.yaml this file always will be part of the deployment, it contains the variables base and the variables with no sensitive information. let's add a new variable NEW_VAR: ... CURRENT_ENVIRONMENT: {{ .Values.global.current_environment }} ENVIRONMENT_NAME: \"{{ .Values.global.environment }}\" NATS_SERVER1: {{ .Values.global.nats_server }} NEW_VAR: \"{{ .Values.config.new_var }}\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! REDIS_HOSTNAME: {{ .Values.global.redis_hostname }} PAPERTRAIL_ACTIVE: \"{{ .Values.global.papertrail_active }}\" PAPERTRAIL_HOST: {{ .Values.global.papertrail_host }} PAPERTRAIL_PORT: \"{{ .Values.global.papertrail_port }}\" PAPERTRAIL_PREFIX: \"{{ .Values.config.papertrail_prefix }}\" ... You can see here two important things, 1. there are variables with quotes and without quotes: this depends on your needs, if you don't put quotes, YAML will interpret the best case for you.. example, if you put a number like 5 as a value, YAML will interpret this as an integer, but careful, this could be a danger if your application expects a string variable; if this is the case, use quotes to define your var. 2. Additionally, we have variables of two types: \"global\" and \"config\". The global ones are common for all microservices, and the \"config\" is specific for this microservice. All the additional variables that we add will be of the type \"config\" helm/charts/automation-engine/charts/new-bridge/templates/secret.yaml this file may or may not exist in the chart and contains variables that have sensitive information. This info will be encoded with base64 to no show in clear text. let's add a new variable NEW_SENSITIVE_VAR: apiVersion: v1 kind: Secret metadata: name: {{ include \"new-bridge.secretName\" . }} labels: {{- include \"new-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" data: NEW_SENSITIVE_VAR: {{ .Values.config.new_sensitive_var | b64enc }} <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! helm/charts/automation-engine/charts/new-bridge/values.yaml Now that we add our new variable in the configmap.yaml, we have to define it in our values file in order to use it. As you can see above, the definition of our variable points to the values file of our microservice; \"Values.config.new_var\" so let's go update it: ... config: <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 in config section!!! papertrail_prefix: \"\" # -- New useful variable with no sensitive information \u00af\u2502______________ here the configmap variable! new_var: \"\" _\u2502 # -- New useful variable with sensitive information \u00af\u2502______________ and here the secret variable! new_sensitive_var: \"\" _\u2502 ... Check that we only define the variable but no put any value, although we can also set a default value if we want. helm/charts/automation-engine/values.yaml This is the values template off the entire automation-engine application. This only have the structure of the values and no contain any real value. For this part we will copy the content of the values file that we just created and paste in the place that correspond (respecting the alphabetical order). It's important to note that we are pasting the values inside another Yaml, so we must adapt the indentation for the destiny file: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: # -- Field to indicate if the lumin-billing-report module is going to be deployed enabled: true # -- Number of replicas of lumin-billing-report module replicaCount: 1 config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"\" # -- URI of Lumin API lumin_uri: \"\" # -- Token credentials for Lumin API lumin_token: \"\" # -- Name of customer to generate lumin-billing-report customer_name: \"\" # -- Email address to send lumin-billing-report billing_recipient: \"\" image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: \"\" service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: 1 \u2502 enabled: true \u2502 config: \u2502 papertrail_prefix: \"\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: \"\" \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: \"\" \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: \"\" \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: # -- Field to indicate if the notifier module is going to be deployed enabled: true # -- Number of replicas of notifier module replicaCount: 1 # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: \"\" # -- notifier Service Configuration ... Things to check: first the indentation!!.. second, the \"global\" config is not set here; it is defined at the beginning of the values file and is common for all microservices. and finally, we remove blank and default configurations to get a shorter file (things removed: autoscaling, the default is false, so we can omit it. nodeSelector. tolerations and affinity). PD: You can keep autoscaling if you will enable it. helm/charts/automation-engine/values.yaml.tpl This is the most important file, it contains the values that will be parsed and used to deploy the Automation-Engine application. Basically it's the same file of values.yaml, but with the variables that will be replaced in the pipeline to deploy a production or develop environment. Let's add our new micro with the variables: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: enabled: ${LUMIN_BILLING_REPORT_ENABLED} replicaCount: ${LUMIN_BILLING_REPORT_DESIRED_TASKS} config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"lumin-billing-report-${LUMIN_BILLING_REPORT_BUILD_NUMBER}\" # -- URI of Lumin API lumin_uri: ${LUMIN_URI} # -- Token credentials for Lumin API lumin_token: ${LUMIN_TOKEN} # -- Name of customer to generate lumin-billing-report customer_name: ${CUSTOMER_NAME_BILLING_REPORT} # -- Email address to send lumin-billing-report billing_recipient: ${BILLING_RECIPIENT} image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: ${LUMIN_BILLING_REPORT_BUILD_NUMBER} service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: ${NEW_BRIDGE_DESIRED_TASKS} \u2502 enabled: ${NEW_BRIDGE_ENABLED} \u2502 config: \u2502 papertrail_prefix: \"new-bridge-${NEW_BRIDGE_BUILD_NUMBER}\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: ${NEW_BRIDGE_NEW_VAR} \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: ${NEW_BRIDGE_NEW_SENSITIVE_VAR} \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: ${NEW_BRIDGE_BUILD_NUMBER} \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: enabled: ${NOTIFIER_ENABLED} replicaCount: ${NOTIFIER_DESIRED_TASKS} # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: ${NOTIFIER_BUILD_NUMBER} # -- notifier Service Configuration service: type: ClusterIP port: 5000 ... With this, we have the entire template of our new microservice. Now we need to set in the pipeline the variables that we just created. ci-utils/environments/deploy_environment_vars.sh In this file, we define the variables that will be used in the values file. Most of the cases are variables that we create in GitLab with the value of dev and production environments. This file is a bash script that has multiple functions to define the variables, each function is for the microservice that requires those variables. If we are adding a new micro that requires variables, we need to define the function and in the bottom of the file execute that function. PD: no all microservices needs specific variables, so in some cases, we wouldn't need to touch this file or even create a secret.yaml. Rebember to respect the alphabetical order: ... function lumin_billing_report_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # lumin-billing-report environment variables for ephemeral environments export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_DEV} else # lumin-billing-report environment variables for production environment export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_PROD} fi } function new_bridge_variables() { \u00af\u2502 if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then \u2502 # new-bridge environment variables for ephemeral environments \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_DEV} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_DEV} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! else \u2502 # new-bridge environment variables for production environment \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_PRO} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_PRO} \u2502 fi \u2502 } _\u2502 function notifier_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # notifier environment variables for ephemeral environments export NOTIFIER_SLACK_URL=${SLACK_URL_DEV} else # notifier environment variables for production environment export NOTIFIER_SLACK_URL=${SLACK_URL_PRO} fi } _\u2502 ... function environments_assign() { # assign enabled variable for each subchart create_enabled_var_for_each_subchart # assign common environment variables for each environment common_variables_by_environment # assign specific environment variables for each subchart bruin_bridge_variables cts_bridge_variables digi_bridge_variables digi_reboot_report_variables email_tagger_monitor_variables hawkeye_bridge_variables links_metrics_api_variables lit_bridge_variables lumin_billing_report_variables new_bridge_variables <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 and here! notifier_variables t7_bridge_variables velocloud_bridge_variables } ... add the variables in gitlab-ci finally, we have all the path until the real value in Gitlab. Let's go to the repository settings/ci-cd section and create the new variables: That's all, with this and the proper commit message the pipeline will run and deploy an ephemeral environment.","title":"CREATE NEW MICROSERVICE"},{"location":"CREATE_NEW_MICROSERVICE/#create-new-microservice","text":"This process describes step by step how to create a new microservice, from the ECR repository to the helm chart templates that define the microservice. All of this is created from the Gitlab repository in the pipeline; no need for more tools or actions by the developer.","title":"CREATE NEW MICROSERVICE"},{"location":"CREATE_NEW_MICROSERVICE/#introduction","text":"The process requires that the steps be carried out in order. Basically, it is necessary to create the ECR repository first so that we can then start developing and testing our new microservice in ephemeral environments or in production.","title":"Introduction"},{"location":"CREATE_NEW_MICROSERVICE/#1-create-ecr-repository","text":"We can't create the ECR repository in the branch where we are developing because the creation or update of the ECR repositories is only in the Master branch. This means that the first thing that we need to do is make a little merge to master to create our new microservice repo, by that way we can deploy our microservice later in dev branches. create a new branch from master create a new terraform ECR repo file in the folder: infra-as-code/basic-infra/3-registry you can copy any of the other repos to have an example. this is an example: resource \"aws_ecr_repository\" \"new-bridge-repository\" { name = \"new-bridge\" tags = { Project = var.common_info.project Provisioning = var.common_info.provisioning Module = \"new-bridge\" } } merge the new repository to Master branch. The pipeline will run and create the new ECR repo new-bridge","title":"1. Create ECR repository"},{"location":"CREATE_NEW_MICROSERVICE/#2-create-our-new-microservice-folder","text":"We can start working on our new microservice based on an existing one. It depends on if is a capability (bridges) or a use case . Select one or other depends on what are you developing. For example let's copy a capability \"bruing-bridge\" and paste in the root of the repo to change his name to \"new-bridge\". from this moment you can start to develop and do your tests locally. Every microservice must have the following directory structure: new-bridge \u251c\u2500\u2500 .gitlab-ci.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 package.json \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u251c\u2500\u2500 app.py \u251c\u2500\u2500 application \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests \u2514\u2500\u2500 ...","title":"2. Create our new microservice folder"},{"location":"CREATE_NEW_MICROSERVICE/#3-update-ci-cd-gitlab-files-with-the-proper-values","text":"It's important to have the .gitlab-ci.yaml files correctly defined to enable pipelines: * new-bridge/.gitlab-ci.yml Change any reference of the template microservice to the new one: example find and replace \"bruin-bridge\" for \"new-bridge\" * .gitlab-ci.yml (the file in the root of the repository) Here we need to specify to gitlab-ci that we define other jobs in a different directories (the .gitlab-ci.yml of our new repo). So locate the root gitlab file and add a new line with the path of the new micro jobs (do it respecting the alphabetical order) ... - local: 'services/links-metrics-api/.gitlab-ci.yml' - local: 'services/links-metrics-collector/.gitlab-ci.yml' - local: 'services/lumin-billing-report/.gitlab-ci.yml' - local: 'services/new-bridge/.gitlab-ci.yml' <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! - local: 'services/notifier/.gitlab-ci.yml' - local: 'services/service-affecting-monitor/.gitlab-ci.yml' - local: 'services/service-outage-monitor/.gitlab-ci.yml' ... Now in the same file, let's define the \"desired_tasks\" variable for this new micro (do it respecting the alphabetical order): ... NATS_SERVER_DESIRED_TASKS: \"1\" NATS_SERVER_1_DESIRED_TASKS: \"1\" NATS_SERVER_2_DESIRED_TASKS: \"1\" NEW_BRIDGE_DESIRED_TASKS: \"1\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! NOTIFIER_DESIRED_TASKS: \"1\" SERVICE_AFFECTING_MONITOR_DESIRED_TASKS: \"1\" SERVICE_OUTAGE_MONITOR_1_DESIRED_TASKS: \"1\" ...","title":"3. Update CI-CD gitlab files with the proper values"},{"location":"CREATE_NEW_MICROSERVICE/#3-configure-semantic-release","text":"We need to edit two files, one in the new micro path and other in the root of the repo: * new-bridge/package.json update the name of the micro with our new working name (do it respecting the alphabetical order): { \"name\": \"new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"version\": \"0.0.1\", \"dependencies\": {}, \"devDependencies\": {} } * package.json (the file in the root of the repository) add to the semantic-release global config our new path to analyze version changes (do it respecting the alphabetical order): ... \"./services/links-metrics-api\", \"./services/links-metrics-collector\", \"./services/lumin-billing-report\", \"./services/new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"./services/notifier\", \"./services/service-affecting-monitor\", \"./services/service-outage-monitor\", ...","title":"3. Configure semantic-release"},{"location":"CREATE_NEW_MICROSERVICE/#3-configure-logs","text":"We use 2 systems to storage logs, papertrail for 3 days and cloudwath for 1 month. Let's add those config in: * ci-utils/papertrail-provisioning/config.py just copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... { \"query\": f\"lumin-billing-report AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[lumin-billing-report] - logs\", \"repository\": \"lumin-billing-report\", }, { \u00af\u2502 \"query\": f\"new-bridge AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \u2502 \"search_name\": f\"[new-bridge] - logs\", \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! \"repository\": \"new-bridge\", \u2502 }, _\u2502 { \"query\": f\"notifier AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[notifier] - logs\", \"repository\": \"notifier\", }, ... helm/charts/fluent-bit-custom/templates/configmap.yaml The same; copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... [OUTPUT] Name cloudwatch Match kube.var.log.containers.lumin-billing-report* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name lumin-billing-report auto_create_group true [OUTPUT] \u00af\u2502 Name cloudwatch \u2502 Match kube.var.log.containers.new-bridge* \u2502 region {{ .Values.config.region }} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! log_group_name {{ .Values.config.logGroupName }} \u2502 log_stream_name new-bridge \u2502 auto_create_group true _\u2502 [OUTPUT] Name cloudwatch Match kube.var.log.containers.notifier* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name notifier auto_create_group true ...","title":"3. Configure logs"},{"location":"CREATE_NEW_MICROSERVICE/#3-update-docker-compose-to-enable-local-deployments","text":"docker-compose.yml here we define our container along with the rest of the microservices. Just add the definition of our container respecting the alphabetical order: ... new-bridge: \u00af\u2502 build: \u2502 # Context must be the root of the monorepo \u2502 context: . \u2502 dockerfile: new-bridge/Dockerfile \u2502 args: \u2502 REPOSITORY_URL: 374050862540.dkr.ecr.us-east-1.amazonaws.com \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! env_file: \u2502 - new-bridge/src/config/env \u2502 depends_on: \u2502 - \"nats-server\" \u2502 - redis \u2502 ports: \u2502 - 5006:5000 _\u2502 notifier: build: ...","title":"3. Update docker-compose to enable local deployments"},{"location":"CREATE_NEW_MICROSERVICE/#4-add-option-to-enable-or-disable-our-microservice","text":"helm/charts/automation-engine/Chart.yaml in this file we define our Automation-engine chart version and his dependencies. Let's add a condition for our microservice to have the possibility of disable or enable in our future deployments. ... - name: lumin-billing-report version: '*.*.*' condition: lumin-billing-report.enabled - name: new-bridge \u00af\u2502 version: '*.*.*' \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! condition: new-bridge.enabled _\u2502 - name: notifier version: '*.*.*' condition: notifier.enabled ...","title":"4. Add option to enable or disable our microservice"},{"location":"CREATE_NEW_MICROSERVICE/#5-helm-templates-and-variables","text":"Here we will define the infrastructure part of our microservice with a helm chart. Is very important to know that this \"how to\" is only to copy an existing microservice, therefore, we take the following statements for granted: 1. The microservice will not have a public endpoint (except email-tagger-monitor) 2. The microservice port will always be 5000 3. Depends on the base chart you use to copy & paste, you will have more or less kubernetes resources. although most microservices have: configmap, deployment, secret and service. Perfect, now let's copy and paste another chart to use as template, if we will develop a use-case, we must copy the most similar use-case. For this example we are creating a \"new-bridge\", so let's copy a \"bruin-bridge\" as a template: * BASE-FOLDER: copy this folder helm/charts/automation-engine/charts/bruin-bridge and paste here helm/charts/automation-engine/charts/ we will have something like bruin-bridge copy change the name to new-bridge . PREPARE BASE_FOLDER: now let's do a find and replace in our new folder new-bridge . find bruin-bridge and replace for new-bridge . many substitutions should appear (at the moment of write this, i can see 46 substitutions in 10 files but over time, this can change). Just remember to do this in the new-bridge folder context to evoid modify other resources. DEPENDENCIES and CHECKS: Now we have to customize our new microservice, first we must ask ourselves, what dependency does my new microservice have on other services? for example, bruin-bridge have a dependency with Nats and Redis, so it have a few checks to see if those services are available and if they are, it can be deployed. We can find this checks in the deployment.yaml file. Specifically in this part: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} ... We can add or remove all the init containers we want. Even, it is very possible that all the dependencies that we need already have the microservice that we use as a base or some other microservice already developed. So we can navigate through the folders of the rest of the microservices and copy any other dependency check and use it in ours. I will add a new dependency for notifier copied from other microservice, and my file will look like the following: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} {{- if .Values.config.capabilities_enabled.notifier }} \u00af\u2502 - name: wait-notifier \u2502 image: busybox:1.28 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! command: ['sh', '-c', 'until wget --spider -S http://notifier:5000/_health; do echo waiting for notifier; sleep 2; done'] \u2502 {{- end }} _\u2502 ... Note that we have a if condition. You will see this in some check, we use this because if we deploy only some microservices, we must contemplate this. If the notifier not exist, the check will not be created. Nats and Redis are always required, that's wy don't have the conditional. VARIABLES: time to update the variables that will use our microservice, this involves various files: helm/charts/automation-engine/charts/new-bridge/templates/configmap.yaml this file always will be part of the deployment, it contains the variables base and the variables with no sensitive information. let's add a new variable NEW_VAR: ... CURRENT_ENVIRONMENT: {{ .Values.global.current_environment }} ENVIRONMENT_NAME: \"{{ .Values.global.environment }}\" NATS_SERVER1: {{ .Values.global.nats_server }} NEW_VAR: \"{{ .Values.config.new_var }}\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! REDIS_HOSTNAME: {{ .Values.global.redis_hostname }} PAPERTRAIL_ACTIVE: \"{{ .Values.global.papertrail_active }}\" PAPERTRAIL_HOST: {{ .Values.global.papertrail_host }} PAPERTRAIL_PORT: \"{{ .Values.global.papertrail_port }}\" PAPERTRAIL_PREFIX: \"{{ .Values.config.papertrail_prefix }}\" ... You can see here two important things, 1. there are variables with quotes and without quotes: this depends on your needs, if you don't put quotes, YAML will interpret the best case for you.. example, if you put a number like 5 as a value, YAML will interpret this as an integer, but careful, this could be a danger if your application expects a string variable; if this is the case, use quotes to define your var. 2. Additionally, we have variables of two types: \"global\" and \"config\". The global ones are common for all microservices, and the \"config\" is specific for this microservice. All the additional variables that we add will be of the type \"config\" helm/charts/automation-engine/charts/new-bridge/templates/secret.yaml this file may or may not exist in the chart and contains variables that have sensitive information. This info will be encoded with base64 to no show in clear text. let's add a new variable NEW_SENSITIVE_VAR: apiVersion: v1 kind: Secret metadata: name: {{ include \"new-bridge.secretName\" . }} labels: {{- include \"new-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" data: NEW_SENSITIVE_VAR: {{ .Values.config.new_sensitive_var | b64enc }} <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! helm/charts/automation-engine/charts/new-bridge/values.yaml Now that we add our new variable in the configmap.yaml, we have to define it in our values file in order to use it. As you can see above, the definition of our variable points to the values file of our microservice; \"Values.config.new_var\" so let's go update it: ... config: <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 in config section!!! papertrail_prefix: \"\" # -- New useful variable with no sensitive information \u00af\u2502______________ here the configmap variable! new_var: \"\" _\u2502 # -- New useful variable with sensitive information \u00af\u2502______________ and here the secret variable! new_sensitive_var: \"\" _\u2502 ... Check that we only define the variable but no put any value, although we can also set a default value if we want. helm/charts/automation-engine/values.yaml This is the values template off the entire automation-engine application. This only have the structure of the values and no contain any real value. For this part we will copy the content of the values file that we just created and paste in the place that correspond (respecting the alphabetical order). It's important to note that we are pasting the values inside another Yaml, so we must adapt the indentation for the destiny file: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: # -- Field to indicate if the lumin-billing-report module is going to be deployed enabled: true # -- Number of replicas of lumin-billing-report module replicaCount: 1 config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"\" # -- URI of Lumin API lumin_uri: \"\" # -- Token credentials for Lumin API lumin_token: \"\" # -- Name of customer to generate lumin-billing-report customer_name: \"\" # -- Email address to send lumin-billing-report billing_recipient: \"\" image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: \"\" service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: 1 \u2502 enabled: true \u2502 config: \u2502 papertrail_prefix: \"\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: \"\" \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: \"\" \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: \"\" \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: # -- Field to indicate if the notifier module is going to be deployed enabled: true # -- Number of replicas of notifier module replicaCount: 1 # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: \"\" # -- notifier Service Configuration ... Things to check: first the indentation!!.. second, the \"global\" config is not set here; it is defined at the beginning of the values file and is common for all microservices. and finally, we remove blank and default configurations to get a shorter file (things removed: autoscaling, the default is false, so we can omit it. nodeSelector. tolerations and affinity). PD: You can keep autoscaling if you will enable it. helm/charts/automation-engine/values.yaml.tpl This is the most important file, it contains the values that will be parsed and used to deploy the Automation-Engine application. Basically it's the same file of values.yaml, but with the variables that will be replaced in the pipeline to deploy a production or develop environment. Let's add our new micro with the variables: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: enabled: ${LUMIN_BILLING_REPORT_ENABLED} replicaCount: ${LUMIN_BILLING_REPORT_DESIRED_TASKS} config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"lumin-billing-report-${LUMIN_BILLING_REPORT_BUILD_NUMBER}\" # -- URI of Lumin API lumin_uri: ${LUMIN_URI} # -- Token credentials for Lumin API lumin_token: ${LUMIN_TOKEN} # -- Name of customer to generate lumin-billing-report customer_name: ${CUSTOMER_NAME_BILLING_REPORT} # -- Email address to send lumin-billing-report billing_recipient: ${BILLING_RECIPIENT} image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: ${LUMIN_BILLING_REPORT_BUILD_NUMBER} service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: ${NEW_BRIDGE_DESIRED_TASKS} \u2502 enabled: ${NEW_BRIDGE_ENABLED} \u2502 config: \u2502 papertrail_prefix: \"new-bridge-${NEW_BRIDGE_BUILD_NUMBER}\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: ${NEW_BRIDGE_NEW_VAR} \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: ${NEW_BRIDGE_NEW_SENSITIVE_VAR} \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: ${NEW_BRIDGE_BUILD_NUMBER} \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: enabled: ${NOTIFIER_ENABLED} replicaCount: ${NOTIFIER_DESIRED_TASKS} # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: ${NOTIFIER_BUILD_NUMBER} # -- notifier Service Configuration service: type: ClusterIP port: 5000 ... With this, we have the entire template of our new microservice. Now we need to set in the pipeline the variables that we just created. ci-utils/environments/deploy_environment_vars.sh In this file, we define the variables that will be used in the values file. Most of the cases are variables that we create in GitLab with the value of dev and production environments. This file is a bash script that has multiple functions to define the variables, each function is for the microservice that requires those variables. If we are adding a new micro that requires variables, we need to define the function and in the bottom of the file execute that function. PD: no all microservices needs specific variables, so in some cases, we wouldn't need to touch this file or even create a secret.yaml. Rebember to respect the alphabetical order: ... function lumin_billing_report_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # lumin-billing-report environment variables for ephemeral environments export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_DEV} else # lumin-billing-report environment variables for production environment export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_PROD} fi } function new_bridge_variables() { \u00af\u2502 if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then \u2502 # new-bridge environment variables for ephemeral environments \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_DEV} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_DEV} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! else \u2502 # new-bridge environment variables for production environment \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_PRO} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_PRO} \u2502 fi \u2502 } _\u2502 function notifier_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # notifier environment variables for ephemeral environments export NOTIFIER_SLACK_URL=${SLACK_URL_DEV} else # notifier environment variables for production environment export NOTIFIER_SLACK_URL=${SLACK_URL_PRO} fi } _\u2502 ... function environments_assign() { # assign enabled variable for each subchart create_enabled_var_for_each_subchart # assign common environment variables for each environment common_variables_by_environment # assign specific environment variables for each subchart bruin_bridge_variables cts_bridge_variables digi_bridge_variables digi_reboot_report_variables email_tagger_monitor_variables hawkeye_bridge_variables links_metrics_api_variables lit_bridge_variables lumin_billing_report_variables new_bridge_variables <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 and here! notifier_variables t7_bridge_variables velocloud_bridge_variables } ... add the variables in gitlab-ci finally, we have all the path until the real value in Gitlab. Let's go to the repository settings/ci-cd section and create the new variables: That's all, with this and the proper commit message the pipeline will run and deploy an ephemeral environment.","title":"5. Helm templates and variables"},{"location":"DOCUMENTATION/","text":"1. DOCS Organization Folder structure diagrams : In this folder you will encounter all the diagrams Intelygenz has done. decisions : Folder where Intelygenz store the decisions made by the team. metrics-definitions : Here is where all the metrics of the project are defined. images : Folder to store images to be use on other MD files. logging : 2. Rules This is the Main source of truth. Docs must be in the docs folder. Always make atomic commits. All the documentation must be reviewed and approved. Before start the code of a new metric, document it, and work on it only after approved by another team member. Organization of the documentation matters. Discuss with your team where is the best place to put new stuff. Always link a new MD file in an index README.md where makes sense. Diagrams are important, before start coding a new system/infrastructure, update the diagrams. 3. Tools Diagrams To create good diagrams centralized in the repository we use diagrams.net , this tool has plugins for the two main IDE that the company use: * IntelliJ * VSCode","title":"Organization"},{"location":"DOCUMENTATION/#1-docs-organization","text":"","title":"1. DOCS Organization"},{"location":"DOCUMENTATION/#folder-structure","text":"diagrams : In this folder you will encounter all the diagrams Intelygenz has done. decisions : Folder where Intelygenz store the decisions made by the team. metrics-definitions : Here is where all the metrics of the project are defined. images : Folder to store images to be use on other MD files. logging :","title":"Folder structure"},{"location":"DOCUMENTATION/#2-rules","text":"This is the Main source of truth. Docs must be in the docs folder. Always make atomic commits. All the documentation must be reviewed and approved. Before start the code of a new metric, document it, and work on it only after approved by another team member. Organization of the documentation matters. Discuss with your team where is the best place to put new stuff. Always link a new MD file in an index README.md where makes sense. Diagrams are important, before start coding a new system/infrastructure, update the diagrams.","title":"2. Rules"},{"location":"DOCUMENTATION/#3-tools","text":"","title":"3. Tools"},{"location":"DOCUMENTATION/#diagrams","text":"To create good diagrams centralized in the repository we use diagrams.net , this tool has plugins for the two main IDE that the company use: * IntelliJ * VSCode","title":"Diagrams"},{"location":"INFRASTRUCTURE_AS_CODE/","text":"INFRASTRUCTURE AS CODE Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. IaC is a key DevOps practice and is used in conjunction with continuous delivery. Azure Introduction Infrastructure as Code enables DevOps to test the deployment of environments before use it in production. IaC can deliver stable environments rapidly and at scale. Avoiding manual configuration of environments and enforce consistency by representing the desired state of their environments via code. This technique improves the automatic deployments in automation-engine, each time the pipelines launch the Continuous delivery will create, update or destroy the infrastructure if it's necessary. IaC in MetTel Automation Automation-engine runs IaC with terraform , this task is/will be included in the automation pipelines . Terraform save the state of the infrastructure in a storage, these files have the extension .tfstate . In MetTel Automation we saves these files in a protected Cloud storage to centralize the states and be accessible each time the pipeline needs to deploy/update the infrastructure. Folder structure infra-as-code/ \u251c\u2500\u2500 basic-infra # basic infrastructure in AWS \u2514\u2500\u2500 data-collector # data-collector infrastructre in AWS \u2514\u2500\u2500 dev # AWS resources for each environment (ECS Cluster, ElastiCache Cluster, etc.) \u2514\u2500\u2500 kre # kre infrastructure \u2514\u2500\u2500 0 -create-bucket # bucket to store EKS information \u2514\u2500\u2500 1 -eks-roles # IAM roles infrastructure for EKS cluster \u2514\u2500\u2500 2 -smtp # SES infrastructure folder \u2514\u2500\u2500 kre-runtimes # kre runtimes infrastructure \u2514\u2500\u2500 modules # custom terraform modules folders used for create KRE infrastructure \u2514\u2500\u2500 runtimes # KRE runtimes folders \u2514\u2500\u2500 network-resources # network resources infrastructure in AWS Al terraform files are located inside ./infra-as-code , in this folder there are four additional folders, basic-infra , dev , ecs-services and network-resources . basic-infra : there are the necessary terraform files to create the Docker images repositories in ECS, and the roles and policies necessary for use these. data-collector : there are the necessary terraform files to create a Lambda, a DocumentDB Cluster, as well as an API Gateway to call the necessary and all the necessary resources to perform the conexion between these elements. These resources will only be created for production environment dev : there are the necessary terraform files for create the resources used for each environment in AWS, these are as follows An ECS Cluster , the ECS Services and its Task Definition for all the microservices present in the project Three ElastiCache Redis Clusters An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A set of Security Groups for all the resources created by the terraform files present in this folder A set of null_resource Terraform type resources to execute the python script in charge of health checking the task instances created in the deployment of capabilities microservices. kre : there are the necessary terraform files for create the infrastructure for the kre component of konstellation , as well as all the components it needs at AWS. There is a series of folders with terraform code that have a number in their names, these will be used to deploy the components in a certain order and are detailed below: 0-create-bucket : In this folder the terraform code is available to create a bucket for each environment and save information about the cluster, such as the SSH key to connect to the worker nodes of the EKS cluster that is going to be created. 1-eks-roles : In this folder the terraform code is available to create different IAM roles to map with EKS users and assign specific permissions for each one, for this purpose, a cli will be used later. 2-create-eks-cluster : In this folder the terraform code is available to create the following resources An EKS cluster to be able to deploy the different kre components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A hosted zone on Route53 for the corresponding kre environment A SSH key to connect to any worker node of EKS 3-smtp : In this folder the terraform code to create a SMTP service through Amazon SES and all the necessary components of it. kre-runtimes : there are the necessary terraform files for create the infrastructure needed by a KRE runtime: modules : Contains the terraform code for custom modules created for provision a KRE runtimes. It will create the following for each KRE runtime: A Route53 Hosted Zone in mettel-automation.net domain. runtimes : Contains the terraform code files for deploy KRE runtimes used the custom module located in modules folder. network-resources : there are the necessary terraform files for create the VPC and all related resources in the environment used for deployment, these being the following: Internet Gateway Elastic IP Addresses NAT Gateways Subnets Route tables for the created subnets A set of Security Groups for all the resources created by the terraform files present in this folder","title":"INFRASTRUCTURE AS CODE"},{"location":"INFRASTRUCTURE_AS_CODE/#infrastructure-as-code","text":"Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. IaC is a key DevOps practice and is used in conjunction with continuous delivery. Azure","title":"INFRASTRUCTURE AS CODE"},{"location":"INFRASTRUCTURE_AS_CODE/#introduction","text":"Infrastructure as Code enables DevOps to test the deployment of environments before use it in production. IaC can deliver stable environments rapidly and at scale. Avoiding manual configuration of environments and enforce consistency by representing the desired state of their environments via code. This technique improves the automatic deployments in automation-engine, each time the pipelines launch the Continuous delivery will create, update or destroy the infrastructure if it's necessary.","title":"Introduction"},{"location":"INFRASTRUCTURE_AS_CODE/#iac-in-mettel-automation","text":"Automation-engine runs IaC with terraform , this task is/will be included in the automation pipelines . Terraform save the state of the infrastructure in a storage, these files have the extension .tfstate . In MetTel Automation we saves these files in a protected Cloud storage to centralize the states and be accessible each time the pipeline needs to deploy/update the infrastructure.","title":"IaC in MetTel Automation"},{"location":"INFRASTRUCTURE_AS_CODE/#folder-structure","text":"infra-as-code/ \u251c\u2500\u2500 basic-infra # basic infrastructure in AWS \u2514\u2500\u2500 data-collector # data-collector infrastructre in AWS \u2514\u2500\u2500 dev # AWS resources for each environment (ECS Cluster, ElastiCache Cluster, etc.) \u2514\u2500\u2500 kre # kre infrastructure \u2514\u2500\u2500 0 -create-bucket # bucket to store EKS information \u2514\u2500\u2500 1 -eks-roles # IAM roles infrastructure for EKS cluster \u2514\u2500\u2500 2 -smtp # SES infrastructure folder \u2514\u2500\u2500 kre-runtimes # kre runtimes infrastructure \u2514\u2500\u2500 modules # custom terraform modules folders used for create KRE infrastructure \u2514\u2500\u2500 runtimes # KRE runtimes folders \u2514\u2500\u2500 network-resources # network resources infrastructure in AWS Al terraform files are located inside ./infra-as-code , in this folder there are four additional folders, basic-infra , dev , ecs-services and network-resources . basic-infra : there are the necessary terraform files to create the Docker images repositories in ECS, and the roles and policies necessary for use these. data-collector : there are the necessary terraform files to create a Lambda, a DocumentDB Cluster, as well as an API Gateway to call the necessary and all the necessary resources to perform the conexion between these elements. These resources will only be created for production environment dev : there are the necessary terraform files for create the resources used for each environment in AWS, these are as follows An ECS Cluster , the ECS Services and its Task Definition for all the microservices present in the project Three ElastiCache Redis Clusters An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A set of Security Groups for all the resources created by the terraform files present in this folder A set of null_resource Terraform type resources to execute the python script in charge of health checking the task instances created in the deployment of capabilities microservices. kre : there are the necessary terraform files for create the infrastructure for the kre component of konstellation , as well as all the components it needs at AWS. There is a series of folders with terraform code that have a number in their names, these will be used to deploy the components in a certain order and are detailed below: 0-create-bucket : In this folder the terraform code is available to create a bucket for each environment and save information about the cluster, such as the SSH key to connect to the worker nodes of the EKS cluster that is going to be created. 1-eks-roles : In this folder the terraform code is available to create different IAM roles to map with EKS users and assign specific permissions for each one, for this purpose, a cli will be used later. 2-create-eks-cluster : In this folder the terraform code is available to create the following resources An EKS cluster to be able to deploy the different kre components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A hosted zone on Route53 for the corresponding kre environment A SSH key to connect to any worker node of EKS 3-smtp : In this folder the terraform code to create a SMTP service through Amazon SES and all the necessary components of it. kre-runtimes : there are the necessary terraform files for create the infrastructure needed by a KRE runtime: modules : Contains the terraform code for custom modules created for provision a KRE runtimes. It will create the following for each KRE runtime: A Route53 Hosted Zone in mettel-automation.net domain. runtimes : Contains the terraform code files for deploy KRE runtimes used the custom module located in modules folder. network-resources : there are the necessary terraform files for create the VPC and all related resources in the environment used for deployment, these being the following: Internet Gateway Elastic IP Addresses NAT Gateways Subnets Route tables for the created subnets A set of Security Groups for all the resources created by the terraform files present in this folder","title":"Folder structure"},{"location":"LOGGING_AND_MONITORING/","text":"Logging and Monitoring Cloudwatch Cloudwatch Log Groups A log group is created in Cloudwatch for the different environments deployed in AWS: Production environment : A log group will be created with the name automation-master in which the different logStreams will be created to store the logs of the different ECS services for the production environment. Ephemeral environments : A log group will be created with the name automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment, the logStreams required for the different ECS services deployed in that environment will be created using the mentioned log group . Cloudwatch Log Streams As mentioned in the previous section, the different logStreams of the deployed services will be stored in a specific logGroup for each environment. A logStream will be created for each of the ECS cluster services tasks created in each environment, which will follow the following scheme <environment_name>-<microservice_image_build_number>/<microservice_name>/<ecs_tasks_id> environment_name : The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. microservice_image_build_number : The pipeline number is used as build number to build the image of those microservices that need to build a new docker image. microservice_name : The name of the microservice deployed in ECS, e.g. bruin-bridge . ecs_task_id> : For each ECS service, one or several tasks are created, depending on the desired number in the service, these tasks are identified with an identifier formed by number and letters, e.g. 961ef51b61834a2e9dd804db564a9fe0 . Cloudwatch Retention Period All environments deployed on AWS have been configured to use Cloudwatch to record the logs of the microservices present in them, although it is important to note the following differences: Production environment : The retention period of the log group created for such an environment is 90 days. Ephemeral environments : The retention period of the log group created for such an environment is 14 days. This retention period is configured in the infra-as-code/dev/logs.tf file. Cloudwatch logs retrieval tool It is possible to obtain the events in logs of a logGroup through a tool designed for this purpose available in Github called log-stream-filter . Download and install This tool is available for Linux, MacOS and Windows, it is possible to download the latest binary for each of these OSs: Linux : It's possible download and install as a deb package curl -o log-stream-filter.deb -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_amd64.deb\")) | .browser_download_url' ) sudo dpkg -i log-stream-filter.deb It's also possible download and install as a simple binary curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_64-bit.tar.gz\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin MacOS : curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"macOS\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin Windows : curl -o log-stream-filter.zip -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"windows\")) | .browser_download_url' ) unzip log-stream-filter.zip Example of usage Example of search text Outage monitoring process finished between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 in log group with name automation-master for log streams that match with name service-outage-monitor-1 using AWS profile with name mettel-automation : $ log-stream-filter -n \"automation-master\" -l \"service-outage-monitor-1\" -a \"mettel-automation\" -s \"04/06/2021 12:00:00\" -e \"04/07/2021 12:00:00\" -T \"Outage monitoring process finished\" -t true Filtering logs for logGroup automation-master params: [ aws-profile mettel-automation ] [ log-stream-filter: service-outage-monitor-1 ] [ search-term-search: true ] [ search-term: Outage monitoring process finished ] [ path: /tmp ] [ start-date: 04 /06/2021 12 :00:00 ] [ end-date: 04 /07/2021 12 :00:00 ] Getting logStreams for logGroup automation-master applying filter service-outage-monitor-1 Getting the logEvents for those logStreams whose last event was inserted between 04 /06/2021 12 :00:00 and 04 /07/2021 12 :00:00 **************************************************************************************************** LogStreamName: automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 CreationTime: 04 /05/2021 23 :08:29 LastEventTime: 04 /06/2021 12 :43:49 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 from time 1617710400000 Event messages for stream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 in log group automation-master are going to be saved in the following files - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 CreationTime: 04 /06/2021 12 :44:40 LastEventTime: 04 /07/2021 10 :39:16 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 from time 1617710400000 Event messages for stream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 CreationTime: 04 /07/2021 10 :41:22 LastEventTime: 04 /07/2021 11 :04:33 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 from time 1617710400000 Event messages for stream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished **************************************************************************************************** 3 files generated for logs of logStreams filtered for logGroup automation-master Location of files where logs of logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 were stored are the following - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 were stored are the following - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 were stored are the following - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished To know all the options it is recommended to read the README of log-stream-filter in github. Papertrail There is a papertrail account used in the project for sending logs, but it will only be used for the production environment , this is because it is a high cost service and it is not feasible to have an account for all ephemeral environments due to its cost and the high volume of logs generated in these environments. Papertrail dashboards The system logs of the production environment are stored in papertrail, the account credentials are in the project's onepassword vault to which all people in the project have access. Below is a screenshot of the main screen of the system. In this system it's possible see three useful dashboards: [production] - master alarms : alarms are defined on different modules with notifications sent to slack through the mettel-alarms-papertrail-production channel, for example when the number of error messages in a module exceeds 100 times in an hour. Below is a screenshot where these alarms have been marked in a red rectangle. [production] - master logs : searches are defined to gather the logs of the replicas of each deployed microservice. Below is a screenshot where these searches have been marked in a red rectangle. [production] - master notifications : Searches on different modules are defined with their notification to slack through the mettel-notifications-papertrail-production channel. Below is a screenshot where these searches have been marked in a red rectangle. Papertrail logging configuration A certain configuration must be made for the microservices to use papertrail for sending logs in production, this configuration is made in the LOG_CONFIG section of the config.py file present in the src/config folder of each microservice, this configuration is shown below: LOG_CONFIG = { 'name' : '<microservice_name>' , 'level' : logging.DEBUG, 'stream_handler' : logging.StreamHandler ( sys.stdout ) , 'format' : f '%(asctime)s: {ENVIRONMENT_NAME}: %(hostname)s: %(module)s::%(lineno)d %(levelname)s: %(message)s' , 'papertrail' : { 'active' : True if os.getenv ( 'PAPERTRAIL_ACTIVE' ) == \"true\" else False, 'prefix' : os.getenv ( 'PAPERTRAIL_PREFIX' , f '{ENVIRONMENT_NAME}-<microservice_name>' ) , 'host' : os.getenv ( 'PAPERTRAIL_HOST' ) , 'port' : int ( os.getenv ( 'PAPERTRAIL_PORT' )) } , } It would be necessary to put the microservice name instead of microservice_name for each microservice in the project Papertrail searches configuration The papertrail-provisioning tool is available in the repository to configure the different groups of searches in papertrail, you must follow the procedure explained in the README of the same for its use. The above mentioned guide should be followed to add log searches for a specific microservice, it is also possible to configure alarms that will send notifications to slack.","title":"LOGGING AND MONITORING"},{"location":"LOGGING_AND_MONITORING/#logging-and-monitoring","text":"","title":"Logging and Monitoring"},{"location":"LOGGING_AND_MONITORING/#cloudwatch","text":"","title":"Cloudwatch"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-log-groups","text":"A log group is created in Cloudwatch for the different environments deployed in AWS: Production environment : A log group will be created with the name automation-master in which the different logStreams will be created to store the logs of the different ECS services for the production environment. Ephemeral environments : A log group will be created with the name automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment, the logStreams required for the different ECS services deployed in that environment will be created using the mentioned log group .","title":"Cloudwatch Log Groups"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-log-streams","text":"As mentioned in the previous section, the different logStreams of the deployed services will be stored in a specific logGroup for each environment. A logStream will be created for each of the ECS cluster services tasks created in each environment, which will follow the following scheme <environment_name>-<microservice_image_build_number>/<microservice_name>/<ecs_tasks_id> environment_name : The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. microservice_image_build_number : The pipeline number is used as build number to build the image of those microservices that need to build a new docker image. microservice_name : The name of the microservice deployed in ECS, e.g. bruin-bridge . ecs_task_id> : For each ECS service, one or several tasks are created, depending on the desired number in the service, these tasks are identified with an identifier formed by number and letters, e.g. 961ef51b61834a2e9dd804db564a9fe0 .","title":"Cloudwatch Log Streams"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-retention-period","text":"All environments deployed on AWS have been configured to use Cloudwatch to record the logs of the microservices present in them, although it is important to note the following differences: Production environment : The retention period of the log group created for such an environment is 90 days. Ephemeral environments : The retention period of the log group created for such an environment is 14 days. This retention period is configured in the infra-as-code/dev/logs.tf file.","title":"Cloudwatch Retention Period"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-logs-retrieval-tool","text":"It is possible to obtain the events in logs of a logGroup through a tool designed for this purpose available in Github called log-stream-filter .","title":"Cloudwatch logs retrieval tool"},{"location":"LOGGING_AND_MONITORING/#download-and-install","text":"This tool is available for Linux, MacOS and Windows, it is possible to download the latest binary for each of these OSs: Linux : It's possible download and install as a deb package curl -o log-stream-filter.deb -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_amd64.deb\")) | .browser_download_url' ) sudo dpkg -i log-stream-filter.deb It's also possible download and install as a simple binary curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_64-bit.tar.gz\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin MacOS : curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"macOS\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin Windows : curl -o log-stream-filter.zip -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"windows\")) | .browser_download_url' ) unzip log-stream-filter.zip","title":"Download and install"},{"location":"LOGGING_AND_MONITORING/#example-of-usage","text":"Example of search text Outage monitoring process finished between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 in log group with name automation-master for log streams that match with name service-outage-monitor-1 using AWS profile with name mettel-automation : $ log-stream-filter -n \"automation-master\" -l \"service-outage-monitor-1\" -a \"mettel-automation\" -s \"04/06/2021 12:00:00\" -e \"04/07/2021 12:00:00\" -T \"Outage monitoring process finished\" -t true Filtering logs for logGroup automation-master params: [ aws-profile mettel-automation ] [ log-stream-filter: service-outage-monitor-1 ] [ search-term-search: true ] [ search-term: Outage monitoring process finished ] [ path: /tmp ] [ start-date: 04 /06/2021 12 :00:00 ] [ end-date: 04 /07/2021 12 :00:00 ] Getting logStreams for logGroup automation-master applying filter service-outage-monitor-1 Getting the logEvents for those logStreams whose last event was inserted between 04 /06/2021 12 :00:00 and 04 /07/2021 12 :00:00 **************************************************************************************************** LogStreamName: automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 CreationTime: 04 /05/2021 23 :08:29 LastEventTime: 04 /06/2021 12 :43:49 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 from time 1617710400000 Event messages for stream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 in log group automation-master are going to be saved in the following files - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 CreationTime: 04 /06/2021 12 :44:40 LastEventTime: 04 /07/2021 10 :39:16 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 from time 1617710400000 Event messages for stream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 CreationTime: 04 /07/2021 10 :41:22 LastEventTime: 04 /07/2021 11 :04:33 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 from time 1617710400000 Event messages for stream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished **************************************************************************************************** 3 files generated for logs of logStreams filtered for logGroup automation-master Location of files where logs of logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 were stored are the following - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 were stored are the following - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 were stored are the following - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished To know all the options it is recommended to read the README of log-stream-filter in github.","title":"Example of usage"},{"location":"LOGGING_AND_MONITORING/#papertrail","text":"There is a papertrail account used in the project for sending logs, but it will only be used for the production environment , this is because it is a high cost service and it is not feasible to have an account for all ephemeral environments due to its cost and the high volume of logs generated in these environments.","title":"Papertrail"},{"location":"LOGGING_AND_MONITORING/#papertrail-dashboards","text":"The system logs of the production environment are stored in papertrail, the account credentials are in the project's onepassword vault to which all people in the project have access. Below is a screenshot of the main screen of the system. In this system it's possible see three useful dashboards: [production] - master alarms : alarms are defined on different modules with notifications sent to slack through the mettel-alarms-papertrail-production channel, for example when the number of error messages in a module exceeds 100 times in an hour. Below is a screenshot where these alarms have been marked in a red rectangle. [production] - master logs : searches are defined to gather the logs of the replicas of each deployed microservice. Below is a screenshot where these searches have been marked in a red rectangle. [production] - master notifications : Searches on different modules are defined with their notification to slack through the mettel-notifications-papertrail-production channel. Below is a screenshot where these searches have been marked in a red rectangle.","title":"Papertrail dashboards"},{"location":"LOGGING_AND_MONITORING/#papertrail-logging-configuration","text":"A certain configuration must be made for the microservices to use papertrail for sending logs in production, this configuration is made in the LOG_CONFIG section of the config.py file present in the src/config folder of each microservice, this configuration is shown below: LOG_CONFIG = { 'name' : '<microservice_name>' , 'level' : logging.DEBUG, 'stream_handler' : logging.StreamHandler ( sys.stdout ) , 'format' : f '%(asctime)s: {ENVIRONMENT_NAME}: %(hostname)s: %(module)s::%(lineno)d %(levelname)s: %(message)s' , 'papertrail' : { 'active' : True if os.getenv ( 'PAPERTRAIL_ACTIVE' ) == \"true\" else False, 'prefix' : os.getenv ( 'PAPERTRAIL_PREFIX' , f '{ENVIRONMENT_NAME}-<microservice_name>' ) , 'host' : os.getenv ( 'PAPERTRAIL_HOST' ) , 'port' : int ( os.getenv ( 'PAPERTRAIL_PORT' )) } , } It would be necessary to put the microservice name instead of microservice_name for each microservice in the project","title":"Papertrail logging configuration"},{"location":"LOGGING_AND_MONITORING/#papertrail-searches-configuration","text":"The papertrail-provisioning tool is available in the repository to configure the different groups of searches in papertrail, you must follow the procedure explained in the README of the same for its use. The above mentioned guide should be followed to add log searches for a specific microservice, it is also possible to configure alarms that will send notifications to slack.","title":"Papertrail searches configuration"},{"location":"MONOREPO/","text":"Monorepo In revision control systems, a monorepo (syllabic abbreviation of monolithic repository) is a software development strategy where code for many projects are stored in the same repository. Wikipedia Advantages Simplified organization : The organization is simplified separating all the projects(called modules) in different folders that are stored in the root folder of the repository. Simplified automation : The automation gets easy with this approach, each time the repo has a commit in develop or master the automation will deploy all the necessary parts of the app to make it work correctly. Refactoring changes : When a project has a dependency with another in the monorepo, the changes are easier to be made. Atomic commits : When projects that work together are contained in separate repositories, releases need to determine which versions of one project are related to the other and then syncing them. Collaboration across team : The integration between projects will be easier thanks to the branches strategy Single source of truth : Like a developer you'll find all the available code, automation and documentation in the same place. What is not a monorepo about Monorepo is not the same that Monolith. Monolith is huge amount of coupled code of one application that is hell to maintain. This is not only a repository, it's a sum of good practices between automation, code and documentation.","title":"MONOREPO"},{"location":"MONOREPO/#monorepo","text":"In revision control systems, a monorepo (syllabic abbreviation of monolithic repository) is a software development strategy where code for many projects are stored in the same repository. Wikipedia","title":"Monorepo"},{"location":"MONOREPO/#advantages","text":"Simplified organization : The organization is simplified separating all the projects(called modules) in different folders that are stored in the root folder of the repository. Simplified automation : The automation gets easy with this approach, each time the repo has a commit in develop or master the automation will deploy all the necessary parts of the app to make it work correctly. Refactoring changes : When a project has a dependency with another in the monorepo, the changes are easier to be made. Atomic commits : When projects that work together are contained in separate repositories, releases need to determine which versions of one project are related to the other and then syncing them. Collaboration across team : The integration between projects will be easier thanks to the branches strategy Single source of truth : Like a developer you'll find all the available code, automation and documentation in the same place.","title":"Advantages"},{"location":"MONOREPO/#what-is-not-a-monorepo-about","text":"Monorepo is not the same that Monolith. Monolith is huge amount of coupled code of one application that is hell to maintain. This is not only a repository, it's a sum of good practices between automation, code and documentation.","title":"What is not a monorepo about"},{"location":"PIPELINES/","text":"Pipelines In this project is implemented Software delivery with total automation, thus avoiding manual intervention and therefore human errors in our product. Human error can and does occur when carrying out these boring and repetitive tasks manually and ultimately does affect the ability to meet deliverables. All of the automation is made with Gitlab CI technology, taking advantage of all the tools that Gitlab has. We separate the automatation in two parts, continuous integration and continuous delivery , that are explained in the next sections. To improve the speed and optimization of the pipelines, only the jobs and stages will be executed on those modules that change in each commit . Launch all jobs in a pipeline Exceptionally, it is possible to launch a pipeline with all the jobs and stages on a branch using the web interface, as shown in the following image . To do so, the following steps must be followed: From the project repository select the CI/CD option in the left sidebar and this Pipelines , as shown in the following image where these options are marked in red. Choose the Run Pipeline option, as shown in the image below in red. Indicate the branch where you want to run the pipeline in the Run for box and then click on Run pipeline . It's possible see an example in the following image, where the box Run for is shown in green and Run pipeline is shown in red. It is important to note that due to the extra time added by the tests of the dispacth-portal-frontend microservice, the tests of this one will only be executed when any of the files within it change or a pipeline with the Gitlab variable TEST_DISPATCH_PORTAL_FRONTEND with value true is executed. Environments Microservices Environments For the microservices there are the following environments Production : The environment is related to everything currently running in AWS related to the latest version of the master branch of the repository. Ephemerals : These environments are created from branches that start with name dev/feature or dev/fix . The name of any environment, regardless of the type, will identify all the resources created in the deployment process. The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. These names will identify all the resources created in AWS during the continuous delivery process, explained in the following sections. KRE Environments For KRE component there are the following environments: dev : This will be used for the various tests and calls made from the project's microservices in any ephemeral environment , ie from the microservices deployed in the ECS cluster with name automation-<environment_id> . production : This will be used for the different calls made from the project's microservices in the production environment , that is, from the microservices deployed in the ECS cluster with the name automation-master . Continuous integration (CI) Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day. Each integration can then be verified by an automated build and automated tests. While automated testing is not strictly part of CI it is typically implied. Codeship Validation steps This stage checks the following: All python microservices comply with the rules of PEP8 Terraform files used to configure the infrastructure are valid from a syntactic point of view. The frontend modules comply with the linter configured for them Unit tests steps All the available unit tests for each service should be run in this stage of the CI process. If the coverage obtained from these tests for a service is not greater than or equal to 80%, it will cause this phase to fail, this will mean that the steps of the next stage will not be executed and the process will fail. In cases in which a module does not reach the minimum coverage mentioned above, a message like the following will be seen in the step executed for that module. Continuous delivery (CD) Continuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements. Puppet.com Basic_infra steps This area covers the checking and creation, if necessary, of all the basic resources for the subsequent deployment, these being the specific image repositories in ECR Docker Container Registry , as well as the roles necessary in AWS to be able to display these images in ECS Container Orchestrator . In this stage there is also a job that must be executed manually if necessary, this is responsible for checking and creating if necessary network resources for the production environment or ephemeral environments. In this stage is also checked whether there are enough free resources in ECS to carry out the deployment with success or not. It's necessary run the basic-infra job the first time a new microservice is created in the project This has been done because ECR repositories are global resources and are stored in the same tfstate file, thus avoiding that when a microservice that creates a repository is created, it is not deleted by other branches that do not have it added. Basic_infra_kre steps In this stage will be the following jobs: * deploy-basic-infra-kre-dev for ephemeral environments and deploy-basic-infra-kre-production for the production environment, these are executed optionally manually . This job is responsible of checking and creation, if necessary, of the EKS cluster used by KRE in each environment and all the necessary resources related (RBAC configuration, helm charts needed for the KRE runtimes, etc) The process followed in this job is as follows: The necessary infrastructure is created in AWS for KRE, creating for them the following components: An S3 bucket for each environment and save information about the cluster, such as the SSH key to connect to the nodes. An EKS cluster to be able to deploy the different KRE components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A SMTP service through Amazon SES and all the necessary components of it A set of IAM roles, one for each user with access to the project. These will be used to assign subsequent permissions in the Kubernetes cluster according to the role they belong to. These are stored as terraform output values , saving the list of user roles belonging to each role in their corresponding variable. Below is an example of a pipeline execution where it's possible see the IAM roles of users generated for each role in the project: Outputs: eks_developer_ops_privileged_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xisco.capllonch\" , \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xoan.mallon.developer\" , ] eks_developer_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-brandon.samudio\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-daniel.fernandez\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-joseluis.vega\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-marc.vivancos\" , ] eks_devops_roles = [ \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-alberto.iglesias\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.costales\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.luis.piquero\" , ] A set of helm charts necessary for any KRE runtime: external-dns , using the helm chart from bitnami repository external-dns is a Kubernetes addon that configures public DNS servers with information about exposed Kubernetes services to make them discoverable. It allows in a simple way that through the creation of an ingress in AWS you can create an entry in Route53 of type alias so that the calls to that ingress redirect to the value configured for the alias, being the most typical the DNS of the balancer created by the ingress. cert-mananger , using the helm chart from jetstack repository . This component automate the management lifecycle of all required certificates used by the KRE component in each environment. nginx ingress controller , using the helm chart from ingress-nginx repository . A series of configurations are provided so that the IP of clients in Kubernetes services can be known, since by default it will always use the internal IP in EKS of the load balancer for requests made from the Internet. A list of allowed IPs is also provided in the chart configuration through a specific configuration key, thus restricting access to the cluster's microservices. This component will create a Classic Load Balancer in AWS to expose nginx ingress component. hostpath provisioner , using the helm chart from rimusz repository Using a Python cli , permissions are assigned in Kubernetes Cluster created for KRE for each of the IAM roles created in the previous step. Deploy_kre_runtimes steps In this stage the KRE runtimes will be deployed in the corresponding environment, creating the necessary infrastructure and resources: A Hosted Zone in Route53 for the runtime in the specified environment using the mettel-automation.net domain name The kre helm chart with the necessary values for the environment creating a specific namespace in the EKS cluster for deploy the helm chart Build steps This area will cover all build steps of all necessary modules to deploy the app to the selected environment. It's typical to build the docker images and push to the repository in this step. Deploy steps In this stage there are one job: deploy-branches for ephemeral environments and deploy-master for the production environment, these are executed automatically . In which MetTel Automation modules in the monorepo will be deployed to the selected environment, as well as all the resources associated to that environment in AWS. The deploy steps will deploy the following in AWS: An ECS Cluster will be created for the environment with a set of resources An ECS Service that will use the new Docker image uploaded for each service of the project, being these services the specified below: bruin-bridge cts-bridge customer-cache dispatch-portal-backend dispatch-portal-frontend last-contact-report lit-bridge lumin-billing-report metrics-prometheus nats-server, nats-server-1, nats-server-2 notifier service-affecting-monitor service-dispatch-monitor service-outage-monitor-1, service-outage-monitor-2, service-outage-monitor-3, service-outage-monitor-4, service-outage-monitor-triage t7-bridge tnba-feedback tnba-monitor velocloud-bridge A Task Definition for each of the above ECS Services In this process, a series of resources will also be created in AWS for the selected environment, as follows: Three ElastiCache Redis Clusters , which are detailed below: <environment> : used to save some data about dispatches, as well as to skip the limitation of messages of more than 1MB when passing them to NATS. <environment>-customer-cache-redis : used to save the mappings between Bruin clients and Velocloud edges, being able to use them between restarts if any occurs. <environment>-tnba-feedback-cache-redis : used to save ticket metrics sent to T7, so tnba-feedback can avoid sending them again afterwards. An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A S3 bucket to store the content of the metrics obtained by Thanos and displayed through Grafana . Also, resources of type null_resource are created to execute some Python scripts: The creation of ECS Services starts only if a Python script launched as a null_resource finishes with success. This script checks that the last ECS service created for NATS is running in HEALTHY state. If the previous step succeeded then ECS services related to capabilities microservices are created, with these being the following: bruin-bridge cts-bridge lit-bridge notifier prometheus t7-bridge velocloud-bridge hawkeye-bridge Once created, the script used for NATS is launched through null_resource to check that the task instances for each of these ECS services were created successfully and are in RUNNING and HEALTHY status. Once all the scripts for the capabilities microservices have finished successfully, ECS services for the use-cases microservices are all created, with these being the following: customer-cache dispatch-portal-backend hawkeye-customer-cache hawkeye-outage-monitor last-contact-report lumin-billing-report service-affecting-monitor service-dispatch-monitor service-outage-monitor-1 service-outage-monitor-2 service-outage-monitor-3 service-outage-monitor-4 service-outage-monitor-triage tnba-feedback tnba-monitor This is achieved by defining explicit dependencies between the ECS services for the capabilities microservices and the set of null resources that perform the healthcheck of the capabilities microservices.\u200b The following is an example of a definition for the use-case microservice service-affecting-monitor using Terraform . Here, the dependency between the corresponding null_resource type resources in charge of performing the health check of the different capabilities microservices in Terraform code for this microservice is established. resource \"aws_ecs_service\" \"automation-service-affecting-monitor\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck ] . . . } This procedure has been done to ensure that use case microservices are not created in ECS until new versions of the capability-type microservices are properly deployed, as use case microservices need to use capability-type microservices. Following the same procedure as in the previous step, a dependency is established between the microservice dispatch-portal-frontend and dispatch-portal-backend . The reason for this is that the dispatch-portal-frontend microservice needs to know the corresponding IP with the DNS entry in Route53 for the dispatch-portal-backend microservice, since if the previous deployment is saved, the new IP corresponding to the DNS entry is not updated. The following is the configuration in the terraform code of the service in ECS for the dispatch-portal-frontend microservice, where the necessary configuration to comply with this restriction can be seen. resource \"aws_ecs_service\" \"automation-dispatch-portal-frontend\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck , null_resource.dispatch-portal-backend-healthcheck , aws_lb.automation-alb ] } The provisioning of the different groups and the searches included in each one of them is done through a python utility , this makes calls to the util go-papertrail-cli who is in charge of the provisioning of the elements mentioned in Papertrail . Destroy steps In this stage a series of manual jobs are available to destroy what was created in the previous stage, both for KRE and for the microservices of the repository in AWS. These are detailed below: destroy-branches for ephemeral environments or destroy-master for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-branches or deploy-master depending on the environment. destroy-branches-aws-nuke : This job is only available for ephemeral environments, it generates a yml file using a specific script to be used by aws-nuke to destroy all the infrastructure created for an ephemeral environment in AWS. This job should only be used when the `destroy-branches' job fails. destroy-basic-infra-kre-dev for ephemeral environments or destroy-basic-infra-kre-production for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-kre-dev or deploy-kre-production depending on the environment.","title":"PIPELINES"},{"location":"PIPELINES/#pipelines","text":"In this project is implemented Software delivery with total automation, thus avoiding manual intervention and therefore human errors in our product. Human error can and does occur when carrying out these boring and repetitive tasks manually and ultimately does affect the ability to meet deliverables. All of the automation is made with Gitlab CI technology, taking advantage of all the tools that Gitlab has. We separate the automatation in two parts, continuous integration and continuous delivery , that are explained in the next sections. To improve the speed and optimization of the pipelines, only the jobs and stages will be executed on those modules that change in each commit .","title":"Pipelines"},{"location":"PIPELINES/#launch-all-jobs-in-a-pipeline","text":"Exceptionally, it is possible to launch a pipeline with all the jobs and stages on a branch using the web interface, as shown in the following image . To do so, the following steps must be followed: From the project repository select the CI/CD option in the left sidebar and this Pipelines , as shown in the following image where these options are marked in red. Choose the Run Pipeline option, as shown in the image below in red. Indicate the branch where you want to run the pipeline in the Run for box and then click on Run pipeline . It's possible see an example in the following image, where the box Run for is shown in green and Run pipeline is shown in red. It is important to note that due to the extra time added by the tests of the dispacth-portal-frontend microservice, the tests of this one will only be executed when any of the files within it change or a pipeline with the Gitlab variable TEST_DISPATCH_PORTAL_FRONTEND with value true is executed.","title":"Launch all jobs in a pipeline"},{"location":"PIPELINES/#environments","text":"","title":"Environments"},{"location":"PIPELINES/#microservices-environments","text":"For the microservices there are the following environments Production : The environment is related to everything currently running in AWS related to the latest version of the master branch of the repository. Ephemerals : These environments are created from branches that start with name dev/feature or dev/fix . The name of any environment, regardless of the type, will identify all the resources created in the deployment process. The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. These names will identify all the resources created in AWS during the continuous delivery process, explained in the following sections.","title":"Microservices Environments"},{"location":"PIPELINES/#kre-environments","text":"For KRE component there are the following environments: dev : This will be used for the various tests and calls made from the project's microservices in any ephemeral environment , ie from the microservices deployed in the ECS cluster with name automation-<environment_id> . production : This will be used for the different calls made from the project's microservices in the production environment , that is, from the microservices deployed in the ECS cluster with the name automation-master .","title":"KRE Environments"},{"location":"PIPELINES/#continuous-integration-ci","text":"Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day. Each integration can then be verified by an automated build and automated tests. While automated testing is not strictly part of CI it is typically implied. Codeship","title":"Continuous integration (CI)"},{"location":"PIPELINES/#validation-steps","text":"This stage checks the following: All python microservices comply with the rules of PEP8 Terraform files used to configure the infrastructure are valid from a syntactic point of view. The frontend modules comply with the linter configured for them","title":"Validation steps"},{"location":"PIPELINES/#unit-tests-steps","text":"All the available unit tests for each service should be run in this stage of the CI process. If the coverage obtained from these tests for a service is not greater than or equal to 80%, it will cause this phase to fail, this will mean that the steps of the next stage will not be executed and the process will fail. In cases in which a module does not reach the minimum coverage mentioned above, a message like the following will be seen in the step executed for that module.","title":"Unit tests steps"},{"location":"PIPELINES/#continuous-delivery-cd","text":"Continuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements. Puppet.com","title":"Continuous delivery (CD)"},{"location":"PIPELINES/#basic_infra-steps","text":"This area covers the checking and creation, if necessary, of all the basic resources for the subsequent deployment, these being the specific image repositories in ECR Docker Container Registry , as well as the roles necessary in AWS to be able to display these images in ECS Container Orchestrator . In this stage there is also a job that must be executed manually if necessary, this is responsible for checking and creating if necessary network resources for the production environment or ephemeral environments. In this stage is also checked whether there are enough free resources in ECS to carry out the deployment with success or not. It's necessary run the basic-infra job the first time a new microservice is created in the project This has been done because ECR repositories are global resources and are stored in the same tfstate file, thus avoiding that when a microservice that creates a repository is created, it is not deleted by other branches that do not have it added.","title":"Basic_infra steps"},{"location":"PIPELINES/#basic_infra_kre-steps","text":"In this stage will be the following jobs: * deploy-basic-infra-kre-dev for ephemeral environments and deploy-basic-infra-kre-production for the production environment, these are executed optionally manually . This job is responsible of checking and creation, if necessary, of the EKS cluster used by KRE in each environment and all the necessary resources related (RBAC configuration, helm charts needed for the KRE runtimes, etc) The process followed in this job is as follows: The necessary infrastructure is created in AWS for KRE, creating for them the following components: An S3 bucket for each environment and save information about the cluster, such as the SSH key to connect to the nodes. An EKS cluster to be able to deploy the different KRE components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A SMTP service through Amazon SES and all the necessary components of it A set of IAM roles, one for each user with access to the project. These will be used to assign subsequent permissions in the Kubernetes cluster according to the role they belong to. These are stored as terraform output values , saving the list of user roles belonging to each role in their corresponding variable. Below is an example of a pipeline execution where it's possible see the IAM roles of users generated for each role in the project: Outputs: eks_developer_ops_privileged_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xisco.capllonch\" , \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xoan.mallon.developer\" , ] eks_developer_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-brandon.samudio\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-daniel.fernandez\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-joseluis.vega\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-marc.vivancos\" , ] eks_devops_roles = [ \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-alberto.iglesias\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.costales\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.luis.piquero\" , ] A set of helm charts necessary for any KRE runtime: external-dns , using the helm chart from bitnami repository external-dns is a Kubernetes addon that configures public DNS servers with information about exposed Kubernetes services to make them discoverable. It allows in a simple way that through the creation of an ingress in AWS you can create an entry in Route53 of type alias so that the calls to that ingress redirect to the value configured for the alias, being the most typical the DNS of the balancer created by the ingress. cert-mananger , using the helm chart from jetstack repository . This component automate the management lifecycle of all required certificates used by the KRE component in each environment. nginx ingress controller , using the helm chart from ingress-nginx repository . A series of configurations are provided so that the IP of clients in Kubernetes services can be known, since by default it will always use the internal IP in EKS of the load balancer for requests made from the Internet. A list of allowed IPs is also provided in the chart configuration through a specific configuration key, thus restricting access to the cluster's microservices. This component will create a Classic Load Balancer in AWS to expose nginx ingress component. hostpath provisioner , using the helm chart from rimusz repository Using a Python cli , permissions are assigned in Kubernetes Cluster created for KRE for each of the IAM roles created in the previous step.","title":"Basic_infra_kre steps"},{"location":"PIPELINES/#deploy_kre_runtimes-steps","text":"In this stage the KRE runtimes will be deployed in the corresponding environment, creating the necessary infrastructure and resources: A Hosted Zone in Route53 for the runtime in the specified environment using the mettel-automation.net domain name The kre helm chart with the necessary values for the environment creating a specific namespace in the EKS cluster for deploy the helm chart","title":"Deploy_kre_runtimes steps"},{"location":"PIPELINES/#build-steps","text":"This area will cover all build steps of all necessary modules to deploy the app to the selected environment. It's typical to build the docker images and push to the repository in this step.","title":"Build steps"},{"location":"PIPELINES/#deploy-steps","text":"In this stage there are one job: deploy-branches for ephemeral environments and deploy-master for the production environment, these are executed automatically . In which MetTel Automation modules in the monorepo will be deployed to the selected environment, as well as all the resources associated to that environment in AWS. The deploy steps will deploy the following in AWS: An ECS Cluster will be created for the environment with a set of resources An ECS Service that will use the new Docker image uploaded for each service of the project, being these services the specified below: bruin-bridge cts-bridge customer-cache dispatch-portal-backend dispatch-portal-frontend last-contact-report lit-bridge lumin-billing-report metrics-prometheus nats-server, nats-server-1, nats-server-2 notifier service-affecting-monitor service-dispatch-monitor service-outage-monitor-1, service-outage-monitor-2, service-outage-monitor-3, service-outage-monitor-4, service-outage-monitor-triage t7-bridge tnba-feedback tnba-monitor velocloud-bridge A Task Definition for each of the above ECS Services In this process, a series of resources will also be created in AWS for the selected environment, as follows: Three ElastiCache Redis Clusters , which are detailed below: <environment> : used to save some data about dispatches, as well as to skip the limitation of messages of more than 1MB when passing them to NATS. <environment>-customer-cache-redis : used to save the mappings between Bruin clients and Velocloud edges, being able to use them between restarts if any occurs. <environment>-tnba-feedback-cache-redis : used to save ticket metrics sent to T7, so tnba-feedback can avoid sending them again afterwards. An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A S3 bucket to store the content of the metrics obtained by Thanos and displayed through Grafana . Also, resources of type null_resource are created to execute some Python scripts: The creation of ECS Services starts only if a Python script launched as a null_resource finishes with success. This script checks that the last ECS service created for NATS is running in HEALTHY state. If the previous step succeeded then ECS services related to capabilities microservices are created, with these being the following: bruin-bridge cts-bridge lit-bridge notifier prometheus t7-bridge velocloud-bridge hawkeye-bridge Once created, the script used for NATS is launched through null_resource to check that the task instances for each of these ECS services were created successfully and are in RUNNING and HEALTHY status. Once all the scripts for the capabilities microservices have finished successfully, ECS services for the use-cases microservices are all created, with these being the following: customer-cache dispatch-portal-backend hawkeye-customer-cache hawkeye-outage-monitor last-contact-report lumin-billing-report service-affecting-monitor service-dispatch-monitor service-outage-monitor-1 service-outage-monitor-2 service-outage-monitor-3 service-outage-monitor-4 service-outage-monitor-triage tnba-feedback tnba-monitor This is achieved by defining explicit dependencies between the ECS services for the capabilities microservices and the set of null resources that perform the healthcheck of the capabilities microservices.\u200b The following is an example of a definition for the use-case microservice service-affecting-monitor using Terraform . Here, the dependency between the corresponding null_resource type resources in charge of performing the health check of the different capabilities microservices in Terraform code for this microservice is established. resource \"aws_ecs_service\" \"automation-service-affecting-monitor\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck ] . . . } This procedure has been done to ensure that use case microservices are not created in ECS until new versions of the capability-type microservices are properly deployed, as use case microservices need to use capability-type microservices. Following the same procedure as in the previous step, a dependency is established between the microservice dispatch-portal-frontend and dispatch-portal-backend . The reason for this is that the dispatch-portal-frontend microservice needs to know the corresponding IP with the DNS entry in Route53 for the dispatch-portal-backend microservice, since if the previous deployment is saved, the new IP corresponding to the DNS entry is not updated. The following is the configuration in the terraform code of the service in ECS for the dispatch-portal-frontend microservice, where the necessary configuration to comply with this restriction can be seen. resource \"aws_ecs_service\" \"automation-dispatch-portal-frontend\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck , null_resource.dispatch-portal-backend-healthcheck , aws_lb.automation-alb ] } The provisioning of the different groups and the searches included in each one of them is done through a python utility , this makes calls to the util go-papertrail-cli who is in charge of the provisioning of the elements mentioned in Papertrail .","title":"Deploy steps"},{"location":"PIPELINES/#destroy-steps","text":"In this stage a series of manual jobs are available to destroy what was created in the previous stage, both for KRE and for the microservices of the repository in AWS. These are detailed below: destroy-branches for ephemeral environments or destroy-master for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-branches or deploy-master depending on the environment. destroy-branches-aws-nuke : This job is only available for ephemeral environments, it generates a yml file using a specific script to be used by aws-nuke to destroy all the infrastructure created for an ephemeral environment in AWS. This job should only be used when the `destroy-branches' job fails. destroy-basic-infra-kre-dev for ephemeral environments or destroy-basic-infra-kre-production for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-kre-dev or deploy-kre-production depending on the environment.","title":"Destroy steps"},{"location":"README_AUTOMATION/","text":"Documentation directory System overview System architecture Monorepo Pipelines Launch all jobs in a pipeline Environments CI CD Infrastructure as code Infrastructure Environment infrastructure Network infrastructure Logging and monitoring Cloudwatch Cloudwatch Log Groups Cloudwatch Log Streams Cloudwatch Retention Periord Cloudwatch logs retrieval tool Papertrail Papertrail Dashboards Papertrail Logging Configuration Papertrail Searches Configuration","title":"README AUTOMATION"},{"location":"README_AUTOMATION/#documentation-directory","text":"System overview System architecture Monorepo Pipelines Launch all jobs in a pipeline Environments CI CD Infrastructure as code Infrastructure Environment infrastructure Network infrastructure Logging and monitoring Cloudwatch Cloudwatch Log Groups Cloudwatch Log Streams Cloudwatch Retention Periord Cloudwatch logs retrieval tool Papertrail Papertrail Dashboards Papertrail Logging Configuration Papertrail Searches Configuration","title":"Documentation directory"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/","text":"Context Configurations are stored in AWS Systems Manager Parameter Store . Parameter Store is a capability of AWS Systems Manager that allows storing configuration values in hierarchical structures. It also provides the ability to encrypt these values through AWS KMS keys, and allows keeping track of configuration changes / versions for auditing purposes. From a pricing perspective, parameters can be defined either as simple or advanced : - Simple . This kind of parameters come with no additional charges, and can hold up to 4KB of data. Parameter Store allows defining up to 10,000 simple parameters per account and region. - Advanced . AWS charges $0.05 per advanced parameter per month. These can hold up to 8KB of data, and Parameter Store allows defining up to 100,000 of them per account and region. Attending to their type, parameters can be defined as: * String . These parameters consist of a block of text. Nothing else. * StringList . These parameters hold comma-separated values, but bear in mind that this is just a convention. AWS will not turn the value into an array before returning it through Parameter Store API; that is, it will be kept as a string. * SecureString . Essentially, these are the same as String parameters, but empowered with encryption features thanks to KMS keys. Conventions Please, take some time to go through the following conventions before publishing new parameters to Parameter Store. Parameter names Any new configuration published to Parameter Store must match the following pattern: /automation-engine/<environment>/<service-name>/<parameter> where: * environment refers to the Kubernetes context whose namespaces will take this parameter. Allowed values are dev , pro and common . * service-name refers to the name of the service this parameter will be loaded to. Examples of allowed values would be bruin-bridge , tnba-monitor , repair-tickets-monitor , and so on. * parameter refers to the name of the parameter that will be loaded to the service through an environment variable. Most configurations will follow the previous pattern, but if the business logic of a service carries out several tasks related to the same domain concept, an alternate form can be used: /automation-engine/<environment>/<domain>/<task>/<parameter> where: * domain refers to a domain concept that can represent multiple tasks accurately. An example would be service-affecting , which is a domain area that represents strongly related tasks (in this case, tasks related to Service Affecting troubles). * task refers to the underlying piece of logic that is independent of other tasks, but is still suitable to have under the subpath defined by domain . Considering that domain is set to service-affecting , acceptable values for task would be monitor , daily-bandwidth-report , or reoccurring-trouble-report . As a rule of thumb, choose the first pattern to name parameters if possible. However, if there is a strong reason to choose the second pattern over the first one, that is totally fine. If there are doubts about choosing one or the other, it can be brought up to discussion with the rest of the team. Values Before getting into how to define new configurations, bear these considerations in mind to keep certain consistency and avoid confusion / potential errors: * Values representing a time duration must always be expressed in seconds. * 24 should not be used to represent the hours in a period of 1 day. * 86400 should be used to represent the hours in a period of 1 day. Values representing percentages must always be expressed in their integer form. 0.75 should not be used to represent the value 75% . 75 should be used to represent the value 75% . JSON-like values must adhere to the JSON specification . The most relevant considerations are: All keys in an object-like JSON must be strings, even if they represent numbers. Example: // NOT valid { 1 : \"foo\" , 2 : \"bar\" } // Valid { \"1\" : \"foo\" , \"2\" : \"bar\" } // Valid { \"foo\" : \"bar\" , \"baz\" : \"hey\" } 2. An array-like JSON can hold a combination of values with different types, be them integers, floats, boolean, arrays, objects, and so on. Example: [ 1 , \"foo\" , 0.75 , \"2\" , \"bar\" , true , [ \"hello\" , \"world\" ], { \"hello\" : \"world\" } ] Consider prettifying JSON-like configurations before publishing to Parameter Store. This enhances readability when dealing with huge JSONs. Developers are responsible for making any necessary transformations related to data types, time conversions, etc. in the config files of a particular service . Encryption A parameter must be encrypted if it holds: * Personally Identifiable Information. This includes e-mail addresses, names, surnames, phone numbers, and so on. * Authentication credentials of any kind. * URLs from third party services. * Information related to application domains, such as: * IDs (even incremental ones) * Organization names * Domain-related values that potentially expose internal details about third party systems Publishing configurations to Parameter Store Adding new configurations to Parameter Store is pretty straightforward. The process should be as easy as: 1. Access the AWS Management Console. 2. Go to the AWS Systems Manager Parameter Store dashboard. 3. Hit Create parameter . 4. Give the parameter a Name that complies with any of the patterns under the Parameter names section. 5. Make sure to add a meaningful Description to the parameter. This is extremely important to give context to anyone in need of making changes to parameters, so take some time to think about a good, meaningful description. 6. Choose the tier that fits better for this parameter: 1. If the parameter is small and is not expected to grow much as time passes, choose Standard . 2. On the other hand, if the parameter is large enough and is expected to grow, choose Advanced . 7. Choose the type that fits better for this parameter: 1. If the parameter is safe to stay unencrypted in AWS: 1. Choose String . 2. Set the Data Type field to text . 2. On the other hand, if the parameter needs to be encrypted (see the Encryption section): 1. Choose SecureString . 2. Set the KMS Key Source field to My current account to pick a KMS key registered in the current account. 3. Set the KMS Key ID field to alias/aws/ssm to pick the default KMS key for Parameter Store. In general, avoid using StringList parameters. These are special cases of the String type, which essentially means that String can be used to create parameters based on comma-separated values as well. 8. Give the parameter a value that adheres to the conventions specified in the Values section. 9. Finally, hit Create parameter to save it. About the different environments When creating a new parameter in the Parameter Store, please decide if it will be different on dev and pro or if it will be same in both environments. If they're different, you should create a parameter for each environment. Otherwise, just create one under common . In general, most parameters will share the same value, unless there is a strong reason to keep them different. For example, parameters used to point to third party systems may differ if their app has not only a Production system, but also a Development / Test one. In that case, the pro version of the parameter should aim at the third party's Production system, and the dev version should aim at their Development / Test system. Hooking AWS Parameter Store with Kubernetes clusters Pre-requisites Before moving on, make sure to install k9s in your system. k9s is a powerful terminal UI that lets users manage Kubernetes clusters from their own machines, and even edit Kubernetes objects in place to reflect those changes immediately. After installing k9s , follow these steps to install a plugin that allows editing decoded Secret objects (more in the next section): 1. Install krew , following the instructions for the desired OS. 2. Install kubectl 's plugin kubectl-modify-secret , following the instructions for the desired OS. 3. Integrate kubectl-modify-secret into k9s 's plugin system: 1. In a terminal, run k9s info to check which folder holds k9s configurations. 2. Create file plugin.yml under that folder, if it does not exist yet. 3. Add the following snippet: plugin : edit-secret : shortCut : Ctrl-X confirm : false description : \"Edit Decoded Secret\" scopes : - secrets command : kubectl background : false args : - modify-secret - --namespace - $NAMESPACE - --context - $CONTEXT - $NAME > By default, the shortcut to edit decoded secrets is set to Ctrl + X . If needed, set a custom one instead. Kubernetes: Secrets and ConfigMaps Secret and ConfigMap objects are the Kubernetes objects used to inject environment variables to one or multiple pods. Like other Kubernetes objects, they are defined through .yaml files. These objects hold mappings between environment variables and their values, along with some metadata. Here is an example of a ConfigMap definition: apiVersion : v1 data : CURRENT_ENVIRONMENT : production ENVIRONMENT_NAME : production REDIS_HOSTNAME : redis.pro.somewhere.com kind : ConfigMap metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2021-08-30T15:08:12Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : case-of-use project : mettel-automation name : some-fancy-bridge-configmap namespace : automation-engine resourceVersion : \"83171937\" selfLink : /api/v1/namespaces/automation-engine/configmaps/some-fancy-bridge-configmap uid : ba625451-8ba4-4ac7-a8da-593cc938eae7 And here is an example of a Secret definition: apiVersion : v1 data : THIRD_PARTY_API_USERNAME : aGVsbG8K THIRD_PARTY_API_PASSWORD : d29ybGQK kind : Secret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reconcile.external-secrets.io/data-hash : 3162fd065a6777587e9a3a604e0c56e2 reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:31:36Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine ownerReferences : - apiVersion : external-secrets.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ExternalSecret name : some-fancy-bridge-secret uid : 42bdad1c-37c4-4890-b86b-d9623333df18 resourceVersion : \"82588568\" selfLink : /api/v1/namespaces/automation-engine/secrets/some-fancy-bridge-secret uid : 81ad3356-8696-406e-bba0-c4ec389676ee type : Opaque Both objects have a data field where the different configurations are stored. The main difference between both objects is that all fields under data remain clear in ConfigMap objects, but in Secret ones, these fields are base64-encoded. These objects lack of mechanisms to pull configurations from external sources, so an additional tool is needed to hook them with AWS Parameter Store. External secrets The (External Secrets Operator)[https://github.com/external-secrets/external-secrets] is a tool that allows setting up Secret objects based on external references through a new kind of object called ExternalSecret . ExternalSecret objects define the external source to pull configurations from, and the references that should be resolved. After these references have been resolved, the ExternalSecret will create a regular Secret object with a data section whose key-value pairs are based on the environment variables the pod expects to see, and the values gotten after resolving the references from the external source. Aside from that, ExternalSecret objects pull configuration values from the external source periodically to keep secrets up to date, also known as reconciling secrets . An example of ExternalSecret object would be this one: apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:11:02Z\" generation : 1 labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine resourceVersion : \"83201847\" selfLink : /apis/external-secrets.io/v1alpha1/namespaces/automation-engine/externalsecrets/some-fancy-bridge-secret uid : ca5c1faf-1b76-4f59-b57f-e10e43154ace spec : data : - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE status : conditions : - lastTransitionTime : \"2022-03-08T18:11:10Z\" message : Secret was synced reason : SecretSynced status : \"True\" type : Ready refreshTime : \"2022-03-10T13:29:12Z\" syncedResourceVersion : 1-3efb62db37d8b935be922ecc6f7ed99f In this example, there are two items under the data section. Each item refers to an external / remote reference (field remoteRef::key ), and the environment variable that the value behind that remote reference should be loaded to (field secretKey ). Manipulating external secrets in the Automation system To add, remove, or update external secrets for a particular service in the Automation system, head to helm/charts/automation-engine/<service>/templates/external-secret.yaml and make the appropriate changes under the data section. For example, consider the following external-secret.yaml : {{ - if and .Values.global.externalSecrets.enabled - }} apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : name : {{ include \"some-fancy-bridge.secretName\" . }} labels : {{ - include \"some-fancy-bridge.labels\" . | nindent 4 }} annotations : reloader.stakater.com/match : \"true\" spec : secretStoreRef : name : {{ .Values.global.environment }} -parameter-store kind : SecretStore target : creationPolicy : 'Owner' Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration) May be set to zero to fetch and create it {{ - if eq .Values.global.current_environment \"dev\" }} refreshInterval : \"0\" {{ else }} refreshInterval : \"5m\" {{ - end }} data : {{ - with .Values.global.externalSecrets.envPath }} - remoteRef : key : {{ .commonPath }} /some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : {{ .envPath }} /some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE {{ - end }} {{ - end }} If a new configuration called THIRD_PARTY_API_URL must be added to the underlying Secret created by this ExternalSecret , a new item should be placed under the data section, and it should look like this: - remoteRef : key : {{ . }} /some-fancy-bridge/third-party-api-url secretKey : THIRD_PARTY_API_URL The change can then be deployed to the Kubernetes cluster. A word of caution about ephemeral environments Although external-secrets is part of the EKS cluster for production and the cluster for ephemeral environments, the truth is it behaves differently across contexts. In the production cluster, ExternalSecret objects are configured to pull configurations from AWS Parameter Store every 5 minutes . That way, if a developer adds, removes, or updated a parameter in AWS, the cluster will realize about that event to update the regular Secret object created by the ExternalSecret with the most recent value. This will trigger another piece in the cluster called reloader , which ultimately will kill any pod that relies on the updated secret to spin up a new one with the configurations in the updated Secret . In ephemeral environments however, this is completely different. The ExternalSecret object will create a Secret object only when the ephemeral environment is deployed for the first time. After it has been deployed, control and management of Secret objects is delegated to developers; that is, external-secrets will never pull parameters from AWS again. The reasoning behind this behavior is that these parameters are shared by all ephemeral environments, so if one of them were to be updated in AWS and the polling rate was set to 5 minutes , all ephemeral environments would be updated because they all share the same reference. So essentially, the set of configurations for ephemeral environments that are stored in AWS are used as a template to populate ExternalSecret and Secret objects in ephemeral environments. If a secret needs to be updated, k9s should be used to edit the Secret in place. Using an AWS param key in our services Add the new variable to automation-engine/installation-utils/environment_files_generator.py SERVICE__DOMAIN__PARAM_KEY_NAME = parameters [ 'environment' ][ 'service' ][ 'domain' ][ 'param-key-name' ] And inside the env dictionary in the same file f 'DOMAIN__PARAM_KEY_NAME= { SERVICE__DOMAIN__PARAM_KEY_NAME } ' , Add the new variable as a template to services/<service>/src/config/.template.env DOMAIN__PARAM_KEY_NAME = Add the new variable retrieving the value from the environment to services/<service>/src/config/config.py 'param_key_name' : ( os . environ [ 'DOMAIN__PARAM_KEY_NAME' ] ), Any parameter that should be used with a primitive type other than str , should be cast here. We MUST use primitive types. DO NOT convert values to complex types like timedelta or datetime . Consumers of the service configuration should be responsible for doing that. Add the new variable to services/<service>/src/config/testconfig.py for being available when testing 'param_key_name' : 86400 After that you can use it in the service self . _config . MONITOR_CONFIG [ 'param_key_name' ] And while testing param_key_name = action . _config . MONITOR_CONFIG [ 'param_key_name' ]","title":"SYSTEM CONFIGURATIONS THROUGH AWS PARAM KEYS"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#context","text":"Configurations are stored in AWS Systems Manager Parameter Store . Parameter Store is a capability of AWS Systems Manager that allows storing configuration values in hierarchical structures. It also provides the ability to encrypt these values through AWS KMS keys, and allows keeping track of configuration changes / versions for auditing purposes. From a pricing perspective, parameters can be defined either as simple or advanced : - Simple . This kind of parameters come with no additional charges, and can hold up to 4KB of data. Parameter Store allows defining up to 10,000 simple parameters per account and region. - Advanced . AWS charges $0.05 per advanced parameter per month. These can hold up to 8KB of data, and Parameter Store allows defining up to 100,000 of them per account and region. Attending to their type, parameters can be defined as: * String . These parameters consist of a block of text. Nothing else. * StringList . These parameters hold comma-separated values, but bear in mind that this is just a convention. AWS will not turn the value into an array before returning it through Parameter Store API; that is, it will be kept as a string. * SecureString . Essentially, these are the same as String parameters, but empowered with encryption features thanks to KMS keys.","title":"Context"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#conventions","text":"Please, take some time to go through the following conventions before publishing new parameters to Parameter Store.","title":"Conventions"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#parameter-names","text":"Any new configuration published to Parameter Store must match the following pattern: /automation-engine/<environment>/<service-name>/<parameter> where: * environment refers to the Kubernetes context whose namespaces will take this parameter. Allowed values are dev , pro and common . * service-name refers to the name of the service this parameter will be loaded to. Examples of allowed values would be bruin-bridge , tnba-monitor , repair-tickets-monitor , and so on. * parameter refers to the name of the parameter that will be loaded to the service through an environment variable. Most configurations will follow the previous pattern, but if the business logic of a service carries out several tasks related to the same domain concept, an alternate form can be used: /automation-engine/<environment>/<domain>/<task>/<parameter> where: * domain refers to a domain concept that can represent multiple tasks accurately. An example would be service-affecting , which is a domain area that represents strongly related tasks (in this case, tasks related to Service Affecting troubles). * task refers to the underlying piece of logic that is independent of other tasks, but is still suitable to have under the subpath defined by domain . Considering that domain is set to service-affecting , acceptable values for task would be monitor , daily-bandwidth-report , or reoccurring-trouble-report . As a rule of thumb, choose the first pattern to name parameters if possible. However, if there is a strong reason to choose the second pattern over the first one, that is totally fine. If there are doubts about choosing one or the other, it can be brought up to discussion with the rest of the team.","title":"Parameter names"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#values","text":"Before getting into how to define new configurations, bear these considerations in mind to keep certain consistency and avoid confusion / potential errors: * Values representing a time duration must always be expressed in seconds. * 24 should not be used to represent the hours in a period of 1 day. * 86400 should be used to represent the hours in a period of 1 day. Values representing percentages must always be expressed in their integer form. 0.75 should not be used to represent the value 75% . 75 should be used to represent the value 75% . JSON-like values must adhere to the JSON specification . The most relevant considerations are: All keys in an object-like JSON must be strings, even if they represent numbers. Example: // NOT valid { 1 : \"foo\" , 2 : \"bar\" } // Valid { \"1\" : \"foo\" , \"2\" : \"bar\" } // Valid { \"foo\" : \"bar\" , \"baz\" : \"hey\" } 2. An array-like JSON can hold a combination of values with different types, be them integers, floats, boolean, arrays, objects, and so on. Example: [ 1 , \"foo\" , 0.75 , \"2\" , \"bar\" , true , [ \"hello\" , \"world\" ], { \"hello\" : \"world\" } ] Consider prettifying JSON-like configurations before publishing to Parameter Store. This enhances readability when dealing with huge JSONs. Developers are responsible for making any necessary transformations related to data types, time conversions, etc. in the config files of a particular service .","title":"Values"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#encryption","text":"A parameter must be encrypted if it holds: * Personally Identifiable Information. This includes e-mail addresses, names, surnames, phone numbers, and so on. * Authentication credentials of any kind. * URLs from third party services. * Information related to application domains, such as: * IDs (even incremental ones) * Organization names * Domain-related values that potentially expose internal details about third party systems","title":"Encryption"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#publishing-configurations-to-parameter-store","text":"Adding new configurations to Parameter Store is pretty straightforward. The process should be as easy as: 1. Access the AWS Management Console. 2. Go to the AWS Systems Manager Parameter Store dashboard. 3. Hit Create parameter . 4. Give the parameter a Name that complies with any of the patterns under the Parameter names section. 5. Make sure to add a meaningful Description to the parameter. This is extremely important to give context to anyone in need of making changes to parameters, so take some time to think about a good, meaningful description. 6. Choose the tier that fits better for this parameter: 1. If the parameter is small and is not expected to grow much as time passes, choose Standard . 2. On the other hand, if the parameter is large enough and is expected to grow, choose Advanced . 7. Choose the type that fits better for this parameter: 1. If the parameter is safe to stay unencrypted in AWS: 1. Choose String . 2. Set the Data Type field to text . 2. On the other hand, if the parameter needs to be encrypted (see the Encryption section): 1. Choose SecureString . 2. Set the KMS Key Source field to My current account to pick a KMS key registered in the current account. 3. Set the KMS Key ID field to alias/aws/ssm to pick the default KMS key for Parameter Store. In general, avoid using StringList parameters. These are special cases of the String type, which essentially means that String can be used to create parameters based on comma-separated values as well. 8. Give the parameter a value that adheres to the conventions specified in the Values section. 9. Finally, hit Create parameter to save it.","title":"Publishing configurations to Parameter Store"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#about-the-different-environments","text":"When creating a new parameter in the Parameter Store, please decide if it will be different on dev and pro or if it will be same in both environments. If they're different, you should create a parameter for each environment. Otherwise, just create one under common . In general, most parameters will share the same value, unless there is a strong reason to keep them different. For example, parameters used to point to third party systems may differ if their app has not only a Production system, but also a Development / Test one. In that case, the pro version of the parameter should aim at the third party's Production system, and the dev version should aim at their Development / Test system.","title":"About the different environments"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#hooking-aws-parameter-store-with-kubernetes-clusters","text":"","title":"Hooking AWS Parameter Store with Kubernetes clusters"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#pre-requisites","text":"Before moving on, make sure to install k9s in your system. k9s is a powerful terminal UI that lets users manage Kubernetes clusters from their own machines, and even edit Kubernetes objects in place to reflect those changes immediately. After installing k9s , follow these steps to install a plugin that allows editing decoded Secret objects (more in the next section): 1. Install krew , following the instructions for the desired OS. 2. Install kubectl 's plugin kubectl-modify-secret , following the instructions for the desired OS. 3. Integrate kubectl-modify-secret into k9s 's plugin system: 1. In a terminal, run k9s info to check which folder holds k9s configurations. 2. Create file plugin.yml under that folder, if it does not exist yet. 3. Add the following snippet: plugin : edit-secret : shortCut : Ctrl-X confirm : false description : \"Edit Decoded Secret\" scopes : - secrets command : kubectl background : false args : - modify-secret - --namespace - $NAMESPACE - --context - $CONTEXT - $NAME > By default, the shortcut to edit decoded secrets is set to Ctrl + X . If needed, set a custom one instead.","title":"Pre-requisites"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#kubernetes-secrets-and-configmaps","text":"Secret and ConfigMap objects are the Kubernetes objects used to inject environment variables to one or multiple pods. Like other Kubernetes objects, they are defined through .yaml files. These objects hold mappings between environment variables and their values, along with some metadata. Here is an example of a ConfigMap definition: apiVersion : v1 data : CURRENT_ENVIRONMENT : production ENVIRONMENT_NAME : production REDIS_HOSTNAME : redis.pro.somewhere.com kind : ConfigMap metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2021-08-30T15:08:12Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : case-of-use project : mettel-automation name : some-fancy-bridge-configmap namespace : automation-engine resourceVersion : \"83171937\" selfLink : /api/v1/namespaces/automation-engine/configmaps/some-fancy-bridge-configmap uid : ba625451-8ba4-4ac7-a8da-593cc938eae7 And here is an example of a Secret definition: apiVersion : v1 data : THIRD_PARTY_API_USERNAME : aGVsbG8K THIRD_PARTY_API_PASSWORD : d29ybGQK kind : Secret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reconcile.external-secrets.io/data-hash : 3162fd065a6777587e9a3a604e0c56e2 reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:31:36Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine ownerReferences : - apiVersion : external-secrets.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ExternalSecret name : some-fancy-bridge-secret uid : 42bdad1c-37c4-4890-b86b-d9623333df18 resourceVersion : \"82588568\" selfLink : /api/v1/namespaces/automation-engine/secrets/some-fancy-bridge-secret uid : 81ad3356-8696-406e-bba0-c4ec389676ee type : Opaque Both objects have a data field where the different configurations are stored. The main difference between both objects is that all fields under data remain clear in ConfigMap objects, but in Secret ones, these fields are base64-encoded. These objects lack of mechanisms to pull configurations from external sources, so an additional tool is needed to hook them with AWS Parameter Store.","title":"Kubernetes: Secrets and ConfigMaps"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#external-secrets","text":"The (External Secrets Operator)[https://github.com/external-secrets/external-secrets] is a tool that allows setting up Secret objects based on external references through a new kind of object called ExternalSecret . ExternalSecret objects define the external source to pull configurations from, and the references that should be resolved. After these references have been resolved, the ExternalSecret will create a regular Secret object with a data section whose key-value pairs are based on the environment variables the pod expects to see, and the values gotten after resolving the references from the external source. Aside from that, ExternalSecret objects pull configuration values from the external source periodically to keep secrets up to date, also known as reconciling secrets . An example of ExternalSecret object would be this one: apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:11:02Z\" generation : 1 labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine resourceVersion : \"83201847\" selfLink : /apis/external-secrets.io/v1alpha1/namespaces/automation-engine/externalsecrets/some-fancy-bridge-secret uid : ca5c1faf-1b76-4f59-b57f-e10e43154ace spec : data : - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE status : conditions : - lastTransitionTime : \"2022-03-08T18:11:10Z\" message : Secret was synced reason : SecretSynced status : \"True\" type : Ready refreshTime : \"2022-03-10T13:29:12Z\" syncedResourceVersion : 1-3efb62db37d8b935be922ecc6f7ed99f In this example, there are two items under the data section. Each item refers to an external / remote reference (field remoteRef::key ), and the environment variable that the value behind that remote reference should be loaded to (field secretKey ).","title":"External secrets"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#manipulating-external-secrets-in-the-automation-system","text":"To add, remove, or update external secrets for a particular service in the Automation system, head to helm/charts/automation-engine/<service>/templates/external-secret.yaml and make the appropriate changes under the data section. For example, consider the following external-secret.yaml : {{ - if and .Values.global.externalSecrets.enabled - }} apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : name : {{ include \"some-fancy-bridge.secretName\" . }} labels : {{ - include \"some-fancy-bridge.labels\" . | nindent 4 }} annotations : reloader.stakater.com/match : \"true\" spec : secretStoreRef : name : {{ .Values.global.environment }} -parameter-store kind : SecretStore target : creationPolicy : 'Owner' Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration) May be set to zero to fetch and create it {{ - if eq .Values.global.current_environment \"dev\" }} refreshInterval : \"0\" {{ else }} refreshInterval : \"5m\" {{ - end }} data : {{ - with .Values.global.externalSecrets.envPath }} - remoteRef : key : {{ .commonPath }} /some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : {{ .envPath }} /some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE {{ - end }} {{ - end }} If a new configuration called THIRD_PARTY_API_URL must be added to the underlying Secret created by this ExternalSecret , a new item should be placed under the data section, and it should look like this: - remoteRef : key : {{ . }} /some-fancy-bridge/third-party-api-url secretKey : THIRD_PARTY_API_URL The change can then be deployed to the Kubernetes cluster.","title":"Manipulating external secrets in the Automation system"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#a-word-of-caution-about-ephemeral-environments","text":"Although external-secrets is part of the EKS cluster for production and the cluster for ephemeral environments, the truth is it behaves differently across contexts. In the production cluster, ExternalSecret objects are configured to pull configurations from AWS Parameter Store every 5 minutes . That way, if a developer adds, removes, or updated a parameter in AWS, the cluster will realize about that event to update the regular Secret object created by the ExternalSecret with the most recent value. This will trigger another piece in the cluster called reloader , which ultimately will kill any pod that relies on the updated secret to spin up a new one with the configurations in the updated Secret . In ephemeral environments however, this is completely different. The ExternalSecret object will create a Secret object only when the ephemeral environment is deployed for the first time. After it has been deployed, control and management of Secret objects is delegated to developers; that is, external-secrets will never pull parameters from AWS again. The reasoning behind this behavior is that these parameters are shared by all ephemeral environments, so if one of them were to be updated in AWS and the polling rate was set to 5 minutes , all ephemeral environments would be updated because they all share the same reference. So essentially, the set of configurations for ephemeral environments that are stored in AWS are used as a template to populate ExternalSecret and Secret objects in ephemeral environments. If a secret needs to be updated, k9s should be used to edit the Secret in place.","title":"A word of caution about ephemeral environments"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#using-an-aws-param-key-in-our-services","text":"Add the new variable to automation-engine/installation-utils/environment_files_generator.py SERVICE__DOMAIN__PARAM_KEY_NAME = parameters [ 'environment' ][ 'service' ][ 'domain' ][ 'param-key-name' ] And inside the env dictionary in the same file f 'DOMAIN__PARAM_KEY_NAME= { SERVICE__DOMAIN__PARAM_KEY_NAME } ' , Add the new variable as a template to services/<service>/src/config/.template.env DOMAIN__PARAM_KEY_NAME = Add the new variable retrieving the value from the environment to services/<service>/src/config/config.py 'param_key_name' : ( os . environ [ 'DOMAIN__PARAM_KEY_NAME' ] ), Any parameter that should be used with a primitive type other than str , should be cast here. We MUST use primitive types. DO NOT convert values to complex types like timedelta or datetime . Consumers of the service configuration should be responsible for doing that. Add the new variable to services/<service>/src/config/testconfig.py for being available when testing 'param_key_name' : 86400 After that you can use it in the service self . _config . MONITOR_CONFIG [ 'param_key_name' ] And while testing param_key_name = action . _config . MONITOR_CONFIG [ 'param_key_name' ]","title":"Using an AWS param key in our services"},{"location":"SYSTEM_OVERVIEW/","text":"System overview Architecture Basic concepts It's a project based on microservices, in which two types are distinguished: Capabilities : They are in charge of carrying out certain common actions for the business logic. I.e.: Collect information from SD-WAN routers For example: Collect information from SD-WAN routers. Use cases : They use capabilities as a base to make specific use cases. I.e.: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for the subsequent storage in the corresponding tickets. For example: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for subsequent storage in the corresponding tickets. It is important to emphasize on the architecture of the system the use of NATS in it. Messaging system It's important to emphasize in the architecture of the system the use of NATS in it. NATS is a simple, secure and performant communications system for digital system, services and devices NATS is used in the microservices system as a communication center for all of them. NATS it's used in cluster mode to safistify more work to be done by it due to the high number of events in the system to be processed Microservices communications There are two types of microservices depending on the connection between them and NATS: Microservices that communicates with NATS , divided into three types: Those that take the role of replier in the context of NATS, usually microservices that contains capabilities : bruin-bridge cts-bridge hawkeye-bridge lit-bridge notifier t7-bridge Those that take the role of requester in the context of NATS, usually microservices that contains use cases : dispatch-portal-backend grafana component, from metrics-prometheus microservice hawkeye-affecting-monitor hawkeye-outage-monitor last-contact-report service-affecting-monitor service-dispatch-monitor service-outage-monitor tnba-feedback tnba-monitor Those that take the role of both requester and replier in the context of NATS. These microservices can be considered a mixture between use use cases and capabilities : customer-cache hawkeye-customer-cache It's important take into account that all microservices that communicate with NATS can also communicate with the Redis Cluster. This is needed to bypass the limit size that NATS enforces for all messages it receives (1MB). Microservices that doesn't communicate with NATS : dispatch-portal-frontend lumin-billing-report prometheus and thanos components, from metrics-prometheus microservice redis cluster (Docker container in local environment or an Elasticache Redis Cluster in AWS environments) NATS is used in the microservice system as a communication center for all of them. It is used in cluster mode to satisfy more work to be done by it. In the following diagram it's possible see a graph with the relationships between the microservices explained previously in this section Relationships between microservices The services that are part of the previously explained architecture are related to each other, in the following diagram it's possible see the relationships between them. Capabilities microservices Bruin-bridge microservice This microservice is in charge of making requests to the bruin API, taking the role of replier in the context of NATS. When another microservice requests bruin data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. Bruin is a third-party system that allows creating and managing support tickets to deal with issues that appear in network devices, among other types of devices. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Cts-bridge microservice This microservice is in charge of making requests to the CTS API, taking the role of replier in the context of NATS. When another microservice requests CTS data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Digi-bridge microservice This microservice is in charge of making requests to the Digi Reboot API, taking the role of replier in the context of NATS. When another microservice asks to reboot a SD-WAN device, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Hawkeye-bridge microservice This microservice is in charge of making requests to the Hawkeye API, taking the role of replier in the context of NATS. When another microservice requests Hawkeye data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Lit-bridge microservice This microservice is in charge of making requests to the LIT API, taking the role of replier in the context of NATS. When another microservice requests LIT data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Notifier microservice This microservice is in charge of sending emails, Slack notifications and SMS. It is important to point out that it is not in charge of the composition of the messages to be sent, that is to say, of their content, but only of sending them. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. T7-bridge microservice The function of this microservice is to embed in the notes of a ticket the prediction calculated by T7, this prediction will store information on the recommendations actions for the ticket. In order to carry out the mentioned actions, it communicates with the API of T7 to obtain the information about the prediction, as it can be seen in the following diagram . Velocloud-bridge microservice This microservice is in charge of making requests to the velocloud API, taking the role of replier in the context of NATS. When another microservice requests velocloud data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Use cases microservices Dispatch-portal-backend microservice In conjunction with dispatch-portal-frontend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It also updates Bruin tickets to keep support people posted about the changes in the dispatch requests. It acts as an intermediary between dispatch-portal-frontend and CTS & LIT APIs by providing a REST API with multiple endpoints that, once they receive a payload from the frontend side, it modifies its fields with the help of some mappers to match the formats expected by CTS and LIT and then forward those customized payloads to their APIs. The following diagram shows the dependencies or interactions of this microservice with the others. Grafana microservice Although Grafana is a visualization tool for metrics, it needs to fetch some data from VeloCloud API to build dashboards for customer Titan America. The following diagram shows the dependencies or interactions of this microservice with the others. Hawkeye-outage-monitor microservice This service is responsible for resolving/unresolving outage tickets depending on the state of a Hawkeye device. It is triggered every 3 minutes. If a device is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the device is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the device was detected to be healthy, the system looks for an open outage ticket for this device and resolves it in case it exists. In the following diagram it's possible see the relationship of this microservice with the others. Last-contact-report microservice The function to be performed by this microservice is to send a monthly report with information about routers that were last contacted more than 30 days ago. The following flow is used to make this report: The last-contact-report microservice communicates with the velocloud-bridge microservice to obtain events from an edge. Once the events are obtained from an edge, it communicates with the notifier microservice to send an email with this information. It is possible to see the relations between the mentioned services for the flow in the following diagram . Service-affecting-monitor microservice In this microservice are defined a series of scales and thresholds, the function of this will be to check if there is loss of packages, latencies or jitter measurements that exceed the thresholds defined. In case the thresholds are exceeded, it will communicate with the notifier service to send a notification by email and slack, by means of which it will warn of the problems detected on a specific edge. This microservice also communicates with the bruin-bridge microservice to create tickets or add notes to an existing one, including in this information about the routers for which a problem is detected. In the following diagram it's possible see the relationships between this microservice and the others. Service-dispatch-monitor microservice This microservice monitor dispatches statuses for different vendors, at the time of writing this document LIT and CTS. Both processes are pretty much the same in concept but with differences in the implementation. A dispatch is general terms can have the following statuses: Requested Confirmed Tech on site Canceled Completed The main use is to monitor: Dispatch status changed Updates in the dispatch like the technician Send sms prior 2 and 12 hours before Send sms tech on site Cancel dispatch The basic algorithm behaves like this: Get all dispatches for a vendor Filter dispatches that are created through the dispatch-portal Discard invalid ticket ids or dispatches with not proper fields Split the dispatches by status and then send them to the function to process them, there are 3 general functions Confirmed dispatch: Send sms and append note to bruin when a dispatch is confirmed Send sms and append note to bruin 12 or 2 hours prior the dispatch Send sms and append note to bruin when a tech has changed Tech on site dispatch: Send sms and append note to bruin when tech on site Canceled dispatch: Append note to bruin when a dispatch is canceled Each vendor has it's own details like how to retrieve some fields or how we identify the tickets with the dispatches, all explained in the service-dispatch-monitor . In the following diagram it's possible see the relationships between this microservice and the others. Service-outage-monitor microservice This microservice orchestrates the execution of two different processes: Outage monitoring. This process is responsible for resolving/unresolving outage tickets depending on the state of an edge. It is triggered every 3 minutes. If an edge is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the edge is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the edge was detected to be healthy, the system looks for an open outage ticket for this edge and resolves it in case it exists. Triage. This process is aimed at updating Bruin tickets with information related to recent edge events. It is triggered every 10 minutes. At the beginning, the process gathers all the open tickets related with the companies that are under triage monitoring. Tickets not related with edges belonging to these companies are discarded before going on. The process starts dealing with every ticket in the set collected in the previous step: * If the outage ticket does not have any triage note from a previous execution of the triage process then a triage note is appended with information of the events related to the edge corresponding to this ticket. Events correspond to the period between 7 days ago and the current moment. If the current environment is DEV instead of PRODUCTION then no note is appended to the ticket; instead, a notification with a summary of the triage results is delivered to a Slack channel. If the outage ticket already has a triage note from a previous execution then the process attempts to append new triage notes to the ticket but only if the last triage note was not appended recently (30 minutes or less ago). In case there's no recent triage note, edge events from the period between the creation date of the last triage note and the current moment are claimed to Velocloud and then they are included in the triage notes, which are finally appended to the ticket. Note that due to Bruin limitations it is not feasible to have a triage note with 1500 characters or more; that is the reason why several triage notes are appended to the ticket (instead of just appending one). In the following diagram it's possible see the relationship of this microservice with the others. TNBA-feedback microservice This microservice is in charge of collecting closed tickets that had a TNBA note appended by tnba-monitor at some point. After collecting them, they are sent to t7-bridge to retrain predictive models and hence improve the accuracy of predictions claimed by tnba-monitor . The following diagram shows the relationship between this microservice and the others. TNBA-monitor microservice This microservice is in charge of appending notes to Bruin tickets indicating what is T he N ext B est A ction a member of the support team of Bruin can take to move forward on the resolution of the ticket. It mostly communicates with bruin-bridge and t7-bridge to embed predictions into tickets, but it also communicates with other capabilities as shown in the following diagram . The following diagram shows the relationship between this microservice and the others. Special microservices (NATS Requester and Replier) Customer-cache microservice This microservice is in charge of crossing Bruin and Velocloud data. More specifically, it focus on associating Bruin customers with Velocloud edges. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of customer-cache , it plays the role of a requester as it asks for data to Velocloud and Bruin to cross it. Hawkeye-customer-cache microservice This microservice is in charge of crossing Bruin and Hawkeye data. More specifically, it focus on associating Bruin customers with Hawkeye devices. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of hawkeye-customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of hawkeye-customer-cache , it plays the role of a requester as it asks for data to Hawkeye and Bruin to cross it. Microservices that don't communicate with NATS dispatch-portal-frontend In conjunction with dispatch-portal-backend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It exposes a UI that communicates directly with a REST API in dispatch-portal-backend to handle the visualization, creation and update of dispatch requests. The following diagram shows the relationship between this microservice and dispatch-portal-backend . lumin-billing-report This service automates requesting billing information for a given customer from the Lumin.AI service provider, generating a summary HTML email and attaching a csv with all data for the current billing period. This service is self-contained, i.e., it does not require access to NATS or Redis, or any other microservice within the Automation Engine. The following diagram shows the relationship between this service and the third-party services it uses. Prometheus & Thanos The purpose of Prometheus is to scrape metrics from HTTP servers placed in those services with the ability to write metrics, nothing else. Thanos is just another component that adds a layer of persistence to Prometheus, thus allowing to save metrics before they are lost when a service is re-deployed. These metrics can be restored after the deployment completes. Metrics are usually displayed in a Grafana instance with a few custom dashboards. The following diagram shows the relationship between Prometheus, the metrics servers it scrapes metrics, and Grafana. Redis Redis is an in-memory key-value store that, in this system, is used mostly for caching purposes, and also as a temporary storage for messages larger than 1 MB, which NATS cannot handle by itself. There are three Redis instances: * redis . Used to store NATS messages larger than 1 MB temporarily. All microservices that communicate with NATS in some way have the ability to store and retrieve messages from this Redis instance. redis-customer-cache . Used to turn customer-cache and hawkeye-customer-cache into fault-tolerant services, so if any of them fail caches will still be available to serve as soon as they come back. redis-tnba-feedback . Used to collect huge amounts of Bruin tickets' task histories before they are sent to T7 by the tnba-feedback service. Technologies and tools Code repository Intelygenz's Gitlab is used to store the project's code Gitlab CI is used as the CI/CD tool for the project Containerization The following containerization tools are used: Docker is used to create o container of this type by microservice > In the folder of each microservice there is a Dockerfile that allows to execute that microservice as a container Docker-compose is used for defining and running project microservices as a multi-container Docker application: > At the root of the repository there is a docker-compose.yml file that allows to run one or more microservices as docker containers Infrastructure Microservices Infrastructure For the microservices ECS is used to deploy a container for each microservice for all environments deployed, as each one has its own repository in the ECR registry used in the project. In the following diagram it's possible see how the microservices of the project are deployed, using the different images available in the registry created for the project in ECR. KRE Infrastructure In this project KRE is used, it has been deployed in an Kubernetes cluster using EKS for each of the necessary environments , as well as all the parts needed for this in AWS. In the following diagram it's possible see how is configured the KRE infrastructure in the project. Network infrastructure For the infrastructure of the network resources there is a distinction according to the microservice environments and also the kre-environmetns to deploy belongs to dev or production . In the following diagram it's possible see the infrastructure relative to the existing network resources in AWS created for the two type of environments. When deploying an environment it will use the resources belonging to the environment type. This approach has been implemented so that regardless of the number of ECS clusters being used, the same public IPs are always used to make requests outward from the different environments. KRE's clusters will also use the VPCs corresponding to each environment, i.e., dev or production .","title":"System overview"},{"location":"SYSTEM_OVERVIEW/#system-overview","text":"","title":"System overview"},{"location":"SYSTEM_OVERVIEW/#architecture","text":"","title":"Architecture"},{"location":"SYSTEM_OVERVIEW/#basic-concepts","text":"It's a project based on microservices, in which two types are distinguished: Capabilities : They are in charge of carrying out certain common actions for the business logic. I.e.: Collect information from SD-WAN routers For example: Collect information from SD-WAN routers. Use cases : They use capabilities as a base to make specific use cases. I.e.: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for the subsequent storage in the corresponding tickets. For example: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for subsequent storage in the corresponding tickets. It is important to emphasize on the architecture of the system the use of NATS in it.","title":"Basic concepts"},{"location":"SYSTEM_OVERVIEW/#messaging-system","text":"It's important to emphasize in the architecture of the system the use of NATS in it. NATS is a simple, secure and performant communications system for digital system, services and devices NATS is used in the microservices system as a communication center for all of them. NATS it's used in cluster mode to safistify more work to be done by it due to the high number of events in the system to be processed","title":"Messaging system"},{"location":"SYSTEM_OVERVIEW/#microservices-communications","text":"There are two types of microservices depending on the connection between them and NATS: Microservices that communicates with NATS , divided into three types: Those that take the role of replier in the context of NATS, usually microservices that contains capabilities : bruin-bridge cts-bridge hawkeye-bridge lit-bridge notifier t7-bridge Those that take the role of requester in the context of NATS, usually microservices that contains use cases : dispatch-portal-backend grafana component, from metrics-prometheus microservice hawkeye-affecting-monitor hawkeye-outage-monitor last-contact-report service-affecting-monitor service-dispatch-monitor service-outage-monitor tnba-feedback tnba-monitor Those that take the role of both requester and replier in the context of NATS. These microservices can be considered a mixture between use use cases and capabilities : customer-cache hawkeye-customer-cache It's important take into account that all microservices that communicate with NATS can also communicate with the Redis Cluster. This is needed to bypass the limit size that NATS enforces for all messages it receives (1MB). Microservices that doesn't communicate with NATS : dispatch-portal-frontend lumin-billing-report prometheus and thanos components, from metrics-prometheus microservice redis cluster (Docker container in local environment or an Elasticache Redis Cluster in AWS environments) NATS is used in the microservice system as a communication center for all of them. It is used in cluster mode to satisfy more work to be done by it. In the following diagram it's possible see a graph with the relationships between the microservices explained previously in this section","title":"Microservices communications"},{"location":"SYSTEM_OVERVIEW/#relationships-between-microservices","text":"The services that are part of the previously explained architecture are related to each other, in the following diagram it's possible see the relationships between them.","title":"Relationships between microservices"},{"location":"SYSTEM_OVERVIEW/#capabilities-microservices","text":"","title":"Capabilities microservices"},{"location":"SYSTEM_OVERVIEW/#bruin-bridge-microservice","text":"This microservice is in charge of making requests to the bruin API, taking the role of replier in the context of NATS. When another microservice requests bruin data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. Bruin is a third-party system that allows creating and managing support tickets to deal with issues that appear in network devices, among other types of devices. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Bruin-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#cts-bridge-microservice","text":"This microservice is in charge of making requests to the CTS API, taking the role of replier in the context of NATS. When another microservice requests CTS data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Cts-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#digi-bridge-microservice","text":"This microservice is in charge of making requests to the Digi Reboot API, taking the role of replier in the context of NATS. When another microservice asks to reboot a SD-WAN device, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Digi-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-bridge-microservice","text":"This microservice is in charge of making requests to the Hawkeye API, taking the role of replier in the context of NATS. When another microservice requests Hawkeye data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Hawkeye-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#lit-bridge-microservice","text":"This microservice is in charge of making requests to the LIT API, taking the role of replier in the context of NATS. When another microservice requests LIT data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Lit-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#notifier-microservice","text":"This microservice is in charge of sending emails, Slack notifications and SMS. It is important to point out that it is not in charge of the composition of the messages to be sent, that is to say, of their content, but only of sending them. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Notifier microservice"},{"location":"SYSTEM_OVERVIEW/#t7-bridge-microservice","text":"The function of this microservice is to embed in the notes of a ticket the prediction calculated by T7, this prediction will store information on the recommendations actions for the ticket. In order to carry out the mentioned actions, it communicates with the API of T7 to obtain the information about the prediction, as it can be seen in the following diagram .","title":"T7-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#velocloud-bridge-microservice","text":"This microservice is in charge of making requests to the velocloud API, taking the role of replier in the context of NATS. When another microservice requests velocloud data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Velocloud-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#use-cases-microservices","text":"","title":"Use cases microservices"},{"location":"SYSTEM_OVERVIEW/#dispatch-portal-backend-microservice","text":"In conjunction with dispatch-portal-frontend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It also updates Bruin tickets to keep support people posted about the changes in the dispatch requests. It acts as an intermediary between dispatch-portal-frontend and CTS & LIT APIs by providing a REST API with multiple endpoints that, once they receive a payload from the frontend side, it modifies its fields with the help of some mappers to match the formats expected by CTS and LIT and then forward those customized payloads to their APIs. The following diagram shows the dependencies or interactions of this microservice with the others.","title":"Dispatch-portal-backend microservice"},{"location":"SYSTEM_OVERVIEW/#grafana-microservice","text":"Although Grafana is a visualization tool for metrics, it needs to fetch some data from VeloCloud API to build dashboards for customer Titan America. The following diagram shows the dependencies or interactions of this microservice with the others.","title":"Grafana microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-outage-monitor-microservice","text":"This service is responsible for resolving/unresolving outage tickets depending on the state of a Hawkeye device. It is triggered every 3 minutes. If a device is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the device is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the device was detected to be healthy, the system looks for an open outage ticket for this device and resolves it in case it exists. In the following diagram it's possible see the relationship of this microservice with the others.","title":"Hawkeye-outage-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#last-contact-report-microservice","text":"The function to be performed by this microservice is to send a monthly report with information about routers that were last contacted more than 30 days ago. The following flow is used to make this report: The last-contact-report microservice communicates with the velocloud-bridge microservice to obtain events from an edge. Once the events are obtained from an edge, it communicates with the notifier microservice to send an email with this information. It is possible to see the relations between the mentioned services for the flow in the following diagram .","title":"Last-contact-report microservice"},{"location":"SYSTEM_OVERVIEW/#service-affecting-monitor-microservice","text":"In this microservice are defined a series of scales and thresholds, the function of this will be to check if there is loss of packages, latencies or jitter measurements that exceed the thresholds defined. In case the thresholds are exceeded, it will communicate with the notifier service to send a notification by email and slack, by means of which it will warn of the problems detected on a specific edge. This microservice also communicates with the bruin-bridge microservice to create tickets or add notes to an existing one, including in this information about the routers for which a problem is detected. In the following diagram it's possible see the relationships between this microservice and the others.","title":"Service-affecting-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#service-dispatch-monitor-microservice","text":"This microservice monitor dispatches statuses for different vendors, at the time of writing this document LIT and CTS. Both processes are pretty much the same in concept but with differences in the implementation. A dispatch is general terms can have the following statuses: Requested Confirmed Tech on site Canceled Completed The main use is to monitor: Dispatch status changed Updates in the dispatch like the technician Send sms prior 2 and 12 hours before Send sms tech on site Cancel dispatch The basic algorithm behaves like this: Get all dispatches for a vendor Filter dispatches that are created through the dispatch-portal Discard invalid ticket ids or dispatches with not proper fields Split the dispatches by status and then send them to the function to process them, there are 3 general functions Confirmed dispatch: Send sms and append note to bruin when a dispatch is confirmed Send sms and append note to bruin 12 or 2 hours prior the dispatch Send sms and append note to bruin when a tech has changed Tech on site dispatch: Send sms and append note to bruin when tech on site Canceled dispatch: Append note to bruin when a dispatch is canceled Each vendor has it's own details like how to retrieve some fields or how we identify the tickets with the dispatches, all explained in the service-dispatch-monitor . In the following diagram it's possible see the relationships between this microservice and the others.","title":"Service-dispatch-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#service-outage-monitor-microservice","text":"This microservice orchestrates the execution of two different processes: Outage monitoring. This process is responsible for resolving/unresolving outage tickets depending on the state of an edge. It is triggered every 3 minutes. If an edge is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the edge is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the edge was detected to be healthy, the system looks for an open outage ticket for this edge and resolves it in case it exists. Triage. This process is aimed at updating Bruin tickets with information related to recent edge events. It is triggered every 10 minutes. At the beginning, the process gathers all the open tickets related with the companies that are under triage monitoring. Tickets not related with edges belonging to these companies are discarded before going on. The process starts dealing with every ticket in the set collected in the previous step: * If the outage ticket does not have any triage note from a previous execution of the triage process then a triage note is appended with information of the events related to the edge corresponding to this ticket. Events correspond to the period between 7 days ago and the current moment. If the current environment is DEV instead of PRODUCTION then no note is appended to the ticket; instead, a notification with a summary of the triage results is delivered to a Slack channel. If the outage ticket already has a triage note from a previous execution then the process attempts to append new triage notes to the ticket but only if the last triage note was not appended recently (30 minutes or less ago). In case there's no recent triage note, edge events from the period between the creation date of the last triage note and the current moment are claimed to Velocloud and then they are included in the triage notes, which are finally appended to the ticket. Note that due to Bruin limitations it is not feasible to have a triage note with 1500 characters or more; that is the reason why several triage notes are appended to the ticket (instead of just appending one). In the following diagram it's possible see the relationship of this microservice with the others.","title":"Service-outage-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#tnba-feedback-microservice","text":"This microservice is in charge of collecting closed tickets that had a TNBA note appended by tnba-monitor at some point. After collecting them, they are sent to t7-bridge to retrain predictive models and hence improve the accuracy of predictions claimed by tnba-monitor . The following diagram shows the relationship between this microservice and the others.","title":"TNBA-feedback microservice"},{"location":"SYSTEM_OVERVIEW/#tnba-monitor-microservice","text":"This microservice is in charge of appending notes to Bruin tickets indicating what is T he N ext B est A ction a member of the support team of Bruin can take to move forward on the resolution of the ticket. It mostly communicates with bruin-bridge and t7-bridge to embed predictions into tickets, but it also communicates with other capabilities as shown in the following diagram . The following diagram shows the relationship between this microservice and the others.","title":"TNBA-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#special-microservices-nats-requester-and-replier","text":"","title":"Special microservices (NATS Requester and Replier)"},{"location":"SYSTEM_OVERVIEW/#customer-cache-microservice","text":"This microservice is in charge of crossing Bruin and Velocloud data. More specifically, it focus on associating Bruin customers with Velocloud edges. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of customer-cache , it plays the role of a requester as it asks for data to Velocloud and Bruin to cross it.","title":"Customer-cache microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-customer-cache-microservice","text":"This microservice is in charge of crossing Bruin and Hawkeye data. More specifically, it focus on associating Bruin customers with Hawkeye devices. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of hawkeye-customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of hawkeye-customer-cache , it plays the role of a requester as it asks for data to Hawkeye and Bruin to cross it.","title":"Hawkeye-customer-cache microservice"},{"location":"SYSTEM_OVERVIEW/#microservices-that-dont-communicate-with-nats","text":"","title":"Microservices that don't communicate with NATS"},{"location":"SYSTEM_OVERVIEW/#dispatch-portal-frontend","text":"In conjunction with dispatch-portal-backend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It exposes a UI that communicates directly with a REST API in dispatch-portal-backend to handle the visualization, creation and update of dispatch requests. The following diagram shows the relationship between this microservice and dispatch-portal-backend .","title":"dispatch-portal-frontend"},{"location":"SYSTEM_OVERVIEW/#lumin-billing-report","text":"This service automates requesting billing information for a given customer from the Lumin.AI service provider, generating a summary HTML email and attaching a csv with all data for the current billing period. This service is self-contained, i.e., it does not require access to NATS or Redis, or any other microservice within the Automation Engine. The following diagram shows the relationship between this service and the third-party services it uses.","title":"lumin-billing-report"},{"location":"SYSTEM_OVERVIEW/#prometheus-thanos","text":"The purpose of Prometheus is to scrape metrics from HTTP servers placed in those services with the ability to write metrics, nothing else. Thanos is just another component that adds a layer of persistence to Prometheus, thus allowing to save metrics before they are lost when a service is re-deployed. These metrics can be restored after the deployment completes. Metrics are usually displayed in a Grafana instance with a few custom dashboards. The following diagram shows the relationship between Prometheus, the metrics servers it scrapes metrics, and Grafana.","title":"Prometheus &amp; Thanos"},{"location":"SYSTEM_OVERVIEW/#redis","text":"Redis is an in-memory key-value store that, in this system, is used mostly for caching purposes, and also as a temporary storage for messages larger than 1 MB, which NATS cannot handle by itself. There are three Redis instances: * redis . Used to store NATS messages larger than 1 MB temporarily. All microservices that communicate with NATS in some way have the ability to store and retrieve messages from this Redis instance. redis-customer-cache . Used to turn customer-cache and hawkeye-customer-cache into fault-tolerant services, so if any of them fail caches will still be available to serve as soon as they come back. redis-tnba-feedback . Used to collect huge amounts of Bruin tickets' task histories before they are sent to T7 by the tnba-feedback service.","title":"Redis"},{"location":"SYSTEM_OVERVIEW/#technologies-and-tools","text":"","title":"Technologies and tools"},{"location":"SYSTEM_OVERVIEW/#code-repository","text":"Intelygenz's Gitlab is used to store the project's code Gitlab CI is used as the CI/CD tool for the project","title":"Code repository"},{"location":"SYSTEM_OVERVIEW/#containerization","text":"The following containerization tools are used: Docker is used to create o container of this type by microservice > In the folder of each microservice there is a Dockerfile that allows to execute that microservice as a container Docker-compose is used for defining and running project microservices as a multi-container Docker application: > At the root of the repository there is a docker-compose.yml file that allows to run one or more microservices as docker containers","title":"Containerization"},{"location":"SYSTEM_OVERVIEW/#infrastructure","text":"","title":"Infrastructure"},{"location":"SYSTEM_OVERVIEW/#microservices-infrastructure","text":"For the microservices ECS is used to deploy a container for each microservice for all environments deployed, as each one has its own repository in the ECR registry used in the project. In the following diagram it's possible see how the microservices of the project are deployed, using the different images available in the registry created for the project in ECR.","title":"Microservices Infrastructure"},{"location":"SYSTEM_OVERVIEW/#kre-infrastructure","text":"In this project KRE is used, it has been deployed in an Kubernetes cluster using EKS for each of the necessary environments , as well as all the parts needed for this in AWS. In the following diagram it's possible see how is configured the KRE infrastructure in the project.","title":"KRE Infrastructure"},{"location":"SYSTEM_OVERVIEW/#network-infrastructure","text":"For the infrastructure of the network resources there is a distinction according to the microservice environments and also the kre-environmetns to deploy belongs to dev or production . In the following diagram it's possible see the infrastructure relative to the existing network resources in AWS created for the two type of environments. When deploying an environment it will use the resources belonging to the environment type. This approach has been implemented so that regardless of the number of ECS clusters being used, the same public IPs are always used to make requests outward from the different environments. KRE's clusters will also use the VPCs corresponding to each environment, i.e., dev or production .","title":"Network infrastructure"},{"location":"WELCOME_PACK/","text":"Welcome Pack Team Composition Julia - Manager Dani - Tech Lead \u00c1ngel - Devops Brandon - Developer David - Developer Sergio - Developer Javier - Developer First steps Please request access to all the things listed below: Project repository https://gitlab.intelygenz.com/mettel/automation-engine/ , docker repository https://gitlab.intelygenz.com/mettel/docker_images/-/tree/master and One password to itcrowd@intelygenz.com through their ticketing system https://docs.google.com/document/d/1YLYdI9Dyq8tNlNy2iJ29InquKDz8r_Dw4XBsxI7pPiM/edit and CC your manager to allow the request Configure vpn https://docs.google.com/document/d/16_LFpkiBWN0mbfjAoqR4BaEB5kPNsuNHrUS7PtrWnEA/edit#heading=h.to49i8wu1vn3 AWS account creation to our devops Jira board to our manager Mettel's Slack channels to our tech lead Project overview Resources Check docs folder inside the mettel gitlab project and read carefully the readme for installing all the required programs and configure the project. After RRHH's on-boarding our team lead will give an overview of the project https://docs.google.com/presentation/d/1Y18vXn6lsSp-6pJVsB5swWg7UcTDWYrIYtXq8_brRMk/edit#slide=id.g6183830b53_0_5 Project guidelines This project uses Black and isort. You just need to install pip install pre-commit and then just run pre-commit run --all-files on the root folder. Please check pre-commit-config.yaml for more info about it. Another option (after adding the project poetry env as interpreter in pycharm) would be running poetry run black . and poetry run isort . on the root folder, the config options will be taken automatically from pyproject.toml . For adding this as autosave option please refer to https://black.readthedocs.io/en/stable/integrations/editors.html When updating a git branch please use rebase instead of merge. Tools k9s https://k9scli.io/ bash-completion","title":"Welcome Pack"},{"location":"WELCOME_PACK/#welcome-pack","text":"","title":"Welcome Pack"},{"location":"WELCOME_PACK/#team-composition","text":"Julia - Manager Dani - Tech Lead \u00c1ngel - Devops Brandon - Developer David - Developer Sergio - Developer Javier - Developer","title":"Team Composition"},{"location":"WELCOME_PACK/#first-steps","text":"Please request access to all the things listed below: Project repository https://gitlab.intelygenz.com/mettel/automation-engine/ , docker repository https://gitlab.intelygenz.com/mettel/docker_images/-/tree/master and One password to itcrowd@intelygenz.com through their ticketing system https://docs.google.com/document/d/1YLYdI9Dyq8tNlNy2iJ29InquKDz8r_Dw4XBsxI7pPiM/edit and CC your manager to allow the request Configure vpn https://docs.google.com/document/d/16_LFpkiBWN0mbfjAoqR4BaEB5kPNsuNHrUS7PtrWnEA/edit#heading=h.to49i8wu1vn3 AWS account creation to our devops Jira board to our manager Mettel's Slack channels to our tech lead","title":"First steps"},{"location":"WELCOME_PACK/#project-overview","text":"","title":"Project overview"},{"location":"WELCOME_PACK/#resources","text":"Check docs folder inside the mettel gitlab project and read carefully the readme for installing all the required programs and configure the project. After RRHH's on-boarding our team lead will give an overview of the project https://docs.google.com/presentation/d/1Y18vXn6lsSp-6pJVsB5swWg7UcTDWYrIYtXq8_brRMk/edit#slide=id.g6183830b53_0_5","title":"Resources"},{"location":"WELCOME_PACK/#project-guidelines","text":"This project uses Black and isort. You just need to install pip install pre-commit and then just run pre-commit run --all-files on the root folder. Please check pre-commit-config.yaml for more info about it. Another option (after adding the project poetry env as interpreter in pycharm) would be running poetry run black . and poetry run isort . on the root folder, the config options will be taken automatically from pyproject.toml . For adding this as autosave option please refer to https://black.readthedocs.io/en/stable/integrations/editors.html When updating a git branch please use rebase instead of merge.","title":"Project guidelines"},{"location":"WELCOME_PACK/#tools","text":"k9s https://k9scli.io/ bash-completion","title":"Tools"},{"location":"api/flows/1-authentication/","text":"Flow Diagram Requisites Okta account/user set up AWS login endpoint exposed to internet (Also whitelisted IP?) create a user on okta that has permisions to check the user and the token Description As a requirement we need to accomplish centralized users and groups in Okta. We must connect Okta with our authentication workflow. Flow User will launch a signin call to our API endpoint Our API endpoint will call to the auth enpoint of okta. create api token https://developer.okta.com/docs/guides/create-an-api-token/main/ https://developer.okta.com/docs/guides/create-an-api-token/main/ login with api token https://developer.okta.com/docs/reference/api/authn/#response-example-for-primary-authentication-with-public-application-factor-enroll token expiration https://developer.okta.com/docs/guides/create-an-api-token/main/ validate access token https://developer.okta.com/docs/guides/validate-access-tokens/python/main/ https://developer.okta.com/docs/guides/implement-grant-type/ropassword/main/#set-up-your-app","title":"1 authentication"},{"location":"api/flows/1-authentication/#flow-diagram","text":"","title":"Flow Diagram"},{"location":"api/flows/1-authentication/#requisites","text":"Okta account/user set up AWS login endpoint exposed to internet (Also whitelisted IP?) create a user on okta that has permisions to check the user and the token Description As a requirement we need to accomplish centralized users and groups in Okta. We must connect Okta with our authentication workflow.","title":"Requisites"},{"location":"api/flows/1-authentication/#flow","text":"User will launch a signin call to our API endpoint Our API endpoint will call to the auth enpoint of okta. create api token https://developer.okta.com/docs/guides/create-an-api-token/main/ https://developer.okta.com/docs/guides/create-an-api-token/main/ login with api token https://developer.okta.com/docs/reference/api/authn/#response-example-for-primary-authentication-with-public-application-factor-enroll token expiration https://developer.okta.com/docs/guides/create-an-api-token/main/ validate access token https://developer.okta.com/docs/guides/validate-access-tokens/python/main/ https://developer.okta.com/docs/guides/implement-grant-type/ropassword/main/#set-up-your-app","title":"Flow"},{"location":"decisions/","text":"GLOBAL DECISIONS Dashboard infrastructure and architecture","title":"MetTel decisions"},{"location":"decisions/01-dashboards-infrastructure-and-architecture/","text":"1: Design of an isolated an unique dashboard user experience Status: Approved Decission: Future is to have an external Grafana that connects to an external Prometheus & InfluxDB 2 Implementation plan as following: - Prometheus will be deployed as an AWS Managed Prometheus. - Grafana will be deployed independently in AWS pointing to AWS Prometheus. - Konstellation KRE will be merged to a single KRE instance as soon as KRE allows it. - KRE will use a dedicated InfluxDB2 in AWS. - Grafana will connect to InfluxDB2 as additional data source. - Chronograf dashboards will be migrated to the dedicated Grafana in AWS. Miro diagram of affected systems: https://miro.com/app/board/uXjVO6bg-zY=/ Alternatives considered: Justification: Current dashboarding solution has several different interfaces, this is confusing for all kind of users. Current stability of dashboarding is dependent in the stability of our K8s deployments, losing all visibility if anything goes wrong on K8s. License limitations on InfluxDB OSS1 could become a problem. Consequences:","title":"**1: Design of an isolated an unique dashboard user experience**"},{"location":"decisions/01-dashboards-infrastructure-and-architecture/#1-design-of-an-isolated-an-unique-dashboard-user-experience","text":"Status: Approved Decission: Future is to have an external Grafana that connects to an external Prometheus & InfluxDB 2 Implementation plan as following: - Prometheus will be deployed as an AWS Managed Prometheus. - Grafana will be deployed independently in AWS pointing to AWS Prometheus. - Konstellation KRE will be merged to a single KRE instance as soon as KRE allows it. - KRE will use a dedicated InfluxDB2 in AWS. - Grafana will connect to InfluxDB2 as additional data source. - Chronograf dashboards will be migrated to the dedicated Grafana in AWS. Miro diagram of affected systems: https://miro.com/app/board/uXjVO6bg-zY=/ Alternatives considered: Justification: Current dashboarding solution has several different interfaces, this is confusing for all kind of users. Current stability of dashboarding is dependent in the stability of our K8s deployments, losing all visibility if anything goes wrong on K8s. License limitations on InfluxDB OSS1 could become a problem. Consequences:","title":"1: Design of an isolated an unique dashboard user experience"},{"location":"diagrams/TOOLS/","text":"https://github.com/mingrammer/diagrams","title":"TOOLS"},{"location":"kafka/LAUNCH_DOCKER_COMPOSE/","text":"1. How to run In this section we explain all the useful information that we can use when working in our local environment. In order to raise our system we will have to go to the root folder of our system and launch the following command docker-compose up This will launch our system by creating a local docker image and deploying our fetcher. It is important to keep in mind that this process launches a multitude of requests against the production environment and it is important to limit them all so as not to overload the system. To facilitate the work in local we also have several local configurations that allow us to test step by step our application avoiding possible problems of launching too many requests. To launch this configuration in Intellij you will need to verify that the following steps were done: - Go to Settings >> Build, Execution, Deployment >> Docker - Select \"TCP socket\" - Enter 'unix:///var/run/docker.sock' under \"Engine API URL\"","title":"Launch docker compose"},{"location":"kafka/LAUNCH_DOCKER_COMPOSE/#1-how-to-run","text":"In this section we explain all the useful information that we can use when working in our local environment. In order to raise our system we will have to go to the root folder of our system and launch the following command docker-compose up This will launch our system by creating a local docker image and deploying our fetcher. It is important to keep in mind that this process launches a multitude of requests against the production environment and it is important to limit them all so as not to overload the system. To facilitate the work in local we also have several local configurations that allow us to test step by step our application avoiding possible problems of launching too many requests. To launch this configuration in Intellij you will need to verify that the following steps were done: - Go to Settings >> Build, Execution, Deployment >> Docker - Select \"TCP socket\" - Enter 'unix:///var/run/docker.sock' under \"Engine API URL\"","title":"1. How to run"},{"location":"lambda/PARAMETER_REPLICATOR/","text":"1. Summary Parameter Store is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. This service is only available in the region where is deployed. We use it to set configuration of Automation-Engine app. But if a desaster occurs in the main region we need to have the parameters replicated en the stand-by region to redeploy the app. Parameter-replicator is a phython lambda that replicate parameters from one region to another. If any parameter change, that change will be replicated in the other region, by this way we have a configuration ready to run the application in the mirror region. This lambda also run ones a day to create a parameter backup and store in S3. 1. Diagram parameter-replicator.drawio.svg","title":"PARAMETER REPLICATOR"},{"location":"lambda/PARAMETER_REPLICATOR/#1-summary","text":"Parameter Store is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. This service is only available in the region where is deployed. We use it to set configuration of Automation-Engine app. But if a desaster occurs in the main region we need to have the parameters replicated en the stand-by region to redeploy the app. Parameter-replicator is a phython lambda that replicate parameters from one region to another. If any parameter change, that change will be replicated in the other region, by this way we have a configuration ready to run the application in the mirror region. This lambda also run ones a day to create a parameter backup and store in S3.","title":"1. Summary"},{"location":"lambda/PARAMETER_REPLICATOR/#1-diagram","text":"parameter-replicator.drawio.svg","title":"1. Diagram"},{"location":"logging/","text":"","title":"Index"},{"location":"logging/events/0-messages-bus/","text":"Messages Bus Event Logging Description The messages bus is the central piece of communication between the services living in the Automation system. Services can talk to each other by publishing messages to subjects. When a service has interest on a particular subject, any messages published to that subject will be consumed by it. The bus provides two communication models for these services: Publish / Subscribe (a.k.a. PUB / SUB). As hinted above, a service publishes a message to a subject which is later consumed by another service with interest in that subject. Request / Reply. In this case, a service asks for data by publishing a message to a subject which is later consumed by another service with interest in that subject. This is pretty much a PUB/SUB flow. The difference here, however, is that the consumer of the message will reply with a response to the requestor. The requestor takes care of consuming that response message once it gets it. Since the messages bus won't allow publishing messages larger than 1MB+, and if necessary, all services take care of storing them to a temporary storage so consumers can restore the original message on their end. Process Workflows List of Decisions made by the Messages Bus PUB-SUB workflow Condition Decision Decision 1 Check for message size before publishing it Message is too large for the message bus to handle (1MB+) Message is small enough for the message bus to handle (<1MB) REQUEST-REPLY workflow Requestor Condition Decision Decision 1 Check for request message size before publishing it Message is too large for the message bus to handle (1MB+) Message is small enough for the message bus to handle (<1MB) 2 Check for response message size before consuming it Message has a token representing a large message (1MB+) Message doesn't have a token representing a large message (1MB+) Replier Condition Decision Decision 1 Check for request message size before consuming it Message has a token representing a large message (1MB+) Message doesn't have a token representing a large message (1MB+) 2 Check for response message size before publishing it Message is too large for the message bus to handle (1MB+) Message is small enough for the message bus to handle (<1MB) Event Descriptions Python 3.10 utils Connect to messages bus connect Subscribe to subject subscribe PUB/SUB workflow publish REQUEST/REPLY workflow request Python 3.6 utils Connect to messages bus connect Subscribe to subject subscribe_consumer PUB/SUB workflow publish_message REQUEST/REPLY workflow rpc_request","title":"Messages Bus Event Logging"},{"location":"logging/events/0-messages-bus/#messages-bus-event-logging","text":"","title":"Messages Bus Event Logging"},{"location":"logging/events/0-messages-bus/#description","text":"The messages bus is the central piece of communication between the services living in the Automation system. Services can talk to each other by publishing messages to subjects. When a service has interest on a particular subject, any messages published to that subject will be consumed by it. The bus provides two communication models for these services: Publish / Subscribe (a.k.a. PUB / SUB). As hinted above, a service publishes a message to a subject which is later consumed by another service with interest in that subject. Request / Reply. In this case, a service asks for data by publishing a message to a subject which is later consumed by another service with interest in that subject. This is pretty much a PUB/SUB flow. The difference here, however, is that the consumer of the message will reply with a response to the requestor. The requestor takes care of consuming that response message once it gets it. Since the messages bus won't allow publishing messages larger than 1MB+, and if necessary, all services take care of storing them to a temporary storage so consumers can restore the original message on their end.","title":"Description"},{"location":"logging/events/0-messages-bus/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/0-messages-bus/#list-of-decisions-made-by-the-messages-bus","text":"","title":"List of Decisions made by the Messages Bus"},{"location":"logging/events/0-messages-bus/#pub-sub-workflow","text":"Condition Decision Decision 1 Check for message size before publishing it Message is too large for the message bus to handle (1MB+) Message is small enough for the message bus to handle (<1MB)","title":"PUB-SUB workflow"},{"location":"logging/events/0-messages-bus/#request-reply-workflow","text":"","title":"REQUEST-REPLY workflow"},{"location":"logging/events/0-messages-bus/#requestor","text":"Condition Decision Decision 1 Check for request message size before publishing it Message is too large for the message bus to handle (1MB+) Message is small enough for the message bus to handle (<1MB) 2 Check for response message size before consuming it Message has a token representing a large message (1MB+) Message doesn't have a token representing a large message (1MB+)","title":"Requestor"},{"location":"logging/events/0-messages-bus/#replier","text":"Condition Decision Decision 1 Check for request message size before consuming it Message has a token representing a large message (1MB+) Message doesn't have a token representing a large message (1MB+) 2 Check for response message size before publishing it Message is too large for the message bus to handle (1MB+) Message is small enough for the message bus to handle (<1MB)","title":"Replier"},{"location":"logging/events/0-messages-bus/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/0-messages-bus/#python-310-utils","text":"","title":"Python 3.10 utils"},{"location":"logging/events/0-messages-bus/#connect-to-messages-bus","text":"connect","title":"Connect to messages bus"},{"location":"logging/events/0-messages-bus/#subscribe-to-subject","text":"subscribe","title":"Subscribe to subject"},{"location":"logging/events/0-messages-bus/#pubsub-workflow","text":"publish","title":"PUB/SUB workflow"},{"location":"logging/events/0-messages-bus/#requestreply-workflow","text":"request","title":"REQUEST/REPLY workflow"},{"location":"logging/events/0-messages-bus/#python-36-utils","text":"","title":"Python 3.6 utils"},{"location":"logging/events/0-messages-bus/#connect-to-messages-bus_1","text":"connect","title":"Connect to messages bus"},{"location":"logging/events/0-messages-bus/#subscribe-to-subject_1","text":"subscribe_consumer","title":"Subscribe to subject"},{"location":"logging/events/0-messages-bus/#pubsub-workflow_1","text":"publish_message","title":"PUB/SUB workflow"},{"location":"logging/events/0-messages-bus/#requestreply-workflow_1","text":"rpc_request","title":"REQUEST/REPLY workflow"},{"location":"logging/events/1-service-outage/","text":"IPA Event Logging Process Workflows List of Decisions made by the IPA System Service Outage Start of Service Outage process 1. Checking edge & link status for outage Outage is detected Outage is not detected Autoresolution 2. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN 3. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 4. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 5. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 6. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times Ticket Creation 7. Checking the results of ticket creation attempt New ticket is created with a task for the SD-WAN and placed in an IPA queue Ticket already exists for the location 8. Determining what to do with an already existing ticket New task for SD-WAN is added to ticket SD-WAN task on existing ticket for is reopened In-progress tasks for SD-WAN already exists IPA queue 9. What type of Outage was caused Outage Detected is an EDGE DOWN Outage Detected is a LINK DOWN 10. Checking time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) DiGi Reboot 11. Are there any DiGi links Edge has at least one offline DiGi link Edge has no offline DiGi links 12. Has a DiGi reboot been attempted DiGi reboot has not been attempted yet DiGi reboot has been attempted 13. Has the DiGi reboot attempt occur after 30 mins 30 min has passed since reboot started 30 min has not passed since reboot started Event Descriptions Service Outage start_service_outage_monitoring","title":"IPA Event Logging"},{"location":"logging/events/1-service-outage/#ipa-event-logging","text":"","title":"IPA Event Logging"},{"location":"logging/events/1-service-outage/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/1-service-outage/#list-of-decisions-made-by-the-ipa-system","text":"","title":"List of Decisions made by the IPA System"},{"location":"logging/events/1-service-outage/#service-outage","text":"","title":"Service Outage"},{"location":"logging/events/1-service-outage/#start-of-service-outage-process","text":"1. Checking edge & link status for outage Outage is detected Outage is not detected","title":"Start of Service Outage process"},{"location":"logging/events/1-service-outage/#autoresolution","text":"2. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN 3. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 4. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 5. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 6. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times","title":"Autoresolution"},{"location":"logging/events/1-service-outage/#ticket-creation","text":"7. Checking the results of ticket creation attempt New ticket is created with a task for the SD-WAN and placed in an IPA queue Ticket already exists for the location 8. Determining what to do with an already existing ticket New task for SD-WAN is added to ticket SD-WAN task on existing ticket for is reopened In-progress tasks for SD-WAN already exists","title":"Ticket Creation"},{"location":"logging/events/1-service-outage/#ipa-queue","text":"9. What type of Outage was caused Outage Detected is an EDGE DOWN Outage Detected is a LINK DOWN 10. Checking time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY)","title":"IPA queue"},{"location":"logging/events/1-service-outage/#digi-reboot","text":"11. Are there any DiGi links Edge has at least one offline DiGi link Edge has no offline DiGi links 12. Has a DiGi reboot been attempted DiGi reboot has not been attempted yet DiGi reboot has been attempted 13. Has the DiGi reboot attempt occur after 30 mins 30 min has passed since reboot started 30 min has not passed since reboot started","title":"DiGi Reboot"},{"location":"logging/events/1-service-outage/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/1-service-outage/#service-outage_1","text":"start_service_outage_monitoring","title":"Service Outage"},{"location":"logging/events/10-ixia-outage-monitoring/","text":"Ixia Outage Monitoring Event Logging Process Workflows List of Decisions made by the Ixia outage monitoring System Ixia outage monitoring queue Start of Ixia outage monitoring workflow 1. Checking probe status The probe is not active The probe is active 2. Check Node to Node and Real service status Node to Node or Real service status is 0 (Offline) Node to Node AND Real status is 1 (Online) 3. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN on a Service Outage ticket 4. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 5. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 6. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 7. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times ## Event Descriptions ### Ixia outage monitoring queue * start_hawkeye_outage_monitoring","title":"Ixia Outage Monitoring Event Logging"},{"location":"logging/events/10-ixia-outage-monitoring/#ixia-outage-monitoring-event-logging","text":"","title":"Ixia Outage Monitoring Event Logging"},{"location":"logging/events/10-ixia-outage-monitoring/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/10-ixia-outage-monitoring/#list-of-decisions-made-by-the-ixia-outage-monitoring-system","text":"","title":"List of Decisions made by the Ixia outage monitoring System"},{"location":"logging/events/10-ixia-outage-monitoring/#ixia-outage-monitoring-queue","text":"","title":"Ixia outage monitoring queue"},{"location":"logging/events/10-ixia-outage-monitoring/#start-of-ixia-outage-monitoring-workflow","text":"1. Checking probe status The probe is not active The probe is active 2. Check Node to Node and Real service status Node to Node or Real service status is 0 (Offline) Node to Node AND Real status is 1 (Online) 3. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN on a Service Outage ticket 4. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 5. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 6. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 7. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times ## Event Descriptions ### Ixia outage monitoring queue * start_hawkeye_outage_monitoring","title":"Start of Ixia outage monitoring workflow"},{"location":"logging/events/11-bruin-bridge/","text":"Bruin Bridge Event Logging Description The mission of this service is to act as a proxy to the Bruin API. It accepts requests from other services and yields the requested data back to those services so they can make the appropriate business decision. Process Workflows List of Decisions made by the Bruin Bridge Subject: bruin.ticket.basic.request (aims at endpoint GET /api/Ticket/basic ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from GET /api/Ticket/basic HTTP response has status 200 HTTP response has NOT status 200 Subject: bruin.get.site (aims at endpoint GET /api/Site ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from GET /api/Site HTTP response has status 200 HTTP response has NOT status 200 3 Check for existence of site's data Site info was found for filters Site info was NOT found for filters Subject: bruin.change.ticket.severity (aims at endpoint PUT /api/Ticket/{ticketId}/severity ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from PUT /api/Ticket/{ticketId}/severity HTTP response has status 200 HTTP response has NOT status 200 Event Descriptions Subject: bruin.ticket.basic.request get_tickets_basic_info Subject: bruin.get.site get_site Subject: bruin.change.ticket.severity change_ticket_severity","title":"Bruin Bridge Event Logging"},{"location":"logging/events/11-bruin-bridge/#bruin-bridge-event-logging","text":"","title":"Bruin Bridge Event Logging"},{"location":"logging/events/11-bruin-bridge/#description","text":"The mission of this service is to act as a proxy to the Bruin API. It accepts requests from other services and yields the requested data back to those services so they can make the appropriate business decision.","title":"Description"},{"location":"logging/events/11-bruin-bridge/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/11-bruin-bridge/#list-of-decisions-made-by-the-bruin-bridge","text":"","title":"List of Decisions made by the Bruin Bridge"},{"location":"logging/events/11-bruin-bridge/#subject-bruinticketbasicrequest-aims-at-endpoint-get-apiticketbasic","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from GET /api/Ticket/basic HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: bruin.ticket.basic.request (aims at endpoint GET /api/Ticket/basic)"},{"location":"logging/events/11-bruin-bridge/#subject-bruingetsite-aims-at-endpoint-get-apisite","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from GET /api/Site HTTP response has status 200 HTTP response has NOT status 200 3 Check for existence of site's data Site info was found for filters Site info was NOT found for filters","title":"Subject: bruin.get.site (aims at endpoint GET /api/Site)"},{"location":"logging/events/11-bruin-bridge/#subject-bruinchangeticketseverity-aims-at-endpoint-put-apiticketticketidseverity","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from PUT /api/Ticket/{ticketId}/severity HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: bruin.change.ticket.severity (aims at endpoint PUT /api/Ticket/{ticketId}/severity)"},{"location":"logging/events/11-bruin-bridge/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/11-bruin-bridge/#subject-bruinticketbasicrequest","text":"get_tickets_basic_info","title":"Subject: bruin.ticket.basic.request"},{"location":"logging/events/11-bruin-bridge/#subject-bruingetsite","text":"get_site","title":"Subject: bruin.get.site"},{"location":"logging/events/11-bruin-bridge/#subject-bruinchangeticketseverity","text":"change_ticket_severity","title":"Subject: bruin.change.ticket.severity"},{"location":"logging/events/12-velocloud-bridge/","text":"VeloCloud Bridge Event Logging Description The mission of this service is to act as a proxy to the VeloCloud API. It accepts requests from other services and yields the requested data back to those services, so they can make the appropriate business decision. Process Workflows List of Decisions made by the VeloCloud Bridge Subject: alert.request.event.edge (aims at endpoint POST /event/getEnterpriseEvents ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /event/getEnterpriseEvents HTTP response has status 200 HTTP response has NOT status 200 Subject: request.enterprises.edges (aims at endpoint POST /enterprise/getEnterpriseEdges ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /enterprise/getEnterpriseEdges HTTP response has status 200 HTTP response has NOT status 200 Subject: alert.request.event.enterprise (aims at endpoint POST /event/getEnterpriseEvents ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /event/getEnterpriseEvents HTTP response has status 200 HTTP response has NOT status 200 Subject: request.enterprises.names (aims at endpoint POST /monitoring/getAggregates ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /monitoring/getAggregates HTTP response has status 200 HTTP response has NOT status 200 Subject: request.edge.links.series (aims at endpoint POST /metrics/getEdgeLinkSeries ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /metrics/getEdgeLinkSeries HTTP response has status 200 HTTP response has NOT status 200 Subject: request.links.configuration (aims at endpoint POST /edge/getEdgeConfigurationModules ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /edge/getEdgeConfigurationModules HTTP response has status 200 HTTP response has NOT status 200 3 Check for existence of WAN module in response WAN configuration exists WAN configuration does NOT exist 4 Check for existence of links configurations in WAN module Links config defined in WAN configuration Links config NOT defined in WAN configuration Subject: get.links.metric.info (aims at endpoint POST /monitoring/getAggregateEdgeLinkMetrics ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /monitoring/getAggregateEdgeLinkMetrics HTTP response has status 200 HTTP response has NOT status 200 Subject: get.links.with.edge.info (aims at endpoint POST /monitoring/getEnterpriseEdgeLinkStatus ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /monitoring/getEnterpriseEdgeLinkStatus HTTP response has status 200 HTTP response has NOT status 200 Subject: request.network.enterprise.edges (aims at endpoint POST /network/getNetworkEnterprises ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /network/getNetworkEnterprises HTTP response has status 200 HTTP response has NOT status 200 3 Check enterprise was found for specified filters Enterprise found for filters Enterprise NOT found for filters 4 Check enterprise has edges Edges found for enterprises Edges NOT found for enterprises Subject: request.network.gateway.list (aims at endpoint POST /network/getNetworkGateways ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /network/getNetworkGateways HTTP response has status 200 HTTP response has NOT status 200 Subject: request.gateway.status.metrics (aims at endpoint POST /metrics/getGatewayStatusMetrics ) Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /metrics/getGatewayStatusMetrics HTTP response has status 200 HTTP response has NOT status 200 Event Descriptions Subject: alert.request.event.edge edge_events_for_alert Subject: request.enterprises.edges enterprise_edge_list Subject: alert.request.event.enterprise enterprise_events_for_alert Subject: request.enterprises.names enterprise_name_list_response Subject: request.edge.links.series get_edge_links_series Subject: request.links.configuration links_configuration Subject: get.links.metric.info links_metric_info Subject: get.links.with.edge.info links_with_edge_info Subject: request.network.enterprise.edges network_enterprise_edge_list Subject: request.network.gateway.list network_gateway_list Subject: request.gateway.status.metrics gateway_status_metrics","title":"VeloCloud Bridge Event Logging"},{"location":"logging/events/12-velocloud-bridge/#velocloud-bridge-event-logging","text":"","title":"VeloCloud Bridge Event Logging"},{"location":"logging/events/12-velocloud-bridge/#description","text":"The mission of this service is to act as a proxy to the VeloCloud API. It accepts requests from other services and yields the requested data back to those services, so they can make the appropriate business decision.","title":"Description"},{"location":"logging/events/12-velocloud-bridge/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/12-velocloud-bridge/#list-of-decisions-made-by-the-velocloud-bridge","text":"","title":"List of Decisions made by the VeloCloud Bridge"},{"location":"logging/events/12-velocloud-bridge/#subject-alertrequesteventedge-aims-at-endpoint-post-eventgetenterpriseevents","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /event/getEnterpriseEvents HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: alert.request.event.edge (aims at endpoint POST /event/getEnterpriseEvents)"},{"location":"logging/events/12-velocloud-bridge/#subject-requestenterprisesedges-aims-at-endpoint-post-enterprisegetenterpriseedges","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /enterprise/getEnterpriseEdges HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: request.enterprises.edges (aims at endpoint POST /enterprise/getEnterpriseEdges)"},{"location":"logging/events/12-velocloud-bridge/#subject-alertrequestevententerprise-aims-at-endpoint-post-eventgetenterpriseevents","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /event/getEnterpriseEvents HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: alert.request.event.enterprise (aims at endpoint POST /event/getEnterpriseEvents)"},{"location":"logging/events/12-velocloud-bridge/#subject-requestenterprisesnames-aims-at-endpoint-post-monitoringgetaggregates","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /monitoring/getAggregates HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: request.enterprises.names (aims at endpoint POST /monitoring/getAggregates)"},{"location":"logging/events/12-velocloud-bridge/#subject-requestedgelinksseries-aims-at-endpoint-post-metricsgetedgelinkseries","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /metrics/getEdgeLinkSeries HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: request.edge.links.series (aims at endpoint POST /metrics/getEdgeLinkSeries)"},{"location":"logging/events/12-velocloud-bridge/#subject-requestlinksconfiguration-aims-at-endpoint-post-edgegetedgeconfigurationmodules","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /edge/getEdgeConfigurationModules HTTP response has status 200 HTTP response has NOT status 200 3 Check for existence of WAN module in response WAN configuration exists WAN configuration does NOT exist 4 Check for existence of links configurations in WAN module Links config defined in WAN configuration Links config NOT defined in WAN configuration","title":"Subject: request.links.configuration (aims at endpoint POST /edge/getEdgeConfigurationModules)"},{"location":"logging/events/12-velocloud-bridge/#subject-getlinksmetricinfo-aims-at-endpoint-post-monitoringgetaggregateedgelinkmetrics","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /monitoring/getAggregateEdgeLinkMetrics HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: get.links.metric.info (aims at endpoint POST /monitoring/getAggregateEdgeLinkMetrics)"},{"location":"logging/events/12-velocloud-bridge/#subject-getlinkswithedgeinfo-aims-at-endpoint-post-monitoringgetenterpriseedgelinkstatus","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /monitoring/getEnterpriseEdgeLinkStatus HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: get.links.with.edge.info (aims at endpoint POST /monitoring/getEnterpriseEdgeLinkStatus)"},{"location":"logging/events/12-velocloud-bridge/#subject-requestnetworkenterpriseedges-aims-at-endpoint-post-networkgetnetworkenterprises","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /network/getNetworkEnterprises HTTP response has status 200 HTTP response has NOT status 200 3 Check enterprise was found for specified filters Enterprise found for filters Enterprise NOT found for filters 4 Check enterprise has edges Edges found for enterprises Edges NOT found for enterprises","title":"Subject: request.network.enterprise.edges (aims at endpoint POST /network/getNetworkEnterprises)"},{"location":"logging/events/12-velocloud-bridge/#subject-requestnetworkgatewaylist-aims-at-endpoint-post-networkgetnetworkgateways","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /network/getNetworkGateways HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: request.network.gateway.list (aims at endpoint POST /network/getNetworkGateways)"},{"location":"logging/events/12-velocloud-bridge/#subject-requestgatewaystatusmetrics-aims-at-endpoint-post-metricsgetgatewaystatusmetrics","text":"Condition Decision Decision 1 Check for shape and content of incoming request Request has valid format Request has invalid format 2 Check for status of response from POST /metrics/getGatewayStatusMetrics HTTP response has status 200 HTTP response has NOT status 200","title":"Subject: request.gateway.status.metrics (aims at endpoint POST /metrics/getGatewayStatusMetrics)"},{"location":"logging/events/12-velocloud-bridge/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/12-velocloud-bridge/#subject-alertrequesteventedge","text":"edge_events_for_alert","title":"Subject: alert.request.event.edge"},{"location":"logging/events/12-velocloud-bridge/#subject-requestenterprisesedges","text":"enterprise_edge_list","title":"Subject: request.enterprises.edges"},{"location":"logging/events/12-velocloud-bridge/#subject-alertrequestevententerprise","text":"enterprise_events_for_alert","title":"Subject: alert.request.event.enterprise"},{"location":"logging/events/12-velocloud-bridge/#subject-requestenterprisesnames","text":"enterprise_name_list_response","title":"Subject: request.enterprises.names"},{"location":"logging/events/12-velocloud-bridge/#subject-requestedgelinksseries","text":"get_edge_links_series","title":"Subject: request.edge.links.series"},{"location":"logging/events/12-velocloud-bridge/#subject-requestlinksconfiguration","text":"links_configuration","title":"Subject: request.links.configuration"},{"location":"logging/events/12-velocloud-bridge/#subject-getlinksmetricinfo","text":"links_metric_info","title":"Subject: get.links.metric.info"},{"location":"logging/events/12-velocloud-bridge/#subject-getlinkswithedgeinfo","text":"links_with_edge_info","title":"Subject: get.links.with.edge.info"},{"location":"logging/events/12-velocloud-bridge/#subject-requestnetworkenterpriseedges","text":"network_enterprise_edge_list","title":"Subject: request.network.enterprise.edges"},{"location":"logging/events/12-velocloud-bridge/#subject-requestnetworkgatewaylist","text":"network_gateway_list","title":"Subject: request.network.gateway.list"},{"location":"logging/events/12-velocloud-bridge/#subject-requestgatewaystatusmetrics","text":"gateway_status_metrics","title":"Subject: request.gateway.status.metrics"},{"location":"logging/events/2-BYOB-IPA-queue/","text":"IPA Event Logging Process Workflows List of Decisions made by the IPA System BYOB IPA queue Start of BYOB workflow 1. Determining whether or not there is an ongoing trouble or not Trouble is detected Trouble stabilized Trouble is detected 2. Determining what kind of trouble occurred Trouble is on the link Trouble is on the Edge 3. Determining what VCO does the troubled device belong too VCO 1, 2, 3 VCO 4 4. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\" Trouble stabilized 5. Determining what kind of trouble last occurred Trouble is on the link Trouble is on the Edge 6. Determining what VCO does the device belong too VCO 1, 2, 3 VCO 4 7. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\" Event Descriptions BYOB IPA queue _attempt_ticket_creation","title":"IPA Event Logging"},{"location":"logging/events/2-BYOB-IPA-queue/#ipa-event-logging","text":"","title":"IPA Event Logging"},{"location":"logging/events/2-BYOB-IPA-queue/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/2-BYOB-IPA-queue/#list-of-decisions-made-by-the-ipa-system","text":"","title":"List of Decisions made by the IPA System"},{"location":"logging/events/2-BYOB-IPA-queue/#byob-ipa-queue","text":"","title":"BYOB IPA queue"},{"location":"logging/events/2-BYOB-IPA-queue/#start-of-byob-workflow","text":"1. Determining whether or not there is an ongoing trouble or not Trouble is detected Trouble stabilized","title":"Start of BYOB workflow"},{"location":"logging/events/2-BYOB-IPA-queue/#trouble-is-detected","text":"2. Determining what kind of trouble occurred Trouble is on the link Trouble is on the Edge 3. Determining what VCO does the troubled device belong too VCO 1, 2, 3 VCO 4 4. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\"","title":"Trouble is detected"},{"location":"logging/events/2-BYOB-IPA-queue/#trouble-stabilized","text":"5. Determining what kind of trouble last occurred Trouble is on the link Trouble is on the Edge 6. Determining what VCO does the device belong too VCO 1, 2, 3 VCO 4 7. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\"","title":"Trouble stabilized"},{"location":"logging/events/2-BYOB-IPA-queue/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/2-BYOB-IPA-queue/#byob-ipa-queue_1","text":"_attempt_ticket_creation","title":"BYOB IPA queue"},{"location":"logging/events/3-HNOC-forwarding/","text":"HNOC Forwarding Event Logging Process Workflows List of Decisions made by the HNOC forwarding System HNOC forwarding queue Start of HNOC forwarding workflow (SO) 1. Service outage detected 2. Attempt to create ticket outage Create new SO ticket Reopen exist ticket 3. If exist ticket and have more than 60 min Forward HNOC END 4. New ticket created Append triage note 5. If edge outage Forward to HNOC END 6. If link outage Check if more than 60 minutes Start of HNOC forwarding workflow (SA) 1. Service affecting trouble detected Not SA ticket for device SA ticket for device 2. Not SA ticket Create new SA ticket and append note 3. SA ticket exist Reopen SA ticket and append note 4 Wait 60 seconds and forward to HNOC Event Descriptions HNOC forwarding outage queue _attempt_ticket_creation Event Descriptions HNOC forwarding affecting queue _attempt_ticket_creation","title":"HNOC Forwarding Event Logging"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-event-logging","text":"","title":"HNOC Forwarding Event Logging"},{"location":"logging/events/3-HNOC-forwarding/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/3-HNOC-forwarding/#list-of-decisions-made-by-the-hnoc-forwarding-system","text":"","title":"List of Decisions made by the HNOC forwarding System"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-queue","text":"","title":"HNOC forwarding queue"},{"location":"logging/events/3-HNOC-forwarding/#start-of-hnoc-forwarding-workflow-so","text":"1. Service outage detected 2. Attempt to create ticket outage Create new SO ticket Reopen exist ticket 3. If exist ticket and have more than 60 min Forward HNOC END 4. New ticket created Append triage note 5. If edge outage Forward to HNOC END 6. If link outage Check if more than 60 minutes","title":"Start of  HNOC forwarding workflow (SO)"},{"location":"logging/events/3-HNOC-forwarding/#start-of-hnoc-forwarding-workflow-sa","text":"1. Service affecting trouble detected Not SA ticket for device SA ticket for device 2. Not SA ticket Create new SA ticket and append note 3. SA ticket exist Reopen SA ticket and append note 4 Wait 60 seconds and forward to HNOC","title":"Start of  HNOC forwarding workflow (SA)"},{"location":"logging/events/3-HNOC-forwarding/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-outage-queue","text":"_attempt_ticket_creation","title":"HNOC forwarding outage queue"},{"location":"logging/events/3-HNOC-forwarding/#event-descriptions_1","text":"","title":"Event Descriptions"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-affecting-queue","text":"_attempt_ticket_creation","title":"HNOC forwarding affecting queue"},{"location":"logging/events/4-SA-forward-to-ASR/","text":"IPA Event Logging Process Workflows List of Decisions made by the IPA System Service Affecting Circuit Instability 1. Check for existing SA ticket for SD-WAN SA ticket does not exist for SD-WAN Resolved SA ticket exists for SD-WAN Open/In progress SA ticket exist for SD-WAN 2. Check for other troubles on the ticket Other trouble is documented on ticket. (Jitter, Latency, Packet Loss, Bandwidth) Only Circuit Instability is the only documented trouble on the ticket 3. Checking what kind of link the instability was detected on Instability was detected for Wireless Link Instability was detected for Wired Link 4. Checking wired link's name for BYOB or Customer Owned Wired Link name contains BYOB or Customer Owned Wired Link name does NOT contain BYOB or Customer Owned 5. Checking if the wired link name is an IP Wired link name is an IP Wired link name is NOT an IP Event Descriptions Service Affecting Bouncing check","title":"IPA Event Logging"},{"location":"logging/events/4-SA-forward-to-ASR/#ipa-event-logging","text":"","title":"IPA Event Logging"},{"location":"logging/events/4-SA-forward-to-ASR/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/4-SA-forward-to-ASR/#list-of-decisions-made-by-the-ipa-system","text":"","title":"List of Decisions made by the IPA System"},{"location":"logging/events/4-SA-forward-to-ASR/#service-affecting","text":"","title":"Service Affecting"},{"location":"logging/events/4-SA-forward-to-ASR/#circuit-instability","text":"1. Check for existing SA ticket for SD-WAN SA ticket does not exist for SD-WAN Resolved SA ticket exists for SD-WAN Open/In progress SA ticket exist for SD-WAN 2. Check for other troubles on the ticket Other trouble is documented on ticket. (Jitter, Latency, Packet Loss, Bandwidth) Only Circuit Instability is the only documented trouble on the ticket 3. Checking what kind of link the instability was detected on Instability was detected for Wireless Link Instability was detected for Wired Link 4. Checking wired link's name for BYOB or Customer Owned Wired Link name contains BYOB or Customer Owned Wired Link name does NOT contain BYOB or Customer Owned 5. Checking if the wired link name is an IP Wired link name is an IP Wired link name is NOT an IP","title":"Circuit Instability"},{"location":"logging/events/4-SA-forward-to-ASR/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/4-SA-forward-to-ASR/#service-affecting_1","text":"Bouncing check","title":"Service Affecting"},{"location":"logging/events/5-TNBA-monitor/","text":"1. <G ent Logging Process Workflows List of Decisions made by the TNBA Monitor 1. Get TNBA prediction for SD-WAN task Prediction not repair request complete Prediction repair request complete Ticket is not outage Ticket is outage Ticket is not created by IPA Ticket created by IPA Confidence level is < 80 Confidence level is > 80 2. Append prediction note SD-WAN - - 3. Check status SD-WAN Edge or link not online Edge and links online 4. TBD - - 5. Autoresolve SD-WAN task - - 6. Append A.I. resolve note - - Start of TNBA Monitor Event description Start TNBA automated process","title":"5 TNBA monitor"},{"location":"logging/events/5-TNBA-monitor/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/5-TNBA-monitor/#list-of-decisions-made-by-the-tnba-monitor","text":"1. Get TNBA prediction for SD-WAN task Prediction not repair request complete Prediction repair request complete Ticket is not outage Ticket is outage Ticket is not created by IPA Ticket created by IPA Confidence level is < 80 Confidence level is > 80 2. Append prediction note SD-WAN - - 3. Check status SD-WAN Edge or link not online Edge and links online 4. TBD - - 5. Autoresolve SD-WAN task - - 6. Append A.I. resolve note - -","title":"List of Decisions made by the TNBA Monitor"},{"location":"logging/events/5-TNBA-monitor/#start-of-tnba-monitor","text":"","title":"Start of TNBA Monitor"},{"location":"logging/events/5-TNBA-monitor/#event-description","text":"Start TNBA automated process","title":"Event description"},{"location":"logging/events/6-ticket-severity/","text":"Ticket severity Process Workflows List of Decisions made by ticket severity Ticket Severity Start of change ticket severity 1. Outage condition is detected Edge DOWN Link DOWN Edge Down 1. Attempt to create a service Outage bruin ticket Status 200, 409, 473, 472, 471 2. Set ticket severity 2 END Link Down 1. Attempt to create a service Outage bruin ticket Status 200 or 473 Status 409, 472 or 471 2. For status 200 or 473 Set ticket severity 3 - 3. For status 409, 472 or 471 Ticket with only 1 Task set severity 3 If ticket have more than 1 task end Autoresolution Event Descriptions Attempt ticket creation","title":"Ticket severity"},{"location":"logging/events/6-ticket-severity/#ticket-severity","text":"","title":"Ticket severity"},{"location":"logging/events/6-ticket-severity/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/6-ticket-severity/#list-of-decisions-made-by-ticket-severity","text":"","title":"List of Decisions made by ticket severity"},{"location":"logging/events/6-ticket-severity/#ticket-severity_1","text":"","title":"Ticket Severity"},{"location":"logging/events/6-ticket-severity/#start-of-change-ticket-severity","text":"1. Outage condition is detected Edge DOWN Link DOWN","title":"Start of change ticket severity"},{"location":"logging/events/6-ticket-severity/#edge-down","text":"1. Attempt to create a service Outage bruin ticket Status 200, 409, 473, 472, 471 2. Set ticket severity 2 END","title":"Edge Down"},{"location":"logging/events/6-ticket-severity/#link-down","text":"1. Attempt to create a service Outage bruin ticket Status 200 or 473 Status 409, 472 or 471 2. For status 200 or 473 Set ticket severity 3 - 3. For status 409, 472 or 471 Ticket with only 1 Task set severity 3 If ticket have more than 1 task end","title":"Link Down"},{"location":"logging/events/6-ticket-severity/#autoresolution","text":"","title":"Autoresolution"},{"location":"logging/events/6-ticket-severity/#event-descriptions","text":"Attempt ticket creation","title":"Event Descriptions"},{"location":"logging/events/7-ticket-creation-outcome/","text":"","title":"7 ticket creation outcome"},{"location":"logging/events/8-service-affecting/","text":"Service affecting Event Logging Process Workflows List of Decisions made by the service affecting System Service affecting queue Start of service affecting workflow 1. Detected service trouble SD-WAN 2. Check if exist ticket No new, in progress or resolved ticket created Exist a resolve ticket New or in progress exist 3. If no exist ticket Create ticket and append note for trouble 4. If resolved ticket exist Reopen ticket and append note for trouble 5. If exist in progress ticket Check if is the same problem, if is the same end process If not the same trouble append new note Event Descriptions Service affecting queue _attempt_ticket_creation","title":"Service affecting Event Logging"},{"location":"logging/events/8-service-affecting/#service-affecting-event-logging","text":"","title":"Service affecting Event Logging"},{"location":"logging/events/8-service-affecting/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/8-service-affecting/#list-of-decisions-made-by-the-service-affecting-system","text":"","title":"List of Decisions made by the service affecting System"},{"location":"logging/events/8-service-affecting/#service-affecting-queue","text":"","title":"Service affecting queue"},{"location":"logging/events/8-service-affecting/#start-of-service-affecting-workflow","text":"1. Detected service trouble SD-WAN 2. Check if exist ticket No new, in progress or resolved ticket created Exist a resolve ticket New or in progress exist 3. If no exist ticket Create ticket and append note for trouble 4. If resolved ticket exist Reopen ticket and append note for trouble 5. If exist in progress ticket Check if is the same problem, if is the same end process If not the same trouble append new note","title":"Start of  service affecting workflow"},{"location":"logging/events/8-service-affecting/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/8-service-affecting/#service-affecting-queue_1","text":"_attempt_ticket_creation","title":"Service affecting queue"},{"location":"logging/events/9-intermapper-monitor/","text":"Intermapper Event Logging Description The mission of this service is to analyze InterMapper events sent by one or more InterMapper instances via e-mail, where each event refers to a device. The outcome of the analysis will determine whether: * A new Bruin ticket is created, if the device is down. * An existing Bruin ticket is re-opened, if the device is down and the ticket is resolved. * An existing Bruin ticket is auto-resolved, if the device is up and the ticket is open. When any of these cases take place, the event information is added to the ticket as a note. In some cases, this note can be enriched with information from the DRI system. Whatever the outcome of the analysis is, the event is finally marked as processed. Process Workflows List of Decisions made by the Intermapper Monitor Process E-mail Batch workflow Condition Decision Decision 1 Check for Circuit ID existence in e-mail Circuit ID is defined Circuit ID is undefined 2 Check for Circuit ID being an SD-WAN Circuit ID is not SD-WAN Circuit ID is SD-WAN Process E-mail workflow Condition Decision Decision Decision 1 Check for Event Type Event type is Alarm, Critical, Warning, Down or Link Warning Event type is Up or OK Event Type is any other Ticket Creation workflow Condition Decision Decision 1 Check for Probe Type Probe Type is Data Remote Probe Type is NOT Data Remote 2 Check to see if we can retrieve data from DRI Successfully retrieved data from DRI for Inventory Could not retrieve any data for inventory from DRI 3 Check Probe Type and Condition of event Probe Type is Data Remote Probe AND Condition is Device Lost Power - Battery is in use Probe Type is NOT Data Remote Probe or Condition is NOT Device Lost Power - Battery is in use Auto-Resolution workflow Condition Decision Decision 1 Check to see if ticket's product category is in the whitelist Ticket's product category is whitelisted Ticket's product category is NOT whitelisted 2 Check for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 3 Check if time passed is more or less than 3 hrs (NIGHT) Less than 3 hrs have passed since an outage has been documented More than 3 hrs have passed since an outage has been documented 4 Check if time passed is more or less than 90 min (DAY) Less than 90 min have passed since an outage has been documented More than 90 min have passed since an outage has been documented 5 Check if max auto-resolves threshold has been exceeded Ticket has been auto-resolved less than 3 times Ticket has been auto-resolved 3+ times already 6 Check if ticket is resolved Ticket is resolved already Ticket is not resolved Event Descriptions Start InterMapper Outage Monitoring start_intermapper_outage_monitoring Process E-mail Batch _process_email_batch Process E-mail _process_email Ticket Creation _create_outage_ticket Auto-Resolution _autoresolve_ticket","title":"Intermapper Event Logging"},{"location":"logging/events/9-intermapper-monitor/#intermapper-event-logging","text":"","title":"Intermapper Event Logging"},{"location":"logging/events/9-intermapper-monitor/#description","text":"The mission of this service is to analyze InterMapper events sent by one or more InterMapper instances via e-mail, where each event refers to a device. The outcome of the analysis will determine whether: * A new Bruin ticket is created, if the device is down. * An existing Bruin ticket is re-opened, if the device is down and the ticket is resolved. * An existing Bruin ticket is auto-resolved, if the device is up and the ticket is open. When any of these cases take place, the event information is added to the ticket as a note. In some cases, this note can be enriched with information from the DRI system. Whatever the outcome of the analysis is, the event is finally marked as processed.","title":"Description"},{"location":"logging/events/9-intermapper-monitor/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/9-intermapper-monitor/#list-of-decisions-made-by-the-intermapper-monitor","text":"","title":"List of Decisions made by the Intermapper Monitor"},{"location":"logging/events/9-intermapper-monitor/#process-e-mail-batch-workflow","text":"Condition Decision Decision 1 Check for Circuit ID existence in e-mail Circuit ID is defined Circuit ID is undefined 2 Check for Circuit ID being an SD-WAN Circuit ID is not SD-WAN Circuit ID is SD-WAN","title":"Process E-mail Batch workflow"},{"location":"logging/events/9-intermapper-monitor/#process-e-mail-workflow","text":"Condition Decision Decision Decision 1 Check for Event Type Event type is Alarm, Critical, Warning, Down or Link Warning Event type is Up or OK Event Type is any other","title":"Process E-mail workflow"},{"location":"logging/events/9-intermapper-monitor/#ticket-creation-workflow","text":"Condition Decision Decision 1 Check for Probe Type Probe Type is Data Remote Probe Type is NOT Data Remote 2 Check to see if we can retrieve data from DRI Successfully retrieved data from DRI for Inventory Could not retrieve any data for inventory from DRI 3 Check Probe Type and Condition of event Probe Type is Data Remote Probe AND Condition is Device Lost Power - Battery is in use Probe Type is NOT Data Remote Probe or Condition is NOT Device Lost Power - Battery is in use","title":"Ticket Creation workflow"},{"location":"logging/events/9-intermapper-monitor/#auto-resolution-workflow","text":"Condition Decision Decision 1 Check to see if ticket's product category is in the whitelist Ticket's product category is whitelisted Ticket's product category is NOT whitelisted 2 Check for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 3 Check if time passed is more or less than 3 hrs (NIGHT) Less than 3 hrs have passed since an outage has been documented More than 3 hrs have passed since an outage has been documented 4 Check if time passed is more or less than 90 min (DAY) Less than 90 min have passed since an outage has been documented More than 90 min have passed since an outage has been documented 5 Check if max auto-resolves threshold has been exceeded Ticket has been auto-resolved less than 3 times Ticket has been auto-resolved 3+ times already 6 Check if ticket is resolved Ticket is resolved already Ticket is not resolved","title":"Auto-Resolution workflow"},{"location":"logging/events/9-intermapper-monitor/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/9-intermapper-monitor/#start-intermapper-outage-monitoring","text":"start_intermapper_outage_monitoring","title":"Start InterMapper Outage Monitoring"},{"location":"logging/events/9-intermapper-monitor/#process-e-mail-batch","text":"_process_email_batch","title":"Process E-mail Batch"},{"location":"logging/events/9-intermapper-monitor/#process-e-mail","text":"_process_email","title":"Process E-mail"},{"location":"logging/events/9-intermapper-monitor/#ticket-creation","text":"_create_outage_ticket","title":"Ticket Creation"},{"location":"logging/events/9-intermapper-monitor/#auto-resolution","text":"_autoresolve_ticket","title":"Auto-Resolution"},{"location":"logging/services/TNBA-monitor/actions/_append_tnba_notes/","text":"Append tnba notes for ticket id in notes ticket id: self._logger.info(f\"Appending {len(notes)} TNBA notes to ticket {ticket_id}...\") append_multiple_notes_to_ticket If append multiple notes status is not OK: self._logger.warning(f\"Bad status calling append multiple notes to ticket id: {ticket_id}.\" f\" Skipping ...\")","title":" append tnba notes"},{"location":"logging/services/TNBA-monitor/actions/_append_tnba_notes/#append-tnba-notes","text":"for ticket id in notes ticket id: self._logger.info(f\"Appending {len(notes)} TNBA notes to ticket {ticket_id}...\") append_multiple_notes_to_ticket If append multiple notes status is not OK: self._logger.warning(f\"Bad status calling append multiple notes to ticket id: {ticket_id}.\" f\" Skipping ...\")","title":"Append tnba notes"},{"location":"logging/services/TNBA-monitor/actions/_autoresolve_ticket_detail/","text":"Autoresolve ticket detail self._logger.info(f\"Running autoresolve for serial {serial_number} of ticket {ticket_id}...\") * If ticket created by automation engine: self._logger.info( f\"Ticket {ticket_id}, where serial {serial_number} is, was not created by Automation Engine. \" \"Skipping autoresolve...\" ) * If is detail in outage ticket: self._logger.info( f\"Serial {serial_number} of ticket {ticket_id} is in outage state. Skipping autoresolve...\" ) * If is detail in affecting ticket and are all metrics within thresholds: self._logger.info( f\"At least one metric from serial {serial_number} of ticket {ticket_id} is not within the threshold.\" f\" Skipping autoresolve...\" ) * If prediction is not confident enough: self._logger.info( f\"The confidence of the best prediction found for ticket {ticket_id}, where serial {serial_number} is, \" f\"did not exceed the minimum threshold. Skipping autoresolve...\" ) * If environment is not PRODUCTION: self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} was about to be resolved, but the \" f\"current environment is {self._config.CURRENT_ENVIRONMENT.upper()}. Skipping autoresolve...\" ) * unpause_ticket_detail * resolve_ticket_detail * If resolve ticket detail status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket detail for ticket id: {ticket_id} \" f\"and ticket detail id: {ticket_detail_id} . Skipping resolve ticket detail\")","title":" autoresolve ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_autoresolve_ticket_detail/#autoresolve-ticket-detail","text":"self._logger.info(f\"Running autoresolve for serial {serial_number} of ticket {ticket_id}...\") * If ticket created by automation engine: self._logger.info( f\"Ticket {ticket_id}, where serial {serial_number} is, was not created by Automation Engine. \" \"Skipping autoresolve...\" ) * If is detail in outage ticket: self._logger.info( f\"Serial {serial_number} of ticket {ticket_id} is in outage state. Skipping autoresolve...\" ) * If is detail in affecting ticket and are all metrics within thresholds: self._logger.info( f\"At least one metric from serial {serial_number} of ticket {ticket_id} is not within the threshold.\" f\" Skipping autoresolve...\" ) * If prediction is not confident enough: self._logger.info( f\"The confidence of the best prediction found for ticket {ticket_id}, where serial {serial_number} is, \" f\"did not exceed the minimum threshold. Skipping autoresolve...\" ) * If environment is not PRODUCTION: self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} was about to be resolved, but the \" f\"current environment is {self._config.CURRENT_ENVIRONMENT.upper()}. Skipping autoresolve...\" ) * unpause_ticket_detail * resolve_ticket_detail * If resolve ticket detail status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket detail for ticket id: {ticket_id} \" f\"and ticket detail id: {ticket_detail_id} . Skipping resolve ticket detail\")","title":"Autoresolve ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_filter_outage_ticket_details_based_on_last_outage/","text":"Filter outage ticket details based on last outage for ticket detail in tickets details: If is outage ticket and last outage detected recently: self._logger.info( f\"Last outage detected for serial {serial_number} in Service Outage ticket {ticket_id} is \" \"too recent. Skipping...\" )","title":" filter outage ticket details based on last outage"},{"location":"logging/services/TNBA-monitor/actions/_filter_outage_ticket_details_based_on_last_outage/#filter-outage-ticket-details-based-on-last-outage","text":"for ticket detail in tickets details: If is outage ticket and last outage detected recently: self._logger.info( f\"Last outage detected for serial {serial_number} in Service Outage ticket {ticket_id} is \" \"too recent. Skipping...\" )","title":"Filter outage ticket details based on last outage"},{"location":"logging/services/TNBA-monitor/actions/_filter_tickets_and_details_related_to_edges_under_monitoring/","text":"Filter tickets and details related to edges under monitoring for ticket in tickets If not relevant ticket: self._logger.warning(f\"Don't found relevant tickets. Skipping ticket ...\")","title":" filter tickets and details related to edges under monitoring"},{"location":"logging/services/TNBA-monitor/actions/_filter_tickets_and_details_related_to_edges_under_monitoring/#filter-tickets-and-details-related-to-edges-under-monitoring","text":"for ticket in tickets If not relevant ticket: self._logger.warning(f\"Don't found relevant tickets. Skipping ticket ...\")","title":"Filter tickets and details related to edges under monitoring"},{"location":"logging/services/TNBA-monitor/actions/_get_all_open_tickets_with_details_for_monitored_companies/","text":"Get all open tickets with details for monitored companies _get_open_tickets_with_details_by_client_id","title":" get all open tickets with details for monitored companies"},{"location":"logging/services/TNBA-monitor/actions/_get_all_open_tickets_with_details_for_monitored_companies/#get-all-open-tickets-with-details-for-monitored-companies","text":"_get_open_tickets_with_details_by_client_id","title":"Get all open tickets with details for monitored companies"},{"location":"logging/services/TNBA-monitor/actions/_get_open_tickets_with_details_by_client_id/","text":"Get open tickets with details by client id get_open_outage_tickets If bad status calling to get outage tickets: self._logger.warning(f\"Bad status calling to get outage tickets. Return empty list ...\") get_open_affecting_tickets self._logger.warning(f\"Bad status calling to get affecting tickets. Return empty list ...\") self._logger.info(f\"Getting all opened tickets for Bruin customer {client_id}...\") For ticket in all tickets: get_ticket_details.md self._logger.warning(f\"Bad status calling to get tickets details with id: {ticket_id}.\" f\"Skipping ticket ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id} of Bruin customer {client_id}!\") self._logger.info(f\"Finished getting all opened tickets for Bruin customer {client_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get open tickets with details for Bruin client {client_id} -> {e}\" )","title":" get open tickets with details by client id"},{"location":"logging/services/TNBA-monitor/actions/_get_open_tickets_with_details_by_client_id/#get-open-tickets-with-details-by-client-id","text":"get_open_outage_tickets If bad status calling to get outage tickets: self._logger.warning(f\"Bad status calling to get outage tickets. Return empty list ...\") get_open_affecting_tickets self._logger.warning(f\"Bad status calling to get affecting tickets. Return empty list ...\") self._logger.info(f\"Getting all opened tickets for Bruin customer {client_id}...\") For ticket in all tickets: get_ticket_details.md self._logger.warning(f\"Bad status calling to get tickets details with id: {ticket_id}.\" f\"Skipping ticket ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id} of Bruin customer {client_id}!\") self._logger.info(f\"Finished getting all opened tickets for Bruin customer {client_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get open tickets with details for Bruin client {client_id} -> {e}\" )","title":"Get open tickets with details by client id"},{"location":"logging/services/TNBA-monitor/actions/_get_predictions_by_ticket_id/","text":"Get predictions by ticket id for ticket in tickets: self._logger.info(f\"Claiming T7 predictions for ticket {ticket_id}...\") get_ticket_task_history self._logger.warning(f\"Bad status calling to get ticket history. \" f\"Skipping for ticket id: {ticket_id} ...\") If any ticket row has asset: self._logger.info(f\"Task history of ticket {ticket_id} doesn't have any asset. Skipping...\") get_prediction self._logger.warning(f\"Bad status calling to t7 predictions. \" f\"Skipping predictions for ticket id: {ticket_id}\") If not predictions: self._logger.info(f\"There are no predictions for ticket {ticket_id}. Skipping...\") self._logger.info(f\"T7 predictions found for ticket {ticket_id}!\")","title":" get predictions by ticket id"},{"location":"logging/services/TNBA-monitor/actions/_get_predictions_by_ticket_id/#get-predictions-by-ticket-id","text":"for ticket in tickets: self._logger.info(f\"Claiming T7 predictions for ticket {ticket_id}...\") get_ticket_task_history self._logger.warning(f\"Bad status calling to get ticket history. \" f\"Skipping for ticket id: {ticket_id} ...\") If any ticket row has asset: self._logger.info(f\"Task history of ticket {ticket_id} doesn't have any asset. Skipping...\") get_prediction self._logger.warning(f\"Bad status calling to t7 predictions. \" f\"Skipping predictions for ticket id: {ticket_id}\") If not predictions: self._logger.info(f\"There are no predictions for ticket {ticket_id}. Skipping...\") self._logger.info(f\"T7 predictions found for ticket {ticket_id}!\")","title":"Get predictions by ticket id"},{"location":"logging/services/TNBA-monitor/actions/_map_ticket_details_with_predictions/","text":"Map ticket details with predictions for detail obj in detail ticket details: If not predictions for ticket: self._logger.info( f\"Ticket {ticket_id} does not have any prediction associated. Skipping serial \" f\"{serial_number}...\" ) If not prediction object for related serial: self._logger.info( f\"No predictions were found for ticket {ticket_id} and serial {serial_number}. Skipping...\" )","title":" map ticket details with predictions"},{"location":"logging/services/TNBA-monitor/actions/_map_ticket_details_with_predictions/#map-ticket-details-with-predictions","text":"for detail obj in detail ticket details: If not predictions for ticket: self._logger.info( f\"Ticket {ticket_id} does not have any prediction associated. Skipping serial \" f\"{serial_number}...\" ) If not prediction object for related serial: self._logger.info( f\"No predictions were found for ticket {ticket_id} and serial {serial_number}. Skipping...\" )","title":"Map ticket details with predictions"},{"location":"logging/services/TNBA-monitor/actions/_process_ticket_detail/","text":"Process ticket detail self._logger.info( f\"Processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}...\" ) If is a tnba note and tnba note is not old enough: self._logger.info( f\"TNBA note found for ticket {ticket_id} and detail {ticket_detail_id} is too recent. \" f\"Skipping detail...\" ) get_next_results_for_ticket_detail If get next result for ticket detail status is not ok: self._logger.warning(f\"Bad status calling get next result for ticket details.\" f\"Skipping process ticket details for ticket id: {ticket_id} and\" f\"ticket detail id: {ticket_detail_id}\") self._logger.info( f\"Filtering predictions available in next results for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If not relevant predictions: self._logger.info(f\"No predictions with name appearing in the next results were found for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}!\") self._logger.info( f\"Predictions available in next results found for ticket {ticket_id}, detail {ticket_detail_id} \" f\"and serial {serial_number}: {relevant_predictions}\" ) If newest note: If best prediction different that the ticket note: self._logger.info( f\"Best prediction for ticket {ticket_id}, detail {ticket_detail_id} and serial {serial_number} \" f\"didn't change since the last TNBA note was appended. Skipping detail...\" ) self._logger.info( f\"Building TNBA note from prediction {best_prediction['name']} for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If request or repair completed prediction: self._logger.info( f\"Best prediction found for serial {serial_number} of ticket {ticket_id} is \" f'{best_prediction[\"name\"]}. Running autoresolve...' ) _autoresolve_ticket_detail If autoresolve status is SUCCES: post_live_automation_metrics If autoresolve status is SKIPPED: self._logger.info( f\"Autoresolve was triggered because the best prediction found for serial {serial_number} of \" f'ticket {ticket_id} was {best_prediction[\"name\"]}, but the process failed. A TNBA note with ' \"this prediction will be built and appended to the ticket later on.\" ) IF autoresolve status is BAD_PREDICTION: post_live_automation_metrics self._logger.info( f\"The prediction for serial {serial_number} of ticket {ticket_id} is considered wrong.\" ) If not TNBA note: self._logger.info(f\"No TNBA note will be appended for serial {serial_number} of ticket {ticket_id}.\") If environment DEV: self._logger.info(f\"TNBA note would have been appended to ticket {ticket_id} and detail {ticket_detail_id} \" f\"(serial: {serial_number}). Note: {tnba_note}. Details at app.bruin.com/t/{ticket_id}\") self._logger.info( f\"Finished processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}!\" )","title":" process ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_process_ticket_detail/#process-ticket-detail","text":"self._logger.info( f\"Processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}...\" ) If is a tnba note and tnba note is not old enough: self._logger.info( f\"TNBA note found for ticket {ticket_id} and detail {ticket_detail_id} is too recent. \" f\"Skipping detail...\" ) get_next_results_for_ticket_detail If get next result for ticket detail status is not ok: self._logger.warning(f\"Bad status calling get next result for ticket details.\" f\"Skipping process ticket details for ticket id: {ticket_id} and\" f\"ticket detail id: {ticket_detail_id}\") self._logger.info( f\"Filtering predictions available in next results for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If not relevant predictions: self._logger.info(f\"No predictions with name appearing in the next results were found for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}!\") self._logger.info( f\"Predictions available in next results found for ticket {ticket_id}, detail {ticket_detail_id} \" f\"and serial {serial_number}: {relevant_predictions}\" ) If newest note: If best prediction different that the ticket note: self._logger.info( f\"Best prediction for ticket {ticket_id}, detail {ticket_detail_id} and serial {serial_number} \" f\"didn't change since the last TNBA note was appended. Skipping detail...\" ) self._logger.info( f\"Building TNBA note from prediction {best_prediction['name']} for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If request or repair completed prediction: self._logger.info( f\"Best prediction found for serial {serial_number} of ticket {ticket_id} is \" f'{best_prediction[\"name\"]}. Running autoresolve...' ) _autoresolve_ticket_detail If autoresolve status is SUCCES: post_live_automation_metrics If autoresolve status is SKIPPED: self._logger.info( f\"Autoresolve was triggered because the best prediction found for serial {serial_number} of \" f'ticket {ticket_id} was {best_prediction[\"name\"]}, but the process failed. A TNBA note with ' \"this prediction will be built and appended to the ticket later on.\" ) IF autoresolve status is BAD_PREDICTION: post_live_automation_metrics self._logger.info( f\"The prediction for serial {serial_number} of ticket {ticket_id} is considered wrong.\" ) If not TNBA note: self._logger.info(f\"No TNBA note will be appended for serial {serial_number} of ticket {ticket_id}.\") If environment DEV: self._logger.info(f\"TNBA note would have been appended to ticket {ticket_id} and detail {ticket_detail_id} \" f\"(serial: {serial_number}). Note: {tnba_note}. Details at app.bruin.com/t/{ticket_id}\") self._logger.info( f\"Finished processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}!\" )","title":"Process ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_remove_erroneous_predictions/","text":"Remove erroneous predictions for ticket in prediction tickets: for prediction in predictions: If error in prediction: self._logger.info( f\"Prediction for serial {serial_number} in ticket {ticket_id} was found but it contains an \" f\"error from T7 API -> {prediction_obj['error']}\" ) If not valid prediction: self._logger.info(f\"All predictions in ticket {ticket_id} were erroneous. Skipping ticket...\")","title":" remove erroneous predictions"},{"location":"logging/services/TNBA-monitor/actions/_remove_erroneous_predictions/#remove-erroneous-predictions","text":"for ticket in prediction tickets: for prediction in predictions: If error in prediction: self._logger.info( f\"Prediction for serial {serial_number} in ticket {ticket_id} was found but it contains an \" f\"error from T7 API -> {prediction_obj['error']}\" ) If not valid prediction: self._logger.info(f\"All predictions in ticket {ticket_id} were erroneous. Skipping ticket...\")","title":"Remove erroneous predictions"},{"location":"logging/services/TNBA-monitor/actions/_run_tickets_polling/","text":"Run tickets polling self._logger.info(\"Starting TNBA process...\") * get_cache_for_tnba_monitoring * If get cache status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping run ticket polling ...\") * get_edges_for_tnba_monitoring * If not edges statuses: self._logger.error(\"No edges statuses were received from VeloCloud. Aborting TNBA monitoring...\") self._logger.info(\"Keeping serials that exist in both the customer cache and the set of edges statuses...\") * get_links_metrics_for_autoresolve * If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") * get_events_by_serial_and_interface self._logger.info(\"Loading customer cache and edges statuses by serial into the monitor instance...\") self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got {len(open_tickets)} open tickets for all customers. \" f\"Filtering them (and their details) to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) self._logger.info(\"Getting T7 predictions for all relevant open tickets...\") * _get_predictions_by_ticket_id self._logger.info(\"Removing erroneous T7 predictions...\") * _remove_erroneous_predictions self._logger.info(\"Creating detail objects based on all tickets...\") self._logger.info(\"Discarding resolved ticket details...\") self._logger.info(\"Discarding ticket details of outage tickets whose last outage happened too recently...\") * _filter_outage_ticket_details_based_on_last_outage self._logger.info(\"Mapping all ticket details with their predictions...\") * _map_ticket_details_with_predictions self._logger.info( f\"{len(ticket_detail_objects)} ticket details were successfully mapped to predictions. \" \"Processing all details...\" ) * _process_ticket_detail self._logger.info(\"All ticket details were processed.\") * If not append notes TNBA: self._logger.info(\"No TNBA notes for append were built for any detail processed.\") * Else: self._logger.info(f\"{len(self._tnba_notes_to_append)} TNBA notes were built for append.\") * _append_tnba_notes self._logger.info(f\"TNBA process finished! Took {round((end_time - start_time) / 60, 2)} minutes.\")","title":" run tickets polling"},{"location":"logging/services/TNBA-monitor/actions/_run_tickets_polling/#run-tickets-polling","text":"self._logger.info(\"Starting TNBA process...\") * get_cache_for_tnba_monitoring * If get cache status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping run ticket polling ...\") * get_edges_for_tnba_monitoring * If not edges statuses: self._logger.error(\"No edges statuses were received from VeloCloud. Aborting TNBA monitoring...\") self._logger.info(\"Keeping serials that exist in both the customer cache and the set of edges statuses...\") * get_links_metrics_for_autoresolve * If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") * get_events_by_serial_and_interface self._logger.info(\"Loading customer cache and edges statuses by serial into the monitor instance...\") self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got {len(open_tickets)} open tickets for all customers. \" f\"Filtering them (and their details) to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) self._logger.info(\"Getting T7 predictions for all relevant open tickets...\") * _get_predictions_by_ticket_id self._logger.info(\"Removing erroneous T7 predictions...\") * _remove_erroneous_predictions self._logger.info(\"Creating detail objects based on all tickets...\") self._logger.info(\"Discarding resolved ticket details...\") self._logger.info(\"Discarding ticket details of outage tickets whose last outage happened too recently...\") * _filter_outage_ticket_details_based_on_last_outage self._logger.info(\"Mapping all ticket details with their predictions...\") * _map_ticket_details_with_predictions self._logger.info( f\"{len(ticket_detail_objects)} ticket details were successfully mapped to predictions. \" \"Processing all details...\" ) * _process_ticket_detail self._logger.info(\"All ticket details were processed.\") * If not append notes TNBA: self._logger.info(\"No TNBA notes for append were built for any detail processed.\") * Else: self._logger.info(f\"{len(self._tnba_notes_to_append)} TNBA notes were built for append.\") * _append_tnba_notes self._logger.info(f\"TNBA process finished! Took {round((end_time - start_time) / 60, 2)} minutes.\")","title":"Run tickets polling"},{"location":"logging/services/TNBA-monitor/actions/start_tnba_automated_process/","text":"Start tnba automated process (Start of service) self._logger.info(\"Scheduling TNBA automated process job...\") * If exec on start: self._logger.info(\"TNBA automated process job is going to be executed immediately\") * _run_tickets_polling * If ConflictingIdError: self._logger.info(f\"Skipping start of TNBA automated process job. Reason: {conflict}\")","title":"Start tnba automated process"},{"location":"logging/services/TNBA-monitor/actions/start_tnba_automated_process/#start-tnba-automated-process-start-of-service","text":"self._logger.info(\"Scheduling TNBA automated process job...\") * If exec on start: self._logger.info(\"TNBA automated process job is going to be executed immediately\") * _run_tickets_polling * If ConflictingIdError: self._logger.info(f\"Skipping start of TNBA automated process job. Reason: {conflict}\")","title":"Start tnba automated process (Start of service)"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/","text":"Append multiple notes to ticket self._logger.info(f\"Posting multiple notes for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") self._logger.info(f\"Posted multiple notes for ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Notes were {notes}. \" f\"Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/#append-multiple-notes-to-ticket","text":"self._logger.info(f\"Posting multiple notes for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") self._logger.info(f\"Posted multiple notes for ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Notes were {notes}. \" f\"Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_next_results_for_ticket_detail/","text":"Get next results for ticket detail self._logger.info( f\"Claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number}...\" ) * If Exception: self._logger.error(f\"An error occurred when claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} -> {e}\") self._logger.info( f\"Got next results for ticket {ticket_id}, detail {detail_id} and service number {service_number}!\" ) * If status not 200: self._logger.error(f\"Error while claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get next results for ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_next_results_for_ticket_detail/#get-next-results-for-ticket-detail","text":"self._logger.info( f\"Claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number}...\" ) * If Exception: self._logger.error(f\"An error occurred when claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} -> {e}\") self._logger.info( f\"Got next results for ticket {ticket_id}, detail {detail_id} and service number {service_number}!\" ) * If status not 200: self._logger.error(f\"Error while claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get next results for ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_affecting_tickets/","text":"Get open affecting tickets get_tickets","title":"Get open affecting tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_affecting_tickets/#get-open-affecting-tickets","text":"get_tickets","title":"Get open affecting tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_outage_tickets/","text":"Get open outage tickets get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_outage_tickets/#get-open-outage-tickets","text":"get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_task_history/","text":"Get ticket task history self._logger.info(f\"Getting task history of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting task history from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got task history of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving task history of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket task history"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_task_history/#get-ticket-task-history","text":"self._logger.info(f\"Getting task history of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting task history from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got task history of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving task history of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket task history"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_tickets/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_tickets/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/resolve_ticket_detail/","text":"Resolve ticket detail self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when resolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\")","title":"Resolve ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/resolve_ticket_detail/#resolve-ticket-detail","text":"self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when resolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\")","title":"Resolve ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache_for_tnba_monitoring/","text":"Get cache for tnba monitoring get_cache","title":"Get cache for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache_for_tnba_monitoring/#get-cache-for-tnba-monitoring","text":"get_cache","title":"Get cache for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/get_prediction/","text":"Get prediction self._logger.info(f\"Claiming T7 prediction for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when claiming T7 prediction for ticket {ticket_id}. Error: {e}\") self._logger.info(f\"Got T7 prediction for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error while claiming T7 prediction for ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Get prediction"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/get_prediction/#get-prediction","text":"self._logger.info(f\"Claiming T7 prediction for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when claiming T7 prediction for ticket {ticket_id}. Error: {e}\") self._logger.info(f\"Got T7 prediction for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error while claiming T7 prediction for ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Get prediction"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/post_live_automation_metrics/","text":"Post live automation metrics self._logger.info(f\"Posting live metric for ticket {ticket_id} to T7...\") * If Exception: self._logger.error(f\"An error occurred when posting live metrics for ticket {ticket_id} to T7. Error: {e}\") * If status is OK: self._logger.info(f\"Live metrics posted for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error when posting live metrics for ticket {ticket_id} to T7 in \" f\"{self._config.ENVIRONMENT_NAME.upper()} \" f\"environment. Error: Error {response_status} - {response_body}\")","title":"Post live automation metrics"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/post_live_automation_metrics/#post-live-automation-metrics","text":"self._logger.info(f\"Posting live metric for ticket {ticket_id} to T7...\") * If Exception: self._logger.error(f\"An error occurred when posting live metrics for ticket {ticket_id} to T7. Error: {e}\") * If status is OK: self._logger.info(f\"Live metrics posted for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error when posting live metrics for ticket {ticket_id} to T7 in \" f\"{self._config.ENVIRONMENT_NAME.upper()} \" f\"environment. Error: Error {response_status} - {response_body}\")","title":"Post live automation metrics"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_all_links_metrics/","text":"Get all links metrics For host in hosts: get_links_metrics_by_host If status is not Ok: self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_all_links_metrics/#get-all-links-metrics","text":"For host in hosts: get_links_metrics_by_host If status is not Ok: self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_edges_for_tnba_monitoring/","text":"Get edges for tnba monitoring For host in host: get_links_with_edge_info If status is Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\") group_links_by_serial","title":"Get edges for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_edges_for_tnba_monitoring/#get-edges-for-tnba-monitoring","text":"For host in host: get_links_with_edge_info If status is Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\") group_links_by_serial","title":"Get edges for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_enterprise_events/","text":"Get enterprise events self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) If Exception self._logger.error(f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\") If status is Ok: self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) Else: self._logger.error(f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\")","title":"Get enterprise events"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_enterprise_events/#get-enterprise-events","text":"self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) If Exception self._logger.error(f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\") If status is Ok: self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) Else: self._logger.error(f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\")","title":"Get enterprise events"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/","text":"Get events by serial and interface for host in hosts: for enterprise id in enterprises ids: get_enterprise_events self._logger.warning(f\" Bad status calling get enterprise events for host: {host} and enterprise\" f\"{enterprise_id}. Skipping enterprise events ...\") For events in enterprise events: If not match edge: self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/#get-events-by-serial-and-interface","text":"for host in hosts: for enterprise id in enterprises ids: get_enterprise_events self._logger.warning(f\" Bad status calling get enterprise events for host: {host} and enterprise\" f\"{enterprise_id}. Skipping enterprise events ...\") For events in enterprise events: If not match edge: self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_by_host/","text":"Get links metrics by host self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) If Exception f\"An error occurred when requesting links metrics from Velocloud -> {e}\" self._logger.info(f\"Got links metrics from Velocloud host {host}!\") Else: self._logger.error(f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links metrics by host"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_by_host/#get-links-metrics-by-host","text":"self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) If Exception f\"An error occurred when requesting links metrics from Velocloud -> {e}\" self._logger.info(f\"Got links metrics from Velocloud host {host}!\") Else: self._logger.error(f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links metrics by host"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/","text":"Get links metrics for autoresolve get_all_links_metrics If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") get_events_by_serial_and_interface","title":"Get links metrics for autoresolve"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/#get-links-metrics-for-autoresolve","text":"get_all_links_metrics If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") get_events_by_serial_and_interface","title":"Get links metrics for autoresolve"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_with_edge_info/","text":"Get Links with edge info Documentation self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links with edge info"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_with_edge_info/#get-links-with-edge-info-documentation","text":"self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get Links with edge info Documentation"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/group_links_by_serial/","text":"Group links by serial For link in links: If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If never activated: self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Group links by serial"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/group_links_by_serial/#group-links-by-serial","text":"For link in links: If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If never activated: self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Group links by serial"},{"location":"logging/services/bruin-bridge/actions/change_ticket_severity/","text":"Subject: bruin.change.ticket.severity Message arrives at subject If message doesn't have a body: self . _logger . error ( f \"Cannot change ticket severity level using { json . dumps ( msg ) } . JSON malformed\" ) END If message body doesn't have ticket_id , severity and reason filters: self . _logger . error ( f \"Cannot change ticket severity level using { json . dumps ( msg ) } . \" 'Need fields \"ticket_id\", \"severity\" and \"reason\".' ) END self . _logger . info ( f \"Changing ticket severity level using parameters { json . dumps ( payload ) } ...\" ) change_ticket_severity self . _logger . info ( f \"Publishing result of changing severity level of ticket { ticket_id } using payload { json . dumps ( payload ) } \" \"to the event bus...\" )","title":"Change ticket severity"},{"location":"logging/services/bruin-bridge/actions/change_ticket_severity/#subject-bruinchangeticketseverity","text":"Message arrives at subject If message doesn't have a body: self . _logger . error ( f \"Cannot change ticket severity level using { json . dumps ( msg ) } . JSON malformed\" ) END If message body doesn't have ticket_id , severity and reason filters: self . _logger . error ( f \"Cannot change ticket severity level using { json . dumps ( msg ) } . \" 'Need fields \"ticket_id\", \"severity\" and \"reason\".' ) END self . _logger . info ( f \"Changing ticket severity level using parameters { json . dumps ( payload ) } ...\" ) change_ticket_severity self . _logger . info ( f \"Publishing result of changing severity level of ticket { ticket_id } using payload { json . dumps ( payload ) } \" \"to the event bus...\" )","title":"Subject: bruin.change.ticket.severity"},{"location":"logging/services/bruin-bridge/actions/get_site/","text":"Subject: bruin.get.site Message arrives at subject If message doesn't have a body: self . _logger . error ( f \"Cannot get bruin site using { json . dumps ( msg ) } . JSON malformed\" ) END If message body doesn't have client_id filter: self . _logger . error ( f 'Cannot get bruin site using { json . dumps ( filters ) } . Need \"client_id\"' ) END If message body doesn't have site_id filter: self . _logger . error ( f 'Cannot get bruin site using { json . dumps ( filters ) } . Need \"site_id\"' ) END self . _logger . info ( f \"Getting Bruin site with filters: { json . dumps ( filters ) } \" ) get_site self . _logger . info ( f \"Bruin get_site published in event bus for request { json . dumps ( msg ) } . Message published was { response } \" )","title":"Get site"},{"location":"logging/services/bruin-bridge/actions/get_site/#subject-bruingetsite","text":"Message arrives at subject If message doesn't have a body: self . _logger . error ( f \"Cannot get bruin site using { json . dumps ( msg ) } . JSON malformed\" ) END If message body doesn't have client_id filter: self . _logger . error ( f 'Cannot get bruin site using { json . dumps ( filters ) } . Need \"client_id\"' ) END If message body doesn't have site_id filter: self . _logger . error ( f 'Cannot get bruin site using { json . dumps ( filters ) } . Need \"site_id\"' ) END self . _logger . info ( f \"Getting Bruin site with filters: { json . dumps ( filters ) } \" ) get_site self . _logger . info ( f \"Bruin get_site published in event bus for request { json . dumps ( msg ) } . Message published was { response } \" )","title":"Subject: bruin.get.site"},{"location":"logging/services/bruin-bridge/actions/get_tickets_basic_info/","text":"Subject: bruin.ticket.basic.request Message arrives at subject If message doesn't have a body: self . _logger . error ( f \"Cannot get tickets basic info using { json . dumps ( msg ) } . JSON malformed\" ) END If message body doesn't have ticket_statuses filter: self . _logger . error ( f \"Cannot get tickets basic info using { json . dumps ( msg ) } . Need a list of ticket statuses to keep going.\" ) END self . _logger . info ( f 'Fetching basic info of all tickets with statuses { \", \" . join ( ticket_statuses ) } and matching filters ' f \" { json . dumps ( bruin_payload ) } ...\" ) get_tickets_basic_info self . _logger . info ( f 'Publishing { len ( response_msg [ \"body\" ]) } tickets to the event bus...' )","title":"Get tickets basic info"},{"location":"logging/services/bruin-bridge/actions/get_tickets_basic_info/#subject-bruinticketbasicrequest","text":"Message arrives at subject If message doesn't have a body: self . _logger . error ( f \"Cannot get tickets basic info using { json . dumps ( msg ) } . JSON malformed\" ) END If message body doesn't have ticket_statuses filter: self . _logger . error ( f \"Cannot get tickets basic info using { json . dumps ( msg ) } . Need a list of ticket statuses to keep going.\" ) END self . _logger . info ( f 'Fetching basic info of all tickets with statuses { \", \" . join ( ticket_statuses ) } and matching filters ' f \" { json . dumps ( bruin_payload ) } ...\" ) get_tickets_basic_info self . _logger . info ( f 'Publishing { len ( response_msg [ \"body\" ]) } tickets to the event bus...' )","title":"Subject: bruin.ticket.basic.request"},{"location":"logging/services/bruin-bridge/app_entrypoint/app/","text":"App entrypoint self . _logger . info ( \"Bruin bridge starting...\" )","title":"App"},{"location":"logging/services/bruin-bridge/app_entrypoint/app/#app-entrypoint","text":"self . _logger . info ( \"Bruin bridge starting...\" )","title":"App entrypoint"},{"location":"logging/services/bruin-bridge/clients/bruin_client/change_ticket_severity/","text":"Change ticket severity self . _logger . info ( f \"Changing severity of ticket { ticket_id } using payload { payload } ...\" ) Call Bruin API endpoint PUT /api/Ticket/{ticket_id}/severity with the desired payload. If there's an error while connecting to Bruin API: self . _logger . error ( f \"A connection error happened while trying to connect to Bruin API -> { e } \" ) END If the status of the HTTP response is 200 : self . _logger . info ( f \"Severity of ticket { ticket_id } changed successfully! Payload used was { payload } \" ) END If the status of the HTTP response is 400 : self . _logger . error ( f \"Got HTTP 400 from Bruin -> { response_json } \" ) END If the status of the HTTP response is 401 : self . _logger . error ( f \"Got HTTP 401 from Bruin. Re-logging in...\" ) login END If the status of the HTTP response is 403 : self . _logger . error ( f \"Got HTTP 403 from Bruin\" ) END If the status of the HTTP response is 404 : self . _logger . error ( f \"Got HTTP 404 from Bruin\" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): self . _logger . error ( f \"Got HTTP { response . status } from Bruin\" ) END","title":"Change ticket severity"},{"location":"logging/services/bruin-bridge/clients/bruin_client/change_ticket_severity/#change-ticket-severity","text":"self . _logger . info ( f \"Changing severity of ticket { ticket_id } using payload { payload } ...\" ) Call Bruin API endpoint PUT /api/Ticket/{ticket_id}/severity with the desired payload. If there's an error while connecting to Bruin API: self . _logger . error ( f \"A connection error happened while trying to connect to Bruin API -> { e } \" ) END If the status of the HTTP response is 200 : self . _logger . info ( f \"Severity of ticket { ticket_id } changed successfully! Payload used was { payload } \" ) END If the status of the HTTP response is 400 : self . _logger . error ( f \"Got HTTP 400 from Bruin -> { response_json } \" ) END If the status of the HTTP response is 401 : self . _logger . error ( f \"Got HTTP 401 from Bruin. Re-logging in...\" ) login END If the status of the HTTP response is 403 : self . _logger . error ( f \"Got HTTP 403 from Bruin\" ) END If the status of the HTTP response is 404 : self . _logger . error ( f \"Got HTTP 404 from Bruin\" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): self . _logger . error ( f \"Got HTTP { response . status } from Bruin\" ) END","title":"Change ticket severity"},{"location":"logging/services/bruin-bridge/clients/bruin_client/get_site/","text":"Get site self . _logger . info ( f \"Getting Bruin Site for params: { params } \" ) Call Bruin API endpoint GET /api/Site with the set of desired query parameters. If there's an error while connecting to Bruin API: self . _logger . error ( f \"A connection error happened while trying to connect to Bruin API. { e } \" ) END If the status of the HTTP response is 200 : self . _logger . info ( f \"Got HTTP 200 from GET /api/Site for params { json . dumps ( params ) } \" ) END If the status of the HTTP response is 400 : self . _logger . error ( f \"Got error from Bruin { response_json } \" ) END If the status of the HTTP response is 401 : self . _logger . error ( f \"Got 401 from Bruin. Re-logging in...\" ) login END If the status of the HTTP response is 403 : self . _logger . error ( f \"Got HTTP 403 from Bruin\" ) END If the status of the HTTP response is 404 : self . _logger . error ( f \"Got HTTP 404 from Bruin\" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): self . _logger . error ( f \"Got HTTP { response . status } from Bruin\" ) END","title":"Get site"},{"location":"logging/services/bruin-bridge/clients/bruin_client/get_site/#get-site","text":"self . _logger . info ( f \"Getting Bruin Site for params: { params } \" ) Call Bruin API endpoint GET /api/Site with the set of desired query parameters. If there's an error while connecting to Bruin API: self . _logger . error ( f \"A connection error happened while trying to connect to Bruin API. { e } \" ) END If the status of the HTTP response is 200 : self . _logger . info ( f \"Got HTTP 200 from GET /api/Site for params { json . dumps ( params ) } \" ) END If the status of the HTTP response is 400 : self . _logger . error ( f \"Got error from Bruin { response_json } \" ) END If the status of the HTTP response is 401 : self . _logger . error ( f \"Got 401 from Bruin. Re-logging in...\" ) login END If the status of the HTTP response is 403 : self . _logger . error ( f \"Got HTTP 403 from Bruin\" ) END If the status of the HTTP response is 404 : self . _logger . error ( f \"Got HTTP 404 from Bruin\" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): self . _logger . error ( f \"Got HTTP { response . status } from Bruin\" ) END","title":"Get site"},{"location":"logging/services/bruin-bridge/clients/bruin_client/get_tickets_basic_info/","text":"Get tickets basic info self . _logger . info ( f \"Getting tickets basic info using params { json . dumps ( request_params ) } ...\" ) Call Bruin API endpoint GET /api/Ticket/basic with the set of desired query parameters. If there's an error while connecting to Bruin API: self . _logger . error ( f \"A connection error happened while trying to connect to Bruin API: { e } \" ) END If the status of the HTTP response is 200 : self . _logger . info ( f \"Got HTTP 200 from GET /api/Ticket/basic for params { json . dumps ( request_params ) } \" ) END If the status of the HTTP response is 400 : self . _logger . error ( f \"Got error from Bruin { response_json } \" ) END If the status of the HTTP response is 401 : self . _logger . error ( f \"Got 401 from Bruin. Re-logging in...\" ) login END If the status of the HTTP response is 403 : self . _logger . error ( f \"Forbidden error from Bruin { response_json } \" ) END If the status of the HTTP response is 404 : self . _logger . error ( f \"Got 404 from Bruin, resource not found for params { request_params } \" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): self . _logger . error ( f \"Got { response . status } .\" ) END","title":"Get tickets basic info"},{"location":"logging/services/bruin-bridge/clients/bruin_client/get_tickets_basic_info/#get-tickets-basic-info","text":"self . _logger . info ( f \"Getting tickets basic info using params { json . dumps ( request_params ) } ...\" ) Call Bruin API endpoint GET /api/Ticket/basic with the set of desired query parameters. If there's an error while connecting to Bruin API: self . _logger . error ( f \"A connection error happened while trying to connect to Bruin API: { e } \" ) END If the status of the HTTP response is 200 : self . _logger . info ( f \"Got HTTP 200 from GET /api/Ticket/basic for params { json . dumps ( request_params ) } \" ) END If the status of the HTTP response is 400 : self . _logger . error ( f \"Got error from Bruin { response_json } \" ) END If the status of the HTTP response is 401 : self . _logger . error ( f \"Got 401 from Bruin. Re-logging in...\" ) login END If the status of the HTTP response is 403 : self . _logger . error ( f \"Forbidden error from Bruin { response_json } \" ) END If the status of the HTTP response is 404 : self . _logger . error ( f \"Got 404 from Bruin, resource not found for params { request_params } \" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): self . _logger . error ( f \"Got { response . status } .\" ) END","title":"Get tickets basic info"},{"location":"logging/services/bruin-bridge/clients/bruin_client/login/","text":"Login self . _logger . info ( \"Logging into Bruin...\" ) Call Bruin Identity Server endpoint POST /identity/connect/token using authentication credentials. If no errors arise while calling the endpoint: self . _logger . info ( \"Logged into Bruin!\" ) Otherwise: self . _logger . error ( f \"An error occurred while trying to login to Bruin: { err } \" )","title":"Login"},{"location":"logging/services/bruin-bridge/clients/bruin_client/login/#login","text":"self . _logger . info ( \"Logging into Bruin...\" ) Call Bruin Identity Server endpoint POST /identity/connect/token using authentication credentials. If no errors arise while calling the endpoint: self . _logger . info ( \"Logged into Bruin!\" ) Otherwise: self . _logger . error ( f \"An error occurred while trying to login to Bruin: { err } \" )","title":"Login"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/__make_paginated_request/","text":"Make paginated request to Bruin API self . _logger . info ( f \"Fetching all pages using { fn . __name__ } ...\" ) While there are pages to fetch: Make a call for the current page to Bruin API through the BruinClient If response status is not ok: self . _logger . warning ( f \"Call to { fn . __name__ } failed for page { current_page } . Checking if max retries threshold has been \" \"reached\" ) If the max attempts threshold hasn't been reached: self . _logger . info ( f \"Max retries threshold hasn't been reached yet. Retrying call to { fn . __name__ } for page \" f \" { current_page } ...\" ) Otherwise: self . _logger . error ( f \"There have been { max_retries } or more errors when calling { fn . __name__ } .\" ) END If there are no more pages to fetch: self . _logger . info ( f \"Finished fetching all pages for { fn . __name__ } .\" ) END","title":"  make paginated request"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/__make_paginated_request/#make-paginated-request-to-bruin-api","text":"self . _logger . info ( f \"Fetching all pages using { fn . __name__ } ...\" ) While there are pages to fetch: Make a call for the current page to Bruin API through the BruinClient If response status is not ok: self . _logger . warning ( f \"Call to { fn . __name__ } failed for page { current_page } . Checking if max retries threshold has been \" \"reached\" ) If the max attempts threshold hasn't been reached: self . _logger . info ( f \"Max retries threshold hasn't been reached yet. Retrying call to { fn . __name__ } for page \" f \" { current_page } ...\" ) Otherwise: self . _logger . error ( f \"There have been { max_retries } or more errors when calling { fn . __name__ } .\" ) END If there are no more pages to fetch: self . _logger . info ( f \"Finished fetching all pages for { fn . __name__ } .\" ) END","title":"Make paginated request to Bruin API"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/change_ticket_severity/","text":"Change ticket severity BruinClient::change_ticket_severity","title":"Change ticket severity"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/change_ticket_severity/#change-ticket-severity","text":"BruinClient::change_ticket_severity","title":"Change ticket severity"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/get_site/","text":"Get site BruinClient::get_site If response is not ok: self . _logger . error ( f \"Got response with status { response [ 'status' ] } while getting site information for params { params } .\" ) END If site information is missing in response: msg = f \"No site information was found for site { params [ 'site_id' ] } and client { params [ 'client_id' ] } \" self . _logger . warning ( msg ) END","title":"Get site"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/get_site/#get-site","text":"BruinClient::get_site If response is not ok: self . _logger . error ( f \"Got response with status { response [ 'status' ] } while getting site information for params { params } .\" ) END If site information is missing in response: msg = f \"No site information was found for site { params [ 'site_id' ] } and client { params [ 'client_id' ] } \" self . _logger . warning ( msg ) END","title":"Get site"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/get_tickets_basic_info/","text":"Get tickets basic info Call __make_paginated_request with BruinClient::get_tickets_basic_info to fetch all available pages for the desired set of filters.","title":"Get tickets basic info"},{"location":"logging/services/bruin-bridge/repositories/bruin_repository/get_tickets_basic_info/#get-tickets-basic-info","text":"Call __make_paginated_request with BruinClient::get_tickets_basic_info to fetch all available pages for the desired set of filters.","title":"Get tickets basic info"},{"location":"logging/services/fraud-monitor/actions/_append_note_to_ticket/","text":"Append note to ticket self._logger.info(f\"Appending Fraud note to ticket {ticket_id}\") * If note existe: self._logger.info( f\"No Fraud trouble note will be appended to ticket {ticket_id}. \" f\"A note for this email was already appended to the ticket after the latest re-open or ticket creation.\" ) * If not PRODUCTION: self._logger.info( f\"No Fraud note will be appended to ticket {ticket_id} since the current environment is not production\" ) * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id}. \" f\"Skipping append note ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":" append note to ticket"},{"location":"logging/services/fraud-monitor/actions/_append_note_to_ticket/#append-note-to-ticket","text":"self._logger.info(f\"Appending Fraud note to ticket {ticket_id}\") * If note existe: self._logger.info( f\"No Fraud trouble note will be appended to ticket {ticket_id}. \" f\"A note for this email was already appended to the ticket after the latest re-open or ticket creation.\" ) * If not PRODUCTION: self._logger.info( f\"No Fraud note will be appended to ticket {ticket_id} since the current environment is not production\" ) * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id}. \" f\"Skipping append note ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":"Append note to ticket"},{"location":"logging/services/fraud-monitor/actions/_create_fraud_ticket/","text":"Create fraud ticket self._logger.info(f\"Creating Fraud ticket for client {client_id} and service number {service_number}\") * If not contacts: self._logger.warning(f\"Not found contacts to create the fraud ticket\") * If environment is not PRODUCTION: self._logger.info(f\"No Fraud ticket will be created since the current environment is not production\") * create_fraud_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to create fraud ticket with client id: {client_id} and\" f\"service number: {service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud ticket was successfully created! Ticket ID is {ticket_id}\") * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\") self._logger.info(f\"Forwarding ticket {ticket_id} to HNOC\") * change_detail_work_queue_to_hnoc","title":" create fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_create_fraud_ticket/#create-fraud-ticket","text":"self._logger.info(f\"Creating Fraud ticket for client {client_id} and service number {service_number}\") * If not contacts: self._logger.warning(f\"Not found contacts to create the fraud ticket\") * If environment is not PRODUCTION: self._logger.info(f\"No Fraud ticket will be created since the current environment is not production\") * create_fraud_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to create fraud ticket with client id: {client_id} and\" f\"service number: {service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud ticket was successfully created! Ticket ID is {ticket_id}\") * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\") self._logger.info(f\"Forwarding ticket {ticket_id} to HNOC\") * change_detail_work_queue_to_hnoc","title":"Create fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_fraud_monitoring_process/","text":"Fraud monitoring process self._logger.info(f'Processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}') * get_unread_emails * If status is no Ok: self._logger.warning(f\"Bad status calling to get unread emails. Skipping fraud monitor process\") * for email in emails: * If message is none or uid -1: self._logger.error(f\"Invalid message: {email}\") * If not email regex: self._logger.info(f\"Email with msg_uid {msg_uid} is not a fraud warning. Skipping...\") * If not found body: self._logger.error(f\"Email with msg_uid {msg_uid} has an unexpected body\") self._logger.info(f\"Processing email with msg_uid {msg_uid}\") * _process_fraud * If processed and PRODUCTION: * If mark email as read status is Ok: self._logger.error(f\"Could not mark email with msg_uid {msg_uid} as read\") * If processed: self._logger.info(f\"Processed email with msg_uid {msg_uid}\") * Else: self._logger.info(f\"Failed to process email with msg_uid {msg_uid}\") self._logger.info( f'Finished processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}. ' f\"Elapsed time: {round((stop - start) / 60, 2)} minutes\" )","title":" fraud monitoring process"},{"location":"logging/services/fraud-monitor/actions/_fraud_monitoring_process/#fraud-monitoring-process","text":"self._logger.info(f'Processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}') * get_unread_emails * If status is no Ok: self._logger.warning(f\"Bad status calling to get unread emails. Skipping fraud monitor process\") * for email in emails: * If message is none or uid -1: self._logger.error(f\"Invalid message: {email}\") * If not email regex: self._logger.info(f\"Email with msg_uid {msg_uid} is not a fraud warning. Skipping...\") * If not found body: self._logger.error(f\"Email with msg_uid {msg_uid} has an unexpected body\") self._logger.info(f\"Processing email with msg_uid {msg_uid}\") * _process_fraud * If processed and PRODUCTION: * If mark email as read status is Ok: self._logger.error(f\"Could not mark email with msg_uid {msg_uid} as read\") * If processed: self._logger.info(f\"Processed email with msg_uid {msg_uid}\") * Else: self._logger.info(f\"Failed to process email with msg_uid {msg_uid}\") self._logger.info( f'Finished processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}. ' f\"Elapsed time: {round((stop - start) / 60, 2)} minutes\" )","title":"Fraud monitoring process"},{"location":"logging/services/fraud-monitor/actions/_get_oldest_fraud_ticket/","text":"Get oldest fraud ticket For ticket in tickets: get_ticket_details If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket details for ticket id: {ticket_id}.\" f\"Skipping get oldest fraud ticket ...\")","title":" get oldest fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_get_oldest_fraud_ticket/#get-oldest-fraud-ticket","text":"For ticket in tickets: get_ticket_details If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket details for ticket id: {ticket_id}.\" f\"Skipping get oldest fraud ticket ...\")","title":"Get oldest fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_process_fraud/","text":"Process fraud get_client_info_by_did If status not Ok: self._logger.warning(f\"Failed to get client info by DID {did}, using default client info\") get_open_fraud_tickets self._logger.warning(f\"fBad status calling to get open fraud tickets for client id: {client_id} and \" f\"service number: {service_number}. Process fraud FALSE ...\") _get_oldest_fraud_ticket If open fraud ticket: self._logger.info(f\"An open Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") If is a task resolved: self._logger.info( f\"Fraud ticket with ID {ticket_id} is open, but the task related to {service_number} is resolved. \" f\"Therefore, the ticket will be considered as Resolved.\" ) Else: _append_note_to_ticket Else: self._logger.info(f\"No open Fraud ticket was found for {service_number}\") get_resolved_fraud_tickets If status is not Ok: self._logger.warning(f\"bad status calling to get resolved fraud tickets for client id: {client_id} \" f\"and service number: {service_number}. Skipping process fraud ...\") _get_oldest_fraud_ticket If resolved fraud ticket: self._logger.info(f\"A resolved Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") _unresolve_task_for_ticket self._logger.info(f\"No open or resolved Fraud ticket was found for {service_number}\") _create_fraud_ticket","title":" process fraud"},{"location":"logging/services/fraud-monitor/actions/_process_fraud/#process-fraud","text":"get_client_info_by_did If status not Ok: self._logger.warning(f\"Failed to get client info by DID {did}, using default client info\") get_open_fraud_tickets self._logger.warning(f\"fBad status calling to get open fraud tickets for client id: {client_id} and \" f\"service number: {service_number}. Process fraud FALSE ...\") _get_oldest_fraud_ticket If open fraud ticket: self._logger.info(f\"An open Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") If is a task resolved: self._logger.info( f\"Fraud ticket with ID {ticket_id} is open, but the task related to {service_number} is resolved. \" f\"Therefore, the ticket will be considered as Resolved.\" ) Else: _append_note_to_ticket Else: self._logger.info(f\"No open Fraud ticket was found for {service_number}\") get_resolved_fraud_tickets If status is not Ok: self._logger.warning(f\"bad status calling to get resolved fraud tickets for client id: {client_id} \" f\"and service number: {service_number}. Skipping process fraud ...\") _get_oldest_fraud_ticket If resolved fraud ticket: self._logger.info(f\"A resolved Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") _unresolve_task_for_ticket self._logger.info(f\"No open or resolved Fraud ticket was found for {service_number}\") _create_fraud_ticket","title":"Process fraud"},{"location":"logging/services/fraud-monitor/actions/_unresolve_task_for_ticket/","text":"Unresolve task for ticket self._logger.info(f\"Unresolving task related to {service_number} of Fraud ticket {ticket_id}...\") * If environment not PRODUCTION: self._logger.info( f\"Task related to {service_number} of Fraud ticket {ticket_id} will not be unresolved \" f\"since the current environment is not production\" ) * open_ticket self._logger.warning(f\"Bad status calling to open ticket with ticket id: {ticket_id}. \" f\"Unresolve task for ticket return FALSE\") self._logger.info(f\"Task related to {service_number} of Fraud ticket {ticket_id} was successfully unresolved!\") * append_note_to_ticket self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Unresolve task for ticket return FALSE\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":" unresolve task for ticket"},{"location":"logging/services/fraud-monitor/actions/_unresolve_task_for_ticket/#unresolve-task-for-ticket","text":"self._logger.info(f\"Unresolving task related to {service_number} of Fraud ticket {ticket_id}...\") * If environment not PRODUCTION: self._logger.info( f\"Task related to {service_number} of Fraud ticket {ticket_id} will not be unresolved \" f\"since the current environment is not production\" ) * open_ticket self._logger.warning(f\"Bad status calling to open ticket with ticket id: {ticket_id}. \" f\"Unresolve task for ticket return FALSE\") self._logger.info(f\"Task related to {service_number} of Fraud ticket {ticket_id} was successfully unresolved!\") * append_note_to_ticket self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Unresolve task for ticket return FALSE\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":"Unresolve task for ticket"},{"location":"logging/services/fraud-monitor/actions/start_fraud_monitoring/","text":"Start fraud monitoring self._logger.info(\"Scheduling Fraud Monitor job...\") * If exec on start: self._logger.info(\"Fraud Monitor job is going to be executed immediately\") * _fraud_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Fraud Monitoring job. Reason: {conflict}\")","title":"Start fraud monitoring"},{"location":"logging/services/fraud-monitor/actions/start_fraud_monitoring/#start-fraud-monitoring","text":"self._logger.info(\"Scheduling Fraud Monitor job...\") * If exec on start: self._logger.info(\"Fraud Monitor job is going to be executed immediately\") * _fraud_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Fraud Monitoring job. Reason: {conflict}\")","title":"Start fraud monitoring"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue/","text":"Change detail work queue self._logger.info( f\"Changing task result of serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) * If Exception: self._logger.error(f\"An error occurred when changing task result of serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\") * If status ok: self._logger.info(f\"Task result of detail serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\") * Else: self._logger.error(f\"Error while changing task result of serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue/#change-detail-work-queue","text":"self._logger.info( f\"Changing task result of serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) * If Exception: self._logger.error(f\"An error occurred when changing task result of serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\") * If status ok: self._logger.info(f\"Task result of detail serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\") * Else: self._logger.error(f\"Error while changing task result of serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/","text":"Change detail work queue to hnoc change_detail_work_queue","title":"Change detail work queue to hnoc"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/#change-detail-work-queue-to-hnoc","text":"change_detail_work_queue","title":"Change detail work queue to hnoc"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/create_fraud_ticket/","text":"Create fraud ticket self._logger.info( f\"Creating fraud ticket for service number {service_number} that belongs to client {client_id}...\" ) * If Exception: f\"An error occurred when creating fraud ticket for service number {service_number} \" f\"that belongs to client {client_id} -> {e}\" * If status ok: self._logger.info(f\"Fraud ticket for service number {service_number} that belongs to client {client_id} created!\") * Else: self._logger.error(f\"Error while creating fraud ticket for service number {service_number} that belongs to client \" f\"{client_id} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Create fraud ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/create_fraud_ticket/#create-fraud-ticket","text":"self._logger.info( f\"Creating fraud ticket for service number {service_number} that belongs to client {client_id}...\" ) * If Exception: f\"An error occurred when creating fraud ticket for service number {service_number} \" f\"that belongs to client {client_id} -> {e}\" * If status ok: self._logger.info(f\"Fraud ticket for service number {service_number} that belongs to client {client_id} created!\") * Else: self._logger.error(f\"Error while creating fraud ticket for service number {service_number} that belongs to client \" f\"{client_id} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Create fraud ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_client_info_by_did/","text":"Get client info by did self._logger.info(f\"Getting client info by DID {did}...\") * If Exception: self._logger.error(f\"An error occurred when getting client info by DID {did} -> {e}\") * If status ok: self._logger.info(f\"Got client info by DID {did}!\") * Else: self._logger.error(f\"Error while getting client info by DID {did} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get client info by did"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_client_info_by_did/#get-client-info-by-did","text":"self._logger.info(f\"Getting client info by DID {did}...\") * If Exception: self._logger.error(f\"An error occurred when getting client info by DID {did} -> {e}\") * If status ok: self._logger.info(f\"Got client info by DID {did}!\") * Else: self._logger.error(f\"Error while getting client info by DID {did} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get client info by did"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_fraud_tickets/","text":"Get fraud tickets get_tickets","title":"Get fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_fraud_tickets/#get-fraud-tickets","text":"get_tickets","title":"Get fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_open_fraud_tickets/","text":"Get open fraud tickets get_fraud_tickets","title":"Get open fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_open_fraud_tickets/#get-open-fraud-tickets","text":"get_fraud_tickets","title":"Get open fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_resolved_fraud_tickets/","text":"Get resolved fraud tickets get_fraud_tickets","title":"Get resolved fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_resolved_fraud_tickets/#get-resolved-fraud-tickets","text":"get_fraud_tickets","title":"Get resolved fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_tickets/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_tickets/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/open_ticket/#open-ticket","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/fraud-monitor/repositories/notifications_repository/get_unread_emails/","text":"Get unread emails self._logger.info( f\"Getting the unread emails from the inbox of {email_account} sent from the users: \" f\"{email_filter}\" ) * If Exception: self._logger.error(f\"An error occurred while getting the unread emails from the inbox of {email_account} -> {e}\") * If status ok: self._logger.info(f\"Got the unread emails from the inbox of {email_account}\") * Else: self._logger.error(f\"Error getting the unread emails from the inbox of {email_account} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get unread emails"},{"location":"logging/services/fraud-monitor/repositories/notifications_repository/get_unread_emails/#get-unread-emails","text":"self._logger.info( f\"Getting the unread emails from the inbox of {email_account} sent from the users: \" f\"{email_filter}\" ) * If Exception: self._logger.error(f\"An error occurred while getting the unread emails from the inbox of {email_account} -> {e}\") * If status ok: self._logger.info(f\"Got the unread emails from the inbox of {email_account}\") * Else: self._logger.error(f\"Error getting the unread emails from the inbox of {email_account} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get unread emails"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_add_device_to_tickets_mapping/","text":"Add device to tickets mapping get_open_affecting_tickets If status is not Ok: self._logger.warning(f\"Bad status calling to get open affecting ticket to serial number \" f\"{serial_number}. Skipping add device to ticket mapping ...\") If not affecting ticket: self._logger.info( f\"No affecting tickets were found for device {serial_number} when building the mapping between \" f\"this serial and tickets.\" ) get_ticket_details self._logger.warning(f\"Bad status calling to get ticket details to ticket id: {ticket_id}.\" f\"Skipping add devices to ticket mapping ...\")","title":" add device to tickets mapping"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_add_device_to_tickets_mapping/#add-device-to-tickets-mapping","text":"get_open_affecting_tickets If status is not Ok: self._logger.warning(f\"Bad status calling to get open affecting ticket to serial number \" f\"{serial_number}. Skipping add device to ticket mapping ...\") If not affecting ticket: self._logger.info( f\"No affecting tickets were found for device {serial_number} when building the mapping between \" f\"this serial and tickets.\" ) get_ticket_details self._logger.warning(f\"Bad status calling to get ticket details to ticket id: {ticket_id}.\" f\"Skipping add devices to ticket mapping ...\")","title":"Add device to tickets mapping"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_affecting_monitoring_process/","text":"Affecting monitoring process self._logger.info(f\"Starting Hawkeye Affecting Monitor!\") * get_cache_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeye affecting monitor process ...\") * get_tests_results_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad request get test results for affectin monitor for probe uids: {probe_uids}.\" f\"Skipping hawkeye affecting monitor ...\") self._logger.info( f\"Looking for Service Affecting tickets for {len(cached_devices_mapped_to_tests_results)} devices...\" ) * _add_device_to_tickets_mapping self._logger.info(f\"Processing {len(cached_devices_mapped_to_tests_results)} devices...\") * _process_device self._logger.info(f\"Hawkeye Affecting Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":" affecting monitoring process"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_affecting_monitoring_process/#affecting-monitoring-process","text":"self._logger.info(f\"Starting Hawkeye Affecting Monitor!\") * get_cache_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeye affecting monitor process ...\") * get_tests_results_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad request get test results for affectin monitor for probe uids: {probe_uids}.\" f\"Skipping hawkeye affecting monitor ...\") self._logger.info( f\"Looking for Service Affecting tickets for {len(cached_devices_mapped_to_tests_results)} devices...\" ) * _add_device_to_tickets_mapping self._logger.info(f\"Processing {len(cached_devices_mapped_to_tests_results)} devices...\") * _process_device self._logger.info(f\"Hawkeye Affecting Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":"Affecting monitoring process"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_append_new_notes_for_device/","text":"Append new notes for device If serial not in ticket: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so no notes can be posted to any ticket. Skipping...\" ) If not notes to append: self._logger.info(f\"No notes to append for serial {serial_number} were found. Skipping...\") If environment is PRODUCTION: self._logger.info( f\"{len(notes_to_append)} affecting notes to append to ticket {ticket_id} were found, but the current \" \"environment is not PRODUCTION. Skipping...\" ) self._logger.info( f\"Posting {len(notes_to_append)} affecting notes to ticket {ticket_id} (serial: {serial_number})...\" )","title":" append new notes for device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_append_new_notes_for_device/#append-new-notes-for-device","text":"If serial not in ticket: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so no notes can be posted to any ticket. Skipping...\" ) If not notes to append: self._logger.info(f\"No notes to append for serial {serial_number} were found. Skipping...\") If environment is PRODUCTION: self._logger.info( f\"{len(notes_to_append)} affecting notes to append to ticket {ticket_id} were found, but the current \" \"environment is not PRODUCTION. Skipping...\" ) self._logger.info( f\"Posting {len(notes_to_append)} affecting notes to ticket {ticket_id} (serial: {serial_number})...\" )","title":"Append new notes for device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_device/","text":"Process device self._logger.info(f\"Processing device {serial_number}...\") * for test result in test results: * If test result passed: * _process_passed_test_result * If test result failed: * _process_failed_test_result * Else: self._logger.info( f'Test result {test_result[\"summary\"][\"id\"]} has state {test_result[\"summary\"][\"status\"].upper()}. ' \"Skipping...\" ) * _append_new_notes_for_device self._logger.info(f\"Finished processing device {serial_number}!\")","title":" process device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_device/#process-device","text":"self._logger.info(f\"Processing device {serial_number}...\") * for test result in test results: * If test result passed: * _process_passed_test_result * If test result failed: * _process_failed_test_result * Else: self._logger.info( f'Test result {test_result[\"summary\"][\"id\"]} has state {test_result[\"summary\"][\"status\"].upper()}. ' \"Skipping...\" ) * _append_new_notes_for_device self._logger.info(f\"Finished processing device {serial_number}!\")","title":"Process device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_failed_test_result/","text":"Process failed test result self._logger.info( f\"Processing FAILED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current FAILED state for test type {test_type} will be ignored. Skipping...\" ) * If not affecting ticket: * If working environment is not PRODUCTION: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}, but the current environment is not PRODUCTION. Skipping ticket creation...\" ) self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}. Creating affecting ticket..\" ) * create_affecting_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling create affecting ticket to serial: {serial_number}.\" f\"Skipping process test failed ...\") self._logger.info( f\"Affecting ticket created for serial {serial_number} (ID: {ticket_id}). A new note reporting the \" f\"current FAILED state for test type {test_type} will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Serial {serial_number} is under affecting ticket {ticket_id} and some troubles were spotted for \" f\"test type {test_type}.\" ) * If detail resolved ticket: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} is \" f\"currently unresolved and a FAILED state was spotted. Unresolving detail...\" ) * unresolve_ticket_detail * If status is not Ok: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"could not be unresolved. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * Else: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"was unresolved successfully. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"A new note reporting the current FAILED state for this test type will be built and appended \" \"to the ticket later on.\" ) * Else: * If passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. A new note reporting the current FAILED state for this test \" \"type will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a previous FAILED state. No new notes will be built to report the current \" \"FAILED state.\" ) self._logger.info(f\"Finished processing FAILED test result {test_result_id}!\")","title":" process failed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_failed_test_result/#process-failed-test-result","text":"self._logger.info( f\"Processing FAILED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current FAILED state for test type {test_type} will be ignored. Skipping...\" ) * If not affecting ticket: * If working environment is not PRODUCTION: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}, but the current environment is not PRODUCTION. Skipping ticket creation...\" ) self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}. Creating affecting ticket..\" ) * create_affecting_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling create affecting ticket to serial: {serial_number}.\" f\"Skipping process test failed ...\") self._logger.info( f\"Affecting ticket created for serial {serial_number} (ID: {ticket_id}). A new note reporting the \" f\"current FAILED state for test type {test_type} will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Serial {serial_number} is under affecting ticket {ticket_id} and some troubles were spotted for \" f\"test type {test_type}.\" ) * If detail resolved ticket: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} is \" f\"currently unresolved and a FAILED state was spotted. Unresolving detail...\" ) * unresolve_ticket_detail * If status is not Ok: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"could not be unresolved. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * Else: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"was unresolved successfully. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"A new note reporting the current FAILED state for this test type will be built and appended \" \"to the ticket later on.\" ) * Else: * If passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. A new note reporting the current FAILED state for this test \" \"type will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a previous FAILED state. No new notes will be built to report the current \" \"FAILED state.\" ) self._logger.info(f\"Finished processing FAILED test result {test_result_id}!\")","title":"Process failed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_passed_test_result/","text":"Process passed test result self._logger.info( f\"Processing PASSED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current PASSED state for test type {test_type} will be ignored. Skipping...\" ) * If affecting tickets: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and all thresholds are normal for \" f\"test type {test_type}. Skipping...\" ) * If affecting ticket detail is solved: self._logger.info( f\"Serial {serial_number} is under an affecting ticket (ID {ticket_id}) whose ticket detail is resolved \" f\"and all thresholds are normal for test type {test_type}, so the current PASSED state will not be \" \"reported. Skipping...\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"Skipping...\" ) * If is passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. Skipping...\" ) self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a FAILED state. A new note reporting the current PASSED state will be built and appended \" \"to the ticket later on.\" ) self._logger.info(f\"Finished processing PASSED test result {test_result_id}!\")","title":" process passed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_passed_test_result/#process-passed-test-result","text":"self._logger.info( f\"Processing PASSED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current PASSED state for test type {test_type} will be ignored. Skipping...\" ) * If affecting tickets: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and all thresholds are normal for \" f\"test type {test_type}. Skipping...\" ) * If affecting ticket detail is solved: self._logger.info( f\"Serial {serial_number} is under an affecting ticket (ID {ticket_id}) whose ticket detail is resolved \" f\"and all thresholds are normal for test type {test_type}, so the current PASSED state will not be \" \"reported. Skipping...\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"Skipping...\" ) * If is passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. Skipping...\" ) self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a FAILED state. A new note reporting the current PASSED state will be built and appended \" \"to the ticket later on.\" ) self._logger.info(f\"Finished processing PASSED test result {test_result_id}!\")","title":"Process passed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/start_hawkeye_affecting_monitoring/","text":"Start hawkeye affecting monitoring (Start service) self._logger.info(\"Scheduling Hawkeye Affecting Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Affecting Monitor job is going to be executed immediately\") * _affecting_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Affecting Monitoring job. Reason: {conflict}\")","title":"Start hawkeye affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/actions/start_hawkeye_affecting_monitoring/#start-hawkeye-affecting-monitoring-start-service","text":"self._logger.info(\"Scheduling Hawkeye Affecting Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Affecting Monitor job is going to be executed immediately\") * _affecting_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Affecting Monitoring job. Reason: {conflict}\")","title":"Start hawkeye affecting monitoring (Start service)"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/","text":"Append multiple notes to ticket self._logger.info(f\"Posting multiple notes to ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") * If status ok: self._logger.info(f\"Posted multiple notes to ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Notes were {notes}. Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/#append-multiple-notes-to-ticket","text":"self._logger.info(f\"Posting multiple notes to ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") * If status ok: self._logger.info(f\"Posted multiple notes to ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Notes were {notes}. Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/","text":"Create Affecting ticket Documentation self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create affecting ticket"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/#create-affecting-ticket-documentation","text":"self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create Affecting ticket Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/","text":"Get affecting tickets Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/#get-affecting-tickets","text":"Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/","text":"Get open affecting tickets Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/#get-open-affecting-tickets","text":"Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get Ticket details Documentation self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get ticket details"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details-documentation","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get Ticket details Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_tickets/","text":"Get tickets Documentation if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_tickets/#get-tickets-documentation","text":"if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/unresolve_ticket_detail/","text":"Unesolve ticket detail self._logger.info(f\"Unresolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unresolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} unresolved successfully!\") * Else: self._logger.error(f\"Error while unresolving detail {detail_id} of affecting ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Unresolve ticket detail"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/unresolve_ticket_detail/#unesolve-ticket-detail","text":"self._logger.info(f\"Unresolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unresolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} unresolved successfully!\") * Else: self._logger.error(f\"Error while unresolving detail {detail_id} of affecting ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Unesolve ticket detail"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/","text":"Get cache for affecting monitoring get_cache","title":"Get cache for affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/#get-cache-for-affecting-monitoring","text":"get_cache","title":"Get cache for affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results/","text":"Get tests results Get probes self._logger.info(f\"Getting tests results for {len(probe_uids)} probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting tests results from Hawkeye -> {e}\") * If status ok: self._logger.info(f\"Error while retrieving tests results: Error {response_status} - {response_body}\") * Else: self._logger.error(f\"Got all tests results for {len(probe_uids)} probes from Hawkeye!\")","title":"Get tests results"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results/#get-tests-results","text":"","title":"Get tests results"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results/#get-probes","text":"self._logger.info(f\"Getting tests results for {len(probe_uids)} probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting tests results from Hawkeye -> {e}\") * If status ok: self._logger.info(f\"Error while retrieving tests results: Error {response_status} - {response_body}\") * Else: self._logger.error(f\"Got all tests results for {len(probe_uids)} probes from Hawkeye!\")","title":"Get probes"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results_for_affecting_monitoring/","text":"Get tests results for affecting monitoring get_tests_results","title":"Get tests results for affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results_for_affecting_monitoring/#get-tests-results-for-affecting-monitoring","text":"get_tests_results","title":"Get tests results for affecting monitoring"},{"location":"logging/services/hawkeye-outage-monitor/actions/_append_triage_note_if_needed/","text":"Append triage note if needed self._logger.info(f\"Checking ticket {ticket_id} to see if device {serial_number} has a triage note already...\") * get_ticket_details * If status is nor Ok: self._logger.warning(f\"Bad status calling to get ticket details. Skipping append triage note ...\") * If triage note: self._logger.info( f\"Triage note already exists in ticket {ticket_id} for serial {serial_number}, so no triage \" f\"note will be appended.\" ) self._logger.info( f\"No triage note was found for serial {serial_number} in ticket {ticket_id}. Appending triage note...\" ) * append_triage_note_to_ticket self._logger.info(f\"Triage note for device {serial_number} appended to ticket {ticket_id}!\")","title":" append triage note if needed"},{"location":"logging/services/hawkeye-outage-monitor/actions/_append_triage_note_if_needed/#append-triage-note-if-needed","text":"self._logger.info(f\"Checking ticket {ticket_id} to see if device {serial_number} has a triage note already...\") * get_ticket_details * If status is nor Ok: self._logger.warning(f\"Bad status calling to get ticket details. Skipping append triage note ...\") * If triage note: self._logger.info( f\"Triage note already exists in ticket {ticket_id} for serial {serial_number}, so no triage \" f\"note will be appended.\" ) self._logger.info( f\"No triage note was found for serial {serial_number} in ticket {ticket_id}. Appending triage note...\" ) * append_triage_note_to_ticket self._logger.info(f\"Triage note for device {serial_number} appended to ticket {ticket_id}!\")","title":"Append triage note if needed"},{"location":"logging/services/hawkeye-outage-monitor/actions/_map_probes_info_with_customer_cache/","text":"Map probes info with customer cache for probe in probes: If not cached info: self._logger.info(f\"No cached info was found for device {serial_number}. Skipping...\")","title":" map probes info with customer cache"},{"location":"logging/services/hawkeye-outage-monitor/actions/_map_probes_info_with_customer_cache/#map-probes-info-with-customer-cache","text":"for probe in probes: If not cached info: self._logger.info(f\"No cached info was found for device {serial_number}. Skipping...\")","title":"Map probes info with customer cache"},{"location":"logging/services/hawkeye-outage-monitor/actions/_outage_monitoring_process/","text":"Outage monitoring process self._logger.info(f\"Starting Hawkeye Outage Monitor!\") * get_cache_for_outage_monitoring * If status is not ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeyey outage monitoring process ...\") * get_probes * If status is not ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If outage devices: self._logger.info( f\"{len(outage_devices)} devices were detected in outage state. \" \"Scheduling re-check job for all of them...\" ) * _schedule_recheck_job_for_devices * Else: self._logger.info(\"No devices were detected in outage state. Re-check job won't be scheduled\") * If healthy edges: self._logger.info( f\"{len(healthy_devices)} devices were detected in healthy state. Running autoresolve for all of them\" ) * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state. Autoresolve won't be triggered\") self._logger.info(f\"Hawkeye Outage Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":" outage monitoring process"},{"location":"logging/services/hawkeye-outage-monitor/actions/_outage_monitoring_process/#outage-monitoring-process","text":"self._logger.info(f\"Starting Hawkeye Outage Monitor!\") * get_cache_for_outage_monitoring * If status is not ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeyey outage monitoring process ...\") * get_probes * If status is not ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If outage devices: self._logger.info( f\"{len(outage_devices)} devices were detected in outage state. \" \"Scheduling re-check job for all of them...\" ) * _schedule_recheck_job_for_devices * Else: self._logger.info(\"No devices were detected in outage state. Re-check job won't be scheduled\") * If healthy edges: self._logger.info( f\"{len(healthy_devices)} devices were detected in healthy state. Running autoresolve for all of them\" ) * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state. Autoresolve won't be triggered\") self._logger.info(f\"Hawkeye Outage Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":"Outage monitoring process"},{"location":"logging/services/hawkeye-outage-monitor/actions/_recheck_devices_for_ticket_creation/","text":"Recheck devices for ticket creation self._logger.info(f\"Re-checking {len(devices)} devices in outage state prior to ticket creation...\") * get_probes * If status not Ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If environment not PRODUCTION: self._logger.info( f\"Process cannot keep going as the current environment is {working_environment.upper()}. \" f\"Healthy devices: {len(healthy_devices)} | Outage devices: {len(devices_still_in_outage)}\" ) * If devices still outage: self._logger.info( f\"{len(devices_still_in_outage)} devices were detected as still in outage state after re-check.\" ) * for device in devices: self._logger.info(f\"Attempting outage ticket creation for faulty device {serial_number}...\") * create_outage_ticket * If create outage ticket status is Ok: self._logger.info(f\"Outage ticket created for device {serial_number}! Ticket ID: {ticket_id}\") self._logger.info(f\"Appending triage note to outage ticket {ticket_id}...\") * append_triage_note_to_ticket * If create outage ticket status is 409: self._logger.info( f\"Faulty device {serial_number} already has an outage ticket in progress (ID = {ticket_id}).\" ) * _append_triage_note_if_needed * If create outage ticket status is 471: self._logger.info( f\"Faulty device {serial_number} has a resolved outage ticket (ID = {ticket_id}). \" \"Re-opening ticket...\" ) * _reopen_outage_ticket * If create outage ticket status is 472: self._logger.info( f\"[outage-recheck] Faulty device {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\" ) * append_note_to_ticket * If create outage ticket status is 473: self._logger.info( f\"[outage-recheck] There is a resolve outage ticket for the same location of faulty device \" f\"{serial_number} (ticket ID = {ticket_id}). The ticket was\" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\" ) * append_triage_note * Else: self._logger.info( \"No devices were detected in outage state after re-check. Outage tickets won't be created\" ) * If healthy devices: self._logger.info(f\"{len(healthy_devices)} devices were detected in healthy state after re-check.\") * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state after re-check.\")","title":" recheck devices for ticket creation"},{"location":"logging/services/hawkeye-outage-monitor/actions/_recheck_devices_for_ticket_creation/#recheck-devices-for-ticket-creation","text":"self._logger.info(f\"Re-checking {len(devices)} devices in outage state prior to ticket creation...\") * get_probes * If status not Ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If environment not PRODUCTION: self._logger.info( f\"Process cannot keep going as the current environment is {working_environment.upper()}. \" f\"Healthy devices: {len(healthy_devices)} | Outage devices: {len(devices_still_in_outage)}\" ) * If devices still outage: self._logger.info( f\"{len(devices_still_in_outage)} devices were detected as still in outage state after re-check.\" ) * for device in devices: self._logger.info(f\"Attempting outage ticket creation for faulty device {serial_number}...\") * create_outage_ticket * If create outage ticket status is Ok: self._logger.info(f\"Outage ticket created for device {serial_number}! Ticket ID: {ticket_id}\") self._logger.info(f\"Appending triage note to outage ticket {ticket_id}...\") * append_triage_note_to_ticket * If create outage ticket status is 409: self._logger.info( f\"Faulty device {serial_number} already has an outage ticket in progress (ID = {ticket_id}).\" ) * _append_triage_note_if_needed * If create outage ticket status is 471: self._logger.info( f\"Faulty device {serial_number} has a resolved outage ticket (ID = {ticket_id}). \" \"Re-opening ticket...\" ) * _reopen_outage_ticket * If create outage ticket status is 472: self._logger.info( f\"[outage-recheck] Faulty device {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\" ) * append_note_to_ticket * If create outage ticket status is 473: self._logger.info( f\"[outage-recheck] There is a resolve outage ticket for the same location of faulty device \" f\"{serial_number} (ticket ID = {ticket_id}). The ticket was\" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\" ) * append_triage_note * Else: self._logger.info( \"No devices were detected in outage state after re-check. Outage tickets won't be created\" ) * If healthy devices: self._logger.info(f\"{len(healthy_devices)} devices were detected in healthy state after re-check.\") * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state after re-check.\")","title":"Recheck devices for ticket creation"},{"location":"logging/services/hawkeye-outage-monitor/actions/_reopen_outage_ticket/","text":"Reopen outage ticket self._logger.info(f\"Reopening Hawkeye outage ticket {ticket_id}...\") * get_ticket_details * If status is not ok: self._logger.warning(f\"Bad status calling get ticket details. Skipping reopen outage ticket ...\") * open_ticket * If status is not Ok: self._logger.error(f\"[outage-ticket-creation] Outage ticket {ticket_id} reopening failed.\") * Else: self._logger.info(f\"Hawkeye outage ticket {ticket_id} reopening succeeded.\") * append_note_to_ticket","title":" reopen outage ticket"},{"location":"logging/services/hawkeye-outage-monitor/actions/_reopen_outage_ticket/#reopen-outage-ticket","text":"self._logger.info(f\"Reopening Hawkeye outage ticket {ticket_id}...\") * get_ticket_details * If status is not ok: self._logger.warning(f\"Bad status calling get ticket details. Skipping reopen outage ticket ...\") * open_ticket * If status is not Ok: self._logger.error(f\"[outage-ticket-creation] Outage ticket {ticket_id} reopening failed.\") * Else: self._logger.info(f\"Hawkeye outage ticket {ticket_id} reopening succeeded.\") * append_note_to_ticket","title":"Reopen outage ticket"},{"location":"logging/services/hawkeye-outage-monitor/actions/_run_ticket_autoresolve/","text":"Run ticket autoresolve self._logger.info(f\"Starting autoresolve for device {serial_number}...\") * get_open_outage_tickets * If status is not Ok: self._logger.warning(f\"Bad status calling to get open outage tickets. \" f\"Skipping run ticket autoresolve ...\") * If not outage tickets: self._logger.info( f\"No open outage ticket found for device {serial_number}. \" f\"Skipping autoresolve...\" ) * If ticket created by automation: self._logger.info( f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\" ) * get_ticket_details * If status not Ok: self._logger.warning(f\"Bad status calling to get ticket details. \" f\"Skipping run ticket autoresolve ...\") * If was last outage detected recently: self._logger.info( f\"Device {device} has been in outage state for a long time, so detail {client_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\" ) * If can't ticket be autoresolve one more time: self._logger.info( f\"Limit to autoresolve ticket {outage_ticket_id} linked to device \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) * If detail is resolved: self._logger.info( f\"Detail {ticket_detail_id} of ticket {outage_ticket_id} is already resolved. \" f\"Skipping autoresolve...\" ) * If working environment PRODUCTION: self._logger.info( f\"Skipping autoresolve for device {serial_number} since the \" f\"current environment is {working_environment.upper()}.\" ) self._logger.info( f\"Autoresolving detail {ticket_detail_id} (serial: {serial_number}) of ticket {outage_ticket_id}...\" ) * unpause_ticket_detail * resolve_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket. Skipping autoresolve ...\") * append_autoresolve_note_to_ticket.md self._logger.info(f\"Ticket {outage_ticket_id} linked to device {serial_number} was autoresolved!\")","title":" run ticket autoresolve"},{"location":"logging/services/hawkeye-outage-monitor/actions/_run_ticket_autoresolve/#run-ticket-autoresolve","text":"self._logger.info(f\"Starting autoresolve for device {serial_number}...\") * get_open_outage_tickets * If status is not Ok: self._logger.warning(f\"Bad status calling to get open outage tickets. \" f\"Skipping run ticket autoresolve ...\") * If not outage tickets: self._logger.info( f\"No open outage ticket found for device {serial_number}. \" f\"Skipping autoresolve...\" ) * If ticket created by automation: self._logger.info( f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\" ) * get_ticket_details * If status not Ok: self._logger.warning(f\"Bad status calling to get ticket details. \" f\"Skipping run ticket autoresolve ...\") * If was last outage detected recently: self._logger.info( f\"Device {device} has been in outage state for a long time, so detail {client_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\" ) * If can't ticket be autoresolve one more time: self._logger.info( f\"Limit to autoresolve ticket {outage_ticket_id} linked to device \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) * If detail is resolved: self._logger.info( f\"Detail {ticket_detail_id} of ticket {outage_ticket_id} is already resolved. \" f\"Skipping autoresolve...\" ) * If working environment PRODUCTION: self._logger.info( f\"Skipping autoresolve for device {serial_number} since the \" f\"current environment is {working_environment.upper()}.\" ) self._logger.info( f\"Autoresolving detail {ticket_detail_id} (serial: {serial_number}) of ticket {outage_ticket_id}...\" ) * unpause_ticket_detail * resolve_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket. Skipping autoresolve ...\") * append_autoresolve_note_to_ticket.md self._logger.info(f\"Ticket {outage_ticket_id} linked to device {serial_number} was autoresolved!\")","title":"Run ticket autoresolve"},{"location":"logging/services/hawkeye-outage-monitor/actions/_schedule_recheck_job_for_devices/","text":"Schedule recheck job for devices self._logger.info(f\"Scheduling recheck job for {len(devices)} devices in outage state...\") * _recheck_devices_for_ticket_creation self._logger.info(f\"Devices scheduled for recheck successfully\")","title":" schedule recheck job for devices"},{"location":"logging/services/hawkeye-outage-monitor/actions/_schedule_recheck_job_for_devices/#schedule-recheck-job-for-devices","text":"self._logger.info(f\"Scheduling recheck job for {len(devices)} devices in outage state...\") * _recheck_devices_for_ticket_creation self._logger.info(f\"Devices scheduled for recheck successfully\")","title":"Schedule recheck job for devices"},{"location":"logging/services/hawkeye-outage-monitor/actions/start_hawkeye_outage_monitoring/","text":"Start hawkeye outage monitoring (Start of service) self._logger.info(\"Scheduling Hawkeye Outage Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Outage Monitoring job. Reason: {conflict}\")","title":"Start hawkeye outage monitoring"},{"location":"logging/services/hawkeye-outage-monitor/actions/start_hawkeye_outage_monitoring/#start-hawkeye-outage-monitoring-start-of-service","text":"self._logger.info(\"Scheduling Hawkeye Outage Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Outage Monitoring job. Reason: {conflict}\")","title":"Start hawkeye outage monitoring (Start of service)"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket","text":"append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_triage_note/","text":"Append triage note If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_triage_note/#append-triage-note","text":"If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/create_outage_ticket/","text":"Create outage ticket Documentation self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/create_outage_ticket/#create-outage-ticket-documentation","text":"self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket Documentation"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/","text":"Get open outage tickets get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/#get-open-outage-tickets","text":"get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_outage_tickets/","text":"Get outage tickets * get_ticket","title":"Get outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_outage_tickets/#get-outage-tickets","text":"* get_ticket","title":"Get outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/open_ticket/#open-ticket","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket detail self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket-detail","text":"self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket detail"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/","text":"Get cache for outage monitoring get_cache","title":"Get cache for outage monitoring"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/#get-cache-for-outage-monitoring","text":"get_cache","title":"Get cache for outage monitoring"},{"location":"logging/services/hawkeye-outage-monitor/repositories/hawkeye_repository/get_probes/","text":"Get probes self._logger.info(f\"Getting all probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting all probes from Hawkeye -> {e}\") * If status ok: self._logger.info(\"Got all probes from Hawkeye!\") * Else: self._logger.error(f\"Error while retrieving probes: Error {response_status} - {response_body}\")","title":"Get probes"},{"location":"logging/services/hawkeye-outage-monitor/repositories/hawkeye_repository/get_probes/#get-probes","text":"self._logger.info(f\"Getting all probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting all probes from Hawkeye -> {e}\") * If status ok: self._logger.info(\"Got all probes from Hawkeye!\") * Else: self._logger.error(f\"Error while retrieving probes: Error {response_status} - {response_body}\")","title":"Get probes"},{"location":"logging/services/intermapper-outage-monitor/actions/_autoresolve_ticket/","text":"Autoresolve ticket self . _logger . info ( \"Starting the autoresolve process\" ) get_ticket_basic_info If response status for get tickets basic info is not ok: self . _logger . warning ( f \"Bad status calling to get ticket basic info for client id: { client_id } .\" f \"Skipping autoresolve ticket ...\" ) self . _logger . info ( f \"Found { len ( tickets_body ) } tickets for service number { service_number } from bruin: { tickets_body } \" ) For ticket in tickets: self . _logger . info ( f \"Posting InterMapper UP note to task of ticket id { ticket_id } \" f \"related to service number { service_number } ...\" ) _append_intermapper_up_note If response status for append InterMapper UP note is not ok: self . _logger . warning ( f \"Bad status calling to append intermapper note to ticket id: { ticket_id } .\" f \"Skipping autoresolve ticket ....\" ) END get_tickets If response status for get tickets is not ok: self . _logger . warning ( f \"Bad status calling to get ticket for client id: { client_id } and \" f \"ticket id: { ticket_id } . Skipping autoresolve ticket ...\" ) END If there's no ticket data: self . _logger . info ( f \"Ticket { ticket_id } couldn't be found in Bruin. Skipping autoresolve...\" ) Continue with next ticket self . _logger . info ( f \"Product category of ticket { ticket_id } from bruin is { product_category } \" ) If ticket's product category is not whitelisted: self . _logger . info ( f \"At least one product category of ticket { ticket_id } from the \" f \"following list is not one of the whitelisted categories for \" f \"auto-resolve: { product_category } . Skipping autoresolve ...\" ) Continue with next ticket self . _logger . info ( f \"Checking to see if ticket { ticket_id } can be autoresolved\" ) get_ticket_details If response status for get ticket details is not ok: self . _logger . warning ( f \"Bad status calling get ticket details to ticket id: { ticket_id } . Skipping autoresolve ...\" ) END If last outage hasn't been detected recently: self . _logger . info ( f \"Edge has been in outage state for a long time, so detail { ticket_detail_id } \" f \"(service number { service_number } ) of ticket { ticket_id } will not be autoresolved. Skipping \" f \"autoresolve...\" ) Continue with next ticket If max auto-resolves threshold has been exceeded: self . _logger . info ( f \"Limit to autoresolve detail { ticket_detail_id } (service number { service_number } ) \" f \"of ticket { ticket_id } has been maxed out already. \" \"Skipping autoresolve...\" ) Continue with next ticket If ticket task is resolved already: self . _logger . info ( f \"Detail { ticket_detail_id } (service number { service_number } ) of ticket { ticket_id } is already \" \"resolved. Skipping autoresolve...\" ) Continue with next ticket If environment is not PRODUCTION : self . _logger . info ( f \"Skipping autoresolve for service number { service_number } \" f \"since the current environment is not production\" ) Continue with next ticket unpause_ticket_detail resolve_ticket If response status for resolve ticket is not ok: self . _logger . warning ( f \"Bad status calling to resolve ticket: { ticket_id } . Skipping autoresolve ...\" ) END self . _logger . info ( f \"Outage ticket { ticket_id } for service_number { service_number } was autoresolved through InterMapper \" f \"emails. Ticket details at https://app.bruin.com/t/ { ticket_id } .\" ) append_autoresolve_note_to_ticket self . _logger . info ( f \"Detail { ticket_detail_id } (service number { service_number } ) of ticket { ticket_id } was autoresolved!\" ) _remove_job_for_autoresolved_task ( HNOC Investigate queue) _remove_job_for_autoresolved_task ( IPA Investigate queue)","title":" autoresolve ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_autoresolve_ticket/#autoresolve-ticket","text":"self . _logger . info ( \"Starting the autoresolve process\" ) get_ticket_basic_info If response status for get tickets basic info is not ok: self . _logger . warning ( f \"Bad status calling to get ticket basic info for client id: { client_id } .\" f \"Skipping autoresolve ticket ...\" ) self . _logger . info ( f \"Found { len ( tickets_body ) } tickets for service number { service_number } from bruin: { tickets_body } \" ) For ticket in tickets: self . _logger . info ( f \"Posting InterMapper UP note to task of ticket id { ticket_id } \" f \"related to service number { service_number } ...\" ) _append_intermapper_up_note If response status for append InterMapper UP note is not ok: self . _logger . warning ( f \"Bad status calling to append intermapper note to ticket id: { ticket_id } .\" f \"Skipping autoresolve ticket ....\" ) END get_tickets If response status for get tickets is not ok: self . _logger . warning ( f \"Bad status calling to get ticket for client id: { client_id } and \" f \"ticket id: { ticket_id } . Skipping autoresolve ticket ...\" ) END If there's no ticket data: self . _logger . info ( f \"Ticket { ticket_id } couldn't be found in Bruin. Skipping autoresolve...\" ) Continue with next ticket self . _logger . info ( f \"Product category of ticket { ticket_id } from bruin is { product_category } \" ) If ticket's product category is not whitelisted: self . _logger . info ( f \"At least one product category of ticket { ticket_id } from the \" f \"following list is not one of the whitelisted categories for \" f \"auto-resolve: { product_category } . Skipping autoresolve ...\" ) Continue with next ticket self . _logger . info ( f \"Checking to see if ticket { ticket_id } can be autoresolved\" ) get_ticket_details If response status for get ticket details is not ok: self . _logger . warning ( f \"Bad status calling get ticket details to ticket id: { ticket_id } . Skipping autoresolve ...\" ) END If last outage hasn't been detected recently: self . _logger . info ( f \"Edge has been in outage state for a long time, so detail { ticket_detail_id } \" f \"(service number { service_number } ) of ticket { ticket_id } will not be autoresolved. Skipping \" f \"autoresolve...\" ) Continue with next ticket If max auto-resolves threshold has been exceeded: self . _logger . info ( f \"Limit to autoresolve detail { ticket_detail_id } (service number { service_number } ) \" f \"of ticket { ticket_id } has been maxed out already. \" \"Skipping autoresolve...\" ) Continue with next ticket If ticket task is resolved already: self . _logger . info ( f \"Detail { ticket_detail_id } (service number { service_number } ) of ticket { ticket_id } is already \" \"resolved. Skipping autoresolve...\" ) Continue with next ticket If environment is not PRODUCTION : self . _logger . info ( f \"Skipping autoresolve for service number { service_number } \" f \"since the current environment is not production\" ) Continue with next ticket unpause_ticket_detail resolve_ticket If response status for resolve ticket is not ok: self . _logger . warning ( f \"Bad status calling to resolve ticket: { ticket_id } . Skipping autoresolve ...\" ) END self . _logger . info ( f \"Outage ticket { ticket_id } for service_number { service_number } was autoresolved through InterMapper \" f \"emails. Ticket details at https://app.bruin.com/t/ { ticket_id } .\" ) append_autoresolve_note_to_ticket self . _logger . info ( f \"Detail { ticket_detail_id } (service number { service_number } ) of ticket { ticket_id } was autoresolved!\" ) _remove_job_for_autoresolved_task ( HNOC Investigate queue) _remove_job_for_autoresolved_task ( IPA Investigate queue)","title":"Autoresolve ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_create_outage_ticket/","text":"Create outage ticket self . _logger . info ( f \"Attempting outage ticket creation for client_id { client_id } and service_number { service_number } \" ) If environment is not PRODUCTION : self . _logger . info ( f \"No outage ticket will be created for client_id { client_id } and circuit_id { circuit_id } \" f \"since the current environment is not production\" ) END create_outage_ticket self . _logger . info ( f \"Bruin response for ticket creation for service number { service_number } : { outage_ticket_response } \" ) If response status for ticket creation is ok: self . _logger . info ( f \"Successfully created outage ticket with ticket_id { ticket_id } \" ) If response status for ticket creation is a Bruin custom status ( 409 , 472 or 473 ): self . _logger . info ( f \"Ticket for service number { service_number } already exists with ticket_id { ticket_id } .\" f \"Status returned was { outage_ticket_status } \" ) If response status for ticket creation is 409 : self . _logger . info ( f \"In Progress ticket exists for location of service number { service_number } \" ) If response status for ticket creation is 472 : self . _logger . info ( f \"Resolved ticket exists for service number { service_number } \" ) If response status for ticket creation is 473 : self . _logger . info ( f \"Resolved ticket exists for location of service number { service_number \") If additional data exists in the DRI system for service number: self . _logger . info ( f \"Appending InterMapper note to ticket id { ticket_id } with dri parameters: \" f \" { dri_parameters } \" ) append_dri_note If response status for append DRI note to ticket is not ok: self . _logger . warning ( f \"Bad status calling append dri note. Skipping create outage ticket ...\" ) Otherwise: END Otherwise: self . _logger . info ( f \"Appending InterMapper note to ticket id { ticket_id } \" ) append_intermapper_note If response status of append InterMapper note is not ok: self . _logger . warning ( f \"Bad status calling append intermapper note. Skipping create outage ticket ...\" ) END If device should be forwarded to the IPA Investigate work queue: _schedule_forward_to_queue ( IPA Investigate queue) _schedule_forward_to_queue ( HNOC Investigate queue)","title":" create outage ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_create_outage_ticket/#create-outage-ticket","text":"self . _logger . info ( f \"Attempting outage ticket creation for client_id { client_id } and service_number { service_number } \" ) If environment is not PRODUCTION : self . _logger . info ( f \"No outage ticket will be created for client_id { client_id } and circuit_id { circuit_id } \" f \"since the current environment is not production\" ) END create_outage_ticket self . _logger . info ( f \"Bruin response for ticket creation for service number { service_number } : { outage_ticket_response } \" ) If response status for ticket creation is ok: self . _logger . info ( f \"Successfully created outage ticket with ticket_id { ticket_id } \" ) If response status for ticket creation is a Bruin custom status ( 409 , 472 or 473 ): self . _logger . info ( f \"Ticket for service number { service_number } already exists with ticket_id { ticket_id } .\" f \"Status returned was { outage_ticket_status } \" ) If response status for ticket creation is 409 : self . _logger . info ( f \"In Progress ticket exists for location of service number { service_number } \" ) If response status for ticket creation is 472 : self . _logger . info ( f \"Resolved ticket exists for service number { service_number } \" ) If response status for ticket creation is 473 : self . _logger . info ( f \"Resolved ticket exists for location of service number { service_number \") If additional data exists in the DRI system for service number: self . _logger . info ( f \"Appending InterMapper note to ticket id { ticket_id } with dri parameters: \" f \" { dri_parameters } \" ) append_dri_note If response status for append DRI note to ticket is not ok: self . _logger . warning ( f \"Bad status calling append dri note. Skipping create outage ticket ...\" ) Otherwise: END Otherwise: self . _logger . info ( f \"Appending InterMapper note to ticket id { ticket_id } \" ) append_intermapper_note If response status of append InterMapper note is not ok: self . _logger . warning ( f \"Bad status calling append intermapper note. Skipping create outage ticket ...\" ) END If device should be forwarded to the IPA Investigate work queue: _schedule_forward_to_queue ( IPA Investigate queue) _schedule_forward_to_queue ( HNOC Investigate queue)","title":"Create outage ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_get_dri_parameters/","text":"Get DRI parameters get_serial_attribute_from_inventory If response status for get serial attribute from inventory is not ok: self . _logger . warning ( f \"Bad status while getting inventory attributes' serial number for service number { service_number } \" f \"and client ID { client_id } . Skipping get DRI parameters...\" ) END If inventory attributes' Serial Number field is undefined: self . _logger . warning ( f \"No inventory attributes' found for service number { service_number } and client ID { client_id } . \" \"Skipping get DRI parameters...\" ) END get_dri_parameters If response status for get DRI parameters is not ok: self . _logger . warning ( f \"Bad status while getting DRI parameters based on inventory attributes' serial number \" f \" { attributes_serial } for service number { service_number } and client ID { client_id } . \" f \"Skipping get DRI parameters...\" ) END","title":" get dri parameters"},{"location":"logging/services/intermapper-outage-monitor/actions/_get_dri_parameters/#get-dri-parameters","text":"get_serial_attribute_from_inventory If response status for get serial attribute from inventory is not ok: self . _logger . warning ( f \"Bad status while getting inventory attributes' serial number for service number { service_number } \" f \"and client ID { client_id } . Skipping get DRI parameters...\" ) END If inventory attributes' Serial Number field is undefined: self . _logger . warning ( f \"No inventory attributes' found for service number { service_number } and client ID { client_id } . \" \"Skipping get DRI parameters...\" ) END get_dri_parameters If response status for get DRI parameters is not ok: self . _logger . warning ( f \"Bad status while getting DRI parameters based on inventory attributes' serial number \" f \" { attributes_serial } for service number { service_number } and client ID { client_id } . \" f \"Skipping get DRI parameters...\" ) END","title":"Get DRI parameters"},{"location":"logging/services/intermapper-outage-monitor/actions/_intermapper_monitoring_process/","text":"Intermapper monitoring process self . _logger . info ( f 'Processing all unread email from { self . _config . INTERMAPPER_CONFIG [ \"inbox_email\" ] } ' ) get_unread_emails If response status of getting unread emails is not ok: self . _logger . warning ( f \"Bad status calling to get unread emails. Skipping intermapper monitoring process...\" ) END Group e-mails by Circuit ID in batches. For every batch of e-mails: _process_email_batch self . _logger . info ( f 'Finished processing unread emails from { self . _config . INTERMAPPER_CONFIG [ \"inbox_email\" ] } . ' f \"Elapsed time: { round (( stop - start ) / 60 , 2 ) } minutes\" )","title":" intermapper monitoring process"},{"location":"logging/services/intermapper-outage-monitor/actions/_intermapper_monitoring_process/#intermapper-monitoring-process","text":"self . _logger . info ( f 'Processing all unread email from { self . _config . INTERMAPPER_CONFIG [ \"inbox_email\" ] } ' ) get_unread_emails If response status of getting unread emails is not ok: self . _logger . warning ( f \"Bad status calling to get unread emails. Skipping intermapper monitoring process...\" ) END Group e-mails by Circuit ID in batches. For every batch of e-mails: _process_email_batch self . _logger . info ( f 'Finished processing unread emails from { self . _config . INTERMAPPER_CONFIG [ \"inbox_email\" ] } . ' f \"Elapsed time: { round (( stop - start ) / 60 , 2 ) } minutes\" )","title":"Intermapper monitoring process"},{"location":"logging/services/intermapper-outage-monitor/actions/_mark_email_as_read/","text":"Mark email as read mark_email_as_read If response status for mark email as read is not ok: self . _logger . error ( f \"Could not mark email with msg_uid: { msg_uid } as read\" )","title":" mark email as read"},{"location":"logging/services/intermapper-outage-monitor/actions/_mark_email_as_read/#mark-email-as-read","text":"mark_email_as_read If response status for mark email as read is not ok: self . _logger . error ( f \"Could not mark email with msg_uid: { msg_uid } as read\" )","title":"Mark email as read"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email/","text":"Process E-mail If message is undefined or its UID is -1 : self . _logger . error ( f \"Invalid message: { email } \" ) END self . _logger . info ( f \"Processing email with msg_uid: { msg_uid } and subject: { subject } \" ) If e-mail represents an UP event: self . _logger . info ( f 'Event from InterMapper was { parsed_email_dict [ \"event\" ] } , there is no need to create ' f \"a new ticket. Checking for autoresolve ...\" ) _autoresolve_ticket If e-mail represents a DOWN event: self . _logger . info ( f 'Event from InterMapper was { parsed_email_dict [ \"event\" ] } , ' f 'condition was { parsed_email_dict [ \"condition\" ] } . Checking for ticket creation ...' ) If device is a PIAB: self . _logger . info ( f \"The probe type from Intermapper is { parsed_email_dict [ 'probe_type' ] } .\" f \"Attempting to get additional parameters from DRI...\" ) _get_dri_parameters _create_outage_ticket If e-mail represents any other kind of event: self . _logger . info ( f 'Event from InterMapper was { parsed_email_dict [ \"event\" ] } , ' f \"so no further action is needs to be taken\" ) If e-mail was processed successfully and environment is PRODUCTION : _mark_email_as_read If event was processed successfully: self . _logger . info ( f \"Processed email: { msg_uid } \" ) Otherwise: self . _logger . error ( f \"Email with msg_uid: { msg_uid } and subject: { subject } \" f \"related to service number: { service_number } could not be processed\" )","title":" process email"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email/#process-e-mail","text":"If message is undefined or its UID is -1 : self . _logger . error ( f \"Invalid message: { email } \" ) END self . _logger . info ( f \"Processing email with msg_uid: { msg_uid } and subject: { subject } \" ) If e-mail represents an UP event: self . _logger . info ( f 'Event from InterMapper was { parsed_email_dict [ \"event\" ] } , there is no need to create ' f \"a new ticket. Checking for autoresolve ...\" ) _autoresolve_ticket If e-mail represents a DOWN event: self . _logger . info ( f 'Event from InterMapper was { parsed_email_dict [ \"event\" ] } , ' f 'condition was { parsed_email_dict [ \"condition\" ] } . Checking for ticket creation ...' ) If device is a PIAB: self . _logger . info ( f \"The probe type from Intermapper is { parsed_email_dict [ 'probe_type' ] } .\" f \"Attempting to get additional parameters from DRI...\" ) _get_dri_parameters _create_outage_ticket If e-mail represents any other kind of event: self . _logger . info ( f 'Event from InterMapper was { parsed_email_dict [ \"event\" ] } , ' f \"so no further action is needs to be taken\" ) If e-mail was processed successfully and environment is PRODUCTION : _mark_email_as_read If event was processed successfully: self . _logger . info ( f \"Processed email: { msg_uid } \" ) Otherwise: self . _logger . error ( f \"Email with msg_uid: { msg_uid } and subject: { subject } \" f \"related to service number: { service_number } could not be processed\" )","title":"Process E-mail"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email_batch/","text":"Process E-mail batch self . _logger . info ( f \"Processing { len ( emails ) } email(s) with circuit ID { circuit_id } ...\" ) If Circuit ID is undefined or Circuit ID is SD-WAN : For each email in batch: mark email as read self . _logger . info ( f \"Invalid circuit_id. Skipping emails with circuit_id { circuit_id } ...\" ) END get_service_number_by_circuit_id If response status for call to get inventory by circuit ID is not ok: self . _logger . error ( f \"Failed to get service number by circuit ID. Skipping emails with circuit_id { circuit_id } ...\" ) END If status = 204: self . _logger . error ( f \"Bruin returned a 204 when getting the service number for circuit_id { circuit_id } . \" f \"Marking all emails with this circuit_id as read\" ) For each email in batch: If environment is PRODUCTION : mark email as read END For email in batch: _process_email self . _logger . info ( f \"Finished processing all emails with circuit_id { circuit_id } !\" )","title":" process email batch"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email_batch/#process-e-mail-batch","text":"self . _logger . info ( f \"Processing { len ( emails ) } email(s) with circuit ID { circuit_id } ...\" ) If Circuit ID is undefined or Circuit ID is SD-WAN : For each email in batch: mark email as read self . _logger . info ( f \"Invalid circuit_id. Skipping emails with circuit_id { circuit_id } ...\" ) END get_service_number_by_circuit_id If response status for call to get inventory by circuit ID is not ok: self . _logger . error ( f \"Failed to get service number by circuit ID. Skipping emails with circuit_id { circuit_id } ...\" ) END If status = 204: self . _logger . error ( f \"Bruin returned a 204 when getting the service number for circuit_id { circuit_id } . \" f \"Marking all emails with this circuit_id as read\" ) For each email in batch: If environment is PRODUCTION : mark email as read END For email in batch: _process_email self . _logger . info ( f \"Finished processing all emails with circuit_id { circuit_id } !\" )","title":"Process E-mail batch"},{"location":"logging/services/intermapper-outage-monitor/actions/_remove_job_for_autoresolved_task/","text":"Remove job for autoresolved task If there is a job scheduled for the work queue: self . _logger . info ( f \"Found job to forward to { target_queue } scheduled for autoresolved ticket { ticket_id } \" f \" related to serial number { serial_number } ! Removing...\" )","title":" remove job for autoresolved task"},{"location":"logging/services/intermapper-outage-monitor/actions/_remove_job_for_autoresolved_task/#remove-job-for-autoresolved-task","text":"If there is a job scheduled for the work queue: self . _logger . info ( f \"Found job to forward to { target_queue } scheduled for autoresolved ticket { ticket_id } \" f \" related to serial number { serial_number } ! Removing...\" )","title":"Remove job for autoresolved task"},{"location":"logging/services/intermapper-outage-monitor/actions/_schedule_forward_to_queue/","text":"Schedule forward to queue self . _logger . info ( f \"Scheduling { target_queue } queue forwarding for ticket_id { ticket_id } and service number { serial_number } \" f \" to happen at timestamp: { forward_task_run_date } \" ) forward_ticket_to_queue","title":" schedule forward to queue"},{"location":"logging/services/intermapper-outage-monitor/actions/_schedule_forward_to_queue/#schedule-forward-to-queue","text":"self . _logger . info ( f \"Scheduling { target_queue } queue forwarding for ticket_id { ticket_id } and service number { serial_number } \" f \" to happen at timestamp: { forward_task_run_date } \" ) forward_ticket_to_queue","title":"Schedule forward to queue"},{"location":"logging/services/intermapper-outage-monitor/actions/change_detail_work_queue/","text":"Change detail work queue change_detail_work_queue If response status for change detail work queue is ok: self . _logger . info ( f \"Successfully forwarded ticket_id { ticket_id } and serial { serial_number } to { target_queue } queue.\" ) If target queue is HNOC Investigate : send_forward_email_milestone_notification If response for send forward email milestone notification is not ok: self . _logger . error ( f \"Forward email related to service number { serial_number } could not be sent for ticket \" f \" { ticket_id } !\" ) Otherwise: self . _logger . error ( f \"Failed to forward ticket_id { ticket_id } and \" f \"serial { serial_number } to { target_queue } queue due to bruin \" f \"returning { change_detail_work_queue_response } when attempting to forward to { target_queue } queue.\" )","title":"Change detail work queue"},{"location":"logging/services/intermapper-outage-monitor/actions/change_detail_work_queue/#change-detail-work-queue","text":"change_detail_work_queue If response status for change detail work queue is ok: self . _logger . info ( f \"Successfully forwarded ticket_id { ticket_id } and serial { serial_number } to { target_queue } queue.\" ) If target queue is HNOC Investigate : send_forward_email_milestone_notification If response for send forward email milestone notification is not ok: self . _logger . error ( f \"Forward email related to service number { serial_number } could not be sent for ticket \" f \" { ticket_id } !\" ) Otherwise: self . _logger . error ( f \"Failed to forward ticket_id { ticket_id } and \" f \"serial { serial_number } to { target_queue } queue due to bruin \" f \"returning { change_detail_work_queue_response } when attempting to forward to { target_queue } queue.\" )","title":"Change detail work queue"},{"location":"logging/services/intermapper-outage-monitor/actions/forward_ticket_to_queue/","text":"Forward ticket to queue self . _logger . info ( f \"Checking if ticket_id { ticket_id } for serial { serial_number } is resolved before \" f \"attempting to forward to { target_queue } queue...\" ) While there are retries left to try to forward to the work queue: change_detail_work_queue If the maximum number of retries was exceeded: self . _logger . error ( f \"An error occurred while trying to forward ticket_id { ticket_id } for serial { serial_number } to\" f \" { target_queue } queue -> { e } \" )","title":"Forward ticket to queue"},{"location":"logging/services/intermapper-outage-monitor/actions/forward_ticket_to_queue/#forward-ticket-to-queue","text":"self . _logger . info ( f \"Checking if ticket_id { ticket_id } for serial { serial_number } is resolved before \" f \"attempting to forward to { target_queue } queue...\" ) While there are retries left to try to forward to the work queue: change_detail_work_queue If the maximum number of retries was exceeded: self . _logger . error ( f \"An error occurred while trying to forward ticket_id { ticket_id } for serial { serial_number } to\" f \" { target_queue } queue -> { e } \" )","title":"Forward ticket to queue"},{"location":"logging/services/intermapper-outage-monitor/actions/start_intermapper_outage_monitoring/","text":"Start InterMapper Outage Monitoring self . _logger . info ( \"Scheduling InterMapper Monitor job...\" ) If job should be executed on service start: self . _logger . info ( \"InterMapper Monitor job is going to be executed immediately\" ) _intermapper_monitoring_process If there's a running job to monitor InterMapper events already: self . _logger . info ( f \"Skipping start of InterMapper Monitoring job. Reason: { conflict } \" )","title":"Start intermapper outage monitoring"},{"location":"logging/services/intermapper-outage-monitor/actions/start_intermapper_outage_monitoring/#start-intermapper-outage-monitoring","text":"self . _logger . info ( \"Scheduling InterMapper Monitor job...\" ) If job should be executed on service start: self . _logger . info ( \"InterMapper Monitor job is going to be executed immediately\" ) _intermapper_monitoring_process If there's a running job to monitor InterMapper events already: self . _logger . info ( f \"Skipping start of InterMapper Monitoring job. Reason: { conflict } \" )","title":"Start InterMapper Outage Monitoring"},{"location":"logging/services/intermapper-outage-monitor/app_entrypoint/app/","text":"App entrypoint self . _logger . info ( f \"InterMapper Outage Monitor starting in { config . CURRENT_ENVIRONMENT } ...\" )","title":"App"},{"location":"logging/services/intermapper-outage-monitor/app_entrypoint/app/#app-entrypoint","text":"self . _logger . info ( f \"InterMapper Outage Monitor starting in { config . CURRENT_ENVIRONMENT } ...\" )","title":"App entrypoint"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket","text":"append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_dri_note/","text":"Append dri note append_note_to_ticket","title":"Append dri note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_dri_note/#append-dri-note","text":"append_note_to_ticket","title":"Append dri note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_note/","text":"Append intermapper note append_note_to_ticket","title":"Append intermapper note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_note/#append-intermapper-note","text":"append_note_to_ticket","title":"Append intermapper note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_up_note/","text":"Append intermapper up note append_note_to_ticket","title":"Append intermapper up note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_up_note/#append-intermapper-up-note","text":"append_note_to_ticket","title":"Append intermapper up note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket self . _logger . info ( f \"Appending note to ticket { ticket_id } ... Note contents: { note } \" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when appending a ticket note to ticket { ticket_id } . \" f \"Ticket note: { note } . Error: { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for append note to ticket is not ok: err_msg = ( f \"Error while appending note to ticket { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment. Note was { note } . Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Note appended to ticket { ticket_id } !\" )","title":"Append note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"self . _logger . info ( f \"Appending note to ticket { ticket_id } ... Note contents: { note } \" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when appending a ticket note to ticket { ticket_id } . \" f \"Ticket note: { note } . Error: { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for append note to ticket is not ok: err_msg = ( f \"Error while appending note to ticket { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment. Note was { note } . Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Note appended to ticket { ticket_id } !\" )","title":"Append note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/change_detail_work_queue/","text":"Change detail work queue self . _logger . info ( f \"Changing task result for ticket { ticket_id } for device { serial_number } to { task_result } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = f \"An error occurred when changing task result for ticket { ticket_id } and serial { serial_number } \" [ ... ] self . _logger . error ( err_msg ) END If response status for change detail work queue is ok: self . _logger . info ( f \"Ticket { ticket_id } and serial { serial_number } task result changed to { task_result } successfully!\" ) Otherwise: err_msg = ( f \"Error while changing task result for ticket { ticket_id } and serial { serial_number } in \" f \" { self . _config . CURRENT_ENVIRONMENT . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Change detail work queue"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/change_detail_work_queue/#change-detail-work-queue","text":"self . _logger . info ( f \"Changing task result for ticket { ticket_id } for device { serial_number } to { task_result } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = f \"An error occurred when changing task result for ticket { ticket_id } and serial { serial_number } \" [ ... ] self . _logger . error ( err_msg ) END If response status for change detail work queue is ok: self . _logger . info ( f \"Ticket { ticket_id } and serial { serial_number } task result changed to { task_result } successfully!\" ) Otherwise: err_msg = ( f \"Error while changing task result for ticket { ticket_id } and serial { serial_number } in \" f \" { self . _config . CURRENT_ENVIRONMENT . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Change detail work queue"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/create_outage_ticket/","text":"Create outage ticket self . _logger . info ( f \"Creating outage ticket for device { service_number } that belongs to client { client_id } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when creating outage ticket for device { service_number } belong to client\" f \" { client_id } -> { e } \" ) [ ... ] self . _logger . error ( err_msg ) END self . _logger . info ( f \"Outage ticket for device { service_number } that belongs to client { client_id } created!\" ) If response status for get serial attribute from inventory is not ok, or is 409 , 471 , 472 or 473 : err_msg = ( f \"Error while creating outage ticket for device { service_number } that belongs to client \" f \" { client_id } in { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" )","title":"Create outage ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/create_outage_ticket/#create-outage-ticket","text":"self . _logger . info ( f \"Creating outage ticket for device { service_number } that belongs to client { client_id } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when creating outage ticket for device { service_number } belong to client\" f \" { client_id } -> { e } \" ) [ ... ] self . _logger . error ( err_msg ) END self . _logger . info ( f \"Outage ticket for device { service_number } that belongs to client { client_id } created!\" ) If response status for get serial attribute from inventory is not ok, or is 409 , 471 , 472 or 473 : err_msg = ( f \"Error while creating outage ticket for device { service_number } that belongs to client \" f \" { client_id } in { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" )","title":"Create outage ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_serial_attribute_from_inventory/","text":"Get serial attribute from inventory self . _logger . info ( f \"Getting inventory attributes' serial number for service number { service_number } and client ID\" f \" { client_id } \" ) If there's an error while asking for the data to the bruin-bridge : err_msg = ( f \"Error while getting inventory attributes' serial number for service number { service_number } and \" f \"client ID { client_id } : { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for get serial attribute from inventory is not ok: err_msg = ( f \"Error while getting inventory attributes' serial number for service number { service_number } and \" f \"client ID { client_id } in { self . _config . ENVIRONMENT_NAME . upper () } environment. Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got inventory attributes' serial number for service number { service_number } and client ID \" f \" { client_id } \" )","title":"Get serial attribute from inventory"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_serial_attribute_from_inventory/#get-serial-attribute-from-inventory","text":"self . _logger . info ( f \"Getting inventory attributes' serial number for service number { service_number } and client ID\" f \" { client_id } \" ) If there's an error while asking for the data to the bruin-bridge : err_msg = ( f \"Error while getting inventory attributes' serial number for service number { service_number } and \" f \"client ID { client_id } : { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for get serial attribute from inventory is not ok: err_msg = ( f \"Error while getting inventory attributes' serial number for service number { service_number } and \" f \"client ID { client_id } in { self . _config . ENVIRONMENT_NAME . upper () } environment. Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got inventory attributes' serial number for service number { service_number } and client ID \" f \" { client_id } \" )","title":"Get serial attribute from inventory"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_service_number_by_circuit_id/","text":"Get service number by circuit ID self . _logger . info ( f \"Getting the translation to service number for circuit_id { circuit_id } \" ) If there's an error while asking for the data to the bruin-bridge : err_msg = f \"Getting the translation to service number for circuit_id { circuit_id } Error: { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get service number by circuit ID is not ok or is 204 : err_msg = ( f \"Getting the translation to service number for circuit_id { circuit_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment. Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Get service number by circuit id"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_service_number_by_circuit_id/#get-service-number-by-circuit-id","text":"self . _logger . info ( f \"Getting the translation to service number for circuit_id { circuit_id } \" ) If there's an error while asking for the data to the bruin-bridge : err_msg = f \"Getting the translation to service number for circuit_id { circuit_id } Error: { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get service number by circuit ID is not ok or is 204 : err_msg = ( f \"Getting the translation to service number for circuit_id { circuit_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment. Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Get service number by circuit ID"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_basic_info/","text":"Get ticket basic info self . _logger . info ( f \"Getting all tickets basic info with any status of { ticket_statuses } , with ticket topic \" f \"VOO, service number { service_number } and belonging to client { client_id } from Bruin...\" ) If there's an error while asking for the data to the bruin-bridge : err_msg = ( f \"An error occurred when requesting tickets basic info from Bruin API with any status\" f \" of { ticket_statuses } , with ticket topic VOO and belonging to client { client_id } -> { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for get ticket basic info is not ok: err_msg = ( f \"Error while retrieving tickets basic info with any status of { ticket_statuses } , \" f \"with ticket topic VOO, service number { service_number } and belonging to client { client_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got all tickets basic info with any status of { ticket_statuses } , with ticket topic \" f \"VOO, service number { service_number } and belonging to client \" f \" { client_id } from Bruin!\" )","title":"Get ticket basic info"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_basic_info/#get-ticket-basic-info","text":"self . _logger . info ( f \"Getting all tickets basic info with any status of { ticket_statuses } , with ticket topic \" f \"VOO, service number { service_number } and belonging to client { client_id } from Bruin...\" ) If there's an error while asking for the data to the bruin-bridge : err_msg = ( f \"An error occurred when requesting tickets basic info from Bruin API with any status\" f \" of { ticket_statuses } , with ticket topic VOO and belonging to client { client_id } -> { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for get ticket basic info is not ok: err_msg = ( f \"Error while retrieving tickets basic info with any status of { ticket_statuses } , \" f \"with ticket topic VOO, service number { service_number } and belonging to client { client_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got all tickets basic info with any status of { ticket_statuses } , with ticket topic \" f \"VOO, service number { service_number } and belonging to client \" f \" { client_id } from Bruin!\" )","title":"Get ticket basic info"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self . _logger . info ( f \"Getting details of ticket { ticket_id } from Bruin...\" ) If there's an error while asking for the data to the bruin-bridge : err_msg = f \"An error occurred when requesting ticket details from Bruin API for ticket { ticket_id } -> { e } \" [ ... ] self . _logger . error ( err_msg ) END self . _logger . info ( f \"Got details of ticket { ticket_id } from Bruin!\" ) If response status for get ticket details is not ok: err_msg = ( f \"Error while retrieving details of ticket { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Get ticket details"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self . _logger . info ( f \"Getting details of ticket { ticket_id } from Bruin...\" ) If there's an error while asking for the data to the bruin-bridge : err_msg = f \"An error occurred when requesting ticket details from Bruin API for ticket { ticket_id } -> { e } \" [ ... ] self . _logger . error ( err_msg ) END self . _logger . info ( f \"Got details of ticket { ticket_id } from Bruin!\" ) If response status for get ticket details is not ok: err_msg = ( f \"Error while retrieving details of ticket { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Get ticket details"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_tickets/","text":"Get tickets self . _logger . info ( f \"Getting all tickets of ticket id { ticket_id } from Bruin...\" ) If there's an error while asking for the data to the bruin-bridge : err_msg = f \"An error occurred when requesting all tickets of ticket id { ticket_id } from Bruin API -> { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get tickets is not ok: err_msg = ( f \"Error while retrieving all tickets of ticket id { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got all tickets of ticket id { ticket_id } from Bruin\" )","title":"Get tickets"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_tickets/#get-tickets","text":"self . _logger . info ( f \"Getting all tickets of ticket id { ticket_id } from Bruin...\" ) If there's an error while asking for the data to the bruin-bridge : err_msg = f \"An error occurred when requesting all tickets of ticket id { ticket_id } from Bruin API -> { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get tickets is not ok: err_msg = ( f \"Error while retrieving all tickets of ticket id { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got all tickets of ticket id { ticket_id } from Bruin\" )","title":"Get tickets"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/post_notification_email_milestone/","text":"Post notification email milestone self . _logger . info ( f \"Sending email for ticket id { ticket_id } , \" f \"service_number { service_number } \" f \"and notification type { notification_type } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when sending email for ticket id { ticket_id } , \" f \"service_number { service_number } \" f \"and notification type { notification_type } ...-> { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for post notification email milestone is ok: self . _logger . info ( f \"Email sent for ticket { ticket_id } , service number { service_number } \" f \"and notification type { notification_type } !\" ) Otherwise: err_msg = ( f \"Error while sending email for ticket id { ticket_id } , service_number { service_number } \" f \"and notification type { notification_type } in \" f \" { self . _config . CURRENT_ENVIRONMENT . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Post notification email milestone"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/post_notification_email_milestone/#post-notification-email-milestone","text":"self . _logger . info ( f \"Sending email for ticket id { ticket_id } , \" f \"service_number { service_number } \" f \"and notification type { notification_type } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when sending email for ticket id { ticket_id } , \" f \"service_number { service_number } \" f \"and notification type { notification_type } ...-> { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for post notification email milestone is ok: self . _logger . info ( f \"Email sent for ticket { ticket_id } , service number { service_number } \" f \"and notification type { notification_type } !\" ) Otherwise: err_msg = ( f \"Error while sending email for ticket id { ticket_id } , service_number { service_number } \" f \"and notification type { notification_type } in \" f \" { self . _config . CURRENT_ENVIRONMENT . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Post notification email milestone"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket self . _logger . info ( f \"Resolving ticket { ticket_id } (affected detail ID: { detail_id } )...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = f \"An error occurred when resolving ticket { ticket_id } -> { e } \" [ ... ] self . _logger . error ( err_msg ) END self . _logger . info ( f \"Ticket { ticket_id } resolved!\" ) If response status for resolve ticket is not ok: err_msg = ( f \"Error while resolving ticket { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } \" f \"environment: Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Resolve ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket","text":"self . _logger . info ( f \"Resolving ticket { ticket_id } (affected detail ID: { detail_id } )...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = f \"An error occurred when resolving ticket { ticket_id } -> { e } \" [ ... ] self . _logger . error ( err_msg ) END self . _logger . info ( f \"Ticket { ticket_id } resolved!\" ) If response status for resolve ticket is not ok: err_msg = ( f \"Error while resolving ticket { ticket_id } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } \" f \"environment: Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg )","title":"Resolve ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/send_forward_email_milestone_notification/","text":"Send forward email milestone notification post_notification_email_milestone","title":"Send forward email milestone notification"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/send_forward_email_milestone_notification/#send-forward-email-milestone-notification","text":"post_notification_email_milestone","title":"Send forward email milestone notification"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self . _logger . info ( f \"Unpausing detail { detail_id } (serial { service_number } ) of ticket { ticket_id } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when unpausing detail { detail_id } (serial { service_number } ) of ticket { ticket_id } . \" f \"Error: { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for unpause ticket detail is not ok: err_msg = ( f \"Error while unpausing detail { detail_id } (serial { service_number } ) of ticket { ticket_id } in \" f \" { self . _config . CURRENT_ENVIRONMENT . upper () } environment. \" f \"Error: Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Detail { detail_id } (serial { service_number } ) of ticket { ticket_id } was unpaused!\" )","title":"Unpause ticket detail"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self . _logger . info ( f \"Unpausing detail { detail_id } (serial { service_number } ) of ticket { ticket_id } ...\" ) If there's an error while posting the data to the bruin-bridge : err_msg = ( f \"An error occurred when unpausing detail { detail_id } (serial { service_number } ) of ticket { ticket_id } . \" f \"Error: { e } \" ) [ ... ] self . _logger . error ( err_msg ) END If response status for unpause ticket detail is not ok: err_msg = ( f \"Error while unpausing detail { detail_id } (serial { service_number } ) of ticket { ticket_id } in \" f \" { self . _config . CURRENT_ENVIRONMENT . upper () } environment. \" f \"Error: Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Detail { detail_id } (serial { service_number } ) of ticket { ticket_id } was unpaused!\" )","title":"Unpause ticket detail"},{"location":"logging/services/intermapper-outage-monitor/repositories/dri_repository/get_dri_parameters/","text":"Get DRI parameters self . _logger . info ( f \"Getting DRI parameters of serial number { serial_number } \" ) If there's an error while asking for the data to the dri-bridge : err_msg = f \"An error occurred while getting DRI parameter for serial number { serial_number } . Error: { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get DRI parameters is not ok: err_msg = ( f \"Error while getting DRI parameter of serial number { serial_number } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment. Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got DRI parameter of serial number { serial_number } !\" )","title":"Get dri parameters"},{"location":"logging/services/intermapper-outage-monitor/repositories/dri_repository/get_dri_parameters/#get-dri-parameters","text":"self . _logger . info ( f \"Getting DRI parameters of serial number { serial_number } \" ) If there's an error while asking for the data to the dri-bridge : err_msg = f \"An error occurred while getting DRI parameter for serial number { serial_number } . Error: { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get DRI parameters is not ok: err_msg = ( f \"Error while getting DRI parameter of serial number { serial_number } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment. Error: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got DRI parameter of serial number { serial_number } !\" )","title":"Get DRI parameters"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/get_unread_emails/","text":"Get unread emails self . _logger . info ( f \"Getting the unread emails from the inbox of { email_account } sent from the users: \" f \" { email_filter } in the last { lookup_days } days\" ) If there's an error while asking for the data to the notifier : err_msg = f \"An error occurred while getting the unread emails from the inbox of { email_account } -> { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get unread emails is not ok: err_msg = ( f \"Error getting the unread emails from the inbox of { email_account } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got the unread emails from the inbox of { email_account } \" )","title":"Get unread emails"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/get_unread_emails/#get-unread-emails","text":"self . _logger . info ( f \"Getting the unread emails from the inbox of { email_account } sent from the users: \" f \" { email_filter } in the last { lookup_days } days\" ) If there's an error while asking for the data to the notifier : err_msg = f \"An error occurred while getting the unread emails from the inbox of { email_account } -> { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for get unread emails is not ok: err_msg = ( f \"Error getting the unread emails from the inbox of { email_account } in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Got the unread emails from the inbox of { email_account } \" )","title":"Get unread emails"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/mark_email_as_read/","text":"Mark email as read self . _logger . info ( f \"Marking message { msg_uid } from the inbox of { email_account } as read\" ) If there's an error while posting the data to the notifier : err_msg = f \"An error occurred while marking message { msg_uid } as read -> { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for mark email as read is not ok: err_msg = ( f \"Error marking message { msg_uid } as read in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Marked message { msg_uid } as read\" )","title":"Mark email as read"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/mark_email_as_read/#mark-email-as-read","text":"self . _logger . info ( f \"Marking message { msg_uid } from the inbox of { email_account } as read\" ) If there's an error while posting the data to the notifier : err_msg = f \"An error occurred while marking message { msg_uid } as read -> { e } \" [ ... ] self . _logger . error ( err_msg ) END If response status for mark email as read is not ok: err_msg = ( f \"Error marking message { msg_uid } as read in \" f \" { self . _config . ENVIRONMENT_NAME . upper () } environment: \" f \"Error { response_status } - { response_body } \" ) [ ... ] self . _logger . error ( err_msg ) Otherwise: self . _logger . info ( f \"Marked message { msg_uid } as read\" )","title":"Mark email as read"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/_pre_recover_cb/","text":"Recover message before consuming it If message is stored to an external storage: logger . warning ( f \"Message received in subject { msg . subject } exceeds the maximum size allowed by NATS. Recovering \" \"it from the external storage...\" ) Depending on the implementation, a call to Redis::recover or RedisLegacy::recover is made","title":"Recover message before consuming it"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/_pre_recover_cb/#recover-message-before-consuming-it","text":"If message is stored to an external storage: logger . warning ( f \"Message received in subject { msg . subject } exceeds the maximum size allowed by NATS. Recovering \" \"it from the external storage...\" ) Depending on the implementation, a call to Redis::recover or RedisLegacy::recover is made","title":"Recover message before consuming it"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/connect/","text":"Connect to the message bus logger . info ( f \"Connecting to NATS servers: { servers } ...\" ) Invoke nats-py's connect method logger . info ( f \"Connected to NATS servers successfully\" )","title":"Connect"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/connect/#connect-to-the-message-bus","text":"logger . info ( f \"Connecting to NATS servers: { servers } ...\" ) Invoke nats-py's connect method logger . info ( f \"Connected to NATS servers successfully\" )","title":"Connect to the message bus"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/publish/","text":"Publish message to the bus If message is too large for NATS to handle (1MB+): logger . warning ( \"Payload exceeds the maximum size allowed by NATS. Storing it to the external storage before \" f \"publishing to subject { subject } ...\" ) Depending on the implementation, a call to Redis::store or RedisLegacy::store is made logger . info ( f \"Publishing payload to subject { subject } ...\" ) Invoke nats-py's publish method The message will be consumed by a subscriber with interest in the subject logger . info ( f \"Payload published to subject { subject } successfully\" ) Consume message from the bus _pre_recover_cb is implicitly called on message arrival Consume message","title":"Publish"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/publish/#publish-message-to-the-bus","text":"If message is too large for NATS to handle (1MB+): logger . warning ( \"Payload exceeds the maximum size allowed by NATS. Storing it to the external storage before \" f \"publishing to subject { subject } ...\" ) Depending on the implementation, a call to Redis::store or RedisLegacy::store is made logger . info ( f \"Publishing payload to subject { subject } ...\" ) Invoke nats-py's publish method The message will be consumed by a subscriber with interest in the subject logger . info ( f \"Payload published to subject { subject } successfully\" )","title":"Publish message to the bus"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/publish/#consume-message-from-the-bus","text":"_pre_recover_cb is implicitly called on message arrival Consume message","title":"Consume message from the bus"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/request/","text":"Publish request message to the bus logger . info ( f \"Requesting a response from subject { subject } ...\" ) Invoke nats-py's request method, which internally calls publish Wait for a response ... logger . info ( f \"Response received from a replier subscribed to subject { subject } \" ) If response message is stored to an external storage: logger . warning ( f \"Response received from subject { subject } exceeds the maximum size allowed by NATS. Recovering it \" \"from the external storage...\" ) Depending on the implementation, a call to Redis::recover or RedisLegacy::recover is made Consume request _pre_recover_cb is implicitly called on request message arrival Consume message Invoke nats-py's respond method with the response message, which internally calls publish","title":"Request"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/request/#publish-request-message-to-the-bus","text":"logger . info ( f \"Requesting a response from subject { subject } ...\" ) Invoke nats-py's request method, which internally calls publish Wait for a response ... logger . info ( f \"Response received from a replier subscribed to subject { subject } \" ) If response message is stored to an external storage: logger . warning ( f \"Response received from subject { subject } exceeds the maximum size allowed by NATS. Recovering it \" \"from the external storage...\" ) Depending on the implementation, a call to Redis::recover or RedisLegacy::recover is made","title":"Publish request message to the bus"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/request/#consume-request","text":"_pre_recover_cb is implicitly called on request message arrival Consume message Invoke nats-py's respond method with the response message, which internally calls publish","title":"Consume request"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/subscribe/","text":"Publish message to the bus logger . info ( f \"Subscribing to subject { subject } with queue group { queue } ...\" ) Invoke nats-py's subscribe method logger . info ( f \"Subscribed to subject successfully\" )","title":"Subscribe"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/client/subscribe/#publish-message-to-the-bus","text":"logger . info ( f \"Subscribing to subject { subject } with queue group { queue } ...\" ) Invoke nats-py's subscribe method logger . info ( f \"Subscribed to subject successfully\" )","title":"Publish message to the bus"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis/recover/","text":"Recover message to Redis (new style) logger . info ( f \"Retrieving payload stored under Redis key { token } ...\" ) Invoke redis' get method logger . info ( f \"Payload stored under Redis key { key } successfully\" )","title":"Recover"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis/recover/#recover-message-to-redis-new-style","text":"logger . info ( f \"Retrieving payload stored under Redis key { token } ...\" ) Invoke redis' get method logger . info ( f \"Payload stored under Redis key { key } successfully\" )","title":"Recover message to Redis (new style)"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis/store/","text":"Store message to Redis (new style) logger . info ( f \"Storing payload of { len ( payload ) } bytes under Redis key { key } \" ) Invoke redis' set method logger . info ( f \"Payload stored under Redis key { key } successfully\" )","title":"Store"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis/store/#store-message-to-redis-new-style","text":"logger . info ( f \"Storing payload of { len ( payload ) } bytes under Redis key { key } \" ) Invoke redis' set method logger . info ( f \"Payload stored under Redis key { key } successfully\" )","title":"Store message to Redis (new style)"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis_legacy/recover/","text":"Recover message to Redis (legacy style) logger . info ( f \"Retrieving payload stored under Redis key { token } ...\" ) Invoke redis' get method logger . info ( f \"Payload stored under Redis key { token } retrieved successfully\" )","title":"Recover"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis_legacy/recover/#recover-message-to-redis-legacy-style","text":"logger . info ( f \"Retrieving payload stored under Redis key { token } ...\" ) Invoke redis' get method logger . info ( f \"Payload stored under Redis key { token } retrieved successfully\" )","title":"Recover message to Redis (legacy style)"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis_legacy/store/","text":"Store message to Redis (legacy style) logger . info ( f \"Storing payload of { len ( payload ) } bytes under Redis key { token } \" ) Invoke redis' set method logger . info ( f \"Payload stored under Redis key { token } retrieved successfully\" )","title":"Store"},{"location":"logging/services/pyutils_automation/py310/src/framework/nats/temp_payload_storage/redis_legacy/store/#store-message-to-redis-legacy-style","text":"logger . info ( f \"Storing payload of { len ( payload ) } bytes under Redis key { token } \" ) Invoke redis' set method logger . info ( f \"Payload stored under Redis key { token } retrieved successfully\" )","title":"Store message to Redis (legacy style)"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/action/execute_stateful_action/","text":"Execute action to consume message If service's action is not properly set up: self . logger . error ( f 'The object { self . state_instance } has no method named { self . target_function } ' ) END __check_large_messages_decorator is implicitly called before executing the action for the message","title":"Execute stateful action"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/action/execute_stateful_action/#execute-action-to-consume-message","text":"If service's action is not properly set up: self . logger . error ( f 'The object { self . state_instance } has no method named { self . target_function } ' ) END __check_large_messages_decorator is implicitly called before executing the action for the message","title":"Execute action to consume message"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/__check_large_messages_decorator/","text":"Recover message before consuming it If message is stored to an external storage: recover_message self . _logger . info ( f 'Message received from topic { event } indicates that the actual message was larger than 1MB ' f 'and was stored with { type ( self . _messages_storage_manager ) . __name__ } .' ) _cb_with_action is implicitly called","title":"  check large messages decorator"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/__check_large_messages_decorator/#recover-message-before-consuming-it","text":"If message is stored to an external storage: recover_message self . _logger . info ( f 'Message received from topic { event } indicates that the actual message was larger than 1MB ' f 'and was stored with { type ( self . _messages_storage_manager ) . __name__ } .' ) _cb_with_action is implicitly called","title":"Recover message before consuming it"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/add_consumer/","text":"Add consumer to event bus self . _logger . info ( f \"Adding consumer { consumer_name } to the event bus...\" ) If consumer has been added to the event bus already: self . _logger . error ( f 'Consumer name { consumer_name } already registered. Skipping...' ) END self . _logger . info ( f \"Consumer { consumer_name } added to the event bus\" )","title":"Add consumer"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/add_consumer/#add-consumer-to-event-bus","text":"self . _logger . info ( f \"Adding consumer { consumer_name } to the event bus...\" ) If consumer has been added to the event bus already: self . _logger . error ( f 'Consumer name { consumer_name } already registered. Skipping...' ) END self . _logger . info ( f \"Consumer { consumer_name } added to the event bus\" )","title":"Add consumer to event bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/close_connections/","text":"Close connections for all consumers self . _logger . info ( \"Closing connection for all consumers in the event bus...\" ) For each consumer in the event bus: * close_nats_connections self . _logger . info ( \"Connections closed for all consumers in the event bus\" ) If there is a producer attached to the event bus: self . _logger . info ( \"Closing connection for producer in the event bus...\" ) close_nats_connections self . _logger . info ( \"Closed connection for producer in the event bus\" )","title":"Close connections"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/close_connections/#close-connections-for-all-consumers","text":"self . _logger . info ( \"Closing connection for all consumers in the event bus...\" ) For each consumer in the event bus: * close_nats_connections self . _logger . info ( \"Connections closed for all consumers in the event bus\" ) If there is a producer attached to the event bus: self . _logger . info ( \"Closing connection for producer in the event bus...\" ) close_nats_connections self . _logger . info ( \"Closed connection for producer in the event bus\" )","title":"Close connections for all consumers"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/connect/","text":"Connect all consumers and producers to the message bus self . _logger . info ( f \"Establishing connection to NATS for all consumers...\" ) For all consumers attached to the event bus: * connect_to_nats self . _logger . info ( f \"Connection to NATS established successfully for all consumers\" ) If there is a producer attached to the event bus: self . _logger . info ( f \"Establishing connection to NATS for producer...\" ) connect_to_nats self . _logger . info ( f \"Connection to NATS established successfully for producer\" )","title":"Connect"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/connect/#connect-all-consumers-and-producers-to-the-message-bus","text":"self . _logger . info ( f \"Establishing connection to NATS for all consumers...\" ) For all consumers attached to the event bus: * connect_to_nats self . _logger . info ( f \"Connection to NATS established successfully for all consumers\" ) If there is a producer attached to the event bus: self . _logger . info ( f \"Establishing connection to NATS for producer...\" ) connect_to_nats self . _logger . info ( f \"Connection to NATS established successfully for producer\" )","title":"Connect all consumers and producers to the message bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/publish_message/","text":"Publish message to the bus self . _logger . info ( f \"Publishing message to subject { topic } ...\" ) If message is too large for NATS to handle (1MB+): store_message self . _logger . info ( 'Message received in publish() was larger than 1MB so it was stored with ' f ' { type ( self . _messages_storage_manager ) . __name__ } . The token needed to recover it is ' f ' { msg [ \"token\" ] } .' ) publish","title":"Publish message"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/publish_message/#publish-message-to-the-bus","text":"self . _logger . info ( f \"Publishing message to subject { topic } ...\" ) If message is too large for NATS to handle (1MB+): store_message self . _logger . info ( 'Message received in publish() was larger than 1MB so it was stored with ' f ' { type ( self . _messages_storage_manager ) . __name__ } . The token needed to recover it is ' f ' { msg [ \"token\" ] } .' ) publish","title":"Publish message to the bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/rpc_request/","text":"Publish request message to the bus If request message is too large for NATS to handle (1MB+): store_message self . _logger . info ( 'Message received in rpc_request() was larger than 1MB so it was stored with ' f ' { type ( self . _messages_storage_manager ) . __name__ } . The token needed to recover it is ' f ' { message [ \"token\" ] } .' ) self . _logger . info ( f \"Requesting a response from subject { subject } ...\" ) rpc_request self . _logger . info ( f \"Response received from a replier subscribed to subject { topic } \" ) If response message is stored to an external storage: self . _logger . info ( f 'Message received from topic { topic } indicates that the actual message was larger than 1MB ' f 'and was stored with { type ( self . _messages_storage_manager ) . __name__ } .' ) recover_message","title":"Rpc request"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/rpc_request/#publish-request-message-to-the-bus","text":"If request message is too large for NATS to handle (1MB+): store_message self . _logger . info ( 'Message received in rpc_request() was larger than 1MB so it was stored with ' f ' { type ( self . _messages_storage_manager ) . __name__ } . The token needed to recover it is ' f ' { message [ \"token\" ] } .' ) self . _logger . info ( f \"Requesting a response from subject { subject } ...\" ) rpc_request self . _logger . info ( f \"Response received from a replier subscribed to subject { topic } \" ) If response message is stored to an external storage: self . _logger . info ( f 'Message received from topic { topic } indicates that the actual message was larger than 1MB ' f 'and was stored with { type ( self . _messages_storage_manager ) . __name__ } .' ) recover_message","title":"Publish request message to the bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/subscribe_consumer/","text":"Subscribe consumer from the event bus to subject self . _logger . info ( f \"Subscribing consumer { consumer_name } from the event bus to subject { topic } and adding it under NATS \" f \"queue { queue } ...\" ) subscribe_action self . _logger . info ( f \"Consumer { consumer_name } from the event bus subscribed successfully\" )","title":"Subscribe consumer"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/eventbus/subscribe_consumer/#subscribe-consumer-from-the-event-bus-to-subject","text":"self . _logger . info ( f \"Subscribing consumer { consumer_name } from the event bus to subject { topic } and adding it under NATS \" f \"queue { queue } ...\" ) subscribe_action self . _logger . info ( f \"Consumer { consumer_name } from the event bus subscribed successfully\" )","title":"Subscribe consumer from the event bus to subject"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/storage_managers/recover_message/","text":"Recover message from Redis self . _logger . info ( f \"Claiming message stored within Redis (payload: { msg } )...\" ) If the token to retrieve the original message is missing: self . _logger . exception ( f 'Key \"token\" was not found within the incoming payload { msg } ' ) END Invoke redis' get method Invoke redis' delete method self . _logger . info ( f 'Message successfully obtained from Redis' )","title":"Recover message"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/storage_managers/recover_message/#recover-message-from-redis","text":"self . _logger . info ( f \"Claiming message stored within Redis (payload: { msg } )...\" ) If the token to retrieve the original message is missing: self . _logger . exception ( f 'Key \"token\" was not found within the incoming payload { msg } ' ) END Invoke redis' get method Invoke redis' delete method self . _logger . info ( f 'Message successfully obtained from Redis' )","title":"Recover message from Redis"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/storage_managers/store_message/","text":"Store message to Redis self . _logger . info ( f 'Storing message within Redis...' ) Invoke redis' set method self . _logger . info ( f 'Message successfully stored within Redis' )","title":"Store message"},{"location":"logging/services/pyutils_automation/py36/igz/packages/eventbus/storage_managers/store_message/#store-message-to-redis","text":"self . _logger . info ( f 'Storing message within Redis...' ) Invoke redis' set method self . _logger . info ( f 'Message successfully stored within Redis' )","title":"Store message to Redis"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/_cb_with_action/","text":"Execute action for message self . _logger . info ( f 'Message received from topic { msg_subject } ' ) If there is no action for target subject: self . _logger . error ( f 'No ActionWrapper defined for topic { msg_subject } .' ) END Execute action for subject If execution failed: self . _logger . exception ( \"NATS Client Exception in client happened. \" f \"Error executing action { self . _topic_action [ msg_subject ] . target_function } \" f \"from { type ( self . _topic_action [ msg_subject ] . state_instance ) . __name__ } instance.\" )","title":" cb with action"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/_cb_with_action/#execute-action-for-message","text":"self . _logger . info ( f 'Message received from topic { msg_subject } ' ) If there is no action for target subject: self . _logger . error ( f 'No ActionWrapper defined for topic { msg_subject } .' ) END Execute action for subject If execution failed: self . _logger . exception ( \"NATS Client Exception in client happened. \" f \"Error executing action { self . _topic_action [ msg_subject ] . target_function } \" f \"from { type ( self . _topic_action [ msg_subject ] . state_instance ) . __name__ } instance.\" )","title":"Execute action for message"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/close_nats_connections/","text":"Close NATS connections self . _logger . info ( \"Draining connections...\" ) For each subscription bound to the NATS client: * Invoke nats-py's drain method self . _logger . info ( \"Connections drained\" ) If the NATS client is connected to NATS: self . _logger . info ( \"Closing connection...\" ) Invoke nats-py's close method self . _logger . info ( \"Connection closed\" )","title":"Close nats connections"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/close_nats_connections/#close-nats-connections","text":"self . _logger . info ( \"Draining connections...\" ) For each subscription bound to the NATS client: * Invoke nats-py's drain method self . _logger . info ( \"Connections drained\" ) If the NATS client is connected to NATS: self . _logger . info ( \"Closing connection...\" ) Invoke nats-py's close method self . _logger . info ( \"Connection closed\" )","title":"Close NATS connections"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/connect_to_nats/","text":"Connect to the messages bus self . _logger . info ( f \"Connecting client to NATS servers: { self . _config [ 'servers' ] } ...\" ) Invoke nats-py's connect method self . _logger . info ( f \"Connected to NATS servers successfully\" )","title":"Connect to nats"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/connect_to_nats/#connect-to-the-messages-bus","text":"self . _logger . info ( f \"Connecting client to NATS servers: { self . _config [ 'servers' ] } ...\" ) Invoke nats-py's connect method self . _logger . info ( f \"Connected to NATS servers successfully\" )","title":"Connect to the messages bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/publish/","text":"Publish message to the bus self . _logger . info ( f \"Publishing message to subject { topic } ...\" ) If NATS client bound to the action is not connected to NATS servers: self . _logger . warning ( f \"NATS client is disconnected from the NATS server. Resetting connection...\" ) close_nats_connections connect_to_nats Invoke nats-py's publish method The message will be consumed by a subscriber with interest in the subject. self . _logger . info ( f \"Message published to subject { topic } successfully\" ) Consume message from the bus execute_stateful_action is implicitly called on message arrival","title":"Publish"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/publish/#publish-message-to-the-bus","text":"self . _logger . info ( f \"Publishing message to subject { topic } ...\" ) If NATS client bound to the action is not connected to NATS servers: self . _logger . warning ( f \"NATS client is disconnected from the NATS server. Resetting connection...\" ) close_nats_connections connect_to_nats Invoke nats-py's publish method The message will be consumed by a subscriber with interest in the subject. self . _logger . info ( f \"Message published to subject { topic } successfully\" )","title":"Publish message to the bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/publish/#consume-message-from-the-bus","text":"execute_stateful_action is implicitly called on message arrival","title":"Consume message from the bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/rpc_request/","text":"Publish request message to the bus self . _logger . info ( f \"Publishing request message to subject { topic } ...\" ) If NATS client bound to the action is not connected to NATS servers: self . _logger . warning ( f \"NATS client is disconnected from the NATS server. Resetting connection...\" ) close_nats_connections connect_to_nats Invoke nats-py's timed_request method Wait for a response ... self . _logger . info ( f \"Got response message from subject { topic } \" ) Consume request execute_stateful_action is implicitly called on message arrival","title":"Rpc request"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/rpc_request/#publish-request-message-to-the-bus","text":"self . _logger . info ( f \"Publishing request message to subject { topic } ...\" ) If NATS client bound to the action is not connected to NATS servers: self . _logger . warning ( f \"NATS client is disconnected from the NATS server. Resetting connection...\" ) close_nats_connections connect_to_nats Invoke nats-py's timed_request method Wait for a response ... self . _logger . info ( f \"Got response message from subject { topic } \" )","title":"Publish request message to the bus"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/rpc_request/#consume-request","text":"execute_stateful_action is implicitly called on message arrival","title":"Consume request"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/subscribe_action/","text":"Subscribe action to subject self . _logger . info ( f \"Subscribing action { type ( action . state_instance ) . __name__ } to subject { topic } under NATS queue \" f \" { queue } ...\" ) If NATS client bound to the action is not connected to NATS servers: self . _logger . warning ( f \"NATS client is disconnected from the NATS server. Resetting connection...\" ) close_nats_connections connect_to_nats Invoke nats-py's subscribe method self . _logger . info ( f \"Action { type ( action . state_instance ) . __name__ } subscribed to subject { topic } successfully\" )","title":"Subscribe action"},{"location":"logging/services/pyutils_automation/py36/igz/packages/nats/clients/subscribe_action/#subscribe-action-to-subject","text":"self . _logger . info ( f \"Subscribing action { type ( action . state_instance ) . __name__ } to subject { topic } under NATS queue \" f \" { queue } ...\" ) If NATS client bound to the action is not connected to NATS servers: self . _logger . warning ( f \"NATS client is disconnected from the NATS server. Resetting connection...\" ) close_nats_connections connect_to_nats Invoke nats-py's subscribe method self . _logger . info ( f \"Action { type ( action . state_instance ) . __name__ } subscribed to subject { topic } successfully\" )","title":"Subscribe action to subject"},{"location":"logging/services/service-affecting-monitor/actions/_append_latest_trouble_to_ticket/","text":"Append latest trouble to ticket Documentation self._logger.info( f\"Appending Service Affecting trouble note to ticket {ticket_id} for {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}...\" ) if is_there_any_note_for_trouble self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}. A note for this trouble was already \" f\"appended to the ticket after the latest re-open (or ticket creation)\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Append note to ticket self._logger.info( f\"Service Affecting trouble note for {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully appended to ticket {ticket_id}!\" )","title":" append latest trouble to ticket"},{"location":"logging/services/service-affecting-monitor/actions/_append_latest_trouble_to_ticket/#append-latest-trouble-to-ticket-documentation","text":"self._logger.info( f\"Appending Service Affecting trouble note to ticket {ticket_id} for {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}...\" ) if is_there_any_note_for_trouble self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}. A note for this trouble was already \" f\"appended to the ticket after the latest re-open (or ticket creation)\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Append note to ticket self._logger.info( f\"Service Affecting trouble note for {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully appended to ticket {ticket_id}!\" )","title":"Append latest trouble to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_append_reminder_note/","text":"Append Reminder Note Documentation Launch append_note_to_ticket","title":" append reminder note"},{"location":"logging/services/service-affecting-monitor/actions/_append_reminder_note/#append-reminder-note-documentation","text":"Launch append_note_to_ticket","title":"Append Reminder Note Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_attempt_forward_to_asr/","text":"Attempt forward to asr Documentation self._logger.info( f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\" ) if link_interface_type != \"WIRED\" self._logger.info( f\"Link {interface} is of type {link_interface_type} and not WIRED. Attempting to forward \" f\"to HNOC...\" ) Forward ticket to hnoc queue self._logger.info( f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f\"following: \" f'{self._config.ASR_CONFIG[\"link_labels_blacklist\"]} ' f\"in the link label\" ) if not _should_be_forwarded_to_asr self._logger.info( f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to {target_queue}.\" ) Get ticket details if other_troubles_in_ticket self._logger.info( f\"Other service affecting troubles were found in ticket id {ticket_id}. Skipping forward\" f\"to asr...\" ) if task_result_note is not None self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{task_result}\"' ) Change detail work queue Append asr forwarding note","title":" attempt forward to asr"},{"location":"logging/services/service-affecting-monitor/actions/_attempt_forward_to_asr/#attempt-forward-to-asr-documentation","text":"self._logger.info( f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\" ) if link_interface_type != \"WIRED\" self._logger.info( f\"Link {interface} is of type {link_interface_type} and not WIRED. Attempting to forward \" f\"to HNOC...\" ) Forward ticket to hnoc queue self._logger.info( f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f\"following: \" f'{self._config.ASR_CONFIG[\"link_labels_blacklist\"]} ' f\"in the link label\" ) if not _should_be_forwarded_to_asr self._logger.info( f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to {target_queue}.\" ) Get ticket details if other_troubles_in_ticket self._logger.info( f\"Other service affecting troubles were found in ticket id {ticket_id}. Skipping forward\" f\"to asr...\" ) if task_result_note is not None self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{task_result}\"' ) Change detail work queue Append asr forwarding note","title":"Attempt forward to asr Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_bandwidth_check/","text":"Bandwidth Check Documentation self._logger.info(\"Looking for bandwidth issues...\") Get links metrics for bandwidth checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bandwidth issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed any bandwidth thresholds\" ) * Process bandwidth trouble self._logger.info(\"Finished looking for bandwidth issues!\")","title":" bandwidth check"},{"location":"logging/services/service-affecting-monitor/actions/_bandwidth_check/#bandwidth-check-documentation","text":"self._logger.info(\"Looking for bandwidth issues...\") Get links metrics for bandwidth checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bandwidth issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed any bandwidth thresholds\" ) * Process bandwidth trouble self._logger.info(\"Finished looking for bandwidth issues!\")","title":"Bandwidth Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_bouncing_check/","text":"Bouncing Check Documentation self._logger.info(\"Looking for bouncing issues...\") Get links metrics for bouncing checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bouncing issues. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if not events self._logger.info( f\"No events were found for {link_status['interface']} from {serial_number} \" f\"while looking for bouncing troubles\" ) if are_bouncing_events_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed bouncing thresholds\" ) * Process bouncing trouble self._logger.info(\"Finished looking for bouncing issues!\")","title":" bouncing check"},{"location":"logging/services/service-affecting-monitor/actions/_bouncing_check/#bouncing-check-documentation","text":"self._logger.info(\"Looking for bouncing issues...\") Get links metrics for bouncing checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bouncing issues. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if not events self._logger.info( f\"No events were found for {link_status['interface']} from {serial_number} \" f\"while looking for bouncing troubles\" ) if are_bouncing_events_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed bouncing thresholds\" ) * Process bouncing trouble self._logger.info(\"Finished looking for bouncing issues!\")","title":"Bouncing Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_create_affecting_ticket/","text":"Create affecting ticket Documentation self._logger.info( f\"Creating Service Affecting ticket to report a {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number}...\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting ticket will be created to report a {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Create affecting ticket self._logger.info( f\"Service Affecting ticket to report {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully created! Ticket ID is {ticket_id}\" ) Append note to ticket if trouble is not AffectingTroubles.BOUNCING if should_schedule_hnoc_forwarding Schedule forward to hnoc queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":" create affecting ticket"},{"location":"logging/services/service-affecting-monitor/actions/_create_affecting_ticket/#create-affecting-ticket-documentation","text":"self._logger.info( f\"Creating Service Affecting ticket to report a {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number}...\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting ticket will be created to report a {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Create affecting ticket self._logger.info( f\"Service Affecting ticket to report {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully created! Ticket ID is {ticket_id}\" ) Append note to ticket if trouble is not AffectingTroubles.BOUNCING if should_schedule_hnoc_forwarding Schedule forward to hnoc queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":"Create affecting ticket Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_forward_ticket_to_hnoc_queue/","text":"Forward ticket to HNOC Documentation self._logger.info( f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\" ) Get ticket details if ticket_details_response[\"status\"] not in range(200, 300) self._logger.error( f\"Getting ticket details of ticket_id: {ticket_id} and serial: {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\" ) if is_task_resolved self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is resolved. \" f\"Skipping forward to HNOC...\" ) self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is not resolved. \" f\"Forwarding to HNOC...\" ) Change detail work queue to hnoc if change_work_queue_response[\"status\"] not in range(200, 300) self._logger.error( f\"Failed to forward ticket_id: {ticket_id} and \" f\"serial: {serial_number} to HNOC Investigate due to bruin \" f\"returning {change_work_queue_response} when attempting to forward to HNOC.\" ) if Exception self._logger.error( f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\" )","title":" forward ticket to hnoc queue"},{"location":"logging/services/service-affecting-monitor/actions/_forward_ticket_to_hnoc_queue/#forward-ticket-to-hnoc-documentation","text":"self._logger.info( f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\" ) Get ticket details if ticket_details_response[\"status\"] not in range(200, 300) self._logger.error( f\"Getting ticket details of ticket_id: {ticket_id} and serial: {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\" ) if is_task_resolved self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is resolved. \" f\"Skipping forward to HNOC...\" ) self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is not resolved. \" f\"Forwarding to HNOC...\" ) Change detail work queue to hnoc if change_work_queue_response[\"status\"] not in range(200, 300) self._logger.error( f\"Failed to forward ticket_id: {ticket_id} and \" f\"serial: {serial_number} to HNOC Investigate due to bruin \" f\"returning {change_work_queue_response} when attempting to forward to HNOC.\" ) if Exception self._logger.error( f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\" )","title":"Forward ticket to HNOC Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_jitter_check/","text":"Jitter Check Documentation self._logger.info(\"Looking for jitter issues...\") Get links metrics for jitter checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking jitter issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_jitter_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed jitter thresholds\" ) * Process jitter trouble self._logger.info(\"Finished looking for jitter issues!\")","title":" jitter check"},{"location":"logging/services/service-affecting-monitor/actions/_jitter_check/#jitter-check-documentation","text":"self._logger.info(\"Looking for jitter issues...\") Get links metrics for jitter checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking jitter issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_jitter_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed jitter thresholds\" ) * Process jitter trouble self._logger.info(\"Finished looking for jitter issues!\")","title":"Jitter Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_latency_check/","text":"Latency Check Documentation self._logger.info(\"Looking for latency issues...\") Get links metrics for latency checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking latency issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_latency_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed latency thresholds\" ) * Process latency trouble self._logger.info(\"Finished looking for latency issues!\")","title":" latency check"},{"location":"logging/services/service-affecting-monitor/actions/_latency_check/#latency-check-documentation","text":"self._logger.info(\"Looking for latency issues...\") Get links metrics for latency checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking latency issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_latency_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed latency thresholds\" ) * Process latency trouble self._logger.info(\"Finished looking for latency issues!\")","title":"Latency Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_map_cached_edges_with_links_metrics_and_contact_info/","text":"Map cached edges with links metrics and contact info documentation for elem in links_metrics if not cached_edge self._logger.info(f\"No cached info was found for edge {serial_number}. Skipping...\")","title":" map cached edges with links metrics and contact info"},{"location":"logging/services/service-affecting-monitor/actions/_map_cached_edges_with_links_metrics_and_contact_info/#map-cached-edges-with-links-metrics-and-contact-info-documentation","text":"for elem in links_metrics if not cached_edge self._logger.info(f\"No cached info was found for edge {serial_number}. Skipping...\")","title":"Map cached edges with links metrics and contact info documentation"},{"location":"logging/services/service-affecting-monitor/actions/_packet_loss_check/","text":"Packet loss Check Documentation self._logger.info(\"Looking for packet loss issues...\") Get links metrics for packet loss checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking packet loss issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_packet_loss_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed packet loss thresholds\" ) * Process packet loss trouble self._logger.info(\"Finished looking for packet loss issues!\")","title":" packet loss check"},{"location":"logging/services/service-affecting-monitor/actions/_packet_loss_check/#packet-loss-check-documentation","text":"self._logger.info(\"Looking for packet loss issues...\") Get links metrics for packet loss checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking packet loss issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_packet_loss_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed packet loss thresholds\" ) * Process packet loss trouble self._logger.info(\"Finished looking for packet loss issues!\")","title":"Packet loss Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_affecting_trouble/","text":"Process affecting trouble Documentation self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number}\" ) Get open affecting tickets if open_affecting_ticket self._logger.info( f\"An open Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) if task_resolved self._logger.info( f\"Service Affecting ticket with ID {ticket_id} is open, but the task related to edge \" f\"{serial_number} is Resolved. Therefore, the ticket will be considered as Resolved.\" ) else Append latest trouble to ticket else no open_affecting_ticket self._logger.info(f\"No open Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not resolved_affecting_ticket Get resolved affecting tickets if not trouble_processed and resolved_affecting_ticket self._logger.info( f\"A resolved Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) Unresolve task for affecting_ticket else self._logger.info(f\"No resolved Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not open_affecting_ticket and not resolved_affecting_ticket: self._logger.info(f\"No open or resolved Service Affecting ticket was found for edge {serial_number}\") Create affecting ticket self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number} has been processed\" )","title":" process affecting trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_affecting_trouble/#process-affecting-trouble-documentation","text":"self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number}\" ) Get open affecting tickets if open_affecting_ticket self._logger.info( f\"An open Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) if task_resolved self._logger.info( f\"Service Affecting ticket with ID {ticket_id} is open, but the task related to edge \" f\"{serial_number} is Resolved. Therefore, the ticket will be considered as Resolved.\" ) else Append latest trouble to ticket else no open_affecting_ticket self._logger.info(f\"No open Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not resolved_affecting_ticket Get resolved affecting tickets if not trouble_processed and resolved_affecting_ticket self._logger.info( f\"A resolved Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) Unresolve task for affecting_ticket else self._logger.info(f\"No resolved Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not open_affecting_ticket and not resolved_affecting_ticket: self._logger.info(f\"No open or resolved Service Affecting ticket was found for edge {serial_number}\") Create affecting ticket self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number} has been processed\" )","title":"Process affecting trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_bandwidth_trouble/","text":"Process bandwidth trouble Documentation Launch _process_affecting_trouble","title":" process bandwidth trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_bandwidth_trouble/#process-bandwidth-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process bandwidth trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_bouncing_trouble/","text":"Process bouncing trouble Documentation Launch _process_affecting_trouble Launch _attempt_forward_to_asr","title":" process bouncing trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_bouncing_trouble/#process-bouncing-trouble-documentation","text":"Launch _process_affecting_trouble Launch _attempt_forward_to_asr","title":"Process bouncing trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_jitter_trouble/","text":"Process jitter trouble Documentation Launch _process_affecting_trouble","title":" process jitter trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_jitter_trouble/#process-jitter-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process jitter trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_latency_trouble/","text":"Process latency trouble Documentation Launch _process_affecting_trouble","title":" process latency trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_latency_trouble/#process-latency-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process latency trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_packet_loss_trouble/","text":"Process packet loss trouble Documentation Launch _process_affecting_trouble","title":" process packet loss trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_packet_loss_trouble/#process-packet-loss-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process packet loss trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_for_edge/","text":"Run autoresolve for edge Documentation self._logger.info(f\"Starting autoresolve for edge {serial_number}...\") if all_metrics_within_thresholds is empty self._logger.info( f\"At least one metric of edge {serial_number} is not within the threshold. Skipping autoresolve...\" ) Get open affecting tickets if affecting tickets is empty self._logger.info( f\"No affecting ticket found for edge with serial number {serial_number}. Skipping autoresolve...\" ) for affecting_ticket in affecting_tickets if not was_ticket_created_by_automation_engine self._logger.info(f\"Ticket {affecting_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") Get ticket details if remove_auto_resolution_restrictions_for_byob self._logger.info( f\"Task for serial {serial_number} in ticket {affecting_ticket_id} is in the IPA Investigate\" f\" queue. Skipping checks for max auto-resolves and grace period to auto-resolve after last\" f\" documented trouble...\" ) else if not last_trouble_was_detected_recently self._logger.info( f\"Edge with serial number {serial_number} has been under an affecting trouble for a long \" f\"time, so the detail of ticket {affecting_ticket_id} related to it will not be \" f\"autoresolved. Skipping autoresolve...\" ) if is_autoresolve_threshold_maxed_out self._logger.info( f\"Limit to autoresolve detail of ticket {affecting_ticket_id} related to serial \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) if is_task_resolved self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial {serial_number} is already \" \"resolved. Skipping autoresolve...\" ) if working_environment != \"production\" self._logger.info( f\"Skipping autoresolve for detail of ticket {affecting_ticket_id} related to serial number \" f\"{serial_number} since the current environment is {working_environment.upper()}\" ) self._logger.info( f\"Autoresolving detail of ticket {affecting_ticket_id} related to serial number {serial_number}...\" ) * Unpause ticket detail * Resolve ticket * Append autoresolve note to ticket self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial number {serial_number} was autoresolved!\" ) self._logger.info(f\"Finished autoresolve for edge {serial_number}!\")","title":" run autoresolve for edge"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_for_edge/#run-autoresolve-for-edge-documentation","text":"self._logger.info(f\"Starting autoresolve for edge {serial_number}...\") if all_metrics_within_thresholds is empty self._logger.info( f\"At least one metric of edge {serial_number} is not within the threshold. Skipping autoresolve...\" ) Get open affecting tickets if affecting tickets is empty self._logger.info( f\"No affecting ticket found for edge with serial number {serial_number}. Skipping autoresolve...\" ) for affecting_ticket in affecting_tickets if not was_ticket_created_by_automation_engine self._logger.info(f\"Ticket {affecting_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") Get ticket details if remove_auto_resolution_restrictions_for_byob self._logger.info( f\"Task for serial {serial_number} in ticket {affecting_ticket_id} is in the IPA Investigate\" f\" queue. Skipping checks for max auto-resolves and grace period to auto-resolve after last\" f\" documented trouble...\" ) else if not last_trouble_was_detected_recently self._logger.info( f\"Edge with serial number {serial_number} has been under an affecting trouble for a long \" f\"time, so the detail of ticket {affecting_ticket_id} related to it will not be \" f\"autoresolved. Skipping autoresolve...\" ) if is_autoresolve_threshold_maxed_out self._logger.info( f\"Limit to autoresolve detail of ticket {affecting_ticket_id} related to serial \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) if is_task_resolved self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial {serial_number} is already \" \"resolved. Skipping autoresolve...\" ) if working_environment != \"production\" self._logger.info( f\"Skipping autoresolve for detail of ticket {affecting_ticket_id} related to serial number \" f\"{serial_number} since the current environment is {working_environment.upper()}\" ) self._logger.info( f\"Autoresolving detail of ticket {affecting_ticket_id} related to serial number {serial_number}...\" ) * Unpause ticket detail * Resolve ticket * Append autoresolve note to ticket self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial number {serial_number} was autoresolved!\" ) self._logger.info(f\"Finished autoresolve for edge {serial_number}!\")","title":"Run autoresolve for edge Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_process/","text":"Run autoresolve process Documentation self._logger.info(\"Starting auto-resolve process...\") * Get links metrics for autoresolve if link metrics is empty self._logger.info(\"List of links metrics arrived empty while running auto-resolve process. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info self._logger.info(f\"Running auto-resolve for {len(edges_with_links_info)} edges\") autoresolve_tasks = [ Run autoresolve for edge for edge in edges_with_links_info] self._logger.info(\"Auto-resolve process finished!\")","title":" run autoresolve process"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_process/#run-autoresolve-process-documentation","text":"self._logger.info(\"Starting auto-resolve process...\") * Get links metrics for autoresolve if link metrics is empty self._logger.info(\"List of links metrics arrived empty while running auto-resolve process. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info self._logger.info(f\"Running auto-resolve for {len(edges_with_links_info)} edges\") autoresolve_tasks = [ Run autoresolve for edge for edge in edges_with_links_info] self._logger.info(\"Auto-resolve process finished!\")","title":"Run autoresolve process Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_schedule_forward_to_hnoc_queue/","text":"Schedule forward to hnoc queue Documentation self._logger.info( f\"Scheduling HNOC forwarding for ticket_id: {ticket_id} and serial: {serial_number} \" f\"to happen at timestamp: {forward_task_run_date}\" ) Schedule Forward ticket to hnoc queue","title":" schedule forward to hnoc queue"},{"location":"logging/services/service-affecting-monitor/actions/_schedule_forward_to_hnoc_queue/#schedule-forward-to-hnoc-queue-documentation","text":"self._logger.info( f\"Scheduling HNOC forwarding for ticket_id: {ticket_id} and serial: {serial_number} \" f\"to happen at timestamp: {forward_task_run_date}\" ) Schedule Forward ticket to hnoc queue","title":"Schedule forward to hnoc queue Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_service_affecting_monitor_process/","text":"Service Affecting process Documentation self._logger.info(f\"Starting Service Affecting Monitor process now...\") Get cache for affecting monitoring Check if customer cache is empty self._logger.info(\"Got an empty customer cache. Process cannot keep going.\") Latency Check Packet Loss Check Jitter Check Bandwidth Check Bouncing Check Run Autoresolve process self._logger.info(f\"Finished processing all links! Took {round((time.time() - start_time) / 60, 2)} minutes\")","title":" service affecting monitor process"},{"location":"logging/services/service-affecting-monitor/actions/_service_affecting_monitor_process/#service-affecting-process-documentation","text":"self._logger.info(f\"Starting Service Affecting Monitor process now...\") Get cache for affecting monitoring Check if customer cache is empty self._logger.info(\"Got an empty customer cache. Process cannot keep going.\") Latency Check Packet Loss Check Jitter Check Bandwidth Check Bouncing Check Run Autoresolve process self._logger.info(f\"Finished processing all links! Took {round((time.time() - start_time) / 60, 2)} minutes\")","title":"Service Affecting process Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_structure_links_metrics/","text":"Structure links metrics Documentation for link_info in links_metrics if edge_state is None self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) if edge_state == \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":" structure links metrics"},{"location":"logging/services/service-affecting-monitor/actions/_structure_links_metrics/#structure-links-metrics-documentation","text":"for link_info in links_metrics if edge_state is None self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) if edge_state == \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Structure links metrics Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_unresolve_task_for_affecting_ticket/","text":"Unresolve task for affecting ticket Documentation self._logger.info( f\"Unresolving task related to edge {serial_number} of Service Affecting ticket {ticket_id} due to a \" f\"{trouble.value} trouble detected in interface {interface}...\" ) if not working_environment == \"production\" self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} will not be unresolved \" f\"because of the {trouble.value} trouble detected in interface {interface}, since the current \" f\"environment is {working_environment.upper()}\" ) Open ticket self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} was successfully \" f\"unresolved! The cause was a {trouble.value} trouble detected in interface {interface}\" ) Append note to ticket if should_schedule_hnoc_forwarding self._logger.info( f\"Forwarding reopened task for serial {serial_number} of ticket {ticket_id} to the HNOC queue...\" ) Schedule forward to HNOC queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for the reopened task of ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":" unresolve task for affecting ticket"},{"location":"logging/services/service-affecting-monitor/actions/_unresolve_task_for_affecting_ticket/#unresolve-task-for-affecting-ticket-documentation","text":"self._logger.info( f\"Unresolving task related to edge {serial_number} of Service Affecting ticket {ticket_id} due to a \" f\"{trouble.value} trouble detected in interface {interface}...\" ) if not working_environment == \"production\" self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} will not be unresolved \" f\"because of the {trouble.value} trouble detected in interface {interface}, since the current \" f\"environment is {working_environment.upper()}\" ) Open ticket self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} was successfully \" f\"unresolved! The cause was a {trouble.value} trouble detected in interface {interface}\" ) Append note to ticket if should_schedule_hnoc_forwarding self._logger.info( f\"Forwarding reopened task for serial {serial_number} of ticket {ticket_id} to the HNOC queue...\" ) Schedule forward to HNOC queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for the reopened task of ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":"Unresolve task for affecting ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_asr_forwarding_note_to_ticket/","text":"Append asr forwarding note to ticket Documentation Launch append_note_to_ticket","title":"Append asr forwarding note to ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_asr_forwarding_note_to_ticket/#append-asr-forwarding-note-to-ticket-documentation","text":"Launch append_note_to_ticket","title":"Append asr forwarding note to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket Documentation Launch append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket-documentation","text":"Launch append_note_to_ticket","title":"Append autoresolve note to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket Documentation self._logger.info(f\"Appending note to ticket {ticket_id}... Note contents: {note}\") self._logger.info(f\"Note appended to ticket {ticket_id}!\") if Exception self._logger.error( f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\" ) if response_status not in range(200, 300) self._logger.error( f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\" )","title":"Append note to ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket-documentation","text":"self._logger.info(f\"Appending note to ticket {ticket_id}... Note contents: {note}\") self._logger.info(f\"Note appended to ticket {ticket_id}!\") if Exception self._logger.error( f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\" ) if response_status not in range(200, 300) self._logger.error( f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\" )","title":"Append note to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue/","text":"Change detail work queue Documentation self._logger.info( f\"Changing task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) if Exception self._logger.error( f\"An error occurred when changing task result of detail {detail_id} / serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\" ) else self._logger.error( f\"Error while changing task result of detail {detail_id} / serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Change detail work queue"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue/#change-detail-work-queue-documentation","text":"self._logger.info( f\"Changing task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) if Exception self._logger.error( f\"An error occurred when changing task result of detail {detail_id} / serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\" ) else self._logger.error( f\"Error while changing task result of detail {detail_id} / serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Change detail work queue Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/","text":"Change detail work queue to hnoc Documentation Launch change_detail_work_queue","title":"Change detail work queue to hnoc"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/#change-detail-work-queue-to-hnoc-documentation","text":"Launch change_detail_work_queue","title":"Change detail work queue to hnoc Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/","text":"Create Affecting ticket Documentation self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create affecting ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/#create-affecting-ticket-documentation","text":"self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create Affecting ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/","text":"Get affecting tickets Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/#get-affecting-tickets","text":"Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/","text":"Get open affecting tickets Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/#get-open-affecting-tickets","text":"Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_resolved_affecting_tickets/","text":"Get resolved affecting tickets Launch Get affecting tickets","title":"Get resolved affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_resolved_affecting_tickets/#get-resolved-affecting-tickets","text":"Launch Get affecting tickets","title":"Get resolved affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get Ticket details Documentation self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get ticket details"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details-documentation","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get Ticket details Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_tickets/","text":"Get tickets Documentation if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_tickets/#get-tickets-documentation","text":"if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket Documentation self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") self._logger.info(f\"Ticket {ticket_id} opened!\") if Exception self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Open ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/open_ticket/#open-ticket-documentation","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") self._logger.info(f\"Ticket {ticket_id} opened!\") if Exception self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Open ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/post_notification_email_milestone/","text":"Post notification email milestone Documentation self._logger.info( f\"Sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...\" ) if Exception self._logger.error( f\"An error occurred when sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...-> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Email sent for ticket {ticket_id}, service number {service_number} \" f\"and notification type {notification_type}!\" ) else self._logger.error( f\"Error while sending email for ticket {ticket_id}, \" f\"service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Post notification email milestone"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/post_notification_email_milestone/#post-notification-email-milestone-documentation","text":"self._logger.info( f\"Sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...\" ) if Exception self._logger.error( f\"An error occurred when sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...-> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Email sent for ticket {ticket_id}, service number {service_number} \" f\"and notification type {notification_type}!\" ) else self._logger.error( f\"Error while sending email for ticket {ticket_id}, \" f\"service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Post notification email milestone Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket Documentation self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") if Exception self._logger.erorr(f\"An error occurred while resolving detail {detail_id} of ticket {ticket_id} -> {e}\") if response_status in range(200, 300) self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\") else self._logger.error( f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Resolve ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket-documentation","text":"self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") if Exception self._logger.erorr(f\"An error occurred while resolving detail {detail_id} of ticket {ticket_id} -> {e}\") if response_status in range(200, 300) self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\") else self._logger.error( f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Resolve ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/","text":"Send initial email milestone notification Documentation Launch post_notification_email_milestone","title":"Send initial email milestone notification"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/#send-initial-email-milestone-notification-documentation","text":"Launch post_notification_email_milestone","title":"Send initial email milestone notification Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail Documentation self._logger.info(f\"Unpausing detail of ticket {ticket_id} related to serial number {service_number}...\") if Exception self._logger.error( f\"An error occurred when unpausing detail of ticket {ticket_id} related to serial number \" f\"{service_number}. Error: {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Detail of ticket {ticket_id} related to serial number {service_number}) was unpaused!\" ) else self._logger.error( f\"Error while unpausing detail of ticket {ticket_id} related to serial number {service_number}) in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\" )","title":"Unpause ticket detail"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail-documentation","text":"self._logger.info(f\"Unpausing detail of ticket {ticket_id} related to serial number {service_number}...\") if Exception self._logger.error( f\"An error occurred when unpausing detail of ticket {ticket_id} related to serial number \" f\"{service_number}. Error: {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Detail of ticket {ticket_id} related to serial number {service_number}) was unpaused!\" ) else self._logger.error( f\"Error while unpausing detail of ticket {ticket_id} related to serial number {service_number}) in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\" )","title":"Unpause ticket detail Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache/","text":"Get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/","text":"Get cache for affecting Documentation Launch get_cache","title":"Get cache for affecting monitoring"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/#get-cache-for-affecting-documentation","text":"Launch get_cache","title":"Get cache for affecting Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_all_links_metrics/","text":"Get all links metrics Documentation for host in self._config.MONITOR_CONFIG[\"velo_filter\"] * Gets links metrics by host * if status from get_links_metrics_by_host return is not in range(200, 300) self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_all_links_metrics/#get-all-links-metrics-documentation","text":"for host in self._config.MONITOR_CONFIG[\"velo_filter\"] * Gets links metrics by host * if status from get_links_metrics_by_host return is not in range(200, 300) self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_enterprise_events/","text":"Get enterprise events Documentation self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) if Exception : self._logger.error( f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) else self._logger.error( f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\" )","title":"Get enterprise events"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_enterprise_events/#get-enterprise-events-documentation","text":"self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) if Exception : self._logger.error( f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) else self._logger.error( f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\" )","title":"Get enterprise events Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/","text":"Get events by serial and interface Documentation for host in edges_by_host_and_enterprise for enterprise_id in edges_by_enterprise Get enterprise events for event in enterprise_events if not matching_edge self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/#get-events-by-serial-and-interface-documentation","text":"for host in edges_by_host_and_enterprise for enterprise_id in edges_by_enterprise Get enterprise events for event in enterprise_events if not matching_edge self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_by_host/","text":"Get links metrics by host Documentation self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) self._logger.info(f\"Got links metrics from Velocloud host {host}!\") if Exception : self._logger.error(f\"An error occurred when requesting links metrics from Velocloud -> {e}\") if response status not in range(200, 300) self._logger.error( f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Get links metrics by host"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_by_host/#get-links-metrics-by-host-documentation","text":"self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) self._logger.info(f\"Got links metrics from Velocloud host {host}!\") if Exception : self._logger.error(f\"An error occurred when requesting links metrics from Velocloud -> {e}\") if response status not in range(200, 300) self._logger.error( f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Get links metrics by host Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/","text":"Get links metrics for autoresolve Documentation Launch get_all_links_metrics","title":"Get links metrics for autoresolve"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/#get-links-metrics-for-autoresolve-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for autoresolve Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bandwidth_checks/","text":"Get links metrics for bandwidth checks Documentation Launch get_all_links_metrics","title":"Get links metrics for bandwidth checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bandwidth_checks/#get-links-metrics-for-bandwidth-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for bandwidth checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bouncing_checks/","text":"Get links metrics for bouncing checks Documentation Launch get_all_links_metrics","title":"Get links metrics for bouncing checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bouncing_checks/#get-links-metrics-for-bouncing-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for bouncing checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_jitter_checks/","text":"Get links metrics for jitter checks Documentation Launch get_all_links_metrics","title":"Get links metrics for jitter checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_jitter_checks/#get-links-metrics-for-jitter-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for jitter checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_latency_checks/","text":"Get links metrics for latency checks Documentation Launch get_all_links_metrics","title":"Get links metrics for latency checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_latency_checks/#get-links-metrics-for-latency-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for latency checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_packet_loss_checks/","text":"Get links metrics for packet loss checks Documentation Launch get_all_links_metrics","title":"Get links metrics for packet loss checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_packet_loss_checks/#get-links-metrics-for-packet-loss-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for packet loss checks Documentation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_reminder_note/","text":"Append reminder note append_note_to_ticket","title":" append reminder note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_reminder_note/#append-reminder-note","text":"append_note_to_ticket","title":"Append reminder note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_triage_note/","text":"Append triage note get_last_edge_events If status not OK: self._logger.warning(f\"Don't found last edge events for edge id: {edge_full_id}. Skipping append triage \" f\"note ...\") get_ticket_details If status not OK: self._logger.warning(f\"Don't found ticket details for ticket id: {ticket_id}. Skipping append triage \" f\"note ...\") build_triage_note self._logger.info(f\"Appending triage note to detail {ticket_detail_id} (serial {serial_number}) of ticket {ticket_id}...\") append_triage_note","title":" append triage note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_triage_note/#append-triage-note","text":"get_last_edge_events If status not OK: self._logger.warning(f\"Don't found last edge events for edge id: {edge_full_id}. Skipping append triage \" f\"note ...\") get_ticket_details If status not OK: self._logger.warning(f\"Don't found ticket details for ticket id: {ticket_id}. Skipping append triage \" f\"note ...\") build_triage_note self._logger.info(f\"Appending triage note to detail {ticket_detail_id} (serial {serial_number}) of ticket {ticket_id}...\") append_triage_note","title":"Append triage note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_forward_to_asr/","text":"Attempt forward to asr self._logger.info(f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\") * If faulty edge: self._logger.info(f\"Outage of serial {serial_number} is caused by a faulty edge. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Searching serial {serial_number} for any wired links\") * If not wired links: self._logger.info(f\"No wired links are disconnected for serial {serial_number}. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f'following: {self._config.MONITOR_CONFIG[\"blacklisted_link_labels_for_asr_forwards\"]} ' f\"in the link label\") * If not whitelist links: self._logger.info(f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to ASR Investigate.\") * get_ticket_details * If status not Ok: self._logger.info(f\"Bad status calling get ticket details. Skipping forward asr ...\") self._logger.info(f\"Notes of ticket {ticket_id}: {notes_from_outage_ticket}\") * If task result note: self._logger.info(f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{target_queue}\"') * change_detail_work_queue * If status of change detail work queue in Ok: * append_asr_forwarding_note","title":" attempt forward to asr"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_forward_to_asr/#attempt-forward-to-asr","text":"self._logger.info(f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\") * If faulty edge: self._logger.info(f\"Outage of serial {serial_number} is caused by a faulty edge. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Searching serial {serial_number} for any wired links\") * If not wired links: self._logger.info(f\"No wired links are disconnected for serial {serial_number}. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f'following: {self._config.MONITOR_CONFIG[\"blacklisted_link_labels_for_asr_forwards\"]} ' f\"in the link label\") * If not whitelist links: self._logger.info(f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to ASR Investigate.\") * get_ticket_details * If status not Ok: self._logger.info(f\"Bad status calling get ticket details. Skipping forward asr ...\") self._logger.info(f\"Notes of ticket {ticket_id}: {notes_from_outage_ticket}\") * If task result note: self._logger.info(f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{target_queue}\"') * change_detail_work_queue * If status of change detail work queue in Ok: * append_asr_forwarding_note","title":"Attempt forward to asr"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_ticket_creation/","text":"Attempt ticket creation self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") create_outage_ticket self._logger.info(f\"[{outage_type.value}] Bruin response for ticket creation for edge {edge}: \" f\"{ticket_creation_response}\") If status is OK: self._logger.info(f\"[{outage_type.value}] Successfully created outage ticket for edge {edge}.\") _append_triage_note _change_severity If should schedule forward to hnoc queue: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email milestone status not ok: self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Else: _append_reminder_note If append reminder note status is not ok: self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" ) _check_for_digi_reboot If status 409: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} already has an outage ticket in \" f\"progress (ID = {ticket_id}). Skipping outage ticket creation for \" \"this edge...\") _change_ticket_severity If change severity is different to NOT_CHANGED: If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") get_ticket_details _send_reminder * * * * * * * * * * * * ** MIRAR ESTO _check_for_failed_digi_reboot _attempt_forward_to_asr If status 471: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Re-opening ticket...\") _reopen_outage_ticket _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append reminder is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 492 self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 473: self._logger.info(f\"[{outage_type.value}] There is a resolve outage ticket for the same location of faulty \" f\"edge {serial_number} (ticket ID = {ticket_id}). The ticket was \" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email status is no Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\")","title":" attempt ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_ticket_creation/#attempt-ticket-creation","text":"self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") create_outage_ticket self._logger.info(f\"[{outage_type.value}] Bruin response for ticket creation for edge {edge}: \" f\"{ticket_creation_response}\") If status is OK: self._logger.info(f\"[{outage_type.value}] Successfully created outage ticket for edge {edge}.\") _append_triage_note _change_severity If should schedule forward to hnoc queue: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email milestone status not ok: self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Else: _append_reminder_note If append reminder note status is not ok: self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" ) _check_for_digi_reboot If status 409: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} already has an outage ticket in \" f\"progress (ID = {ticket_id}). Skipping outage ticket creation for \" \"this edge...\") _change_ticket_severity If change severity is different to NOT_CHANGED: If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") get_ticket_details _send_reminder * * * * * * * * * * * * ** MIRAR ESTO _check_for_failed_digi_reboot _attempt_forward_to_asr If status 471: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Re-opening ticket...\") _reopen_outage_ticket _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append reminder is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 492 self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 473: self._logger.info(f\"[{outage_type.value}] There is a resolve outage ticket for the same location of faulty \" f\"edge {serial_number} (ticket ID = {ticket_id}). The ticket was \" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email status is no Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\")","title":"Attempt ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_change_ticket_severity/","text":"Change ticket severity self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") If is a faulty edge: self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that edge {serial_number} is offline.\") change_ticket_severity_for_offline_edge Else: If check ticket tasks get_ticket_details If response status is not OK: self._logger.warning(f\"Bad response calling get ticket details for ticket id: {ticket_id}. \" f\"The ticket severity don't change\") If ticket have multiple unresolved task self._logger.info(f\"Severity level of ticket {ticket_id} will remain the same, as the root cause of the outage \" f\"issue is that at least one link of edge {serial_number} is disconnected, and this ticket \" f\"has multiple unresolved tasks.\") self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that at least one link of edge {serial_number} is disconnected, and this ticket has a single \" \"unresolved task.\") get_ticket_details self._logger.warning( f\"Bad response calling get ticket for ticket id: {ticket_id}. The ticket severity don't change!\") If ticket already in severity level: self._logger.info( f\"Ticket {ticket_id} is already in severity level {target_severity}, so there is no need \" \"to change it.\") If change severity task response is not ok self._logger.info( f\"Bad response for change severity task. The ticket severity don't change\") self._logger.info( f\"Finished changing severity level of ticket {ticket_id} to {target_severity}!\" )","title":" change ticket severity"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_change_ticket_severity/#change-ticket-severity","text":"self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") If is a faulty edge: self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that edge {serial_number} is offline.\") change_ticket_severity_for_offline_edge Else: If check ticket tasks get_ticket_details If response status is not OK: self._logger.warning(f\"Bad response calling get ticket details for ticket id: {ticket_id}. \" f\"The ticket severity don't change\") If ticket have multiple unresolved task self._logger.info(f\"Severity level of ticket {ticket_id} will remain the same, as the root cause of the outage \" f\"issue is that at least one link of edge {serial_number} is disconnected, and this ticket \" f\"has multiple unresolved tasks.\") self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that at least one link of edge {serial_number} is disconnected, and this ticket has a single \" \"unresolved task.\") get_ticket_details self._logger.warning( f\"Bad response calling get ticket for ticket id: {ticket_id}. The ticket severity don't change!\") If ticket already in severity level: self._logger.info( f\"Ticket {ticket_id} is already in severity level {target_severity}, so there is no need \" \"to change it.\") If change severity task response is not ok self._logger.info( f\"Bad response for change severity task. The ticket severity don't change\") self._logger.info( f\"Finished changing severity level of ticket {ticket_id} to {target_severity}!\" )","title":"Change ticket severity"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_digi_reboot/","text":"Check for digi reboot self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * reboot_link * If status of reboot link is not Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note * If status not Ok: self._logger.warning(f\" Bad status calling to append digi reboot note. Can't append the note\")","title":" check for digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_digi_reboot/#check-for-digi-reboot","text":"self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * reboot_link * If status of reboot link is not Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note * If status not Ok: self._logger.warning(f\" Bad status calling to append digi reboot note. Can't append the note\")","title":"Check for digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_failed_digi_reboot/","text":"Check for failed digi reboot self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * for link in digi_links: * get_ticket_details * If status is not Ok: self._logger.info(f\"Bad status calling to get ticket details checking failed digi reboot.\" f\" Skipping link ...\") * If not find digi note: self._logger.info(f\"No DiGi note was found for ticket {ticket_id}\") * If rebooted recently: self._logger.info(f\"The last DiGi reboot attempt for Edge {serial_number} did not occur \" f'{self._config.MONITOR_CONFIG[\"last_digi_reboot_seconds\"] / 60} or more mins ago.') * If interface note is same that link: * If not find wireless: self._logger.info(f'Task results has already been changed to \"{target_queue}\"') * change_detail_work_queue * If status Ok: * append_task_result_change_note * Else: * reboot_link * If reboot link status Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note","title":" check for failed digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_failed_digi_reboot/#check-for-failed-digi-reboot","text":"self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * for link in digi_links: * get_ticket_details * If status is not Ok: self._logger.info(f\"Bad status calling to get ticket details checking failed digi reboot.\" f\" Skipping link ...\") * If not find digi note: self._logger.info(f\"No DiGi note was found for ticket {ticket_id}\") * If rebooted recently: self._logger.info(f\"The last DiGi reboot attempt for Edge {serial_number} did not occur \" f'{self._config.MONITOR_CONFIG[\"last_digi_reboot_seconds\"] / 60} or more mins ago.') * If interface note is same that link: * If not find wireless: self._logger.info(f'Task results has already been changed to \"{target_queue}\"') * change_detail_work_queue * If status Ok: * append_task_result_change_note * Else: * reboot_link * If reboot link status Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note","title":"Check for failed digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_map_cached_edges_with_edges_status/","text":"Map cached edges with edges status Documentation for edge in edges: If not edge status: self._logger.info(f'No edge status was found for cached edge {cached_edge[\"serial_number\"]}. ' \"Skipping...\") If host == metvco03.mettel.net and enterprise id == 124: self._logger.info(f\"Edge {edge} was appended to the list of edges that have no status but\" f\"are in the customer cache.\")","title":" map cached edges with edges status"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_map_cached_edges_with_edges_status/#map-cached-edges-with-edges-status-documentation","text":"for edge in edges: If not edge status: self._logger.info(f'No edge status was found for cached edge {cached_edge[\"serial_number\"]}. ' \"Skipping...\") If host == metvco03.mettel.net and enterprise id == 124: self._logger.info(f\"Edge {edge} was appended to the list of edges that have no status but\" f\"are in the customer cache.\")","title":"Map cached edges with edges status Documentation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_outage_monitoring_process/","text":"Outage monitor process self._logger.info(f\"[outage_monitoring_process] Start with map cache!\") Get cache for outage monitor Check if cache status is not 200 self._logger.warning(\"Not found cache for service outage. Stop the outage monitoring process\" self._logger.info(\"[outage_monitoring_process] Ignoring blacklisted edges...\") self._logger.info(f\"List of serials from customer cache: {serials_for_monitoring}\") self._logger.info(\"[outage_monitoring_process] Creating list of whitelisted serials for autoresolve...\") self._logger.info(\"[outage_monitoring_process] Splitting cache by host\") self._logger.info(\"[outage_monitoring_process] Cache split\") _process_velocloud_host self._logger.info( f\"[outage_monitoring_process] Outage monitoring process finished! Elapsed time:\" f\"{round((stop - start) / 60, 2)} minutes\" )","title":" outage monitoring process"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_outage_monitoring_process/#outage-monitor-process","text":"self._logger.info(f\"[outage_monitoring_process] Start with map cache!\") Get cache for outage monitor Check if cache status is not 200 self._logger.warning(\"Not found cache for service outage. Stop the outage monitoring process\" self._logger.info(\"[outage_monitoring_process] Ignoring blacklisted edges...\") self._logger.info(f\"List of serials from customer cache: {serials_for_monitoring}\") self._logger.info(\"[outage_monitoring_process] Creating list of whitelisted serials for autoresolve...\") self._logger.info(\"[outage_monitoring_process] Splitting cache by host\") self._logger.info(\"[outage_monitoring_process] Cache split\") _process_velocloud_host self._logger.info( f\"[outage_monitoring_process] Outage monitoring process finished! Elapsed time:\" f\"{round((stop - start) / 60, 2)} minutes\" )","title":"Outage monitor process"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_process_velocloud_host/","text":"Process velocloud host Documentation self._logger.info(f\"Processing {len(host_cache)} edges in Velocloud {host}...\") get_link_with_edge_info If status get link with edge info not OK: self._logger.warning(f\"Not found links with edge info for host: {host}. Stop process velocloud host\") get_network_enterprises If status get network enterprises not OK: self._logger.warning(f\"Not found network enterprises for host: {host}. Stop process velocloud host\") self._logger.warning(f\"Link status with edge info from Velocloud: {links_with_edge_info}\") grouped_links_by_edge self._logger.info( \"Adding HA info to existing edges, and putting standby edges under monitoring as if they were \" \"standalone edges...\" ) map_edges_with_ha_info self._logger.info(f\"Service Outage monitoring is about to check {len(all_edges)} edges\") self._logger.info(f\"{len(serials_with_ha_disabled)} edges have HA disabled: {serials_with_ha_disabled}\") self._logger.info(f\"{len(serials_with_ha_enabled)} edges have HA enabled: {serials_with_ha_enabled}\") self._logger.info(f\"{len(primary_serials)} edges are the primary of a HA pair: {primary_serials}\") self._logger.info(f\"{len(standby_serials)} edges are the standby of a HA pair: {standby_serials}\") map_cached_edges_with_edges_status self._logger.info(f\"Mapped cache serials with status: {mapped_serials_w_status}\") For outage in outages: self._logger.info(f'{outage_type.value} serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in down_edges]}') self._logger.info( f\"{outage_type.value} serials that should be documented: \" f'{[e[\"status\"][\"edgeSerialNumber\"] for e in relevant_down_edges]}' ) If relevant down edges: self._logger.info(f\"{len(relevant_down_edges)} edges were detected in {outage_type.value} state.\") attempt_ticket_creation If ticket creation None: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge \" f\"with Business Grade Link(s): {ex}\") _schedule_recheck_job_for_edges Else: self._logger.info( f\"No edges were detected in {outage_type.value} state. \" f\"No ticket creations will trigger for this outage type\" ) self._logger.info(f'Healthy serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in healthy_edges]}') IF healthy edges: self._logger.info( f\"{len(healthy_edges)} edges were detected in healthy state. Running autoresolve for all of them...\" ) _run_ticket_autoresolve_for_edge Else: self._logger.info( \"No edges were detected in healthy state. Autoresolve won't be triggered\" ) self._logger.info(f\"Elapsed time processing edges in host {host}: {round((stop - start) / 60, 2)} minutes\")","title":" process velocloud host"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_process_velocloud_host/#process-velocloud-host-documentation","text":"self._logger.info(f\"Processing {len(host_cache)} edges in Velocloud {host}...\") get_link_with_edge_info If status get link with edge info not OK: self._logger.warning(f\"Not found links with edge info for host: {host}. Stop process velocloud host\") get_network_enterprises If status get network enterprises not OK: self._logger.warning(f\"Not found network enterprises for host: {host}. Stop process velocloud host\") self._logger.warning(f\"Link status with edge info from Velocloud: {links_with_edge_info}\") grouped_links_by_edge self._logger.info( \"Adding HA info to existing edges, and putting standby edges under monitoring as if they were \" \"standalone edges...\" ) map_edges_with_ha_info self._logger.info(f\"Service Outage monitoring is about to check {len(all_edges)} edges\") self._logger.info(f\"{len(serials_with_ha_disabled)} edges have HA disabled: {serials_with_ha_disabled}\") self._logger.info(f\"{len(serials_with_ha_enabled)} edges have HA enabled: {serials_with_ha_enabled}\") self._logger.info(f\"{len(primary_serials)} edges are the primary of a HA pair: {primary_serials}\") self._logger.info(f\"{len(standby_serials)} edges are the standby of a HA pair: {standby_serials}\") map_cached_edges_with_edges_status self._logger.info(f\"Mapped cache serials with status: {mapped_serials_w_status}\") For outage in outages: self._logger.info(f'{outage_type.value} serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in down_edges]}') self._logger.info( f\"{outage_type.value} serials that should be documented: \" f'{[e[\"status\"][\"edgeSerialNumber\"] for e in relevant_down_edges]}' ) If relevant down edges: self._logger.info(f\"{len(relevant_down_edges)} edges were detected in {outage_type.value} state.\") attempt_ticket_creation If ticket creation None: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge \" f\"with Business Grade Link(s): {ex}\") _schedule_recheck_job_for_edges Else: self._logger.info( f\"No edges were detected in {outage_type.value} state. \" f\"No ticket creations will trigger for this outage type\" ) self._logger.info(f'Healthy serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in healthy_edges]}') IF healthy edges: self._logger.info( f\"{len(healthy_edges)} edges were detected in healthy state. Running autoresolve for all of them...\" ) _run_ticket_autoresolve_for_edge Else: self._logger.info( \"No edges were detected in healthy state. Autoresolve won't be triggered\" ) self._logger.info(f\"Elapsed time processing edges in host {host}: {round((stop - start) / 60, 2)} minutes\")","title":"Process velocloud host Documentation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_recheck_edges_for_ticket_creation/","text":"Recheck edges for ticket creation self._logger.info(f\"[{outage_type.value}] Re-checking {len(outage_edges)} edges in outage state prior to ticket creation...\") self._logger.info(f\"[{outage_type.value}] Edges in outage before quarantine recheck: {outage_edges}\") * get_links_with_edge_info * If get links with edge info status not Ok: self._logger.warning(f\"Bad status calling to get links with edge info for host: {host}. Skipping recheck ...\") * get_network_enterprises * If get network enterprises tatus not Ok: self._logger.warning(f\"Bad status calling to get network enterprises info for host: {host}. Skipping recheck ...\") self._logger.info(f\"[{outage_type.value}] Velocloud edge status response in quarantine recheck: \" f\"{links_with_edge_info_response}\") * group_links_by_edge self._logger.info(f\"[{outage_type.value}] Adding HA info to existing edges, and putting standby edges under monitoring as if \" \"they were standalone edges...\") * map_edges_with_ha_info * get_edges_with_standbys_as_standalone_edges * _map_cached_edges_with_edges_status self._logger.info(f\"[{outage_type.value}] Current status of edges that were in outage state: {edges_full_info}\") self._logger.info(f\"[{outage_type.value}] Edges still in outage state after recheck: {edges_still_down}\") self._logger.info(f\"[{outage_type.value}] Serials still in outage state after recheck: {serials_still_down}\") self._logger.info(f\"[{outage_type.value}] Edges that are healthy after recheck: {healthy_edges}\") self._logger.info(f\"[{outage_type.value}] Serials that are healthy after recheck: {healthy_serials}\") * If edges still down: self._logger.info(f\"[{outage_type.value}] {len(edges_still_down)} edges are still in outage state after re-check. \" \"Attempting outage ticket creation for all of them...\") * If environment PRODUCTION: * _attempt_ticket_creation * If error in attempt ticket creation: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge in \" f\"the quarantine: {ex}\") * Else: self._logger.info(f\"[{outage_type.value}] Not starting outage ticket creation for {len(edges_still_down)} faulty \" f\"edges because the current working environment is {working_environment.upper()}.\") * Else: self._logger.info(f\"[{outage_type.value}] No edges were detected in outage state after re-check. \" \"Outage tickets won't be created\") * If healthy edges: self._logger.info( f\"[{outage_type.value}] {len(healthy_edges)} edges were detected in healthy state after re-check. '\" \"Running autoresolve for all of them...\" ) self._logger.info( f\"[{outage_type.value}] Edges that are going to be attempted to autoresolve: {healthy_edges}\" ) * _run_ticket_autoresolve_for_edge * Else: self._logger.info( f\"[{outage_type.value}] No edges were detected in healthy state. \" \"Autoresolve won't be triggered\" ) self._logger.info(f\"[{outage_type.value}] Re-check process finished for {len(outage_edges)} edges\")","title":" recheck edges for ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_recheck_edges_for_ticket_creation/#recheck-edges-for-ticket-creation","text":"self._logger.info(f\"[{outage_type.value}] Re-checking {len(outage_edges)} edges in outage state prior to ticket creation...\") self._logger.info(f\"[{outage_type.value}] Edges in outage before quarantine recheck: {outage_edges}\") * get_links_with_edge_info * If get links with edge info status not Ok: self._logger.warning(f\"Bad status calling to get links with edge info for host: {host}. Skipping recheck ...\") * get_network_enterprises * If get network enterprises tatus not Ok: self._logger.warning(f\"Bad status calling to get network enterprises info for host: {host}. Skipping recheck ...\") self._logger.info(f\"[{outage_type.value}] Velocloud edge status response in quarantine recheck: \" f\"{links_with_edge_info_response}\") * group_links_by_edge self._logger.info(f\"[{outage_type.value}] Adding HA info to existing edges, and putting standby edges under monitoring as if \" \"they were standalone edges...\") * map_edges_with_ha_info * get_edges_with_standbys_as_standalone_edges * _map_cached_edges_with_edges_status self._logger.info(f\"[{outage_type.value}] Current status of edges that were in outage state: {edges_full_info}\") self._logger.info(f\"[{outage_type.value}] Edges still in outage state after recheck: {edges_still_down}\") self._logger.info(f\"[{outage_type.value}] Serials still in outage state after recheck: {serials_still_down}\") self._logger.info(f\"[{outage_type.value}] Edges that are healthy after recheck: {healthy_edges}\") self._logger.info(f\"[{outage_type.value}] Serials that are healthy after recheck: {healthy_serials}\") * If edges still down: self._logger.info(f\"[{outage_type.value}] {len(edges_still_down)} edges are still in outage state after re-check. \" \"Attempting outage ticket creation for all of them...\") * If environment PRODUCTION: * _attempt_ticket_creation * If error in attempt ticket creation: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge in \" f\"the quarantine: {ex}\") * Else: self._logger.info(f\"[{outage_type.value}] Not starting outage ticket creation for {len(edges_still_down)} faulty \" f\"edges because the current working environment is {working_environment.upper()}.\") * Else: self._logger.info(f\"[{outage_type.value}] No edges were detected in outage state after re-check. \" \"Outage tickets won't be created\") * If healthy edges: self._logger.info( f\"[{outage_type.value}] {len(healthy_edges)} edges were detected in healthy state after re-check. '\" \"Running autoresolve for all of them...\" ) self._logger.info( f\"[{outage_type.value}] Edges that are going to be attempted to autoresolve: {healthy_edges}\" ) * _run_ticket_autoresolve_for_edge * Else: self._logger.info( f\"[{outage_type.value}] No edges were detected in healthy state. \" \"Autoresolve won't be triggered\" ) self._logger.info(f\"[{outage_type.value}] Re-check process finished for {len(outage_edges)} edges\")","title":"Recheck edges for ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_reopen_outage_ticket/","text":"Reopen outage ticket self._logger.info(f\"Reopening outage ticket {ticket_id} for serial {serial_number}...\") * get_ticket_details * If get ticket details status is not Ok: self._logger.info(f\"Bad status calling to get ticket details. Skipping reopen ticket ...\") * open_ticket * If open ticket status is Ok: self._logger.info(f\"Detail {detail_id_for_reopening} of outage ticket {ticket_id} reopened successfully.\") * _append_triage_note * Else: self._logger.error(f\"Reopening for detail {detail_id_for_reopening} of outage ticket {ticket_id} failed.\")","title":" reopen outage ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_reopen_outage_ticket/#reopen-outage-ticket","text":"self._logger.info(f\"Reopening outage ticket {ticket_id} for serial {serial_number}...\") * get_ticket_details * If get ticket details status is not Ok: self._logger.info(f\"Bad status calling to get ticket details. Skipping reopen ticket ...\") * open_ticket * If open ticket status is Ok: self._logger.info(f\"Detail {detail_id_for_reopening} of outage ticket {ticket_id} reopened successfully.\") * _append_triage_note * Else: self._logger.error(f\"Reopening for detail {detail_id_for_reopening} of outage ticket {ticket_id} failed.\")","title":"Reopen outage ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_run_ticket_autoresolve_for_edge/","text":"Run ticket autoresolve for edge self._logger.info(f\"[ticket-autoresolve] Starting autoresolve for edge {serial_number}...\") If serial number not in autoresolve serial whitelist: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} because its \" f\"serial ({serial_number}) is not whitelisted.\") get_open_outage_tickets self._logger.info(f\"Bad status calling for outage tickets for client id: {client_id} and serial: {serial_number}. \" f\"Skipping autoresolve ...\") If not found outage tickets: self._logger.info(f\"[ticket-autoresolve] No outage ticket found for edge {serial_number}. \" f\"Skipping autoresolve...\") If ticket not created by automation: self._logger.info(f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") get_ticket_details self._logger.info(f\"Bad status calling get ticket details for outage ticket: {outage_ticket_id}. \" f\"Skipping autoresolve ...\") If is ticket task in ipa queue: self._logger.info(f\"Task for serial {serial_number} in ticket {outage_ticket_id} is in the IPA Investigate queue. \" f\"Skipping checks for max auto-resolves and grace period to auto-resolve after last documented \" f\"outage...\") Else: If was last outage detect recently: self._logger.info(f\"Edge {serial_number} has been in outage state for a long time, so detail {ticket_detail_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\") If can't autoresolve one time more: self._logger.info(f\"[ticket-autoresolve] Limit to autoresolve detail {ticket_detail_id} (serial {serial_number}) \" f\"of ticket {outage_ticket_id} linked to edge {serial_number} has been maxed out already. \" \"Skipping autoresolve...\") If is detail resolved: self._logger.info(f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} is already \" \"resolved. Skipping autoresolve...\") If not PRODUCTION: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} since the \" f\"current environment is {working_environment.upper()}.\") self._logger.info(f\"Autoresolving detail {ticket_detail_id} of ticket {outage_ticket_id} linked to edge \" f\"{serial_number} with serial number {serial_number}...\") unpause_ticket_detail resolve_ticket self._logger.warning(f\"Bad status calling resolve ticket for outage ticket_id: {outage_ticket_id} and\" f\"ticket detail: {ticket_detail_id}. Skipping autoresolve ...\") append_autoresolve_note_to_ticket self._logger.info( f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} linked to \" f\"edge {serial_number} was autoresolved!\" )","title":" run ticket autoresolve for edge"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_run_ticket_autoresolve_for_edge/#run-ticket-autoresolve-for-edge","text":"self._logger.info(f\"[ticket-autoresolve] Starting autoresolve for edge {serial_number}...\") If serial number not in autoresolve serial whitelist: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} because its \" f\"serial ({serial_number}) is not whitelisted.\") get_open_outage_tickets self._logger.info(f\"Bad status calling for outage tickets for client id: {client_id} and serial: {serial_number}. \" f\"Skipping autoresolve ...\") If not found outage tickets: self._logger.info(f\"[ticket-autoresolve] No outage ticket found for edge {serial_number}. \" f\"Skipping autoresolve...\") If ticket not created by automation: self._logger.info(f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") get_ticket_details self._logger.info(f\"Bad status calling get ticket details for outage ticket: {outage_ticket_id}. \" f\"Skipping autoresolve ...\") If is ticket task in ipa queue: self._logger.info(f\"Task for serial {serial_number} in ticket {outage_ticket_id} is in the IPA Investigate queue. \" f\"Skipping checks for max auto-resolves and grace period to auto-resolve after last documented \" f\"outage...\") Else: If was last outage detect recently: self._logger.info(f\"Edge {serial_number} has been in outage state for a long time, so detail {ticket_detail_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\") If can't autoresolve one time more: self._logger.info(f\"[ticket-autoresolve] Limit to autoresolve detail {ticket_detail_id} (serial {serial_number}) \" f\"of ticket {outage_ticket_id} linked to edge {serial_number} has been maxed out already. \" \"Skipping autoresolve...\") If is detail resolved: self._logger.info(f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} is already \" \"resolved. Skipping autoresolve...\") If not PRODUCTION: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} since the \" f\"current environment is {working_environment.upper()}.\") self._logger.info(f\"Autoresolving detail {ticket_detail_id} of ticket {outage_ticket_id} linked to edge \" f\"{serial_number} with serial number {serial_number}...\") unpause_ticket_detail resolve_ticket self._logger.warning(f\"Bad status calling resolve ticket for outage ticket_id: {outage_ticket_id} and\" f\"ticket detail: {ticket_detail_id}. Skipping autoresolve ...\") append_autoresolve_note_to_ticket self._logger.info( f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} linked to \" f\"edge {serial_number} was autoresolved!\" )","title":"Run ticket autoresolve for edge"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_schedule_recheck_job_for_edges/","text":"Schedule recheck job for edges self._logger.info(f\"Scheduling recheck job for {len(edges)} edges in {outage_type.value} state...\") * _recheck_edges_for_ticket_creation self._logger.info(f\"Edges in {outage_type.value} state scheduled for recheck successfully\")","title":" schedule recheck job for edges"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_schedule_recheck_job_for_edges/#schedule-recheck-job-for-edges","text":"self._logger.info(f\"Scheduling recheck job for {len(edges)} edges in {outage_type.value} state...\") * _recheck_edges_for_ticket_creation self._logger.info(f\"Edges in {outage_type.value} state scheduled for recheck successfully\")","title":"Schedule recheck job for edges"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_send_reminder/","text":"Send reminder self._logger.info(f\"Attempting to send reminder for service number {service_number} to ticket {ticket_id}\") * If not should reminder notification: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id},\" f\" since either the last documentation cycle started or the last reminder\" f\" was sent too recently\") * If working environment not PRODUCTION: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id} since \" f\"the current environment is {working_environment.upper()}\") * send_reminder_email_milestone_notification * If status not OK: self._logger.error(f\"Reminder email of edge {service_number} could not be sent for ticket\" f\" {ticket_id}!\") * _append_reminder_note * If status to append reminder note not Ok: self._logger.error(f\"Reminder note of edge {service_number} could not be appended to ticket\" f\" {ticket_id}!\") self._logger.error(f\"Reminder note of edge {service_number} was successfully appended to ticket\" f\" {ticket_id}!\")","title":" send reminder"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_send_reminder/#send-reminder","text":"self._logger.info(f\"Attempting to send reminder for service number {service_number} to ticket {ticket_id}\") * If not should reminder notification: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id},\" f\" since either the last documentation cycle started or the last reminder\" f\" was sent too recently\") * If working environment not PRODUCTION: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id} since \" f\"the current environment is {working_environment.upper()}\") * send_reminder_email_milestone_notification * If status not OK: self._logger.error(f\"Reminder email of edge {service_number} could not be sent for ticket\" f\" {ticket_id}!\") * _append_reminder_note * If status to append reminder note not Ok: self._logger.error(f\"Reminder note of edge {service_number} could not be appended to ticket\" f\" {ticket_id}!\") self._logger.error(f\"Reminder note of edge {service_number} was successfully appended to ticket\" f\" {ticket_id}!\")","title":"Send reminder"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/append_note_to_ticket/","text":"Append note to ticket","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/append_note_to_ticket/#append-note-to-ticket","text":"","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/change_detail_work_queue_to_hnoc/","text":"Change detail work queue to hnoc change_detail_work_queue If change detail work queue status is ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") Else: self._logger.error( f\"Failed to forward ticket_id {ticket_id} and \" f\"serial {serial_number} to {target_queue} due to bruin \" f\"returning {change_detail_work_queue_response} when attempting to forward to HNOC.\" )","title":"Change detail work queue to hnoc"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/change_detail_work_queue_to_hnoc/#change-detail-work-queue-to-hnoc","text":"change_detail_work_queue If change detail work queue status is ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") Else: self._logger.error( f\"Failed to forward ticket_id {ticket_id} and \" f\"serial {serial_number} to {target_queue} due to bruin \" f\"returning {change_detail_work_queue_response} when attempting to forward to HNOC.\" )","title":"Change detail work queue to hnoc"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/forward_ticket_to_hnoc_queue/","text":"Forward_ticket_to_hnoc_queue self._logger.info(f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\") * get_ticket_details * If ticket details status is not OK self._logger.info(f\"Getting ticket details of ticket_id {ticket_id} and serial {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\") * If detail is resolved: self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is resolved. \" f\"Skipping forward to HNOC...\") self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is not resolved. \" f\"Forwarding to HNOC...\") * change_detail_work_queue_to_hnoc * If Exception: self._logger.error(f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\")","title":"Forward ticket to hnoc queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/forward_ticket_to_hnoc_queue/#forward_ticket_to_hnoc_queue","text":"self._logger.info(f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\") * get_ticket_details * If ticket details status is not OK self._logger.info(f\"Getting ticket details of ticket_id {ticket_id} and serial {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\") * If detail is resolved: self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is resolved. \" f\"Skipping forward to HNOC...\") self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is not resolved. \" f\"Forwarding to HNOC...\") * change_detail_work_queue_to_hnoc * If Exception: self._logger.error(f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\")","title":"Forward_ticket_to_hnoc_queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/schedule_forward_to_hnoc_queue/","text":"Schedule forward to hnoc queue self._logger.info(f\"Scheduling HNOC forwarding for ticket_id {ticket_id} and serial {serial_number}\" f\" to happen at timestamp: {forward_task_run_date}\") * Add job * forward_ticket_to_hnoc_queue","title":"Schedule forward to hnoc queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/schedule_forward_to_hnoc_queue/#schedule-forward-to-hnoc-queue","text":"self._logger.info(f\"Scheduling HNOC forwarding for ticket_id {ticket_id} and serial {serial_number}\" f\" to happen at timestamp: {forward_task_run_date}\") * Add job * forward_ticket_to_hnoc_queue","title":"Schedule forward to hnoc queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/start_service_outage_monitoring/","text":"Start service outage monitoring ( start the process of outage ) self._logger.info(\"Scheduling Service Outage Monitor job...\") * If exe on start: self._logger.info(\"Service Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process * If Exception: self._logger.error(f\"Skipping start of Service Outage Monitoring job. Reason: {conflict}\")","title":"Start service outage monitoring"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/start_service_outage_monitoring/#start-service-outage-monitoring-start-the-process-of-outage","text":"self._logger.info(\"Scheduling Service Outage Monitor job...\") * If exe on start: self._logger.info(\"Service Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process * If Exception: self._logger.error(f\"Skipping start of Service Outage Monitoring job. Reason: {conflict}\")","title":"Start service outage monitoring (start the process of outage)"},{"location":"logging/services/service-outage-monitor/actions/triage/_append_new_triage_notes_based_on_recent_events/","text":"Append new triage notes based on recent events self._logger.info(f\"Appending new triage note to detail {ticket_detail_id} of ticket {ticket_id}...\") self._logger.info( f\"Getting events for serial {service_number} (detail {ticket_detail_id}) in ticket \" f\"{ticket_id} before applying triage...\" ) * get_last_edge_events * If get last events status is not Ok: self._logger.warning(f\"Bad status calling get last edge events for edge: {edge_full_id}. \" f\"Skipping append triage notes based in recent events ...\") * If not recent events: self._logger.info( f\"No events were found for edge {service_number} starting from {events_lookup_timestamp}. \" f\"Not appending any new triage notes to detail {ticket_detail_id} of ticket {ticket_id}.\" ) * For chunk in event chunked: * If environment is PRODUCTION: * append_note_to_ticket * If append note status is not Ok: self._logger.warning(f\"Bad status apeending note to ticket: {ticket_id}. Skipping append note ...\") self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * Else: self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * If note appended: * _notify_triage_note_was_appended_to_ticket","title":" append new triage notes based on recent events"},{"location":"logging/services/service-outage-monitor/actions/triage/_append_new_triage_notes_based_on_recent_events/#append-new-triage-notes-based-on-recent-events","text":"self._logger.info(f\"Appending new triage note to detail {ticket_detail_id} of ticket {ticket_id}...\") self._logger.info( f\"Getting events for serial {service_number} (detail {ticket_detail_id}) in ticket \" f\"{ticket_id} before applying triage...\" ) * get_last_edge_events * If get last events status is not Ok: self._logger.warning(f\"Bad status calling get last edge events for edge: {edge_full_id}. \" f\"Skipping append triage notes based in recent events ...\") * If not recent events: self._logger.info( f\"No events were found for edge {service_number} starting from {events_lookup_timestamp}. \" f\"Not appending any new triage notes to detail {ticket_detail_id} of ticket {ticket_id}.\" ) * For chunk in event chunked: * If environment is PRODUCTION: * append_note_to_ticket * If append note status is not Ok: self._logger.warning(f\"Bad status apeending note to ticket: {ticket_id}. Skipping append note ...\") self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * Else: self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * If note appended: * _notify_triage_note_was_appended_to_ticket","title":"Append new triage notes based on recent events"},{"location":"logging/services/service-outage-monitor/actions/triage/_build_edges_status_by_serial/","text":"Build edges status by serial get_edges_for_triage get_network_enterprises_for_triage map_edges_with_ha_info","title":" build edges status by serial"},{"location":"logging/services/service-outage-monitor/actions/triage/_build_edges_status_by_serial/#build-edges-status-by-serial","text":"get_edges_for_triage get_network_enterprises_for_triage map_edges_with_ha_info","title":"Build edges status by serial"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_irrelevant_notes_in_tickets/","text":"Filter irrelevant notes in tickets For ticket in tickets: self._logger.info(f'Filtering notes for ticket_id: {ticket[\"ticket_id\"]} to contain relevant notes')","title":" filter irrelevant notes in tickets"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_irrelevant_notes_in_tickets/#filter-irrelevant-notes-in-tickets","text":"For ticket in tickets: self._logger.info(f'Filtering notes for ticket_id: {ticket[\"ticket_id\"]} to contain relevant notes')","title":"Filter irrelevant notes in tickets"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_tickets_and_details_related_to_edges_under_monitoring/","text":"Filter tickets and details related to edges under monitoring For ticket in tickets: self._logger.info(f'Checking ticket_id: {ticket[\"ticket_id\"]} for relevant details') If not relevant details: self._logger.info(f'Ticket with ticket_id: {ticket[\"ticket_id\"]} has no relevant details') self._logger.info( f'Ticket with ticket_id: {ticket[\"ticket_id\"]} contains relevant details.' f\"Appending to relevant_tickets list ...\" )","title":" filter tickets and details related to edges under monitoring"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_tickets_and_details_related_to_edges_under_monitoring/#filter-tickets-and-details-related-to-edges-under-monitoring","text":"For ticket in tickets: self._logger.info(f'Checking ticket_id: {ticket[\"ticket_id\"]} for relevant details') If not relevant details: self._logger.info(f'Ticket with ticket_id: {ticket[\"ticket_id\"]} has no relevant details') self._logger.info( f'Ticket with ticket_id: {ticket[\"ticket_id\"]} contains relevant details.' f\"Appending to relevant_tickets list ...\" )","title":"Filter tickets and details related to edges under monitoring"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_all_open_tickets_with_details_for_monitored_companies/","text":"Get all open tickets with details for monitored companies get_open_outage_tickets If get open outage status is not Ok: self._logger.warning(f\"Bad status calling to open tickets. Return an empty list ...\") self._logger.info(\"Getting all opened tickets details for each open ticket ...\") _get_open_tickets_with_details_by_ticket_id self._logger.info(\"Finished getting all opened ticket details!\")","title":" get all open tickets with details for monitored companies"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_all_open_tickets_with_details_for_monitored_companies/#get-all-open-tickets-with-details-for-monitored-companies","text":"get_open_outage_tickets If get open outage status is not Ok: self._logger.warning(f\"Bad status calling to open tickets. Return an empty list ...\") self._logger.info(\"Getting all opened tickets details for each open ticket ...\") _get_open_tickets_with_details_by_ticket_id self._logger.info(\"Finished getting all opened ticket details!\")","title":"Get all open tickets with details for monitored companies"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_open_tickets_with_details_by_ticket_id/","text":"Get open tickets with details by ticket id get_ticket_details If get ticket detail status is not Ok: self._logger.warning( f\"Bad status calling get ticket details for ticket id: {ticket_id}. \" f\"Skipping get ticket details ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get tickets details for ticket_id {ticket_id} -> {e}\" )","title":" get open tickets with details by ticket id"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_open_tickets_with_details_by_ticket_id/#get-open-tickets-with-details-by-ticket-id","text":"get_ticket_details If get ticket detail status is not Ok: self._logger.warning( f\"Bad status calling get ticket details for ticket id: {ticket_id}. \" f\"Skipping get ticket details ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get tickets details for ticket_id {ticket_id} -> {e}\" )","title":"Get open tickets with details by ticket id"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_ticket_details_with_and_without_triage/","text":"Get ticket details with and without triage For ticket in tickets: self._logger.info(f\"Checking details of ticket_id: {ticket_id}\") For detail in ticket details: self._logger.info( f\"Checking for triage notes in ticket_id: {ticket_id} \" f\"relating to serial number: {serial_number}\" ) If notes related to serial: self._logger.info( f\"No triage notes found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_without_triage list...\" ) Else: sself._logger.info( f\"Triage note found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_with_triage list...\" )","title":" get ticket details with and without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_ticket_details_with_and_without_triage/#get-ticket-details-with-and-without-triage","text":"For ticket in tickets: self._logger.info(f\"Checking details of ticket_id: {ticket_id}\") For detail in ticket details: self._logger.info( f\"Checking for triage notes in ticket_id: {ticket_id} \" f\"relating to serial number: {serial_number}\" ) If notes related to serial: self._logger.info( f\"No triage notes found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_without_triage list...\" ) Else: sself._logger.info( f\"Triage note found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_with_triage list...\" )","title":"Get ticket details with and without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_notify_triage_note_was_appended_to_ticket/","text":"Notify triage note was appended to ticket self._logger.info(f\"Triage appended to detail {ticket_detail_id} (serial: {service_number}) of ticket {ticket_id}. \" f\"Details at https://app.bruin.com/t/{ticket_id}\")","title":" notify triage note was appended to ticket"},{"location":"logging/services/service-outage-monitor/actions/triage/_notify_triage_note_was_appended_to_ticket/#notify-triage-note-was-appended-to-ticket","text":"self._logger.info(f\"Triage appended to detail {ticket_detail_id} (serial: {service_number}) of ticket {ticket_id}. \" f\"Details at https://app.bruin.com/t/{ticket_id}\")","title":"Notify triage note was appended to ticket"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_with_triage/","text":"Process ticket details with triage self._logger.info(\"Processing ticket details with triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} with triage of ticket {ticket_id}...\") self._logger.info( f\"Checking if events need to be appended to detail {ticket_detail_id} of ticket {ticket_id}...\" ) * If ticket note append recently: self._logger.info( f\"The last triage note was appended to detail {ticket_detail_id} of ticket \" f\"{ticket_id} not long ago so no new triage note will be appended for now\" ) self._logger.info(f\"Appending events to detail {ticket_detail_id} of ticket {ticket_id}...\") * _append_new_triage_notes_based_on_recent_events self._logger.info(f\"Events appended to detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details with triage!\")","title":" process ticket details with triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_with_triage/#process-ticket-details-with-triage","text":"self._logger.info(\"Processing ticket details with triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} with triage of ticket {ticket_id}...\") self._logger.info( f\"Checking if events need to be appended to detail {ticket_detail_id} of ticket {ticket_id}...\" ) * If ticket note append recently: self._logger.info( f\"The last triage note was appended to detail {ticket_detail_id} of ticket \" f\"{ticket_id} not long ago so no new triage note will be appended for now\" ) self._logger.info(f\"Appending events to detail {ticket_detail_id} of ticket {ticket_id}...\") * _append_new_triage_notes_based_on_recent_events self._logger.info(f\"Events appended to detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details with triage!\")","title":"Process ticket details with triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_without_triage/","text":"Process ticket details without triage self._logger.info(\"Processing ticket details without triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} without triage of ticket {ticket_id}...\") * If not outage type: self._logger.info( f\"Edge {serial_number} is no longer down, so the initial triage note won't be posted to ticket \" f\"{ticket_id}. Posting events of the last 24 hours to the ticket so it's not blank...\" ) * _append_new_triage_notes_based_on_recent_events * Else: self._logger.info( f\"Edge {serial_number} is in {outage_type.value} state. Posting initial triage note to ticket \" f\"{ticket_id}...\" ) * If not document outage: self._logger.info( f\"Edge {serial_number} is down, but it doesn't qualify to be documented as a Service Outage in \" f\"ticket {ticket_id}. Most probable thing is that the edge is the standby of a HA pair, and \" \"standbys in outage state are only documented in the event of a Soft Down. Skipping...\" ) * get_last_edge_events * If get last edge events not Ok: self._logger.warning(f\"Bad status calling to get last edge events. \" f\"Skipping process details without details ...\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details without triage!\")","title":" process ticket details without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_without_triage/#process-ticket-details-without-triage","text":"self._logger.info(\"Processing ticket details without triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} without triage of ticket {ticket_id}...\") * If not outage type: self._logger.info( f\"Edge {serial_number} is no longer down, so the initial triage note won't be posted to ticket \" f\"{ticket_id}. Posting events of the last 24 hours to the ticket so it's not blank...\" ) * _append_new_triage_notes_based_on_recent_events * Else: self._logger.info( f\"Edge {serial_number} is in {outage_type.value} state. Posting initial triage note to ticket \" f\"{ticket_id}...\" ) * If not document outage: self._logger.info( f\"Edge {serial_number} is down, but it doesn't qualify to be documented as a Service Outage in \" f\"ticket {ticket_id}. Most probable thing is that the edge is the standby of a HA pair, and \" \"standbys in outage state are only documented in the event of a Soft Down. Skipping...\" ) * get_last_edge_events * If get last edge events not Ok: self._logger.warning(f\"Bad status calling to get last edge events. \" f\"Skipping process details without details ...\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details without triage!\")","title":"Process ticket details without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_run_tickets_polling/","text":"Run tickets polling self._logger.info(f\"Starting triage process...\") * get_cache_for_triage_monitoring self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got all {len(open_tickets)} open tickets for all customers. \" f\"Filtering them to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) * _filter_irrelevant_notes_in_tickets self._logger.info(f\"Splitting relevant tickets in tickets with and without triage...\") * _get_ticket_details_with_and_without_triage self._logger.info( f\"Ticket details split successfully. \" f\"Ticket details with triage: {len(details_with_triage)}. \" f\"Ticket details without triage: {len(details_without_triage)}. \" \"Processing both sets...\" ) * _build_edges_status_by_serial * _process_ticket_details_with_triage self._logger.info(f\"Triage process finished! took {time.time() - total_start_time} seconds\")","title":" run tickets polling"},{"location":"logging/services/service-outage-monitor/actions/triage/_run_tickets_polling/#run-tickets-polling","text":"self._logger.info(f\"Starting triage process...\") * get_cache_for_triage_monitoring self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got all {len(open_tickets)} open tickets for all customers. \" f\"Filtering them to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) * _filter_irrelevant_notes_in_tickets self._logger.info(f\"Splitting relevant tickets in tickets with and without triage...\") * _get_ticket_details_with_and_without_triage self._logger.info( f\"Ticket details split successfully. \" f\"Ticket details with triage: {len(details_with_triage)}. \" f\"Ticket details without triage: {len(details_without_triage)}. \" \"Processing both sets...\" ) * _build_edges_status_by_serial * _process_ticket_details_with_triage self._logger.info(f\"Triage process finished! took {time.time() - total_start_time} seconds\")","title":"Run tickets polling"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_asr_forwarding_note/","text":"Append asr forwarding note append_note_to_ticket","title":"Append asr forwarding note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_asr_forwarding_note/#append-asr-forwarding-note","text":"append_note_to_ticket","title":"Append asr forwarding note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket","text":"append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_digi_reboot_note/","text":"Append digi reboot note append_note_to_ticket","title":"Append digi reboot note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_digi_reboot_note/#append-digi-reboot-note","text":"append_note_to_ticket","title":"Append digi reboot note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_task_result_change_note/","text":"Append task result change note append_note_to_ticket","title":"Append task result change note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_task_result_change_note/#append-task-result-change-note","text":"append_note_to_ticket","title":"Append task result change note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_triage_note/","text":"Append triage note If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_triage_note/#append-triage-note","text":"If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_detail_work_queue/","text":"Change detail work queue self._logger.info(f\"Changing task result for ticket {ticket_id} and detail id {detail_id} for device {serial_number} to {task_result}...\") * If Exception: self._logger.error(f\"An error occurred when changing task result for ticket {ticket_id} and serial{serial_number}\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.info(f\"Error while changing task result for ticket {ticket_id} and serial {serial_number} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_detail_work_queue/#change-detail-work-queue","text":"self._logger.info(f\"Changing task result for ticket {ticket_id} and detail id {detail_id} for device {serial_number} to {task_result}...\") * If Exception: self._logger.error(f\"An error occurred when changing task result for ticket {ticket_id} and serial{serial_number}\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.info(f\"Error while changing task result for ticket {ticket_id} and serial {serial_number} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity/","text":"Change severity self._logger.info(f\"Changing severity level of ticket {ticket_id} to {severity_level}...\") If Exception: self._logger.error(f\"An error occurred when changing the severity level of ticket {ticket_id} to {severity_level} -> {e}\") If status is 200: self._logger.info(f\"Severity level of ticket {ticket_id} successfully changed to {severity_level}!\") Else: self._logger.error(f\"Error while changing severity of ticket {ticket_id} to {severity_level} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change severity"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity/#change-severity","text":"self._logger.info(f\"Changing severity level of ticket {ticket_id} to {severity_level}...\") If Exception: self._logger.error(f\"An error occurred when changing the severity level of ticket {ticket_id} to {severity_level} -> {e}\") If status is 200: self._logger.info(f\"Severity level of ticket {ticket_id} successfully changed to {severity_level}!\") Else: self._logger.error(f\"Error while changing severity of ticket {ticket_id} to {severity_level} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change severity"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_disconnected_links/","text":"Change ticket severity for disconnected links change_ticket_severity","title":"Change ticket severity for disconnected links"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_disconnected_links/#change-ticket-severity-for-disconnected-links","text":"change_ticket_severity","title":"Change ticket severity for disconnected links"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_offline_edge/","text":"Change ticket severity for offline edges change_ticket_severity","title":"Change ticket severity for offline edge"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_offline_edge/#change-ticket-severity-for-offline-edges","text":"change_ticket_severity","title":"Change ticket severity for offline edges"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/create_outage_ticket/","text":"Create outage ticket Documentation self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/create_outage_ticket/#create-outage-ticket-documentation","text":"self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket Documentation"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/","text":"Get open outage tickets get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/#get-open-outage-tickets","text":"get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_outage_tickets/","text":"Get outage tickets * get_ticket","title":"Get outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_outage_tickets/#get-outage-tickets","text":"* get_ticket","title":"Get outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/open_ticket/#open-ticket","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/post_notification_email_milestone/","text":"post notification email milestone self._logger.info(f\"Sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...\") * If Exception: self._logger.info(f\"An error occurred when sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...-> {e}\") * If status ok: self._logger.info(f\"Email sent for ticket {ticket_id}, service number {service_number} and notification type {notification_type}!\") * Else: self._logger.info(f\"Error while sending email for ticket {ticket_id}, service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Post notification email milestone"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/post_notification_email_milestone/#post-notification-email-milestone","text":"self._logger.info(f\"Sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...\") * If Exception: self._logger.info(f\"An error occurred when sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...-> {e}\") * If status ok: self._logger.info(f\"Email sent for ticket {ticket_id}, service number {service_number} and notification type {notification_type}!\") * Else: self._logger.info(f\"Error while sending email for ticket {ticket_id}, service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"post notification email milestone"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket detail self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket-detail","text":"self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket detail"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/","text":"send initial email milestone notification post_notification_email_milestone","title":"Send initial email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/#send-initial-email-milestone-notification","text":"post_notification_email_milestone","title":"send initial email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_reminder_email_milestone_notification/","text":"Send reminder email milestone notification post_notification_email_milestone","title":"Send reminder email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_reminder_email_milestone_notification/#send-reminder-email-milestone-notification","text":"post_notification_email_milestone","title":"Send reminder email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/","text":"get cache for outage Documentation Launch get_cache","title":"Get cache for outage monitoring"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/#get-cache-for-outage-documentation","text":"Launch get_cache","title":"get cache for outage Documentation"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_triage_monitoring/","text":"Get cache for triage monitoring get_cache","title":"Get cache for triage monitoring"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_triage_monitoring/#get-cache-for-triage-monitoring","text":"get_cache","title":"Get cache for triage monitoring"},{"location":"logging/services/service-outage-monitor/repositories/digi_repository/reboot_link/","text":"Reboot link self._logger.info(f\"Rebooting DiGi link of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when attempting a DiGi reboot for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not ok: self._logger.error(f\"Error while attempting a DiGi reboot for ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Reboot link"},{"location":"logging/services/service-outage-monitor/repositories/digi_repository/reboot_link/#reboot-link","text":"self._logger.info(f\"Rebooting DiGi link of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when attempting a DiGi reboot for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not ok: self._logger.error(f\"Error while attempting a DiGi reboot for ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Reboot link"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/get_edges_with_standbys_as_standalone_edges/","text":"Map edges with HA info Documentation","title":"Get edges with standbys as standalone edges"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/get_edges_with_standbys_as_standalone_edges/#map-edges-with-ha-info-documentation","text":"","title":"Map edges with HA info Documentation"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/map_edges_with_ha_info/","text":"Map edges with HA info Documentation for edge in edges If not edge ha info: self._logger.warning(f\"No HA info was found for edge {serial_number}. Skipping...\") If is not a raw ha state under monitoring self._logger.info( f\"HA partner for {serial_number} is in state {ha_state}, so HA will be considered as disabled for \" \"this edge\" )","title":"Map edges with ha info"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/map_edges_with_ha_info/#map-edges-with-ha-info-documentation","text":"for edge in edges If not edge ha info: self._logger.warning(f\"No HA info was found for edge {serial_number}. Skipping...\") If is not a raw ha state under monitoring self._logger.info( f\"HA partner for {serial_number} is in state {ha_state}, so HA will be considered as disabled for \" \"this edge\" )","title":"Map edges with HA info Documentation"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/build_triage_note/","text":"Build triage note This function don't have logs, only generate a note as string","title":"Build triage note"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/build_triage_note/#build-triage-note","text":"This function don't have logs, only generate a note as string","title":"Build triage note"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/start_triage_job/","text":"Start triage job (star process of triage) self._logger.info( f\"Scheduled task: service outage triage configured to run every \" f'{self._config.TRIAGE_CONFIG[\"polling_minutes\"]} minutes' ) * If exec on start: self._logger.info(f\"It will be executed now\") * _run_tickets_polling","title":"Start triage job"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/start_triage_job/#start-triage-job-star-process-of-triage","text":"self._logger.info( f\"Scheduled task: service outage triage configured to run every \" f'{self._config.TRIAGE_CONFIG[\"polling_minutes\"]} minutes' ) * If exec on start: self._logger.info(f\"It will be executed now\") * _run_tickets_polling","title":"Start triage job (star process of triage)"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edge_events/","text":"Get edge events self._logger.info(f\"Getting events of edge {json.dumps(edge_full_id)} having any type of {event_types} that took place \" f\"between {from_} and {to} from Velocloud...\") * If Exception: self._logger.error(f\"An error occurred when requesting edge events from Velocloud for edge \" f\"{json.dumps(edge_full_id)} -> {e}\") self._logger.info(f\"Got events of edge {json.dumps(edge_full_id)} having any type in {event_types} that took place \" f\"between {from_} and {to} from Velocloud!\") * If status not ok: self._logger.error(f\"Error while retrieving events of edge {json.dumps(edge_full_id)} having any type in \" f\"{event_types} that took place between {from_} and {to} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edge_events/#get-edge-events","text":"self._logger.info(f\"Getting events of edge {json.dumps(edge_full_id)} having any type of {event_types} that took place \" f\"between {from_} and {to} from Velocloud...\") * If Exception: self._logger.error(f\"An error occurred when requesting edge events from Velocloud for edge \" f\"{json.dumps(edge_full_id)} -> {e}\") self._logger.info(f\"Got events of edge {json.dumps(edge_full_id)} having any type in {event_types} that took place \" f\"between {from_} and {to} from Velocloud!\") * If status not ok: self._logger.error(f\"Error while retrieving events of edge {json.dumps(edge_full_id)} having any type in \" f\"{event_types} that took place between {from_} and {to} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edges_for_triage/","text":"Get edges for triage For host in triage host: get_links_with_edge_info Is get links with edge info status is not Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\")","title":"Get edges for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edges_for_triage/#get-edges-for-triage","text":"For host in triage host: get_links_with_edge_info Is get links with edge info status is not Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\")","title":"Get edges for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_last_edge_events/","text":"Get last edge events get_edge_events","title":"Get last edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_last_edge_events/#get-last-edge-events","text":"get_edge_events","title":"Get last edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_links_with_edge_info/","text":"Get Links with edge info Documentation self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links with edge info"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_links_with_edge_info/#get-links-with-edge-info-documentation","text":"self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get Links with edge info Documentation"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises/","text":"Get network enterprises Documentation If enterprises ids: self._logger.info( f\"Getting network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}...\" ) * Else: self._logger.info( \"Getting network information for all edges belonging to all enterprises in host \" f\"{velocloud_host}...\" ) If Exception self._logger.error(f\"An error occurred when requesting network info from Velocloud host {velocloud_host} -> {e}\") * If status OK: If enterprises ids: self._logger.info( f\"Got network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}!\" ) * Else: self._logger.info( f\"Got network information for all edges belonging to all enterprises in host {velocloud_host}!\" ) Else: self._logger.error(f\"Error while retrieving network info from Velocloud host {velocloud_host} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get network enterprises"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises/#get-network-enterprises-documentation","text":"If enterprises ids: self._logger.info( f\"Getting network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}...\" ) * Else: self._logger.info( \"Getting network information for all edges belonging to all enterprises in host \" f\"{velocloud_host}...\" ) If Exception self._logger.error(f\"An error occurred when requesting network info from Velocloud host {velocloud_host} -> {e}\") * If status OK: If enterprises ids: self._logger.info( f\"Got network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}!\" ) * Else: self._logger.info( f\"Got network information for all edges belonging to all enterprises in host {velocloud_host}!\" ) Else: self._logger.error(f\"Error while retrieving network info from Velocloud host {velocloud_host} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get network enterprises Documentation"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises_for_triage/","text":"Get network enterprises for triage For host in triage host: get_network_enterprises If get network enterprises status is not ok: self._logger.error(f\"Could not retrieve network enterprises for triage using host {host}\")","title":"Get network enterprises for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises_for_triage/#get-network-enterprises-for-triage","text":"For host in triage host: get_network_enterprises If get network enterprises status is not ok: self._logger.error(f\"Could not retrieve network enterprises for triage using host {host}\")","title":"Get network enterprises for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/group_links_by_edge/","text":"Process velocloud host Documentation for link in links If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If edge state is \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Group links by edge"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/group_links_by_edge/#process-velocloud-host-documentation","text":"for link in links If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If edge state is \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Process velocloud host Documentation"},{"location":"logging/services/velocloud-bridge/actions/edge_events_for_alert/","text":"Subject: alert.request.event.edge Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get edge events with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have edge , start_date , or end_date filters: logger . error ( f 'Cannot get edge events with { json . dumps ( payload ) } . Need parameters \"edge\", \"start_date\" and ' f '\"end_date\"' ) END If filter field is specified in filters to pull specific event types: logger . info ( f \"Event types filter { filter_ } will be used to get events for edge { edge } \" ) If limit field is specified in filters to pull a certain number of events: logger . info ( f \"Will fetch up to { limit } events for edge { edge } \" ) logger . info ( f \"Getting events for edge { edge } ...\" ) get_all_edge_events logger . info ( f \"Edge events published for request { json . dumps ( payload ) } . Message published was { response } \" )","title":"Edge events for alert"},{"location":"logging/services/velocloud-bridge/actions/edge_events_for_alert/#subject-alertrequesteventedge","text":"Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get edge events with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have edge , start_date , or end_date filters: logger . error ( f 'Cannot get edge events with { json . dumps ( payload ) } . Need parameters \"edge\", \"start_date\" and ' f '\"end_date\"' ) END If filter field is specified in filters to pull specific event types: logger . info ( f \"Event types filter { filter_ } will be used to get events for edge { edge } \" ) If limit field is specified in filters to pull a certain number of events: logger . info ( f \"Will fetch up to { limit } events for edge { edge } \" ) logger . info ( f \"Getting events for edge { edge } ...\" ) get_all_edge_events logger . info ( f \"Edge events published for request { json . dumps ( payload ) } . Message published was { response } \" )","title":"Subject: alert.request.event.edge"},{"location":"logging/services/velocloud-bridge/actions/enterprise_edge_list/","text":"Subject: request.enterprises.edges Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get enterprise edges with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have edge , start_date , or end_date filters: logger . error ( f 'Cannot get edge events with { json . dumps ( payload ) } . Need parameters \"edge\", \"start_date\" and \"end_date\"' ) END logger . info ( f \"Getting edges for host { host } and enterprise { enterprise_id } ...\" ) get_enterprise_edges logger . info ( f \"Sent list of enterprise edges for enterprise { enterprise_id } and host { host } \" )","title":"Enterprise edge list"},{"location":"logging/services/velocloud-bridge/actions/enterprise_edge_list/#subject-requestenterprisesedges","text":"Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get enterprise edges with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have edge , start_date , or end_date filters: logger . error ( f 'Cannot get edge events with { json . dumps ( payload ) } . Need parameters \"edge\", \"start_date\" and \"end_date\"' ) END logger . info ( f \"Getting edges for host { host } and enterprise { enterprise_id } ...\" ) get_enterprise_edges logger . info ( f \"Sent list of enterprise edges for enterprise { enterprise_id } and host { host } \" )","title":"Subject: request.enterprises.edges"},{"location":"logging/services/velocloud-bridge/actions/enterprise_events_for_alert/","text":"Subject: alert.request.event.enterprise Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get enterprise events with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have host , enterprise_id start_date , or end_date filters: logger . error ( f 'Cannot get edge events with { json . dumps ( payload ) } . Need parameters \"host\", \"enterprise_id\", ' f '\"start_date\" and \"end_date\"' ) END If filter field is specified in filters to pull specific event types: logger . info ( f \"Event types filter { filter_ } will be used to get events for enterprise { enterprise_id } of host { host } \" ) If limit field is specified in filters to pull a certain number of events: logger . info ( f \"Will fetch up to { limit } events for enterprise { enterprise_id } of host { host } \" ) logger . info ( f \"Getting events for enterprise { enterprise_id } of host { host } ...\" ) get_all_enterprise_events logger . info ( f \"Enterprise events published for request { json . dumps ( payload ) } . Message published was { response } \" )","title":"Enterprise events for alert"},{"location":"logging/services/velocloud-bridge/actions/enterprise_events_for_alert/#subject-alertrequestevententerprise","text":"Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get enterprise events with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have host , enterprise_id start_date , or end_date filters: logger . error ( f 'Cannot get edge events with { json . dumps ( payload ) } . Need parameters \"host\", \"enterprise_id\", ' f '\"start_date\" and \"end_date\"' ) END If filter field is specified in filters to pull specific event types: logger . info ( f \"Event types filter { filter_ } will be used to get events for enterprise { enterprise_id } of host { host } \" ) If limit field is specified in filters to pull a certain number of events: logger . info ( f \"Will fetch up to { limit } events for enterprise { enterprise_id } of host { host } \" ) logger . info ( f \"Getting events for enterprise { enterprise_id } of host { host } ...\" ) get_all_enterprise_events logger . info ( f \"Enterprise events published for request { json . dumps ( payload ) } . Message published was { response } \" )","title":"Subject: alert.request.event.enterprise"},{"location":"logging/services/velocloud-bridge/actions/enterprise_name_list_response/","text":"Subject: request.enterprises.names Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get enterprise names with { json . dumps ( payload ) } . JSON malformed\" ) END logger . info ( \"Sending enterprise name list\" ) get_all_enterprise_names logger . info ( \"Enterprise name list sent\" )","title":"Enterprise name list response"},{"location":"logging/services/velocloud-bridge/actions/enterprise_name_list_response/#subject-requestenterprisesnames","text":"Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get enterprise names with { json . dumps ( payload ) } . JSON malformed\" ) END logger . info ( \"Sending enterprise name list\" ) get_all_enterprise_names logger . info ( \"Enterprise name list sent\" )","title":"Subject: request.enterprises.names"},{"location":"logging/services/velocloud-bridge/actions/gateway_status_metrics/","text":"Subject: request.gateway.status.metrics Message arrives at subject If message doesn't have the expected format: logger . warning ( f \"Wrong request message: msg= { msg } , validation_error= { e } \" ) END logger . info ( f \"Getting gateway status metrics for gateway { gateway_id } on host { host } in interval { interval } ...\" ) get_gateway_status_metrics logger . info ( f \"Sent gateway status metrics for gateway { gateway_id } on host { host } in interval { interval } \" )","title":"Gateway status metrics"},{"location":"logging/services/velocloud-bridge/actions/gateway_status_metrics/#subject-requestgatewaystatusmetrics","text":"Message arrives at subject If message doesn't have the expected format: logger . warning ( f \"Wrong request message: msg= { msg } , validation_error= { e } \" ) END logger . info ( f \"Getting gateway status metrics for gateway { gateway_id } on host { host } in interval { interval } ...\" ) get_gateway_status_metrics logger . info ( f \"Sent gateway status metrics for gateway { gateway_id } on host { host } in interval { interval } \" )","title":"Subject: request.gateway.status.metrics"},{"location":"logging/services/velocloud-bridge/actions/get_edge_links_series/","text":"Subject: request.edge.links.series Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get edge links series with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have the appropriate shape: logger . error ( f \"Cannot get edge links series with { json . dumps ( payload ) } . Make sure it complies with the shape of \" f \" { REQUEST_MODEL } \" ) END If body's payload doesn't have any filter of enterpriseId , edgeId , interval and metrics : logger . error ( f 'Cannot get edge links series with { json . dumps ( payload ) } . Need parameters \"enterpriseId\", \"edgeId\", ' f '\"interval\" and \"metrics\" under \"payload\"' ) END logger . info ( f \"Getting edge links series from host { host } using payload { payload } ...\" ) get_edge_links_series logger . info ( f \"Published edge links series for host { host } and payload { payload } \" )","title":"Get edge links series"},{"location":"logging/services/velocloud-bridge/actions/get_edge_links_series/#subject-requestedgelinksseries","text":"Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get edge links series with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have the appropriate shape: logger . error ( f \"Cannot get edge links series with { json . dumps ( payload ) } . Make sure it complies with the shape of \" f \" { REQUEST_MODEL } \" ) END If body's payload doesn't have any filter of enterpriseId , edgeId , interval and metrics : logger . error ( f 'Cannot get edge links series with { json . dumps ( payload ) } . Need parameters \"enterpriseId\", \"edgeId\", ' f '\"interval\" and \"metrics\" under \"payload\"' ) END logger . info ( f \"Getting edge links series from host { host } using payload { payload } ...\" ) get_edge_links_series logger . info ( f \"Published edge links series for host { host } and payload { payload } \" )","title":"Subject: request.edge.links.series"},{"location":"logging/services/velocloud-bridge/actions/links_configuration/","text":"Subject: request.links.configuration Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get links configuration with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have edge , start_date , or end_date filters: logger . error ( f 'Cannot get links configuration with { json . dumps ( payload ) } . Need parameters \"host\", \"enterprise_id\" ' f 'and \"edge_id\"' ) END logger . info ( f \"Getting links configuration for edge { edge_full_id } ...\" ) get_links_configuration logger . info ( f \"Published links configuration for edge { edge_full_id } \" )","title":"Links configuration"},{"location":"logging/services/velocloud-bridge/actions/links_configuration/#subject-requestlinksconfiguration","text":"Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get links configuration with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have edge , start_date , or end_date filters: logger . error ( f 'Cannot get links configuration with { json . dumps ( payload ) } . Need parameters \"host\", \"enterprise_id\" ' f 'and \"edge_id\"' ) END logger . info ( f \"Getting links configuration for edge { edge_full_id } ...\" ) get_links_configuration logger . info ( f \"Published links configuration for edge { edge_full_id } \" )","title":"Subject: request.links.configuration"},{"location":"logging/services/velocloud-bridge/actions/links_metric_info/","text":"Subject: get.links.metric.info Message arrives at subject If message doesn't have a body: logger . error ( f 'Cannot get links metric info: \"body\" is missing in the request' ) END If message body doesn't have host filter: logger . error ( f 'Cannot get links metric info: \"host\" is missing in the body of the request' ) END If message body doesn't have interval filter: logger . error ( f 'Cannot get links metric info: \"interval\" is missing in the body of the request' ) END logger . info ( f 'Getting links metric info from Velocloud host \" { velocloud_host } \"...' ) get_links_metric_info logger . info ( f \"Published links metric info for request { payload } \" )","title":"Links metric info"},{"location":"logging/services/velocloud-bridge/actions/links_metric_info/#subject-getlinksmetricinfo","text":"Message arrives at subject If message doesn't have a body: logger . error ( f 'Cannot get links metric info: \"body\" is missing in the request' ) END If message body doesn't have host filter: logger . error ( f 'Cannot get links metric info: \"host\" is missing in the body of the request' ) END If message body doesn't have interval filter: logger . error ( f 'Cannot get links metric info: \"interval\" is missing in the body of the request' ) END logger . info ( f 'Getting links metric info from Velocloud host \" { velocloud_host } \"...' ) get_links_metric_info logger . info ( f \"Published links metric info for request { payload } \" )","title":"Subject: get.links.metric.info"},{"location":"logging/services/velocloud-bridge/actions/links_with_edge_info/","text":"Subject: get.links.with.edge.info Message arrives at subject If message doesn't have a body: logger . error ( f 'Cannot get links with edge info: \"body\" is missing in the request' ) END If message body doesn't have host filter: logger . error ( f 'Cannot get links with edge info: \"host\" is missing in the body of the request' ) END logger . info ( f 'Getting links with edge info from Velocloud host \" { velocloud_host } \"...' ) get_links_with_edge_info logger . info ( f \"Response sent for request { payload } \" )","title":"Links with edge info"},{"location":"logging/services/velocloud-bridge/actions/links_with_edge_info/#subject-getlinkswithedgeinfo","text":"Message arrives at subject If message doesn't have a body: logger . error ( f 'Cannot get links with edge info: \"body\" is missing in the request' ) END If message body doesn't have host filter: logger . error ( f 'Cannot get links with edge info: \"host\" is missing in the body of the request' ) END logger . info ( f 'Getting links with edge info from Velocloud host \" { velocloud_host } \"...' ) get_links_with_edge_info logger . info ( f \"Response sent for request { payload } \" )","title":"Subject: get.links.with.edge.info"},{"location":"logging/services/velocloud-bridge/actions/network_enterprise_edge_list/","text":"Subject: request.network.enterprise.edges Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get network enterprise edge list with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have host and enterprise_ids filters: logger . error ( f 'Cannot get network enterprise edge list with { json . dumps ( payload ) } . Need parameters \"host\" and ' f '\"enterprise_ids\"' ) END logger . info ( f \"Getting network enterprise edge list for host { host } and enterprises { enterprise_ids } \" ) get_network_enterprise_edges logger . info ( f \"Sent list of network enterprises edges for enterprises: { enterprise_ids } and host { host } \" )","title":"Network enterprise edge list"},{"location":"logging/services/velocloud-bridge/actions/network_enterprise_edge_list/#subject-requestnetworkenterpriseedges","text":"Message arrives at subject If message doesn't have a body: logger . error ( f \"Cannot get network enterprise edge list with { json . dumps ( payload ) } . JSON malformed\" ) END If message body doesn't have host and enterprise_ids filters: logger . error ( f 'Cannot get network enterprise edge list with { json . dumps ( payload ) } . Need parameters \"host\" and ' f '\"enterprise_ids\"' ) END logger . info ( f \"Getting network enterprise edge list for host { host } and enterprises { enterprise_ids } \" ) get_network_enterprise_edges logger . info ( f \"Sent list of network enterprises edges for enterprises: { enterprise_ids } and host { host } \" )","title":"Subject: request.network.enterprise.edges"},{"location":"logging/services/velocloud-bridge/actions/network_gateway_list/","text":"Subject: request.network.gateway.list Message arrives at subject If message doesn't have the expected format: logger . warning ( f \"Wrong request message: msg= { msg } , validation_error= { e } \" ) END logger . info ( f \"Getting network gateway list on host { host } ...\" ) get_network_gateways logger . info ( f \"Sent network gateway list on host { host } \" )","title":"Network gateway list"},{"location":"logging/services/velocloud-bridge/actions/network_gateway_list/#subject-requestnetworkgatewaylist","text":"Message arrives at subject If message doesn't have the expected format: logger . warning ( f \"Wrong request message: msg= { msg } , validation_error= { e } \" ) END logger . info ( f \"Getting network gateway list on host { host } ...\" ) get_network_gateways logger . info ( f \"Sent network gateway list on host { host } \" )","title":"Subject: request.network.gateway.list"},{"location":"logging/services/velocloud-bridge/app_entrypoint/app/","text":"App entrypoint app_logger . info ( \"Velocloud bridge starting...\" )","title":"App"},{"location":"logging/services/velocloud-bridge/app_entrypoint/app/#app-entrypoint","text":"app_logger . info ( \"Velocloud bridge starting...\" )","title":"App entrypoint"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/__log_result/","text":"Log result If result status is 400 : logger . error ( f \"Got error from Velocloud -> { body } \" ) If result status is 401 : logger . error ( f \"Authentication error -> { body } \" ) If result status is between 500 and 512 (both inclusive): logger . error ( f \"Got { status } from Velocloud\" )","title":"  log result"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/__log_result/#log-result","text":"If result status is 400 : logger . error ( f \"Got error from Velocloud -> { body } \" ) If result status is 401 : logger . error ( f \"Authentication error -> { body } \" ) If result status is between 500 and 512 (both inclusive): logger . error ( f \"Got { status } from Velocloud\" )","title":"Log result"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/__schedule_relogin_job_if_needed/","text":"Schedule relogin job if needed If auth token expired for a particular VeloCloud host: logger . info ( f \"Auth token expired for host { velocloud_host } . Scheduling re-login job...\" ) _start_relogin_job","title":"  schedule relogin job if needed"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/__schedule_relogin_job_if_needed/#schedule-relogin-job-if-needed","text":"If auth token expired for a particular VeloCloud host: logger . info ( f \"Auth token expired for host { velocloud_host } . Scheduling re-login job...\" ) _start_relogin_job","title":"Schedule relogin job if needed"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_create_and_connect_client/","text":"Create and connect client logger . info ( f \"Logging in host: { host } to velocloud\" ) _create_headers_by_host If VeloCloud responded with valid authentication headers: logger . info ( f \"Authentication headers refreshed for host { host } successfully\" ) Otherwise: logger . error ( f \"Authentication headers could not be refreshed for host { host } . Got response: { headers } \" )","title":" create and connect client"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_create_and_connect_client/#create-and-connect-client","text":"logger . info ( f \"Logging in host: { host } to velocloud\" ) _create_headers_by_host If VeloCloud responded with valid authentication headers: logger . info ( f \"Authentication headers refreshed for host { host } successfully\" ) Otherwise: logger . error ( f \"Authentication headers could not be refreshed for host { host } . Got response: { headers } \" )","title":"Create and connect client"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_create_headers_by_host/","text":"Create headers by host logger . info ( f \"Logging in to host { host } ...\" ) Make HTTP call to POST /login/operatorLogin . If response status for logging in to the VeloCloud host is ok: logger . info ( f \"Logged in to host { host } successfully\" ) If response status for logging in to the VeloCloud host is 302 : logger . error ( f \"Got HTTP 302 while logging in to host { host } \" )","title":" create headers by host"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_create_headers_by_host/#create-headers-by-host","text":"logger . info ( f \"Logging in to host { host } ...\" ) Make HTTP call to POST /login/operatorLogin . If response status for logging in to the VeloCloud host is ok: logger . info ( f \"Logged in to host { host } successfully\" ) If response status for logging in to the VeloCloud host is 302 : logger . error ( f \"Got HTTP 302 while logging in to host { host } \" )","title":"Create headers by host"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_json_return/","text":"JSON return If the response has errors: If the response indicates that the authentication token expired: logger . info ( f \"Response returned: { response } . Attempting to relogin\" ) _start_relogin_job Otherwise: logger . error ( f \"Error response returned: { response } \" )","title":" json return"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_json_return/#json-return","text":"If the response has errors: If the response indicates that the authentication token expired: logger . info ( f \"Response returned: { response } . Attempting to relogin\" ) _start_relogin_job Otherwise: logger . error ( f \"Error response returned: { response } \" )","title":"JSON return"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_relogin_client/","text":"Relogin client logger . info ( f \"Relogging in host: { host } to velocloud\" ) If the VeloCloud host is in the list of supported hosts: logger . info ( f \"Host { host } is in the list of available clients. Refreshing authentication headers...\" ) _create_and_connect_client","title":" relogin client"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_relogin_client/#relogin-client","text":"logger . info ( f \"Relogging in host: { host } to velocloud\" ) If the VeloCloud host is in the list of supported hosts: logger . info ( f \"Host { host } is in the list of available clients. Refreshing authentication headers...\" ) _create_and_connect_client","title":"Relogin client"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_start_relogin_job/","text":"Start relogin job logger . info ( f \"Scheduling relogin job for host { host } ...\" ) _relogin_client is scheduled to trigger immediately If there's a relogin job already scheduled for that VeloCloud host: logger . error ( f \"Skipping start of relogin job for host { host } . Reason: { conflict } \" ) END logger . info ( f \"Relogin job for host { host } has been scheduled\" )","title":" start relogin job"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/_start_relogin_job/#start-relogin-job","text":"logger . info ( f \"Scheduling relogin job for host { host } ...\" ) _relogin_client is scheduled to trigger immediately If there's a relogin job already scheduled for that VeloCloud host: logger . error ( f \"Skipping start of relogin job for host { host } . Reason: { conflict } \" ) END logger . info ( f \"Relogin job for host { host } has been scheduled\" )","title":"Start relogin job"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_all_enterprise_names/","text":"Get all enterprise names For each available VeloCloud host: get_monitoring_aggregates If response status of get monitoring aggregates for current VeloCloud host is not ok: logger . error ( f \"Function [get_all_enterprise_names] Error: \\n \" f \"Status : { res [ 'status' ] } , \\n \" f \"Error Message: { res [ 'body' ] } \" ) Continue to next VeloCloud host","title":"Get all enterprise names"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_all_enterprise_names/#get-all-enterprise-names","text":"For each available VeloCloud host: get_monitoring_aggregates If response status of get monitoring aggregates for current VeloCloud host is not ok: logger . error ( f \"Function [get_all_enterprise_names] Error: \\n \" f \"Status : { res [ 'status' ] } , \\n \" f \"Error Message: { res [ 'body' ] } \" ) Continue to next VeloCloud host","title":"Get all enterprise names"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_all_events/","text":"Get all events If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { host } \" ) _start_relogin_job END logger . info ( f \"Getting all events from host { host } using payload { body } ...\" ) Call VeloCloud API endpoint POST /event/getEnterpriseEvents with the set of desired parameters. If the status of the HTTP response is 200 : logger . info ( f \"Got HTTP 200 from POST /event/getEnterpriseEvents for host { host } and payload { body } \" ) END If the status of the HTTP response is 400 : logger . error ( f \"Got HTTP 400 from Velocloud: { response_json } \" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): logger . error ( f \"Got HTTP { response . status } from Velocloud\" ) END","title":"Get all events"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_all_events/#get-all-events","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { host } \" ) _start_relogin_job END logger . info ( f \"Getting all events from host { host } using payload { body } ...\" ) Call VeloCloud API endpoint POST /event/getEnterpriseEvents with the set of desired parameters. If the status of the HTTP response is 200 : logger . info ( f \"Got HTTP 200 from POST /event/getEnterpriseEvents for host { host } and payload { body } \" ) END If the status of the HTTP response is 400 : logger . error ( f \"Got HTTP 400 from Velocloud: { response_json } \" ) END If the status of the HTTP response is between 500 and 512 (both inclusive): logger . error ( f \"Got HTTP { response . status } from Velocloud\" ) END","title":"Get all events"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_edge_configuration_modules/","text":"Get edge configuration modules If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Trying to get edge links series for payload { payload } from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /edge/getEdgeConfigurationModules with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after fetching edge link series for { payload } \" f \"and host { velocloud_host } \" ) __log_result","title":"Get edge configuration modules"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_edge_configuration_modules/#get-edge-configuration-modules","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Trying to get edge links series for payload { payload } from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /edge/getEdgeConfigurationModules with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after fetching edge link series for { payload } \" f \"and host { velocloud_host } \" ) __log_result","title":"Get edge configuration modules"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_edge_links_series/","text":"Get edge links series If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Trying to get edge links series for payload { payload } from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /metrics/getEdgeLinkSeries with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after fetching edge link series for { payload } \" f \"and host { velocloud_host } \" ) __log_result","title":"Get edge links series"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_edge_links_series/#get-edge-links-series","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Trying to get edge links series for payload { payload } from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /metrics/getEdgeLinkSeries with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after fetching edge link series for { payload } \" f \"and host { velocloud_host } \" ) __log_result","title":"Get edge links series"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_enterprise_edges/","text":"Get enterprise edges If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f \"Getting all enterprise edges from enterprise ID { enterprise_id } and\" f ' from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /enterprise/getEnterpriseEdges with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting enterprise edges for enterprise { enterprise_id } \" f \"and host { velocloud_host } \" ) __log_result","title":"Get enterprise edges"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_enterprise_edges/#get-enterprise-edges","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f \"Getting all enterprise edges from enterprise ID { enterprise_id } and\" f ' from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /enterprise/getEnterpriseEdges with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting enterprise edges for enterprise { enterprise_id } \" f \"and host { velocloud_host } \" ) __log_result","title":"Get enterprise edges"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_gateway_status_metrics/","text":"Get gateway status metrics If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f \"Getting gateway status metrics for gateway { gateway_id } and host { velocloud_host } in \" f \"interval { interval } ...\" ) Call VeloCloud API endpoint POST /metrics/getGatewayStatusMetrics with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END If the status of the HTTP response is 400 : __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting gateway status metrics for gateway { gateway_id } \" f \"and host { velocloud_host } in interval { interval } \" ) __log_result","title":"Get gateway status metrics"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_gateway_status_metrics/#get-gateway-status-metrics","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f \"Getting gateway status metrics for gateway { gateway_id } and host { velocloud_host } in \" f \"interval { interval } ...\" ) Call VeloCloud API endpoint POST /metrics/getGatewayStatusMetrics with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END If the status of the HTTP response is 400 : __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting gateway status metrics for gateway { gateway_id } \" f \"and host { velocloud_host } in interval { interval } \" ) __log_result","title":"Get gateway status metrics"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_links_metric_info/","text":"Get links metric info If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Getting links metric info from Velocloud host \" { velocloud_host } \" for interval { interval } ...' ) Call VeloCloud API endpoint POST /monitoring/getAggregateEdgeLinkMetrics with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after claiming links metric info for host { velocloud_host } and \" f \"interval { interval } \" ) __log_result","title":"Get links metric info"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_links_metric_info/#get-links-metric-info","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Getting links metric info from Velocloud host \" { velocloud_host } \" for interval { interval } ...' ) Call VeloCloud API endpoint POST /monitoring/getAggregateEdgeLinkMetrics with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after claiming links metric info for host { velocloud_host } and \" f \"interval { interval } \" ) __log_result","title":"Get links metric info"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_links_with_edge_info/","text":"Get links with edge info If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Getting links with edge info from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /monitoring/getEnterpriseEdgeLinkStatus with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after claiming links with edge info for host { velocloud_host } \" ) __log_result","title":"Get links with edge info"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_links_with_edge_info/#get-links-with-edge-info","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f 'Getting links with edge info from Velocloud host \" { velocloud_host } \"...' ) Call VeloCloud API endpoint POST /monitoring/getEnterpriseEdgeLinkStatus with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after claiming links with edge info for host { velocloud_host } \" ) __log_result","title":"Get links with edge info"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_monitoring_aggregates/","text":"Get monitoring aggregates logger . info ( f \"Getting monitoring aggregates for host { client [ 'host' ] } \" ) Call VeloCloud API endpoint POST /monitoring/getAggregates . If the status of the HTTP response is 200 : logger . info ( f \"Got HTTP 200 from POST /monitoring/getAggregates for host { client [ 'host' ] } \" ) _json_return END If the status of the HTTP response is 400 : logger . error ( f \"Got HTTP 400 from Velocloud { response_json } \" ) If the status of the HTTP response is between 500 and 512 (both inclusive): logger . error ( f \"Got HTTP { response . status } \" )","title":"Get monitoring aggregates"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_monitoring_aggregates/#get-monitoring-aggregates","text":"logger . info ( f \"Getting monitoring aggregates for host { client [ 'host' ] } \" ) Call VeloCloud API endpoint POST /monitoring/getAggregates . If the status of the HTTP response is 200 : logger . info ( f \"Got HTTP 200 from POST /monitoring/getAggregates for host { client [ 'host' ] } \" ) _json_return END If the status of the HTTP response is 400 : logger . error ( f \"Got HTTP 400 from Velocloud { response_json } \" ) If the status of the HTTP response is between 500 and 512 (both inclusive): logger . error ( f \"Got HTTP { response . status } \" )","title":"Get monitoring aggregates"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_network_enterprises/","text":"Get network enterprises logger . info ( f \"Getting network enterprise edges for host { velocloud_host } and enterprises { enterprise_ids } ...\" ) Call VeloCloud API endpoint POST /network/getNetworkEnterprises with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END If the status of the HTTP response is 400 : __log_result END If the status of the HTTP response is any other: __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting enterprise ids: { enterprise_ids } \" f \"from host { velocloud_host } \" ) __log_result","title":"Get network enterprises"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_network_enterprises/#get-network-enterprises","text":"logger . info ( f \"Getting network enterprise edges for host { velocloud_host } and enterprises { enterprise_ids } ...\" ) Call VeloCloud API endpoint POST /network/getNetworkEnterprises with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END If the status of the HTTP response is 400 : __log_result END If the status of the HTTP response is any other: __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting enterprise ids: { enterprise_ids } \" f \"from host { velocloud_host } \" ) __log_result","title":"Get network enterprises"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_network_gateways/","text":"Get network gateways If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f \"Getting network gateways for host { velocloud_host } ...\" ) Call VeloCloud API endpoint POST /network/getNetworkGateways with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END If the status of the HTTP response is 400 : __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting network gateways for host { velocloud_host } \" ) __log_result","title":"Get network gateways"},{"location":"logging/services/velocloud-bridge/clients/velocloud_client/get_network_gateways/#get-network-gateways","text":"If there's no client authenticated against the VeloCloud host: logger . error ( f \"Cannot find a client to connect to { velocloud_host } \" ) _start_relogin_job __log_result END logger . info ( f \"Getting network gateways for host { velocloud_host } ...\" ) Call VeloCloud API endpoint POST /network/getNetworkGateways with the set of desired parameters. If there's an error while connecting to VeloCloud API: __log_result END If the status of the HTTP response is between 500 and 512 (both inclusive): __log_result END If the status of the HTTP response is 400 : __log_result END __schedule_relogin_job_if_needed logger . info ( f \"Got HTTP { response . status } from Velocloud after getting network gateways for host { velocloud_host } \" ) __log_result","title":"Get network gateways"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/_get_all_events/","text":"Get all events If event types filter is defined: logger . info ( f \"Using event type filter { filter_events_status_list } to get all events from host { host } \" ) logger . info ( f \"Getting all events from host { host } using filters { body } \" ) get_all_events If response status for get all events is not ok: logger . error ( f \"Could not get all events from { host } using filters { body } . Response: { response } \" )","title":"Get all events"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/_get_all_events/#get-all-events","text":"If event types filter is defined: logger . info ( f \"Using event type filter { filter_events_status_list } to get all events from host { host } \" ) logger . info ( f \"Getting all events from host { host } using filters { body } \" ) get_all_events If response status for get all events is not ok: logger . error ( f \"Could not get all events from { host } using filters { body } . Response: { response } \" )","title":"Get all events"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_all_edge_events/","text":"Get all edge events logger . info ( f \"Getting events for edge { edge } between { start } and { end } ...\" ) _get_all_events","title":"Get all edge events"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_all_edge_events/#get-all-edge-events","text":"logger . info ( f \"Getting events for edge { edge } between { start } and { end } ...\" ) _get_all_events","title":"Get all edge events"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_all_enterprise_events/","text":"Get all enterprise events logger . info ( f \"Getting events from enterprise { enterprise } of host { host } between { start } and { end } \" ) _get_all_events","title":"Get all enterprise events"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_all_enterprise_events/#get-all-enterprise-events","text":"logger . info ( f \"Getting events from enterprise { enterprise } of host { host } between { start } and { end } \" ) _get_all_events","title":"Get all enterprise events"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_all_enterprise_names/","text":"Get all enterprise names logger . info ( \"Getting all enterprise names\" ) get_all_enterprise_names If response status for get all enterprise names is not ok: logger . error ( f \"Error { enterprises [ 'status' ] } , error: { enterprises [ 'body' ] } \" )","title":"Get all enterprise names"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_all_enterprise_names/#get-all-enterprise-names","text":"logger . info ( \"Getting all enterprise names\" ) get_all_enterprise_names If response status for get all enterprise names is not ok: logger . error ( f \"Error { enterprises [ 'status' ] } , error: { enterprises [ 'body' ] } \" )","title":"Get all enterprise names"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_edge_links_series/","text":"Get edge links series get_edge_links_series","title":"Get edge links series"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_edge_links_series/#get-edge-links-series","text":"get_edge_links_series","title":"Get edge links series"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_enterprise_edges/","text":"Get enterprise edges get_enterprise_edges","title":"Get enterprise edges"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_enterprise_edges/#get-enterprise-edges","text":"get_enterprise_edges","title":"Get enterprise edges"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_gateway_status_metrics/","text":"Get gateway status metrics get_gateway_status_metrics","title":"Get gateway status metrics"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_gateway_status_metrics/#get-gateway-status-metrics","text":"get_gateway_status_metrics","title":"Get gateway status metrics"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_links_configuration/","text":"Get links configuration get_edge_configuration_modules If response status for get edge configuration modules is not ok: logger . error ( f \"Could not get links configuration for edge { edge_full_id } . Response: { config_modules_response } \" ) END If response does not have a WAN configuration module: logger . warning ( f \"No WAN module was found for edge { edge_full_id } \" ) END If WAN configuration module does not have links configurations: logger . warning ( f \"No links configuration was found in WAN module of edge { edge_full_id } \" ) END","title":"Get links configuration"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_links_configuration/#get-links-configuration","text":"get_edge_configuration_modules If response status for get edge configuration modules is not ok: logger . error ( f \"Could not get links configuration for edge { edge_full_id } . Response: { config_modules_response } \" ) END If response does not have a WAN configuration module: logger . warning ( f \"No WAN module was found for edge { edge_full_id } \" ) END If WAN configuration module does not have links configurations: logger . warning ( f \"No links configuration was found in WAN module of edge { edge_full_id } \" ) END","title":"Get links configuration"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_links_metric_info/","text":"Get links metric info get_links_metric_info If response status for get links metric info is not ok: logger . error ( f \"Could not get links metric info for host { velocloud_host } and interval { interval } . Response: \" f \" { links_metric_info_response } \" ) END","title":"Get links metric info"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_links_metric_info/#get-links-metric-info","text":"get_links_metric_info If response status for get links metric info is not ok: logger . error ( f \"Could not get links metric info for host { velocloud_host } and interval { interval } . Response: \" f \" { links_metric_info_response } \" ) END","title":"Get links metric info"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_links_with_edge_info/","text":"Get links with edge info get_links_with_edge_info If response status for get links with edge info is not ok: logger . error ( f \"Could not get links with edge info for host { velocloud_host } . Response: \" f \" { links_with_edge_info_response } \" ) END","title":"Get links with edge info"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_links_with_edge_info/#get-links-with-edge-info","text":"get_links_with_edge_info If response status for get links with edge info is not ok: logger . error ( f \"Could not get links with edge info for host { velocloud_host } . Response: \" f \" { links_with_edge_info_response } \" ) END","title":"Get links with edge info"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_network_enterprise_edges/","text":"Get network enterprise edges get_network_enterprises If response status for get network enterprises is not ok: logger . error ( f \"Could not get network enterprise edges for host { host } and enterprises { enterprise_ids } . Response: \" f \" { enterprise_edges_response } \" ) END If the list of enterprises from the response is empty: logger . warning ( f \"No enterprises found for host { host } and enterprise ids { enterprise_ids } \" ) END If enterprises have edges associated: logger . info ( f \"Found { len ( edges ) } edges for host { host } and enterprise ids { enterprise_ids } \" ) Otherwise: logger . warning ( f \"No edges found for host { host } and enterprise ids { enterprise_ids } \" )","title":"Get network enterprise edges"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_network_enterprise_edges/#get-network-enterprise-edges","text":"get_network_enterprises If response status for get network enterprises is not ok: logger . error ( f \"Could not get network enterprise edges for host { host } and enterprises { enterprise_ids } . Response: \" f \" { enterprise_edges_response } \" ) END If the list of enterprises from the response is empty: logger . warning ( f \"No enterprises found for host { host } and enterprise ids { enterprise_ids } \" ) END If enterprises have edges associated: logger . info ( f \"Found { len ( edges ) } edges for host { host } and enterprise ids { enterprise_ids } \" ) Otherwise: logger . warning ( f \"No edges found for host { host } and enterprise ids { enterprise_ids } \" )","title":"Get network enterprise edges"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_network_gateways/","text":"Get network gateways get_network_gateways If response status for get network gateways is not ok: logger . error ( f \"Could not get network gateways for host { host } . Response: { response } \" ) END","title":"Get network gateways"},{"location":"logging/services/velocloud-bridge/repositories/velocloud_repository/get_network_gateways/#get-network-gateways","text":"get_network_gateways If response status for get network gateways is not ok: logger . error ( f \"Could not get network gateways for host { host } . Response: { response } \" ) END","title":"Get network gateways"},{"location":"manual_configurations/INIT_AUTOMATION_PROJECT/","text":"Pre requisites AWS user credentials AWS SSH keys credentials Terraform Description The Automation Engine project at this moment is deployed in AWS infrastructure with all FEDRAMP requirements meet, this is the reason why we select the CI/CD tool of AWS to manage the application life cycle; to take advantage of the OKTA and AWS SSO access, permission and logs of every action in the Automation APP. To init the project and be able to start using these CI/CD tool we need to follow a manual steps. Considerations Ensure that your AWS credentials have the required permissions to create the listed resources , if not terraform will fail. In terraform we use a feature called workspace as a representation of the environment . The actual environments are: pro and mirror , but could be more in the future. Any change must be applied manually in all regions and pushed to the repository by the authorized operator. The definition of this resources are located in a separated repository of Automation Engine one. A manual git repository was already created that contain the terraform definition of the CI/CD tool in the master branch. This manual repository has subsequently created a job in the pipelines (automated) that performs backups together with the rest of the infrastructure. Steps Configure AWS credentials by follow the official docs . Configure AWS SSH keys credentials by follow the official docs . Clone the repository: git clone ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/fedramp-pipelines CD to the repo folder cd fedramp-pipelines/ Initializate terraformn: terraform init Select workspace/environment: terraform workspace select <environemt> Verify changes and apply (if theres a problem in the plan step, just fix an try again): terraform plan terraform apply Do the last two steps for the other environments Push the changes to the repository: git commit git push Application results Afther terraform apply of all environments, the pipelines will be configured in each separated region and will be prepared to mange Automation Engine application life cycle. This terraform project will create the follow resources in each environment: aws_codebuild_project.stages[\"deploy\"] aws_codebuild_project.stages[\"integrity\"] aws_codebuild_project.stages[\"observability\"] aws_codebuild_project.stages[\"terraform\"] aws_codebuild_project.stages[\"validate\"] aws_codecommit_approval_rule_template.approval aws_codecommit_approval_rule_template_association.approval_association aws_codecommit_repository.git aws_codepipeline.pipeline_master aws_iam_policy.pipelines aws_iam_role.pipelines aws_iam_role_policy_attachment.pipelines aws_s3_bucket.pipelines aws_s3_bucket_acl.pipelines aws_security_group.codebuild","title":"Init Automation Engine project"},{"location":"manual_configurations/INIT_AUTOMATION_PROJECT/#pre-requisites","text":"AWS user credentials AWS SSH keys credentials Terraform","title":"Pre requisites"},{"location":"manual_configurations/INIT_AUTOMATION_PROJECT/#description","text":"The Automation Engine project at this moment is deployed in AWS infrastructure with all FEDRAMP requirements meet, this is the reason why we select the CI/CD tool of AWS to manage the application life cycle; to take advantage of the OKTA and AWS SSO access, permission and logs of every action in the Automation APP. To init the project and be able to start using these CI/CD tool we need to follow a manual steps.","title":"Description"},{"location":"manual_configurations/INIT_AUTOMATION_PROJECT/#considerations","text":"Ensure that your AWS credentials have the required permissions to create the listed resources , if not terraform will fail. In terraform we use a feature called workspace as a representation of the environment . The actual environments are: pro and mirror , but could be more in the future. Any change must be applied manually in all regions and pushed to the repository by the authorized operator. The definition of this resources are located in a separated repository of Automation Engine one. A manual git repository was already created that contain the terraform definition of the CI/CD tool in the master branch. This manual repository has subsequently created a job in the pipelines (automated) that performs backups together with the rest of the infrastructure.","title":"Considerations"},{"location":"manual_configurations/INIT_AUTOMATION_PROJECT/#steps","text":"Configure AWS credentials by follow the official docs . Configure AWS SSH keys credentials by follow the official docs . Clone the repository: git clone ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/fedramp-pipelines CD to the repo folder cd fedramp-pipelines/ Initializate terraformn: terraform init Select workspace/environment: terraform workspace select <environemt> Verify changes and apply (if theres a problem in the plan step, just fix an try again): terraform plan terraform apply Do the last two steps for the other environments Push the changes to the repository: git commit git push","title":"Steps"},{"location":"manual_configurations/INIT_AUTOMATION_PROJECT/#application-results","text":"Afther terraform apply of all environments, the pipelines will be configured in each separated region and will be prepared to mange Automation Engine application life cycle. This terraform project will create the follow resources in each environment: aws_codebuild_project.stages[\"deploy\"] aws_codebuild_project.stages[\"integrity\"] aws_codebuild_project.stages[\"observability\"] aws_codebuild_project.stages[\"terraform\"] aws_codebuild_project.stages[\"validate\"] aws_codecommit_approval_rule_template.approval aws_codecommit_approval_rule_template_association.approval_association aws_codecommit_repository.git aws_codepipeline.pipeline_master aws_iam_policy.pipelines aws_iam_role.pipelines aws_iam_role_policy_attachment.pipelines aws_s3_bucket.pipelines aws_s3_bucket_acl.pipelines aws_security_group.codebuild","title":"Application results"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/","text":"Pre requisites Okta Account AWS Account Description Because of FEDRAMP we need to implement as a team a IdP that control all users and permissions related to MetTel projects that are made by Intelygenz. For automatic synchronization we are going to create SCIM sync between OKTA and AWS SSO, because of that, groups and users are going to be synced if someone deletes/create a group/user in Okta. Considerations Remove a user/group don't revoke the session tokens in AWS, the minimum duration of these tokens are of 1h. Info here Using the same Okta group for both assignments and group push is not currently supported. To maintain consistent group memberships between Okta and AWS SSO, you need to create a separate group and configure it to push groups to AWS SSO. Info here If you update a user\u2019s address you must have streetAddress, city, state, zipCode and the countryCode value specified. If any of these values are not specified for the Okta user at the time of synchronization, the user or changes to the user will not be provisioned. Info here Entitlements and role attributes are not supported and cannot be synced to AWS SSO. Info here Steps Configure IdP with Okta, this is the guide . Create the following groups: OKTA-IPA-FED-INT-PRIVILEGED: Internal users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-INT-NON-PRIVILEGED: Internal users Federated non privileged group on the federal account. OKTA-IPA-COM-INT-PRIVILEGED: Internal users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-INT-NON-PRIVILEGED: Internal users Federated privileged group on the commercial account. Administration accounts. OKTA-IPA-FED-EXT-PRIVILEGED: External users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-EXT-NON-PRIVILEGED: External users Federated non privileged group on the federal account. OKTA-IPA-COM-EXT-PRIVILEGED: External users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-EXT-NON-PRIVILEGED: External users Federated privileged group on the commercial account. Administration accounts. Associate permissions to groups. Guide Privileged accounts will have general administrator permissions Non privileged accounts will only have access to logs on cloud watch and grafana Revoke permissions Because of the problem of token duration of 1h that can not be revoked from okta, there is a manual procedure to delete the access from AWS SSO. For revoking access follow this guide","title":"AWS SSO Okta JWT token"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#pre-requisites","text":"Okta Account AWS Account","title":"Pre requisites"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#description","text":"Because of FEDRAMP we need to implement as a team a IdP that control all users and permissions related to MetTel projects that are made by Intelygenz. For automatic synchronization we are going to create SCIM sync between OKTA and AWS SSO, because of that, groups and users are going to be synced if someone deletes/create a group/user in Okta.","title":"Description"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#considerations","text":"Remove a user/group don't revoke the session tokens in AWS, the minimum duration of these tokens are of 1h. Info here Using the same Okta group for both assignments and group push is not currently supported. To maintain consistent group memberships between Okta and AWS SSO, you need to create a separate group and configure it to push groups to AWS SSO. Info here If you update a user\u2019s address you must have streetAddress, city, state, zipCode and the countryCode value specified. If any of these values are not specified for the Okta user at the time of synchronization, the user or changes to the user will not be provisioned. Info here Entitlements and role attributes are not supported and cannot be synced to AWS SSO. Info here","title":"Considerations"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#steps","text":"Configure IdP with Okta, this is the guide . Create the following groups: OKTA-IPA-FED-INT-PRIVILEGED: Internal users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-INT-NON-PRIVILEGED: Internal users Federated non privileged group on the federal account. OKTA-IPA-COM-INT-PRIVILEGED: Internal users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-INT-NON-PRIVILEGED: Internal users Federated privileged group on the commercial account. Administration accounts. OKTA-IPA-FED-EXT-PRIVILEGED: External users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-EXT-NON-PRIVILEGED: External users Federated non privileged group on the federal account. OKTA-IPA-COM-EXT-PRIVILEGED: External users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-EXT-NON-PRIVILEGED: External users Federated privileged group on the commercial account. Administration accounts. Associate permissions to groups. Guide Privileged accounts will have general administrator permissions Non privileged accounts will only have access to logs on cloud watch and grafana","title":"Steps"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#revoke-permissions","text":"Because of the problem of token duration of 1h that can not be revoked from okta, there is a manual procedure to delete the access from AWS SSO. For revoking access follow this guide","title":"Revoke permissions"},{"location":"manual_configurations/OKTA_JWT/","text":"Creation of Oauth Application for Data Highway API Pre requisites Okta Account AWS Account Description API needs a way to verify the access of vendors and internal users provided from Okta. They way to do it is about provide a JWT from Okta with a company name information on it. Group API access First steep is about the creation of a group for the API access, only members with his group will access the API. The groups must have the next values: * Name: IGZ-INT-DATA-HIGHWAY-API * Description: API Data highway access group. Name: IGZ-EXT-DATA-HIGHWAY-API Description: API Data highway access group. Create Application server To create an application destined for the API, we are gonna need to go to \"Applications/Applications\" and click on create app integration A new form will show and we need to fill it with the next information: App integration name: IGZ-DATA-HIGHWAY-API Grant type: Select \"Authorization Code\" and \"Resource Owner Password\" Controlled access: Select \"Limit access to selected groups\" Selected groups: IGZ-INT-DATA-HIGHWAY-API, IGZ-EXT-DATA-HIGHWAY-API Click Save. Create Authorization server In the Admin Dashboard, go to security/API and click on add authorization server. We will need to fill the name, audience, and description. the next values are the selected ones: Name: API-DATA-HIGHWAY Audience: API-DATA-HIGHWAY Description: API data highway access. After filling this form, click on the save button. 3. With the Authorization server created, we need to add some extra configurations. Go to the Claims menu following the next image 1. Click on \"Add claim\" 2. Fill the new claim with the next information: 1. Name: organization 2. Include in token type: \"Id Token\" - \"Always\" 3. Value type: \"Expression\" 4. Value: user.organization 5. Include in: \"Any scope\" 4. Go to access policies menu following the next image: 1. in this menu click in the button \"Add new access policy\" 2. it will appear a new form, fill it with the next values and click on create policy: 1. Name: IGZ-DATA-HIGHWAY-API-JWT-ACCESS-POLICY 2. Description: Api JWT access policy. 3. Assign to: IGZ-INT-DATA-HIGHWAY-API, IGZ-EXT-DATA-HIGHWAY-API 3. After closing the form, select the policy created and click on the \"Add rule\" button. And fill the form with the next information: 1. Rule Name: IGZ-DATA-HIGHWAY-API-ACCESS-RULE 2. Grant type is: Select \"Client credentials\" and \"Resource Owner Password\" 3. Assigned the app and a member of one of the following: Add the groups \"IGZ-INT-DATA-HIGHWAY-API, IGZ-EXT-DATA-HIGHWAY-API\". 4. The following scopes: profiles and openid 5. but will expire if not used every: 1 h Update Application with new parameters we are going to need to go to \"Applications/Applications\" and update the configuration of the application \"IGZ-DATA-HIGHWAY-API\". Disable User consent in general configurations.","title":"AWS SSO Okta JWT token"},{"location":"manual_configurations/OKTA_JWT/#creation-of-oauth-application-for-data-highway-api","text":"","title":"Creation of Oauth Application for Data Highway API"},{"location":"manual_configurations/OKTA_JWT/#pre-requisites","text":"Okta Account AWS Account","title":"Pre requisites"},{"location":"manual_configurations/OKTA_JWT/#description","text":"API needs a way to verify the access of vendors and internal users provided from Okta. They way to do it is about provide a JWT from Okta with a company name information on it.","title":"Description"},{"location":"manual_configurations/OKTA_JWT/#group-api-access","text":"First steep is about the creation of a group for the API access, only members with his group will access the API. The groups must have the next values: * Name: IGZ-INT-DATA-HIGHWAY-API * Description: API Data highway access group. Name: IGZ-EXT-DATA-HIGHWAY-API Description: API Data highway access group.","title":"Group API access"},{"location":"manual_configurations/OKTA_JWT/#create-application-server","text":"To create an application destined for the API, we are gonna need to go to \"Applications/Applications\" and click on create app integration A new form will show and we need to fill it with the next information: App integration name: IGZ-DATA-HIGHWAY-API Grant type: Select \"Authorization Code\" and \"Resource Owner Password\" Controlled access: Select \"Limit access to selected groups\" Selected groups: IGZ-INT-DATA-HIGHWAY-API, IGZ-EXT-DATA-HIGHWAY-API Click Save.","title":"Create Application server"},{"location":"manual_configurations/OKTA_JWT/#create-authorization-server","text":"In the Admin Dashboard, go to security/API and click on add authorization server. We will need to fill the name, audience, and description. the next values are the selected ones: Name: API-DATA-HIGHWAY Audience: API-DATA-HIGHWAY Description: API data highway access. After filling this form, click on the save button. 3. With the Authorization server created, we need to add some extra configurations. Go to the Claims menu following the next image 1. Click on \"Add claim\" 2. Fill the new claim with the next information: 1. Name: organization 2. Include in token type: \"Id Token\" - \"Always\" 3. Value type: \"Expression\" 4. Value: user.organization 5. Include in: \"Any scope\" 4. Go to access policies menu following the next image: 1. in this menu click in the button \"Add new access policy\" 2. it will appear a new form, fill it with the next values and click on create policy: 1. Name: IGZ-DATA-HIGHWAY-API-JWT-ACCESS-POLICY 2. Description: Api JWT access policy. 3. Assign to: IGZ-INT-DATA-HIGHWAY-API, IGZ-EXT-DATA-HIGHWAY-API 3. After closing the form, select the policy created and click on the \"Add rule\" button. And fill the form with the next information: 1. Rule Name: IGZ-DATA-HIGHWAY-API-ACCESS-RULE 2. Grant type is: Select \"Client credentials\" and \"Resource Owner Password\" 3. Assigned the app and a member of one of the following: Add the groups \"IGZ-INT-DATA-HIGHWAY-API, IGZ-EXT-DATA-HIGHWAY-API\". 4. The following scopes: profiles and openid 5. but will expire if not used every: 1 h","title":"Create Authorization server"},{"location":"manual_configurations/OKTA_JWT/#update-application-with-new-parameters","text":"we are going to need to go to \"Applications/Applications\" and update the configuration of the application \"IGZ-DATA-HIGHWAY-API\". Disable User consent in general configurations.","title":"Update Application with new parameters"},{"location":"manual_procedures/API_VENDOR_ACCESS/","text":"Pre requisites Okta Account AWS Account Description This procedure explains how to give access to a specific vendor to the data highway API. Vendor requirements Provide a static trusted IP of the vendor MetTel configurations MetTel needs to do these next steps: The Okta vendor account into OKTA group DATA-HIGHWAY-API Vendor profile filled with the organization name of their company. API gateway configurations On the data highway team: Add the trusted static IP on the API gateway access policy.","title":"Vendor access to the API"},{"location":"manual_procedures/API_VENDOR_ACCESS/#pre-requisites","text":"Okta Account AWS Account","title":"Pre requisites"},{"location":"manual_procedures/API_VENDOR_ACCESS/#description","text":"This procedure explains how to give access to a specific vendor to the data highway API.","title":"Description"},{"location":"manual_procedures/API_VENDOR_ACCESS/#vendor-requirements","text":"Provide a static trusted IP of the vendor","title":"Vendor requirements"},{"location":"manual_procedures/API_VENDOR_ACCESS/#mettel-configurations","text":"MetTel needs to do these next steps: The Okta vendor account into OKTA group DATA-HIGHWAY-API Vendor profile filled with the organization name of their company.","title":"MetTel configurations"},{"location":"manual_procedures/API_VENDOR_ACCESS/#api-gateway-configurations","text":"On the data highway team: Add the trusted static IP on the API gateway access policy.","title":"API gateway configurations"},{"location":"manual_procedures/SWITCH_AUTOMATION_ENGINE_REGION/","text":"Pre requisites AWS user credentials AWS SSH keys credentials Verify AWS Personal Health Dashboard Verify AWS Services Health Dashboard Description The Automation Engine application is prepared to be deployed in different AWS regions, each region is associated with an environment . At this moment we have two environments: pro and mirror in us-east-1 and us-west-1 respectively; but we can have more in the future. By default the application is deployed in pro environment but can change by updating a variable in the repository. This way if we suffer a disaster and lose the main region we can easily deploy the application in the mirror region, this is the goal of this procedure. Considerations Ensure that your AWS credentials have Codecommit permission to interact with Automation Engine repository in all available regions. We have configured a git mirroring from pro to mirror environment, by that way we always have updated the repo in the mirror region. The actual environments are: pro and mirror , but could be more in the future. Master branch is protected, you can't commit directly to it, you must create a new branch and do a pull request. An authorized team member must accept your PR once the changes was verified. Once the pipeline run, if we check logs, we can verify that is possible that terraform failed the execution of union resources(AWS resources that create links between regions), this is originated by AWS unavailability in one region. This behavior is expected since we assume that an AWS Region is not working properly; this job is allowed to fail for this reason, if any other task fails the pipeline will end with an error. Posible situation We have all active infrastructure and application in pro ( us-east-1 ), then AWS suffer a disaster in Nort Virginia region( us-east-1 ) and we lost the activity and respondes from AWS resources; we must assume that all resources are unavailable. It's time to star using the resources in the mirror region( us-west-1 ) by following the next steps: Steps Verify AWS Health Dashboards to confirm AWS services availabily per region. Confirm that our services in the main region( us-east-1 ) are unavailable. Clone the repository from the mirror region( us-west-1 ): git clone ssh://git-codecommit.us-west-1.amazonaws.com/v1/repos/fedramp-automation-engine Create a new branch with the name switch-automation-engine-region . cd fedramp-automation-engine git branch -b switch-automation-engine-region Edit ACTIVE_ENVIRONMENT default value in infra-as-code/basic-infra/common_info.tf file variable \"ACTIVE_ENVIRONMENT\" { default = \"pro\" <---- put the desired environment here, example: \"mirror\" description = \"Active environment in aws\" } Push the changes to the repository: git commit git push Go to AWS Codecommit console in mirror region( us-west-1 ), in pull request section of the automation repo and create a new PR After the approve from the authorized team member, merge to master in the same AWS codecommit console. Git merge results Once the merge is complete, this will trigger a pipeline in the mirror region( us-west-1 ) that will create the infrastructure based in the variable ACTIVE_ENVIRONMENT , this means that the application will be deployed in the region where we previously confirm that AWS has no issues.","title":"Switch Automation Engine region"},{"location":"manual_procedures/SWITCH_AUTOMATION_ENGINE_REGION/#pre-requisites","text":"AWS user credentials AWS SSH keys credentials Verify AWS Personal Health Dashboard Verify AWS Services Health Dashboard","title":"Pre requisites"},{"location":"manual_procedures/SWITCH_AUTOMATION_ENGINE_REGION/#description","text":"The Automation Engine application is prepared to be deployed in different AWS regions, each region is associated with an environment . At this moment we have two environments: pro and mirror in us-east-1 and us-west-1 respectively; but we can have more in the future. By default the application is deployed in pro environment but can change by updating a variable in the repository. This way if we suffer a disaster and lose the main region we can easily deploy the application in the mirror region, this is the goal of this procedure.","title":"Description"},{"location":"manual_procedures/SWITCH_AUTOMATION_ENGINE_REGION/#considerations","text":"Ensure that your AWS credentials have Codecommit permission to interact with Automation Engine repository in all available regions. We have configured a git mirroring from pro to mirror environment, by that way we always have updated the repo in the mirror region. The actual environments are: pro and mirror , but could be more in the future. Master branch is protected, you can't commit directly to it, you must create a new branch and do a pull request. An authorized team member must accept your PR once the changes was verified. Once the pipeline run, if we check logs, we can verify that is possible that terraform failed the execution of union resources(AWS resources that create links between regions), this is originated by AWS unavailability in one region. This behavior is expected since we assume that an AWS Region is not working properly; this job is allowed to fail for this reason, if any other task fails the pipeline will end with an error.","title":"Considerations"},{"location":"manual_procedures/SWITCH_AUTOMATION_ENGINE_REGION/#posible-situation","text":"We have all active infrastructure and application in pro ( us-east-1 ), then AWS suffer a disaster in Nort Virginia region( us-east-1 ) and we lost the activity and respondes from AWS resources; we must assume that all resources are unavailable. It's time to star using the resources in the mirror region( us-west-1 ) by following the next steps:","title":"Posible situation"},{"location":"manual_procedures/SWITCH_AUTOMATION_ENGINE_REGION/#steps","text":"Verify AWS Health Dashboards to confirm AWS services availabily per region. Confirm that our services in the main region( us-east-1 ) are unavailable. Clone the repository from the mirror region( us-west-1 ): git clone ssh://git-codecommit.us-west-1.amazonaws.com/v1/repos/fedramp-automation-engine Create a new branch with the name switch-automation-engine-region . cd fedramp-automation-engine git branch -b switch-automation-engine-region Edit ACTIVE_ENVIRONMENT default value in infra-as-code/basic-infra/common_info.tf file variable \"ACTIVE_ENVIRONMENT\" { default = \"pro\" <---- put the desired environment here, example: \"mirror\" description = \"Active environment in aws\" } Push the changes to the repository: git commit git push Go to AWS Codecommit console in mirror region( us-west-1 ), in pull request section of the automation repo and create a new PR After the approve from the authorized team member, merge to master in the same AWS codecommit console.","title":"Steps"},{"location":"manual_procedures/SWITCH_AUTOMATION_ENGINE_REGION/#git-merge-results","text":"Once the merge is complete, this will trigger a pipeline in the mirror region( us-west-1 ) that will create the infrastructure based in the variable ACTIVE_ENVIRONMENT , this means that the application will be deployed in the region where we previously confirm that AWS has no issues.","title":"Git merge results"},{"location":"metrics-definitions/","text":"Metrics Definitions This folder will contain all the metrics created to track functional and business values that improve the overall observability of the system. There will be one markdown file per metric in this folder. The filename will be the metric name and it will contain all the descriptions and possible label combinations. Naming conventions must follow the Prometheus Best Practices for naming and units . List of metrics Metric Description tasks_created Task Creations tasks_reopened Task Re-Opens tasks_forwarded Task Forwards tasks_autoresolved Task Auto-Resolves velocloud_fetcher_to_kafka_messages_attempts VeloCloud fetcher attempts to kafka velocloud_fetcher_to_kafka_messages_status VeloCloud fetcher errors when pushing to kafka","title":"Metrics definitions"},{"location":"metrics-definitions/#metrics-definitions","text":"This folder will contain all the metrics created to track functional and business values that improve the overall observability of the system. There will be one markdown file per metric in this folder. The filename will be the metric name and it will contain all the descriptions and possible label combinations. Naming conventions must follow the Prometheus Best Practices for naming and units .","title":"Metrics Definitions"},{"location":"metrics-definitions/#list-of-metrics","text":"Metric Description tasks_created Task Creations tasks_reopened Task Re-Opens tasks_forwarded Task Forwards tasks_autoresolved Task Auto-Resolves velocloud_fetcher_to_kafka_messages_attempts VeloCloud fetcher attempts to kafka velocloud_fetcher_to_kafka_messages_status VeloCloud fetcher errors when pushing to kafka","title":"List of metrics"},{"location":"metrics-definitions/tasks_autoresolved/","text":"Task Auto-Resolves Metric name: tasks_autoresolved Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been auto-resolved by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA) | Unknown] severity: [2 | 3] has_digi: [True | False | Unknown] has_byob: [True | False | Unknown] link_types: [Wired | Wireless | Both | None | Unknown] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been auto-resolved by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability | Multiple | Unknown] has_byob: [True | False] link_types: [Wired | Wireless | Unknown] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been auto-resolved by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] TNBA Monitor Description: Number of tasks related to VeloCloud edges that have been auto-resolved by the tnba-monitor . Labels: feature: TNBA Monitor system: VeloCloud topic: [VOO | VAS] client: [<client> | FIS | Other] host: <host> severity: [2 | 3] Hawkeye Outage Monitor Description: Number of service outage tasks related to Ixia probes that have been auto-resolved by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None | Unknown] severity: 2","title":"Task Auto-Resolves #"},{"location":"metrics-definitions/tasks_autoresolved/#task-auto-resolves","text":"Metric name: tasks_autoresolved Metric type: Counter Data store: Prometheus","title":"Task Auto-Resolves"},{"location":"metrics-definitions/tasks_autoresolved/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been auto-resolved by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA) | Unknown] severity: [2 | 3] has_digi: [True | False | Unknown] has_byob: [True | False | Unknown] link_types: [Wired | Wireless | Both | None | Unknown]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been auto-resolved by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability | Multiple | Unknown] has_byob: [True | False] link_types: [Wired | Wireless | Unknown]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been auto-resolved by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#tnba-monitor","text":"Description: Number of tasks related to VeloCloud edges that have been auto-resolved by the tnba-monitor . Labels: feature: TNBA Monitor system: VeloCloud topic: [VOO | VAS] client: [<client> | FIS | Other] host: <host> severity: [2 | 3]","title":"TNBA Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#hawkeye-outage-monitor","text":"Description: Number of service outage tasks related to Ixia probes that have been auto-resolved by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None | Unknown] severity: 2","title":"Hawkeye Outage Monitor"},{"location":"metrics-definitions/tasks_created/","text":"Task Creations Metric name: tasks_created Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been created by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been created by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been created by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] Gateway Monitor Description: Number of ServiceNow incidents related to VeloCloud gateways that have been created by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count Hawkeye Outage Monitor Description: Number of service outage tasks related to Ixia probes that have been created by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2 Fraud Monitor Description: Number of service affecting tasks related to Fraud alerts that have been created by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Task Creations #"},{"location":"metrics-definitions/tasks_created/#task-creations","text":"Metric name: tasks_created Metric type: Counter Data store: Prometheus","title":"Task Creations"},{"location":"metrics-definitions/tasks_created/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been created by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_created/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been created by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_created/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been created by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_created/#gateway-monitor","text":"Description: Number of ServiceNow incidents related to VeloCloud gateways that have been created by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count","title":"Gateway Monitor"},{"location":"metrics-definitions/tasks_created/#hawkeye-outage-monitor","text":"Description: Number of service outage tasks related to Ixia probes that have been created by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2","title":"Hawkeye Outage Monitor"},{"location":"metrics-definitions/tasks_created/#fraud-monitor","text":"Description: Number of service affecting tasks related to Fraud alerts that have been created by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Fraud Monitor"},{"location":"metrics-definitions/tasks_forwarded/","text":"Task Forwards Metric name: tasks_forwarded Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been forwarded by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] target_queue: [HNOC Investigate | ASR Investigate | Wireless Repair Intervention Needed] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been forwarded by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] target_queue: [HNOC Investigate | ASR Investigate] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been forwarded by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] target_queue: [HNOC Investigate | IPA Investigate] Fraud Monitor Description: Number of service affecting tasks related to Fraud alerts that have been forwarded by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation] target_queue: HNOC Investigate","title":"Task Forwards #"},{"location":"metrics-definitions/tasks_forwarded/#task-forwards","text":"Metric name: tasks_forwarded Metric type: Counter Data store: Prometheus","title":"Task Forwards"},{"location":"metrics-definitions/tasks_forwarded/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been forwarded by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] target_queue: [HNOC Investigate | ASR Investigate | Wireless Repair Intervention Needed]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_forwarded/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been forwarded by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] target_queue: [HNOC Investigate | ASR Investigate]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_forwarded/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been forwarded by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] target_queue: [HNOC Investigate | IPA Investigate]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_forwarded/#fraud-monitor","text":"Description: Number of service affecting tasks related to Fraud alerts that have been forwarded by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation] target_queue: HNOC Investigate","title":"Fraud Monitor"},{"location":"metrics-definitions/tasks_reopened/","text":"Task Re-Opens Metric name: tasks_reopened Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been re-opened by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been re-opened by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been re-opened by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] Gateway Monitor Description: Number of ServiceNow incidents related to VeloCloud gateways that have been re-opened by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count Hawkeye Outage Monitor Description: Number of service outage tasks related to Ixia probes that have been re-opened by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2 Fraud Monitor Description: Number of service affecting tasks related to Fraud alerts that have been re-opened by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Task Re-Opens #"},{"location":"metrics-definitions/tasks_reopened/#task-re-opens","text":"Metric name: tasks_reopened Metric type: Counter Data store: Prometheus","title":"Task Re-Opens"},{"location":"metrics-definitions/tasks_reopened/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been re-opened by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_reopened/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been re-opened by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_reopened/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been re-opened by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_reopened/#gateway-monitor","text":"Description: Number of ServiceNow incidents related to VeloCloud gateways that have been re-opened by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count","title":"Gateway Monitor"},{"location":"metrics-definitions/tasks_reopened/#hawkeye-outage-monitor","text":"Description: Number of service outage tasks related to Ixia probes that have been re-opened by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2","title":"Hawkeye Outage Monitor"},{"location":"metrics-definitions/tasks_reopened/#fraud-monitor","text":"Description: Number of service affecting tasks related to Fraud alerts that have been re-opened by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Fraud Monitor"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/","text":"VeloCloud fetcher attempts to kafka Metric name: velocloud_fetcher_to_kafka_messages_attempts Metric type: Counter Data store: Prometheus VeloCloud - Attempts total messages to Kafka Description: Number of attempts sending messages to kafka. Labels: schema_name: <schema_name> environment: [develop | master]","title":"VeloCloud fetcher attempts to kafka #"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/#velocloud-fetcher-attempts-to-kafka","text":"Metric name: velocloud_fetcher_to_kafka_messages_attempts Metric type: Counter Data store: Prometheus","title":"VeloCloud fetcher attempts to kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/#velocloud-attempts-total-messages-to-kafka","text":"Description: Number of attempts sending messages to kafka. Labels: schema_name: <schema_name> environment: [develop | master]","title":"VeloCloud - Attempts total messages to Kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/","text":"VeloCloud fetcher errors when pushing to kafka Metric name: velocloud_fetcher_to_kafka_messages_status Metric type: Counter Data store: Prometheus VeloCloud - Status of Messages sent to Kafka Description: Number of OK calls or Errors when pushing data to the kafka server. Labels: schema_name: <schema_name> status: [OK | ERROR] environment: [develop | master]","title":"VeloCloud fetcher errors when pushing to kafka #"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/#velocloud-fetcher-errors-when-pushing-to-kafka","text":"Metric name: velocloud_fetcher_to_kafka_messages_status Metric type: Counter Data store: Prometheus","title":"VeloCloud fetcher errors when pushing to kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/#velocloud-status-of-messages-sent-to-kafka","text":"Description: Number of OK calls or Errors when pushing data to the kafka server. Labels: schema_name: <schema_name> status: [OK | ERROR] environment: [develop | master]","title":"VeloCloud - Status of Messages sent to Kafka"},{"location":"parameters/parameters/","text":"Parameter Store parameters Name Description Link to AWS /automation-engine/common/autoresolve-day-end-hour Defines the hour at which the day ends and the night starts for dynamic auto-resolution times https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/autoresolve-day-end-hour/description?region=us-east-1&tab=Table /automation-engine/common/autoresolve-day-start-hour Defines the hour at which the night ends and the day starts for dynamic auto-resolution times https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/autoresolve-day-start-hour/description?region=us-east-1&tab=Table /automation-engine/common/bruin-ipa-system-username Name of the user that performs operations in Bruin on behalf of the IPA system https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/bruin-ipa-system-username/description?region=us-east-1&tab=Table /automation-engine/common/customer-cache/refresh-check-interval Defines how often the next refresh flag is checked to decide if it's time to refresh the cache or not https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/customer-cache/refresh-check-interval/description?region=us-east-1&tab=Table /automation-engine/common/customer-cache/refresh-job-interval Defines how often the cache is refreshed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/customer-cache/refresh-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/customer-cache/whitelisted-management-statuses/description?region=us-east-1&tab=Table /automation-engine/common/digi-bridge/digi-headers List of possible headers included in all DiGi links https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-bridge/digi-headers/description?region=us-east-1&tab=Table /automation-engine/common/digi-bridge/digi-reboot-api-token-ttl Auth tokens TTL https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-bridge/digi-reboot-api-token-ttl/description?region=us-east-1&tab=Table /automation-engine/common/digi-reboot-report/logs-lookup-interval Defines how much time back to look for DiGi Reboot logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-reboot-report/logs-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/digi-reboot-report/report-job-interval Defines how often the report is built and sent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-reboot-report/report-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/base-url Base URL for DRI API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/dri-data-redis-ttl Defines how much time the data retrieved from DRI for a specific device can be stored and served from Redis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/dri-data-redis-ttl/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/password Password to log into DRI API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/password/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/username Username to log into DRI API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/username/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/api-endpoint-prefix API server endpoint prefix for incoming requests https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/api-endpoint-prefix/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/max-concurrent-emails Defines how many simultaneous emails are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/max-concurrent-emails/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/new-emails-job-interval Defines how often new emails received from Bruin are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/new-emails-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/new-tickets-job-interval Defines how often new tickets received from Bruin are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/new-tickets-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/reply-email-ttl Reply emails time to live (in milliseconds) when storing them to Redis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/reply-email-ttl/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/alerts-lookup-days How many days to look back for Fraud alerts in the desired e-mail inbox https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/alerts-lookup-days/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/default-client-info-for-did-without-inventory Default client info used when the DID device in the Fraud alert does not have an inventory assigned in Bruin https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/default-client-info-for-did-without-inventory/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/default-contact-for-new-tickets Default contact details used when a Fraud is reported as a Service Affecting ticket https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/default-contact-for-new-tickets/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/monitoring-job-interval Defines how often Fraud e-mails are checked to report them as Service Affecting tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent Fraud alerts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/observed-inbox-senders/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/first-gateway-lookup-interval Lookup interval for the first time gateway statuses are retrieved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/first-gateway-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/monitoring-job-interval Defines how often gateways are checked to find and report incidents https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/second-gateway-lookup-interval Lookup interval for the second time gateway statuses are retrieved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/second-gateway-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/tunnel-count-threshold Threshold for tunnel count incidents https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/tunnel-count-threshold/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-affecting-monitor/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-affecting-monitor/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-affecting-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-affecting-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-affecting-monitor/probes-tests-results-lookup-interval Defines how much time back to look for probes' tests results https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-affecting-monitor/probes-tests-results-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-bridge/base-url Base URL to access Hawkeye API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-bridge/client-password Client password to log into Hawkeye API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-bridge/client-password/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-bridge/client-username Client username to log into Hawkeye API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-bridge/client-username/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-customer-cache/refresh-job-interval Defines how often the cache is refreshed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-customer-cache/refresh-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-customer-cache/whitelisted-management-statuses/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage Defines for how long a ticket can be auto-resolved after the last documented outage https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/quarantine-for-devices-in-outage Defines how much time to wait before checking if a particular device is still in outage state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/quarantine-for-devices-in-outage/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/dri-parameters-for-piab-notes Parameters to fetch from DRI to include them in InterMapper notes for PIAB devices https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/dri-parameters-for-piab-notes/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/max-autoresolves-per-ticket/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/max-concurrent-email-batches Defines how many simultaneous email batches related to the same InterMapper asset are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/max-concurrent-email-batches/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/monitored-up-events InterMapper events considered as UP https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/monitored-up-events/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/monitoring-job-interval Defines how often InterMapper events are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent InterMapper events https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/observed-inbox-senders/description?region=us-east-1&tab=Table /automation-engine/common/timezone Timezone used for periodic jobs, timestamps, etc https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/timezone/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/events-lookup-days How many days to look back for InterMapper events in the desired e-mail inbox https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/events-lookup-days/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/monitored-down-events InterMapper events considered as DOWN https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/monitored-down-events/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/whitelisted-product-categories-for-autoresolve Defines which Bruin product categories are taken into account when auto-resolving tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/whitelisted-product-categories-for-autoresolve/description?region=us-east-1&tab=Table /automation-engine/common/link-labels-blacklisted-from-asr-forwards List of link labels that are excluded from forwards to the ASR queue https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/link-labels-blacklisted-from-asr-forwards/description?region=us-east-1&tab=Table /automation-engine/common/link-labels-blacklisted-from-hnoc-forwards List of link labels that are excluded from forwards to the HNOC queue https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/link-labels-blacklisted-from-hnoc-forwards/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/access-token Access token for Lumin's Billing API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/access-token/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/customer-name Name of the customer for which the Billing Report will be generated https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/customer-name/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/email-account-for-message-delivery-password/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/lumin-billing-api-base-url Base URL for Lumin's Billing API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/lumin-billing-api-base-url/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/recipient Email address to send the report to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/recipient/description?region=us-east-1&tab=Table /automation-engine/common/metrics/relevant-clients List of relevant client names to use on Prometheus metrics labels https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/metrics/relevant-clients/description?region=us-east-1&tab=Table /automation-engine/common/nats/endpoint-url Nats endpoint URL for messaging https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/nats/endpoint-url/description?region=us-east-1&tab=Table /automation-engine/common/notifier/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/notifier/email-account-for-message-delivery-password/description?region=us-east-1&tab=Table /automation-engine/common/notifier/email-account-for-message-delivery-username Email account used to send messages to other accounts (username) https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/notifier/email-account-for-message-delivery-username/description?region=us-east-1&tab=Table /automation-engine/common/papertrail/host Papertrail host to send logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/papertrail/host/description?region=us-east-1&tab=Table /automation-engine/common/papertrail/port Papertrail port to send logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/papertrail/port/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/max-concurrent-closed-tickets-for-feedback Defines how many simultaneous new closed tickets are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/max-concurrent-closed-tickets-for-feedback/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/max-concurrent-created-tickets-for-feedback Defines how many simultaneous new created tickets are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/max-concurrent-created-tickets-for-feedback/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/max-concurrent-emails-for-monitoring Defines how many simultaneous tagged emails are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/max-concurrent-emails-for-monitoring/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/new-closed-tickets-feedback-job-interval Defines how often new closed tickets fetched from Bruin are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/new-closed-tickets-feedback-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/new-created-tickets-feedback-job-interval Defines how often new created tickets fetched from Bruin are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/new-created-tickets-feedback-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/rta-monitor-job-interval Defines how often new emails tagged by the E-mail Tagger are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/rta-monitor-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/tag-ids-mapping Mapping of tag names and their corresponding numeric ID, as defined in the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/tag-ids-mapping/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/enabled-customers-per-host Mapping of VeloCloud hosts and Bruin customer IDs for whom this report will trigger periodically https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/enabled-customers-per-host/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/execution-cron-expression Cron expression that determines when to build and deliver this report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/execution-cron-expression/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/lookup-interval Defines how much time back to look for bandwidth metrics and Bruin tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/recipients List of recipients that will get these reports https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/recipients/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/autoresolve-lookup-interval Defines how much time back to look for all kinds of metrics while running auto-resolves https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/autoresolve-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-lookup-interval Defines how much time back to look for Bandwidth metrics in Bandwidth Over Utilization checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-threshold Threshold for Bandwidth Over Utilization troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/circuit-instability-autoresolve-threshold Max DOWN events allowed in Circuit Instability checks while auto-resolving tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/circuit-instability-autoresolve-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-lookup-interval Defines how much time back to look for DOWN events in Circuit Instability checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-threshold Threshold for Circuit Instability troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/customers-to-always-use-default-contact-info [Monitoring] List Bruin customers that should always use the default contact info https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/customers-to-always-use-default-contact-info/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/customers-with-bandwidth-over-utilization-monitoring List of client IDs for which Bandwidth Over Utilization checks are enabled https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/customers-with-bandwidth-over-utilization-monitoring/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/default-contact-info-per-customer Mapping of VeloCloud hosts, Bruin customers and default contact info https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/default-contact-info-per-customer/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-day-time/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-night-time/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/jitter-monitoring-lookup-interval Defines how much time back to look for Jitter metrics in Jitter checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/jitter-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/jitter-monitoring-threshold Threshold for Jitter troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/jitter-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/latency-monitoring-lookup-interval Defines how much time back to look for Latency metrics in Latency checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/latency-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/latency-monitoring-threshold Threshold for Latency troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/latency-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/max-autoresolves-per-ticket/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-lookup-interval Defines how much time back to look for Packet Loss metrics in Packet Loss checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/packet-loss-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-threshold Threshold for Packet Loss troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/packet-loss-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/wait-time-before-sending-new-milestone-reminder/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/recipients-per-host-and-customer Mapping of VeloCloud hosts, Bruin customer IDs and recipients of these reports https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/recipients-per-host-and-customer/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/reported-troubles Troubles that will be reported https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/reported-troubles/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/default-contacts List of default contacts to whom this report will always be delivered to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/default-contacts/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/execution-cron-expression Cron expression that determines when to build and deliver this report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/execution-cron-expression/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/reoccurring-trouble-tickets-threshold Number of different tickets a trouble must appear in for a particular edge and interface to include it in the report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/reoccurring-trouble-tickets-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/tickets-lookup-interval Defines how much time back to look for Bruin tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/tickets-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/business-grade-link-labels List of labels that define a link as business grade https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/business-grade-link-labels/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/grace-period-before-attempting-new-digi-reboots Defines for how long the monitor will wait before attempting a new DiGi Reboot on an edge https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/grace-period-before-attempting-new-digi-reboots/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/max-autoresolves-per-ticket/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/missing-edges-from-cache-report-recipient E-mail address that will receive a tiny report showing which edges from VeloCloud responses are not in the cache of customers https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/missing-edges-from-cache-report-recipient/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down (HA) state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-hard-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down (HA) state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-link-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-soft-down-outage Defines how much time to wait before re-checking an edge currently in Soft Down (HA) state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-soft-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-hard-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-link-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/severity-for-edge-down-outages Severity level for Edge Down outages https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/severity-for-edge-down-outages/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/severity-for-link-down-outages Severity level for Link Down outages https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/severity-for-link-down-outages/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/wait-time-before-sending-new-milestone-reminder/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/triage/last-note-interval Defines how long the last note on a ticket is considered recent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/triage/last-note-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/triage/max-events-per-event-note Defines how many events will be included in events notes https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/triage/max-events-per-event-note/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/triage/monitoring-job-interval Defines how often tickets are checked to see if it needs an initial triage or events note https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/triage/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/tnba-feedback/feedback-job-interval Defines how often tickets are pulled from Bruin and sent to the KRE to train the predictive model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-feedback/feedback-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/tnba-feedback/grace-period-before-resending-tickets Defines for how long a ticket needs to wait before being re-sent to the KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-feedback/grace-period-before-resending-tickets/description?region=us-east-1&tab=Table /automation-engine/common/tnba-feedback/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-feedback/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/grace-period-before-appending-new-tnba-notes Defines for how long a ticket needs to wait since it was opened before appending a new TNBA note https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/grace-period-before-appending-new-tnba-notes/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/grace-period-before-monitoring-tickets-based-on-last-documented-outage Defines for how long a Service Outage ticket needs to wait after the last documented outage to get a new TNBA note appended https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/grace-period-before-monitoring-tickets-based-on-last-documented-outage/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/min-required-confidence-for-request-and-repair-completed-predictions Defines the minimum confidence level required to consider a Request Completed / Repair Completed prediction accurate in TNBA auto-resolves https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/min-required-confidence-for-request-and-repair-completed-predictions/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/monitoring-job-interval Defines how often tickets are checked to see if they need a new TNBA note https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/base-url Base URL for Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/client-id Client ID credential to authenticate against Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/client-id/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/client-secret Client secret credential to authenticate against Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/login-url Login URL for Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/login-url/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-base-url Base URL for Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-client-id Client ID credential to authenticate against Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-client-id/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-client-secret Client secret credential to authenticate against Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-login-url Login URL for Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-login-url/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/blacklisted-clients-with-pending-status Client IDs whose edges have Pending management status that should be ignored in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/blacklisted-clients-with-pending-status/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/blacklisted-edges VeloCloud edges that should be ignored in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/velocloud-hosts VeloCloud hosts whose edges will be stored to the cache https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/digi-bridge/digi-reboot-api-base-url Base URL for Digi API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-bridge/digi-reboot-api-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/digi-bridge/digi-reboot-api-client-id Client ID credentials for Digi API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-bridge/digi-reboot-api-client-id/description?region=us-east-1&tab=Table /automation-engine/dev/digi-bridge/digi-reboot-api-client-secret Client Secret credentials for Digi API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-bridge/digi-reboot-api-client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/digi-reboot-report/report-recipient Email address to send the report to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-reboot-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/email-tagger-kre-bridge/kre-base-url Base URL for E-mail Tagger's KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/email-tagger-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/email-tagger-monitor/api-request-key API request key for incoming requests https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/email-tagger-monitor/api-request-key/description?region=us-east-1&tab=Table /automation-engine/dev/email-tagger-monitor/api-request-signature-secret-key API signature secret key for incoming requests https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/email-tagger-monitor/api-request-signature-secret-key/description?region=us-east-1&tab=Table /automation-engine/dev/external-secrets/iam-role-arn The ARN of the AWS IAM role necesary to manage parameter store and secret manager https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/external-secrets/iam-role-arn/description?region=us-east-1&tab=Table /automation-engine/dev/fraud-monitor/observed-inbox-email-address E-mail account that receives Fraud e-mails for later analysis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/fraud-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/dev/gateway-monitor/monitored-velocloud-hosts VeloCloud hosts whose gateways will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/gateway-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/hawkeye-customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/hawkeye-customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/intermapper-outage-monitor/observed-inbox-email-address E-mail account that receives InterMapper events for later analysis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/intermapper-outage-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/dev/last-contact-report/monitored-velocloud-hosts VeloCloud hosts whose edges will be used to build the report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/last-contact-report/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/last-contact-report/report-recipient Email address to send the report to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/last-contact-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/notifier/monitorable-email-accounts Mapping of e-mail addresses and passwords whose inboxes can be read for later analysis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/notifier/monitorable-email-accounts/description?region=us-east-1&tab=Table /automation-engine/dev/notifier/slack-webhook-url Slack webhook to send messages https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/notifier/slack-webhook-url/description?region=us-east-1&tab=Table /automation-engine/dev/papertrail/enabled Enable/Disable Papertrail logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/papertrail/enabled/description?region=us-east-1&tab=Table /automation-engine/dev/redis/customer-cache-hostname Customer Cache Redis hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/customer-cache-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/redis/email-tagger-hostname Email Tagger Redis Hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/email-tagger-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/redis/main-hostname Main Redis server for Automation-Engine https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/main-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/redis/tnba-feedback-hostname TNBA Feedback hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/tnba-feedback-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/repair-tickets-kre-bridge/kre-base-url Base URL for RTA's KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/repair-tickets-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/service-affecting/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-affecting/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/service-outage/monitor/blacklisted-edges List of edges that are excluded from Service Outage monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-outage/monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/dev/service-outage/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-outage/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/service-outage/triage/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-outage/triage/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/base-url Base URL for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/client-id OAuth client ID for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/client-id/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/client-secret OAuth client secret for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/password Password for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/password/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/username Username for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/username/description?region=us-east-1&tab=Table /automation-engine/dev/t7-bridge/kre-base-url Base URL for TNBA's KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/t7-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/tnba-feedback/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/tnba-feedback/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/tnba-monitor/blacklisted-edges List of edges that are excluded from TNBA monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/tnba-monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/dev/tnba-monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/tnba-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/velocloud-bridge/velocloud-credentials Velocloud credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/velocloud-bridge/velocloud-credentials/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/client-id https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/client-id/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/client-secret https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/client-secret/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/login-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/login-url/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-client-id https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-client-id/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-client-secret https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-client-secret/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-login-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-login-url/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/blacklisted-clients-with-pending-status https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/blacklisted-clients-with-pending-status/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/blacklisted-edges https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/duplicate-inventories-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-client-id https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-client-id/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-client-secret https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-client-secret/description?region=us-east-1&tab=Table /automation-engine/pro/digi-reboot-report/report-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-reboot-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-kre-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/digi-reboot-report/report-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-reboot-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-kre-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-monitor/api-request-key https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-monitor/api-request-key/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-monitor/api-request-signature-secret-key https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-monitor/api-request-signature-secret-key/description?region=us-east-1&tab=Table /automation-engine/pro/external-secrets/iam-role-arn The ARN of the AWS IAM role necesary to manage parameter store and secret manager https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/external-secrets/iam-role-arn/description?region=us-east-1&tab=Table /automation-engine/pro/fraud-monitor/observed-inbox-email-address https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/fraud-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/pro/gateway-monitor/monitored-velocloud-hosts VeloCloud hosts whose gateways will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/gateway-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/hawkeye-customer-cache/duplicate-inventories-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/hawkeye-customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/intermapper-outage-monitor/observed-inbox-email-address https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/intermapper-outage-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/pro/last-contact-report/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/last-contact-report/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/last-contact-report/report-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/last-contact-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/notifier/monitorable-email-accounts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/notifier/monitorable-email-accounts/description?region=us-east-1&tab=Table /automation-engine/pro/notifier/slack-webhook-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/notifier/slack-webhook-url/description?region=us-east-1&tab=Table /automation-engine/pro/papertrail/enabled https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/papertrail/enabled/description?region=us-east-1&tab=Table /automation-engine/pro/redis/customer-cache-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/customer-cache-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/redis/email-tagger-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/email-tagger-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/redis/main-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/main-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/redis/tnba-feedback-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/tnba-feedback-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/repair-tickets-kre-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/repair-tickets-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/service-affecting/monitor/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-affecting/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/service-outage/monitor/blacklisted-edges https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-outage/monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/pro/service-outage/monitor/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-outage/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/service-outage/triage/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-outage/triage/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/t7-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/t7-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/tnba-feedback/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/tnba-feedback/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/tnba-monitor/blacklisted-edges https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/tnba-monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/pro/tnba-monitor/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/tnba-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/velocloud-bridge/velocloud-credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/velocloud-bridge/velocloud-credentials/description?region=us-east-1&tab=Table /data-highway/develop/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/metrics-velocloud-api/ALLOW_ORIGINS/description?region=us-east-1&tab=Table /data-highway/develop/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/metrics-velocloud-api/WAREHOUSE/description?region=us-east-1&tab=Table /data-highway/develop/shared/SECRET_JWT Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/shared/SECRET_JWT/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/BLACKLISTED_VENDORS Blacklisted vendors https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/BLACKLISTED_VENDORS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/BOOTSTRAP_SERVERS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_PASSWORD Kafka develop password to publish velocloud data on Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_PASSWORD/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka develop ssl ca path of Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_SSL_CA_FILE/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_USERNAME Kafka develop username to publish velocloud data on kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_USERNAME/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/METRICS_LIST Metrics list https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/METRICS_LIST/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_HOSTS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_VERIFY_SSL/description?region=us-east-1&tab=Table /data-highway/master/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/metrics-velocloud-api/ALLOW_ORIGINS/description?region=us-east-1&tab=Table /data-highway/master/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/metrics-velocloud-api/WAREHOUSE/description?region=us-east-1&tab=Table /data-highway/master/shared/SECRET_JWT Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/shared/SECRET_JWT/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/BOOTSTRAP_SERVERS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/ENTERPRISES_ID Enterprises IDs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/ENTERPRISES_ID/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_PASSWORD Kafka master password to publish velocloud data on Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_PASSWORD/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka master ssl ca path of Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_SSL_CA_FILE/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_USERNAME Kafka master username to publish velocloud data on kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_USERNAME/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_HOSTS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_VERIFY_SSL/description?region=us-east-1&tab=Table","title":"Parameter Store parameters"},{"location":"parameters/parameters/#parameter-store-parameters","text":"Name Description Link to AWS /automation-engine/common/autoresolve-day-end-hour Defines the hour at which the day ends and the night starts for dynamic auto-resolution times https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/autoresolve-day-end-hour/description?region=us-east-1&tab=Table /automation-engine/common/autoresolve-day-start-hour Defines the hour at which the night ends and the day starts for dynamic auto-resolution times https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/autoresolve-day-start-hour/description?region=us-east-1&tab=Table /automation-engine/common/bruin-ipa-system-username Name of the user that performs operations in Bruin on behalf of the IPA system https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/bruin-ipa-system-username/description?region=us-east-1&tab=Table /automation-engine/common/customer-cache/refresh-check-interval Defines how often the next refresh flag is checked to decide if it's time to refresh the cache or not https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/customer-cache/refresh-check-interval/description?region=us-east-1&tab=Table /automation-engine/common/customer-cache/refresh-job-interval Defines how often the cache is refreshed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/customer-cache/refresh-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/customer-cache/whitelisted-management-statuses/description?region=us-east-1&tab=Table /automation-engine/common/digi-bridge/digi-headers List of possible headers included in all DiGi links https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-bridge/digi-headers/description?region=us-east-1&tab=Table /automation-engine/common/digi-bridge/digi-reboot-api-token-ttl Auth tokens TTL https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-bridge/digi-reboot-api-token-ttl/description?region=us-east-1&tab=Table /automation-engine/common/digi-reboot-report/logs-lookup-interval Defines how much time back to look for DiGi Reboot logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-reboot-report/logs-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/digi-reboot-report/report-job-interval Defines how often the report is built and sent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/digi-reboot-report/report-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/base-url Base URL for DRI API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/dri-data-redis-ttl Defines how much time the data retrieved from DRI for a specific device can be stored and served from Redis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/dri-data-redis-ttl/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/password Password to log into DRI API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/password/description?region=us-east-1&tab=Table /automation-engine/common/dri-bridge/username Username to log into DRI API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/dri-bridge/username/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/api-endpoint-prefix API server endpoint prefix for incoming requests https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/api-endpoint-prefix/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/max-concurrent-emails Defines how many simultaneous emails are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/max-concurrent-emails/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/new-emails-job-interval Defines how often new emails received from Bruin are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/new-emails-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/new-tickets-job-interval Defines how often new tickets received from Bruin are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/new-tickets-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/email-tagger-monitor/reply-email-ttl Reply emails time to live (in milliseconds) when storing them to Redis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/email-tagger-monitor/reply-email-ttl/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/alerts-lookup-days How many days to look back for Fraud alerts in the desired e-mail inbox https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/alerts-lookup-days/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/default-client-info-for-did-without-inventory Default client info used when the DID device in the Fraud alert does not have an inventory assigned in Bruin https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/default-client-info-for-did-without-inventory/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/default-contact-for-new-tickets Default contact details used when a Fraud is reported as a Service Affecting ticket https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/default-contact-for-new-tickets/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/monitoring-job-interval Defines how often Fraud e-mails are checked to report them as Service Affecting tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/fraud-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent Fraud alerts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/fraud-monitor/observed-inbox-senders/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/first-gateway-lookup-interval Lookup interval for the first time gateway statuses are retrieved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/first-gateway-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/monitoring-job-interval Defines how often gateways are checked to find and report incidents https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/second-gateway-lookup-interval Lookup interval for the second time gateway statuses are retrieved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/second-gateway-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/gateway-monitor/tunnel-count-threshold Threshold for tunnel count incidents https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/gateway-monitor/tunnel-count-threshold/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-affecting-monitor/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-affecting-monitor/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-affecting-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-affecting-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-affecting-monitor/probes-tests-results-lookup-interval Defines how much time back to look for probes' tests results https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-affecting-monitor/probes-tests-results-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-bridge/base-url Base URL to access Hawkeye API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-bridge/client-password Client password to log into Hawkeye API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-bridge/client-password/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-bridge/client-username Client username to log into Hawkeye API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-bridge/client-username/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-customer-cache/refresh-job-interval Defines how often the cache is refreshed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-customer-cache/refresh-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-customer-cache/whitelisted-management-statuses/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage Defines for how long a ticket can be auto-resolved after the last documented outage https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/hawkeye-outage-monitor/quarantine-for-devices-in-outage Defines how much time to wait before checking if a particular device is still in outage state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/hawkeye-outage-monitor/quarantine-for-devices-in-outage/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/dri-parameters-for-piab-notes Parameters to fetch from DRI to include them in InterMapper notes for PIAB devices https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/dri-parameters-for-piab-notes/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/max-autoresolves-per-ticket/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/max-concurrent-email-batches Defines how many simultaneous email batches related to the same InterMapper asset are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/max-concurrent-email-batches/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/monitored-up-events InterMapper events considered as UP https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/monitored-up-events/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/monitoring-job-interval Defines how often InterMapper events are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent InterMapper events https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/observed-inbox-senders/description?region=us-east-1&tab=Table /automation-engine/common/timezone Timezone used for periodic jobs, timestamps, etc https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/timezone/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/events-lookup-days How many days to look back for InterMapper events in the desired e-mail inbox https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/events-lookup-days/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/monitored-down-events InterMapper events considered as DOWN https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/monitored-down-events/description?region=us-east-1&tab=Table /automation-engine/common/intermapper-outage-monitor/whitelisted-product-categories-for-autoresolve Defines which Bruin product categories are taken into account when auto-resolving tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/intermapper-outage-monitor/whitelisted-product-categories-for-autoresolve/description?region=us-east-1&tab=Table /automation-engine/common/link-labels-blacklisted-from-asr-forwards List of link labels that are excluded from forwards to the ASR queue https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/link-labels-blacklisted-from-asr-forwards/description?region=us-east-1&tab=Table /automation-engine/common/link-labels-blacklisted-from-hnoc-forwards List of link labels that are excluded from forwards to the HNOC queue https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/link-labels-blacklisted-from-hnoc-forwards/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/access-token Access token for Lumin's Billing API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/access-token/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/customer-name Name of the customer for which the Billing Report will be generated https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/customer-name/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/email-account-for-message-delivery-password/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/lumin-billing-api-base-url Base URL for Lumin's Billing API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/lumin-billing-api-base-url/description?region=us-east-1&tab=Table /automation-engine/common/lumin-billing-report/recipient Email address to send the report to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/lumin-billing-report/recipient/description?region=us-east-1&tab=Table /automation-engine/common/metrics/relevant-clients List of relevant client names to use on Prometheus metrics labels https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/metrics/relevant-clients/description?region=us-east-1&tab=Table /automation-engine/common/nats/endpoint-url Nats endpoint URL for messaging https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/nats/endpoint-url/description?region=us-east-1&tab=Table /automation-engine/common/notifier/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/notifier/email-account-for-message-delivery-password/description?region=us-east-1&tab=Table /automation-engine/common/notifier/email-account-for-message-delivery-username Email account used to send messages to other accounts (username) https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/notifier/email-account-for-message-delivery-username/description?region=us-east-1&tab=Table /automation-engine/common/papertrail/host Papertrail host to send logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/papertrail/host/description?region=us-east-1&tab=Table /automation-engine/common/papertrail/port Papertrail port to send logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/papertrail/port/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/max-concurrent-closed-tickets-for-feedback Defines how many simultaneous new closed tickets are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/max-concurrent-closed-tickets-for-feedback/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/max-concurrent-created-tickets-for-feedback Defines how many simultaneous new created tickets are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/max-concurrent-created-tickets-for-feedback/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/max-concurrent-emails-for-monitoring Defines how many simultaneous tagged emails are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/max-concurrent-emails-for-monitoring/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/new-closed-tickets-feedback-job-interval Defines how often new closed tickets fetched from Bruin are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/new-closed-tickets-feedback-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/new-created-tickets-feedback-job-interval Defines how often new created tickets fetched from Bruin are sent to the KRE to train the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/new-created-tickets-feedback-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/rta-monitor-job-interval Defines how often new emails tagged by the E-mail Tagger are processed https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/rta-monitor-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/repair-tickets-monitor/tag-ids-mapping Mapping of tag names and their corresponding numeric ID, as defined in the AI model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/repair-tickets-monitor/tag-ids-mapping/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/enabled-customers-per-host Mapping of VeloCloud hosts and Bruin customer IDs for whom this report will trigger periodically https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/enabled-customers-per-host/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/execution-cron-expression Cron expression that determines when to build and deliver this report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/execution-cron-expression/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/lookup-interval Defines how much time back to look for bandwidth metrics and Bruin tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/daily-bandwidth-report/recipients List of recipients that will get these reports https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/daily-bandwidth-report/recipients/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/autoresolve-lookup-interval Defines how much time back to look for all kinds of metrics while running auto-resolves https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/autoresolve-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-lookup-interval Defines how much time back to look for Bandwidth metrics in Bandwidth Over Utilization checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-threshold Threshold for Bandwidth Over Utilization troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/circuit-instability-autoresolve-threshold Max DOWN events allowed in Circuit Instability checks while auto-resolving tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/circuit-instability-autoresolve-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-lookup-interval Defines how much time back to look for DOWN events in Circuit Instability checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-threshold Threshold for Circuit Instability troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/customers-to-always-use-default-contact-info [Monitoring] List Bruin customers that should always use the default contact info https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/customers-to-always-use-default-contact-info/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/customers-with-bandwidth-over-utilization-monitoring List of client IDs for which Bandwidth Over Utilization checks are enabled https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/customers-with-bandwidth-over-utilization-monitoring/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/default-contact-info-per-customer Mapping of VeloCloud hosts, Bruin customers and default contact info https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/default-contact-info-per-customer/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-day-time/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-night-time/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/jitter-monitoring-lookup-interval Defines how much time back to look for Jitter metrics in Jitter checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/jitter-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/jitter-monitoring-threshold Threshold for Jitter troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/jitter-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/latency-monitoring-lookup-interval Defines how much time back to look for Latency metrics in Latency checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/latency-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/latency-monitoring-threshold Threshold for Latency troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/latency-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/max-autoresolves-per-ticket/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-lookup-interval Defines how much time back to look for Packet Loss metrics in Packet Loss checks https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/packet-loss-monitoring-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-threshold Threshold for Packet Loss troubles https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/packet-loss-monitoring-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitor/wait-time-before-sending-new-milestone-reminder/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/recipients-per-host-and-customer Mapping of VeloCloud hosts, Bruin customer IDs and recipients of these reports https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/recipients-per-host-and-customer/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/reported-troubles Troubles that will be reported https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/reported-troubles/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/default-contacts List of default contacts to whom this report will always be delivered to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/default-contacts/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/execution-cron-expression Cron expression that determines when to build and deliver this report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/execution-cron-expression/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/reoccurring-trouble-tickets-threshold Number of different tickets a trouble must appear in for a particular edge and interface to include it in the report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/reoccurring-trouble-tickets-threshold/description?region=us-east-1&tab=Table /automation-engine/common/service-affecting/reoccurring-trouble-report/tickets-lookup-interval Defines how much time back to look for Bruin tickets https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-affecting/reoccurring-trouble-report/tickets-lookup-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/business-grade-link-labels List of labels that define a link as business grade https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/business-grade-link-labels/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/grace-period-before-attempting-new-digi-reboots Defines for how long the monitor will wait before attempting a new DiGi Reboot on an edge https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/grace-period-before-attempting-new-digi-reboots/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/max-autoresolves-per-ticket/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/missing-edges-from-cache-report-recipient E-mail address that will receive a tiny report showing which edges from VeloCloud responses are not in the cache of customers https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/missing-edges-from-cache-report-recipient/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down (HA) state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-hard-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down (HA) state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-link-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-soft-down-outage Defines how much time to wait before re-checking an edge currently in Soft Down (HA) state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-soft-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-hard-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down state https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/quarantine-for-edges-in-link-down-outage/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/severity-for-edge-down-outages Severity level for Edge Down outages https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/severity-for-edge-down-outages/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/severity-for-link-down-outages Severity level for Link Down outages https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/severity-for-link-down-outages/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitor/wait-time-before-sending-new-milestone-reminder/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/triage/last-note-interval Defines how long the last note on a ticket is considered recent https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/triage/last-note-interval/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/triage/max-events-per-event-note Defines how many events will be included in events notes https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/triage/max-events-per-event-note/description?region=us-east-1&tab=Table /automation-engine/common/service-outage/triage/monitoring-job-interval Defines how often tickets are checked to see if it needs an initial triage or events note https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/service-outage/triage/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/tnba-feedback/feedback-job-interval Defines how often tickets are pulled from Bruin and sent to the KRE to train the predictive model https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-feedback/feedback-job-interval/description?region=us-east-1&tab=Table /automation-engine/common/tnba-feedback/grace-period-before-resending-tickets Defines for how long a ticket needs to wait before being re-sent to the KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-feedback/grace-period-before-resending-tickets/description?region=us-east-1&tab=Table /automation-engine/common/tnba-feedback/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-feedback/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/grace-period-before-appending-new-tnba-notes Defines for how long a ticket needs to wait since it was opened before appending a new TNBA note https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/grace-period-before-appending-new-tnba-notes/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/grace-period-before-monitoring-tickets-based-on-last-documented-outage Defines for how long a Service Outage ticket needs to wait after the last documented outage to get a new TNBA note appended https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/grace-period-before-monitoring-tickets-based-on-last-documented-outage/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/min-required-confidence-for-request-and-repair-completed-predictions Defines the minimum confidence level required to consider a Request Completed / Repair Completed prediction accurate in TNBA auto-resolves https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/min-required-confidence-for-request-and-repair-completed-predictions/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/monitored-product-category Bruin's product category under monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/monitored-product-category/description?region=us-east-1&tab=Table /automation-engine/common/tnba-monitor/monitoring-job-interval Defines how often tickets are checked to see if they need a new TNBA note https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/common/tnba-monitor/monitoring-job-interval/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/base-url Base URL for Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/client-id Client ID credential to authenticate against Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/client-id/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/client-secret Client secret credential to authenticate against Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/login-url Login URL for Bruin's DEV API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/login-url/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-base-url Base URL for Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-client-id Client ID credential to authenticate against Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-client-id/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-client-secret Client secret credential to authenticate against Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/bruin-bridge/test-login-url Login URL for Bruin's TEST API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/bruin-bridge/test-login-url/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/blacklisted-clients-with-pending-status Client IDs whose edges have Pending management status that should be ignored in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/blacklisted-clients-with-pending-status/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/blacklisted-edges VeloCloud edges that should be ignored in the caching process https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/customer-cache/velocloud-hosts VeloCloud hosts whose edges will be stored to the cache https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/customer-cache/velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/digi-bridge/digi-reboot-api-base-url Base URL for Digi API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-bridge/digi-reboot-api-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/digi-bridge/digi-reboot-api-client-id Client ID credentials for Digi API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-bridge/digi-reboot-api-client-id/description?region=us-east-1&tab=Table /automation-engine/dev/digi-bridge/digi-reboot-api-client-secret Client Secret credentials for Digi API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-bridge/digi-reboot-api-client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/digi-reboot-report/report-recipient Email address to send the report to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/digi-reboot-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/email-tagger-kre-bridge/kre-base-url Base URL for E-mail Tagger's KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/email-tagger-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/email-tagger-monitor/api-request-key API request key for incoming requests https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/email-tagger-monitor/api-request-key/description?region=us-east-1&tab=Table /automation-engine/dev/email-tagger-monitor/api-request-signature-secret-key API signature secret key for incoming requests https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/email-tagger-monitor/api-request-signature-secret-key/description?region=us-east-1&tab=Table /automation-engine/dev/external-secrets/iam-role-arn The ARN of the AWS IAM role necesary to manage parameter store and secret manager https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/external-secrets/iam-role-arn/description?region=us-east-1&tab=Table /automation-engine/dev/fraud-monitor/observed-inbox-email-address E-mail account that receives Fraud e-mails for later analysis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/fraud-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/dev/gateway-monitor/monitored-velocloud-hosts VeloCloud hosts whose gateways will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/gateway-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/hawkeye-customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/hawkeye-customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/intermapper-outage-monitor/observed-inbox-email-address E-mail account that receives InterMapper events for later analysis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/intermapper-outage-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/dev/last-contact-report/monitored-velocloud-hosts VeloCloud hosts whose edges will be used to build the report https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/last-contact-report/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/last-contact-report/report-recipient Email address to send the report to https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/last-contact-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/dev/notifier/monitorable-email-accounts Mapping of e-mail addresses and passwords whose inboxes can be read for later analysis https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/notifier/monitorable-email-accounts/description?region=us-east-1&tab=Table /automation-engine/dev/notifier/slack-webhook-url Slack webhook to send messages https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/notifier/slack-webhook-url/description?region=us-east-1&tab=Table /automation-engine/dev/papertrail/enabled Enable/Disable Papertrail logs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/papertrail/enabled/description?region=us-east-1&tab=Table /automation-engine/dev/redis/customer-cache-hostname Customer Cache Redis hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/customer-cache-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/redis/email-tagger-hostname Email Tagger Redis Hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/email-tagger-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/redis/main-hostname Main Redis server for Automation-Engine https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/main-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/redis/tnba-feedback-hostname TNBA Feedback hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/redis/tnba-feedback-hostname/description?region=us-east-1&tab=Table /automation-engine/dev/repair-tickets-kre-bridge/kre-base-url Base URL for RTA's KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/repair-tickets-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/service-affecting/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-affecting/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/service-outage/monitor/blacklisted-edges List of edges that are excluded from Service Outage monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-outage/monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/dev/service-outage/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-outage/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/service-outage/triage/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/service-outage/triage/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/base-url Base URL for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/client-id OAuth client ID for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/client-id/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/client-secret OAuth client secret for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/client-secret/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/password Password for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/password/description?region=us-east-1&tab=Table /automation-engine/dev/servicenow-bridge/username Username for the ServiceNow API https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/servicenow-bridge/username/description?region=us-east-1&tab=Table /automation-engine/dev/t7-bridge/kre-base-url Base URL for TNBA's KRE https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/t7-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/dev/tnba-feedback/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/tnba-feedback/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/tnba-monitor/blacklisted-edges List of edges that are excluded from TNBA monitoring https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/tnba-monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/dev/tnba-monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/tnba-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/dev/velocloud-bridge/velocloud-credentials Velocloud credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/dev/velocloud-bridge/velocloud-credentials/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/base-url/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/client-id https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/client-id/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/client-secret https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/client-secret/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/login-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/login-url/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-client-id https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-client-id/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-client-secret https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-client-secret/description?region=us-east-1&tab=Table /automation-engine/pro/bruin-bridge/test-login-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/bruin-bridge/test-login-url/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/blacklisted-clients-with-pending-status https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/blacklisted-clients-with-pending-status/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/blacklisted-edges https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/duplicate-inventories-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/customer-cache/velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/customer-cache/velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-client-id https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-client-id/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-client-secret https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-client-secret/description?region=us-east-1&tab=Table /automation-engine/pro/digi-reboot-report/report-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-reboot-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-kre-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/digi-bridge/digi-reboot-api-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-bridge/digi-reboot-api-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/digi-reboot-report/report-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/digi-reboot-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-kre-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-monitor/api-request-key https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-monitor/api-request-key/description?region=us-east-1&tab=Table /automation-engine/pro/email-tagger-monitor/api-request-signature-secret-key https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/email-tagger-monitor/api-request-signature-secret-key/description?region=us-east-1&tab=Table /automation-engine/pro/external-secrets/iam-role-arn The ARN of the AWS IAM role necesary to manage parameter store and secret manager https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/external-secrets/iam-role-arn/description?region=us-east-1&tab=Table /automation-engine/pro/fraud-monitor/observed-inbox-email-address https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/fraud-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/pro/gateway-monitor/monitored-velocloud-hosts VeloCloud hosts whose gateways will be monitored https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/gateway-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/hawkeye-customer-cache/duplicate-inventories-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/hawkeye-customer-cache/duplicate-inventories-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/intermapper-outage-monitor/observed-inbox-email-address https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/intermapper-outage-monitor/observed-inbox-email-address/description?region=us-east-1&tab=Table /automation-engine/pro/last-contact-report/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/last-contact-report/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/last-contact-report/report-recipient https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/last-contact-report/report-recipient/description?region=us-east-1&tab=Table /automation-engine/pro/notifier/monitorable-email-accounts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/notifier/monitorable-email-accounts/description?region=us-east-1&tab=Table /automation-engine/pro/notifier/slack-webhook-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/notifier/slack-webhook-url/description?region=us-east-1&tab=Table /automation-engine/pro/papertrail/enabled https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/papertrail/enabled/description?region=us-east-1&tab=Table /automation-engine/pro/redis/customer-cache-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/customer-cache-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/redis/email-tagger-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/email-tagger-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/redis/main-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/main-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/redis/tnba-feedback-hostname https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/redis/tnba-feedback-hostname/description?region=us-east-1&tab=Table /automation-engine/pro/repair-tickets-kre-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/repair-tickets-kre-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/service-affecting/monitor/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-affecting/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/service-outage/monitor/blacklisted-edges https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-outage/monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/pro/service-outage/monitor/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-outage/monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/service-outage/triage/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/service-outage/triage/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/t7-bridge/kre-base-url https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/t7-bridge/kre-base-url/description?region=us-east-1&tab=Table /automation-engine/pro/tnba-feedback/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/tnba-feedback/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/tnba-monitor/blacklisted-edges https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/tnba-monitor/blacklisted-edges/description?region=us-east-1&tab=Table /automation-engine/pro/tnba-monitor/monitored-velocloud-hosts https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/tnba-monitor/monitored-velocloud-hosts/description?region=us-east-1&tab=Table /automation-engine/pro/velocloud-bridge/velocloud-credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/automation-engine/pro/velocloud-bridge/velocloud-credentials/description?region=us-east-1&tab=Table /data-highway/develop/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/metrics-velocloud-api/ALLOW_ORIGINS/description?region=us-east-1&tab=Table /data-highway/develop/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/metrics-velocloud-api/WAREHOUSE/description?region=us-east-1&tab=Table /data-highway/develop/shared/SECRET_JWT Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/shared/SECRET_JWT/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/BLACKLISTED_VENDORS Blacklisted vendors https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/BLACKLISTED_VENDORS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/BOOTSTRAP_SERVERS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_PASSWORD Kafka develop password to publish velocloud data on Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_PASSWORD/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka develop ssl ca path of Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_SSL_CA_FILE/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/KAFKA_USERNAME Kafka develop username to publish velocloud data on kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/KAFKA_USERNAME/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/METRICS_LIST Metrics list https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/METRICS_LIST/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_HOSTS/description?region=us-east-1&tab=Table /data-highway/develop/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/develop/velocloud-fetcher/VELOCLOUD_VERIFY_SSL/description?region=us-east-1&tab=Table /data-highway/master/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/metrics-velocloud-api/ALLOW_ORIGINS/description?region=us-east-1&tab=Table /data-highway/master/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/metrics-velocloud-api/WAREHOUSE/description?region=us-east-1&tab=Table /data-highway/master/shared/SECRET_JWT Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/shared/SECRET_JWT/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/BOOTSTRAP_SERVERS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/ENTERPRISES_ID Enterprises IDs https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/ENTERPRISES_ID/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_PASSWORD Kafka master password to publish velocloud data on Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_PASSWORD/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka master ssl ca path of Kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_SSL_CA_FILE/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/KAFKA_USERNAME Kafka master username to publish velocloud data on kafka https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/KAFKA_USERNAME/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_CREDENTIALS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_HOSTS/description?region=us-east-1&tab=Table /data-highway/master/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl https://us-east-1.console.aws.amazon.com/systems-manager/parameters/data-highway/master/velocloud-fetcher/VELOCLOUD_VERIFY_SSL/description?region=us-east-1&tab=Table","title":"Parameter Store parameters"},{"location":"pipeline/BASIC_CI_CONFIGURATION/","text":"1. CI/CD PROJECT CONFIGURATION FROM 0 To release this project an make it work we need to configure next stuff: - semantic release - AIVEN - AWS 1.1 Semantic Release Semantic release is depending of a base image in this repository to be faster on this project CI/CD. The only two configurations we need to make it work here is: - Have prepared the base image repository and point to that image in the .gitlab-ci.yml(variable SEMANTIC_RELEASE_IMAGE) - Get an access token on the project (Settings/Access tokens) and get all permissions to interacts with the API. Create a variable called GITLAB_TOKEN and put the token you crete there. 1.2 AIVEN TODO 1.3 AWS It's recommended to create an account for terraform and a group of permissions with admin access, also it's necessary to create an S3 bucket that the user can access to. To accomplish aws connection in some steps of the CI/CD we need to add next variables (on the settings section, not in the YAML) in the CI: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY It's necessary to connect to the S3 from amazon, where we store the tfstate files to maintain the state of our infrastructure deployments. 1.4 Snowflake","title":"Basic configurations"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#1-cicd-project-configuration-from-0","text":"To release this project an make it work we need to configure next stuff: - semantic release - AIVEN - AWS","title":"1. CI/CD PROJECT CONFIGURATION FROM 0"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#11-semantic-release","text":"Semantic release is depending of a base image in this repository to be faster on this project CI/CD. The only two configurations we need to make it work here is: - Have prepared the base image repository and point to that image in the .gitlab-ci.yml(variable SEMANTIC_RELEASE_IMAGE) - Get an access token on the project (Settings/Access tokens) and get all permissions to interacts with the API. Create a variable called GITLAB_TOKEN and put the token you crete there.","title":"1.1 Semantic Release"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#12-aiven","text":"TODO","title":"1.2 AIVEN"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#13-aws","text":"It's recommended to create an account for terraform and a group of permissions with admin access, also it's necessary to create an S3 bucket that the user can access to. To accomplish aws connection in some steps of the CI/CD we need to add next variables (on the settings section, not in the YAML) in the CI: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY It's necessary to connect to the S3 from amazon, where we store the tfstate files to maintain the state of our infrastructure deployments.","title":"1.3 AWS"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#14-snowflake","text":"","title":"1.4 Snowflake"},{"location":"pipeline/PIPELINE_RULES/","text":"PIPELINES RULES Add new section To add new section of jobs in the pipeline, we must include the section adding an included in the general .gitlab-ci.yml: include : - local : microservices/.gitlab-ci.yml Could be possible that an included .gitlab-ci.yml of another section include more sub-sections(Example microservices): include : - local : microservices/fetchers/velocloud/.gitlab-ci.yml We should follow this organization technique to isolate code to their respective area and avoid big files with spaghetti code. Add new template To add a new template we should take a look in the folder structure: # templates (folder where project templating is going to be saved) ## gitlab (section of the templating) Inside gitlab folder we should create template YAML files that fits with a section, for example, for microservices we created a microservice-ci.yml to store the templates of that section. Same in infrastructure. Also, we should include new templates in the index.yml in templates/index.yml include : # CI templates - local : templates/gitlab/microservice-ci.yml - local : templates/gitlab/infrastructure-ci.yml","title":"Pipeline rules"},{"location":"pipeline/PIPELINE_RULES/#pipelines-rules","text":"","title":"PIPELINES RULES"},{"location":"pipeline/PIPELINE_RULES/#add-new-section","text":"To add new section of jobs in the pipeline, we must include the section adding an included in the general .gitlab-ci.yml: include : - local : microservices/.gitlab-ci.yml Could be possible that an included .gitlab-ci.yml of another section include more sub-sections(Example microservices): include : - local : microservices/fetchers/velocloud/.gitlab-ci.yml We should follow this organization technique to isolate code to their respective area and avoid big files with spaghetti code.","title":"Add new section"},{"location":"pipeline/PIPELINE_RULES/#add-new-template","text":"To add a new template we should take a look in the folder structure: # templates (folder where project templating is going to be saved) ## gitlab (section of the templating) Inside gitlab folder we should create template YAML files that fits with a section, for example, for microservices we created a microservice-ci.yml to store the templates of that section. Same in infrastructure. Also, we should include new templates in the index.yml in templates/index.yml include : # CI templates - local : templates/gitlab/microservice-ci.yml - local : templates/gitlab/infrastructure-ci.yml","title":"Add new template"},{"location":"snowflake/","text":"1. Create a private key for a user/service To create a new user follow the link from snowflake about key creation It's only allowed to create users with keys to automate connection between services. If we need a key for testing purposes should be a temporary user or in a development infrastructure. Important to know that only a SECURITYADMIN user can modify users to have a key pair access. 2. Key rotation policy Key rotation is a must to have a high security standard, at this moment is important to know is a manual process, each first week of the month should be a calendar task in the devops team to change it a redeployment the infrastructure with these new keys. Right now is not automated because no make sense to do it, you have to have a static user that can modify these stuff manually in the web, if we discover a way to do it automatically without a static user or a bastion one we could think about it. 3. Add a New provider 4. Rules Each private key must have a passphrase. Each new provider must have their own key Devops team must renew keys each month and redeploy the infra.","title":"Datalake"},{"location":"snowflake/#1-create-a-private-key-for-a-userservice","text":"To create a new user follow the link from snowflake about key creation It's only allowed to create users with keys to automate connection between services. If we need a key for testing purposes should be a temporary user or in a development infrastructure. Important to know that only a SECURITYADMIN user can modify users to have a key pair access.","title":"1. Create a private key for a user/service"},{"location":"snowflake/#2-key-rotation-policy","text":"Key rotation is a must to have a high security standard, at this moment is important to know is a manual process, each first week of the month should be a calendar task in the devops team to change it a redeployment the infrastructure with these new keys. Right now is not automated because no make sense to do it, you have to have a static user that can modify these stuff manually in the web, if we discover a way to do it automatically without a static user or a bastion one we could think about it.","title":"2. Key rotation policy"},{"location":"snowflake/#3-add-a-new-provider","text":"","title":"3. Add a New provider"},{"location":"snowflake/#4-rules","text":"Each private key must have a passphrase. Each new provider must have their own key Devops team must renew keys each month and redeploy the infra.","title":"4. Rules"},{"location":"snowflake/queries/QoE/qoe/","text":"QOE Snowflake query USE DATABASE METTEL_DEVELOP ; USE SCHEMA VELOCLOUD ; SELECT TOP 1 * FROM LINKS_QOE_METRICS_PLAIN ; Results: * RECORD_METADATA * CreateTime: Unix time. Time the record was pull from velocloud. * Offset: Index of the data in Kafka. * Partition: kafka partition parent. * Topic: kafka queue streaming the data. Example of RECORD_METADATA: { \"CreateTime\" : 1657092759530 , \"offset\" : 2430 , \"partition\" : 1 , \"topic\" : \"velocloud_get_links_qoe_metrics_plain_develop\" } RECORD_CONTENT The schema of the data is equal to the body response from the Velocloud API: Link: Velocloud API Link Section: POST /linkQualityEvent/getLinkQualityEvents (see attached file QoE.json for a sample of QoE content) Sample queries Find host and enterprise ID by company name: SELECT * FROM V_COMPANY_IDENTIFIERS WHERE COMPANY LIKE '%FIS%' ; Discover edges for FIS SELECT RECORD_CONTENT : edge_id FROM LINKS_QOE_METRICS_PLAIN WHERE RECORD_CONTENT : host LIKE 'metvco04.mettel.net' AND RECORD_CONTENT : enterprise_id = 269 GROUP BY RECORD_CONTENT : edge_id ; All edges data for one FIS instance SELECT * FROM LINKS_QOE_METRICS_PLAIN WHERE RECORD_CONTENT : host LIKE 'metvco04.mettel.net' AND RECORD_CONTENT : enterprise_id = 269 ;","title":"Qoe"},{"location":"snowflake/queries/QoE/qoe/#qoe","text":"Snowflake query USE DATABASE METTEL_DEVELOP ; USE SCHEMA VELOCLOUD ; SELECT TOP 1 * FROM LINKS_QOE_METRICS_PLAIN ; Results: * RECORD_METADATA * CreateTime: Unix time. Time the record was pull from velocloud. * Offset: Index of the data in Kafka. * Partition: kafka partition parent. * Topic: kafka queue streaming the data. Example of RECORD_METADATA: { \"CreateTime\" : 1657092759530 , \"offset\" : 2430 , \"partition\" : 1 , \"topic\" : \"velocloud_get_links_qoe_metrics_plain_develop\" } RECORD_CONTENT The schema of the data is equal to the body response from the Velocloud API: Link: Velocloud API Link Section: POST /linkQualityEvent/getLinkQualityEvents (see attached file QoE.json for a sample of QoE content)","title":"QOE"},{"location":"snowflake/queries/QoE/qoe/#sample-queries","text":"Find host and enterprise ID by company name: SELECT * FROM V_COMPANY_IDENTIFIERS WHERE COMPANY LIKE '%FIS%' ; Discover edges for FIS SELECT RECORD_CONTENT : edge_id FROM LINKS_QOE_METRICS_PLAIN WHERE RECORD_CONTENT : host LIKE 'metvco04.mettel.net' AND RECORD_CONTENT : enterprise_id = 269 GROUP BY RECORD_CONTENT : edge_id ; All edges data for one FIS instance SELECT * FROM LINKS_QOE_METRICS_PLAIN WHERE RECORD_CONTENT : host LIKE 'metvco04.mettel.net' AND RECORD_CONTENT : enterprise_id = 269 ;","title":"Sample queries"}]}