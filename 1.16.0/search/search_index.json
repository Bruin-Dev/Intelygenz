{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CONCEPTS Monorepo Semantic Release Infrastructure as code Datalake Kafka PIPELINES BASIC CONFIGURATIONS Semantic release AIVEN AWS Snowflake PIPELINES RULES Add new section Add new template DOCUMENTATION Organization Rules Tools MetTel Decisions Metrics definitions Diagrams Logging DEVELOPMENT RULES Branch name convention Semantic release WORK IN A LOCAL ENVIRONMENT Launch docker compose DATALAKE Create private key for a user/service Key rotation policy Add a new provider Rules MANUAL PROCESSES ... MANUAL CONFIGURATIONS AWS SSO Okta identity provider Revoke Session token AWS SSO Okta JWT token Gitlab maintenance RECOVERY PROCESSES Gitlab recovery AUDIT EVENTS Automation Service outage BYOB IPA Queue HNOC forwarding SA forwarding to ASR TNBA Monitor Ticket severity Ticket creation outcomes Service affecting Infrastructure Lambda Parameter-Replicator","title":"Manual processes"},{"location":"CONCEPTS/","text":"","title":"Concepts"},{"location":"CREATE_NEW_MICROSERVICE/","text":"CREATE NEW MICROSERVICE This process describes step by step how to create a new microservice, from the ECR repository to the helm chart templates that define the microservice. All of this is created from the Gitlab repository in the pipeline; no need for more tools or actions by the developer. Introduction The process requires that the steps be carried out in order. Basically, it is necessary to create the ECR repository first so that we can then start developing and testing our new microservice in ephemeral environments or in production. 1. Create ECR repository We can't create the ECR repository in the branch where we are developing because the creation or update of the ECR repositories is only in the Master branch. This means that the first thing that we need to do is make a little merge to master to create our new microservice repo, by that way we can deploy our microservice later in dev branches. create a new branch from master create a new terraform ECR repo file in the folder: infra-as-code/basic-infra/3-registry you can copy any of the other repos to have an example. this is an example: resource \"aws_ecr_repository\" \"new-bridge-repository\" { name = \"new-bridge\" tags = { Project = var.common_info.project Provisioning = var.common_info.provisioning Module = \"new-bridge\" } } merge the new repository to Master branch. The pipeline will run and create the new ECR repo new-bridge 2. Create our new microservice folder We can start working on our new microservice based on an existing one. It depends on if is a capability (bridges) or a use case . Select one or other depends on what are you developing. For example let's copy a capability \"bruing-bridge\" and paste in the root of the repo to change his name to \"new-bridge\". from this moment you can start to develop and do your tests locally. Every microservice must have the following directory structure: new-bridge \u251c\u2500\u2500 .gitlab-ci.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 package.json \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u251c\u2500\u2500 app.py \u251c\u2500\u2500 application \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests \u2514\u2500\u2500 ... 3. Update CI-CD gitlab files with the proper values It's important to have the .gitlab-ci.yaml files correctly defined to enable pipelines: * new-bridge/.gitlab-ci.yml Change any reference of the template microservice to the new one: example find and replace \"bruin-bridge\" for \"new-bridge\" * .gitlab-ci.yml (the file in the root of the repository) Here we need to specify to gitlab-ci that we define other jobs in a different directories (the .gitlab-ci.yml of our new repo). So locate the root gitlab file and add a new line with the path of the new micro jobs (do it respecting the alphabetical order) ... - local: 'services/links-metrics-api/.gitlab-ci.yml' - local: 'services/links-metrics-collector/.gitlab-ci.yml' - local: 'services/lumin-billing-report/.gitlab-ci.yml' - local: 'services/new-bridge/.gitlab-ci.yml' <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! - local: 'services/notifier/.gitlab-ci.yml' - local: 'services/service-affecting-monitor/.gitlab-ci.yml' - local: 'services/service-outage-monitor/.gitlab-ci.yml' ... Now in the same file, let's define the \"desired_tasks\" variable for this new micro (do it respecting the alphabetical order): ... NATS_SERVER_DESIRED_TASKS: \"1\" NATS_SERVER_1_DESIRED_TASKS: \"1\" NATS_SERVER_2_DESIRED_TASKS: \"1\" NEW_BRIDGE_DESIRED_TASKS: \"1\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! NOTIFIER_DESIRED_TASKS: \"1\" SERVICE_AFFECTING_MONITOR_DESIRED_TASKS: \"1\" SERVICE_OUTAGE_MONITOR_1_DESIRED_TASKS: \"1\" ... 3. Configure semantic-release We need to edit two files, one in the new micro path and other in the root of the repo: * new-bridge/package.json update the name of the micro with our new working name (do it respecting the alphabetical order): { \"name\": \"new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"version\": \"0.0.1\", \"dependencies\": {}, \"devDependencies\": {} } * package.json (the file in the root of the repository) add to the semantic-release global config our new path to analyze version changes (do it respecting the alphabetical order): ... \"./services/links-metrics-api\", \"./services/links-metrics-collector\", \"./services/lumin-billing-report\", \"./services/new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"./services/notifier\", \"./services/service-affecting-monitor\", \"./services/service-outage-monitor\", ... 3. Configure logs We use 2 systems to storage logs, papertrail for 3 days and cloudwath for 1 month. Let's add those config in: * ci-utils/papertrail-provisioning/config.py just copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... { \"query\": f\"lumin-billing-report AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[lumin-billing-report] - logs\", \"repository\": \"lumin-billing-report\", }, { \u00af\u2502 \"query\": f\"new-bridge AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \u2502 \"search_name\": f\"[new-bridge] - logs\", \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! \"repository\": \"new-bridge\", \u2502 }, _\u2502 { \"query\": f\"notifier AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[notifier] - logs\", \"repository\": \"notifier\", }, ... helm/charts/fluent-bit-custom/templates/configmap.yaml The same; copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... [OUTPUT] Name cloudwatch Match kube.var.log.containers.lumin-billing-report* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name lumin-billing-report auto_create_group true [OUTPUT] \u00af\u2502 Name cloudwatch \u2502 Match kube.var.log.containers.new-bridge* \u2502 region {{ .Values.config.region }} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! log_group_name {{ .Values.config.logGroupName }} \u2502 log_stream_name new-bridge \u2502 auto_create_group true _\u2502 [OUTPUT] Name cloudwatch Match kube.var.log.containers.notifier* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name notifier auto_create_group true ... 3. Update docker-compose to enable local deployments docker-compose.yml here we define our container along with the rest of the microservices. Just add the definition of our container respecting the alphabetical order: ... new-bridge: \u00af\u2502 build: \u2502 # Context must be the root of the monorepo \u2502 context: . \u2502 dockerfile: new-bridge/Dockerfile \u2502 args: \u2502 REPOSITORY_URL: 374050862540.dkr.ecr.us-east-1.amazonaws.com \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! DOCKER_BASE_IMAGE_VERSION: 2.2.0 \u2502 env_file: \u2502 - new-bridge/src/config/env \u2502 depends_on: \u2502 - \"nats-server\" \u2502 - redis \u2502 ports: \u2502 - 5006:5000 _\u2502 notifier: build: ... 4. Add option to enable or disable our microservice helm/charts/automation-engine/Chart.yaml in this file we define our Automation-engine chart version and his dependencies. Let's add a condition for our microservice to have the possibility of disable or enable in our future deployments. ... - name: lumin-billing-report version: '*.*.*' condition: lumin-billing-report.enabled - name: new-bridge \u00af\u2502 version: '*.*.*' \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! condition: new-bridge.enabled _\u2502 - name: notifier version: '*.*.*' condition: notifier.enabled ... 5. Helm templates and variables Here we will define the infrastructure part of our microservice with a helm chart. Is very important to know that this \"how to\" is only to copy an existing microservice, therefore, we take the following statements for granted: 1. The microservice will not have a public endpoint (except email-tagger-monitor) 2. The microservice port will always be 5000 3. Depends on the base chart you use to copy & paste, you will have more or less kubernetes resources. although most microservices have: configmap, deployment, secret and service. Perfect, now let's copy and paste another chart to use as template, if we will develop a use-case, we must copy the most similar use-case. For this example we are creating a \"new-bridge\", so let's copy a \"bruin-bridge\" as a template: * BASE-FOLDER: copy this folder helm/charts/automation-engine/charts/bruin-bridge and paste here helm/charts/automation-engine/charts/ we will have something like bruin-bridge copy change the name to new-bridge . PREPARE BASE_FOLDER: now let's do a find and replace in our new folder new-bridge . find bruin-bridge and replace for new-bridge . many substitutions should appear (at the moment of write this, i can see 46 substitutions in 10 files but over time, this can change). Just remember to do this in the new-bridge folder context to evoid modify other resources. DEPENDENCIES and CHECKS: Now we have to customize our new microservice, first we must ask ourselves, what dependency does my new microservice have on other services? for example, bruin-bridge have a dependency with Nats and Redis, so it have a few checks to see if those services are available and if they are, it can be deployed. We can find this checks in the deployment.yaml file. Specifically in this part: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} ... We can add or remove all the init containers we want. Even, it is very possible that all the dependencies that we need already have the microservice that we use as a base or some other microservice already developed. So we can navigate through the folders of the rest of the microservices and copy any other dependency check and use it in ours. I will add a new dependency for notifier copied from other microservice, and my file will look like the following: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} {{- if .Values.config.capabilities_enabled.notifier }} \u00af\u2502 - name: wait-notifier \u2502 image: busybox:1.28 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! command: ['sh', '-c', 'until wget --spider -S http://notifier:5000/_health; do echo waiting for notifier; sleep 2; done'] \u2502 {{- end }} _\u2502 ... Note that we have a if condition. You will see this in some check, we use this because if we deploy only some microservices, we must contemplate this. If the notifier not exist, the check will not be created. Nats and Redis are always required, that's wy don't have the conditional. VARIABLES: time to update the variables that will use our microservice, this involves various files: helm/charts/automation-engine/charts/new-bridge/templates/configmap.yaml this file always will be part of the deployment, it contains the variables base and the variables with no sensitive information. let's add a new variable NEW_VAR: ... CURRENT_ENVIRONMENT: {{ .Values.global.current_environment }} ENVIRONMENT_NAME: \"{{ .Values.global.environment }}\" NATS_SERVER1: {{ .Values.global.nats_server }} NEW_VAR: \"{{ .Values.config.new_var }}\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! REDIS_HOSTNAME: {{ .Values.global.redis_hostname }} PAPERTRAIL_ACTIVE: \"{{ .Values.global.papertrail_active }}\" PAPERTRAIL_HOST: {{ .Values.global.papertrail_host }} PAPERTRAIL_PORT: \"{{ .Values.global.papertrail_port }}\" PAPERTRAIL_PREFIX: \"{{ .Values.config.papertrail_prefix }}\" ... You can see here two important things, 1. there are variables with quotes and without quotes: this depends on your needs, if you don't put quotes, YAML will interpret the best case for you.. example, if you put a number like 5 as a value, YAML will interpret this as an integer, but careful, this could be a danger if your application expects a string variable; if this is the case, use quotes to define your var. 2. Additionally, we have variables of two types: \"global\" and \"config\". The global ones are common for all microservices, and the \"config\" is specific for this microservice. All the additional variables that we add will be of the type \"config\" helm/charts/automation-engine/charts/new-bridge/templates/secret.yaml this file may or may not exist in the chart and contains variables that have sensitive information. This info will be encoded with base64 to no show in clear text. let's add a new variable NEW_SENSITIVE_VAR: apiVersion: v1 kind: Secret metadata: name: {{ include \"new-bridge.secretName\" . }} labels: {{- include \"new-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" data: NEW_SENSITIVE_VAR: {{ .Values.config.new_sensitive_var | b64enc }} <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! helm/charts/automation-engine/charts/new-bridge/values.yaml Now that we add our new variable in the configmap.yaml, we have to define it in our values file in order to use it. As you can see above, the definition of our variable points to the values file of our microservice; \"Values.config.new_var\" so let's go update it: ... config: <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 in config section!!! papertrail_prefix: \"\" # -- New useful variable with no sensitive information \u00af\u2502______________ here the configmap variable! new_var: \"\" _\u2502 # -- New useful variable with sensitive information \u00af\u2502______________ and here the secret variable! new_sensitive_var: \"\" _\u2502 ... Check that we only define the variable but no put any value, although we can also set a default value if we want. helm/charts/automation-engine/values.yaml This is the values template off the entire automation-engine application. This only have the structure of the values and no contain any real value. For this part we will copy the content of the values file that we just created and paste in the place that correspond (respecting the alphabetical order). It's important to note that we are pasting the values inside another Yaml, so we must adapt the indentation for the destiny file: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: # -- Field to indicate if the lumin-billing-report module is going to be deployed enabled: true # -- Number of replicas of lumin-billing-report module replicaCount: 1 config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"\" # -- URI of Lumin API lumin_uri: \"\" # -- Token credentials for Lumin API lumin_token: \"\" # -- Name of customer to generate lumin-billing-report customer_name: \"\" # -- Email address to send lumin-billing-report billing_recipient: \"\" image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: \"\" service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: 1 \u2502 enabled: true \u2502 config: \u2502 papertrail_prefix: \"\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: \"\" \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: \"\" \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: \"\" \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: # -- Field to indicate if the notifier module is going to be deployed enabled: true # -- Number of replicas of notifier module replicaCount: 1 # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: \"\" # -- notifier Service Configuration ... Things to check: first the indentation!!.. second, the \"global\" config is not set here; it is defined at the beginning of the values file and is common for all microservices. and finally, we remove blank and default configurations to get a shorter file (things removed: autoscaling, the default is false, so we can omit it. nodeSelector. tolerations and affinity). PD: You can keep autoscaling if you will enable it. helm/charts/automation-engine/values.yaml.tpl This is the most important file, it contains the values that will be parsed and used to deploy the Automation-Engine application. Basically it's the same file of values.yaml, but with the variables that will be replaced in the pipeline to deploy a production or develop environment. Let's add our new micro with the variables: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: enabled: ${LUMIN_BILLING_REPORT_ENABLED} replicaCount: ${LUMIN_BILLING_REPORT_DESIRED_TASKS} config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"lumin-billing-report-${LUMIN_BILLING_REPORT_BUILD_NUMBER}\" # -- URI of Lumin API lumin_uri: ${LUMIN_URI} # -- Token credentials for Lumin API lumin_token: ${LUMIN_TOKEN} # -- Name of customer to generate lumin-billing-report customer_name: ${CUSTOMER_NAME_BILLING_REPORT} # -- Email address to send lumin-billing-report billing_recipient: ${BILLING_RECIPIENT} image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: ${LUMIN_BILLING_REPORT_BUILD_NUMBER} service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: ${NEW_BRIDGE_DESIRED_TASKS} \u2502 enabled: ${NEW_BRIDGE_ENABLED} \u2502 config: \u2502 papertrail_prefix: \"new-bridge-${NEW_BRIDGE_BUILD_NUMBER}\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: ${NEW_BRIDGE_NEW_VAR} \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: ${NEW_BRIDGE_NEW_SENSITIVE_VAR} \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: ${NEW_BRIDGE_BUILD_NUMBER} \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: enabled: ${NOTIFIER_ENABLED} replicaCount: ${NOTIFIER_DESIRED_TASKS} # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: ${NOTIFIER_BUILD_NUMBER} # -- notifier Service Configuration service: type: ClusterIP port: 5000 ... With this, we have the entire template of our new microservice. Now we need to set in the pipeline the variables that we just created. ci-utils/environments/deploy_environment_vars.sh In this file, we define the variables that will be used in the values file. Most of the cases are variables that we create in GitLab with the value of dev and production environments. This file is a bash script that has multiple functions to define the variables, each function is for the microservice that requires those variables. If we are adding a new micro that requires variables, we need to define the function and in the bottom of the file execute that function. PD: no all microservices needs specific variables, so in some cases, we wouldn't need to touch this file or even create a secret.yaml. Rebember to respect the alphabetical order: ... function lumin_billing_report_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # lumin-billing-report environment variables for ephemeral environments export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_DEV} else # lumin-billing-report environment variables for production environment export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_PROD} fi } function new_bridge_variables() { \u00af\u2502 if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then \u2502 # new-bridge environment variables for ephemeral environments \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_DEV} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_DEV} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! else \u2502 # new-bridge environment variables for production environment \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_PRO} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_PRO} \u2502 fi \u2502 } _\u2502 function notifier_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # notifier environment variables for ephemeral environments export NOTIFIER_SLACK_URL=${SLACK_URL_DEV} else # notifier environment variables for production environment export NOTIFIER_SLACK_URL=${SLACK_URL_PRO} fi } _\u2502 ... function environments_assign() { # assign enabled variable for each subchart create_enabled_var_for_each_subchart # assign common environment variables for each environment common_variables_by_environment # assign specific environment variables for each subchart bruin_bridge_variables cts_bridge_variables digi_bridge_variables digi_reboot_report_variables email_tagger_monitor_variables hawkeye_bridge_variables links_metrics_api_variables lit_bridge_variables lumin_billing_report_variables new_bridge_variables <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 and here! notifier_variables t7_bridge_variables ticket_collector_variables velocloud_bridge_variables } ... add the variables in gitlab-ci finally, we have all the path until the real value in Gitlab. Let's go to the repository settings/ci-cd section and create the new variables: That's all, with this and the proper commit message the pipeline will run and deploy an ephemeral environment.","title":"CREATE NEW MICROSERVICE"},{"location":"CREATE_NEW_MICROSERVICE/#create-new-microservice","text":"This process describes step by step how to create a new microservice, from the ECR repository to the helm chart templates that define the microservice. All of this is created from the Gitlab repository in the pipeline; no need for more tools or actions by the developer.","title":"CREATE NEW MICROSERVICE"},{"location":"CREATE_NEW_MICROSERVICE/#introduction","text":"The process requires that the steps be carried out in order. Basically, it is necessary to create the ECR repository first so that we can then start developing and testing our new microservice in ephemeral environments or in production.","title":"Introduction"},{"location":"CREATE_NEW_MICROSERVICE/#1-create-ecr-repository","text":"We can't create the ECR repository in the branch where we are developing because the creation or update of the ECR repositories is only in the Master branch. This means that the first thing that we need to do is make a little merge to master to create our new microservice repo, by that way we can deploy our microservice later in dev branches. create a new branch from master create a new terraform ECR repo file in the folder: infra-as-code/basic-infra/3-registry you can copy any of the other repos to have an example. this is an example: resource \"aws_ecr_repository\" \"new-bridge-repository\" { name = \"new-bridge\" tags = { Project = var.common_info.project Provisioning = var.common_info.provisioning Module = \"new-bridge\" } } merge the new repository to Master branch. The pipeline will run and create the new ECR repo new-bridge","title":"1. Create ECR repository"},{"location":"CREATE_NEW_MICROSERVICE/#2-create-our-new-microservice-folder","text":"We can start working on our new microservice based on an existing one. It depends on if is a capability (bridges) or a use case . Select one or other depends on what are you developing. For example let's copy a capability \"bruing-bridge\" and paste in the root of the repo to change his name to \"new-bridge\". from this moment you can start to develop and do your tests locally. Every microservice must have the following directory structure: new-bridge \u251c\u2500\u2500 .gitlab-ci.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 package.json \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u251c\u2500\u2500 app.py \u251c\u2500\u2500 application \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests \u2514\u2500\u2500 ...","title":"2. Create our new microservice folder"},{"location":"CREATE_NEW_MICROSERVICE/#3-update-ci-cd-gitlab-files-with-the-proper-values","text":"It's important to have the .gitlab-ci.yaml files correctly defined to enable pipelines: * new-bridge/.gitlab-ci.yml Change any reference of the template microservice to the new one: example find and replace \"bruin-bridge\" for \"new-bridge\" * .gitlab-ci.yml (the file in the root of the repository) Here we need to specify to gitlab-ci that we define other jobs in a different directories (the .gitlab-ci.yml of our new repo). So locate the root gitlab file and add a new line with the path of the new micro jobs (do it respecting the alphabetical order) ... - local: 'services/links-metrics-api/.gitlab-ci.yml' - local: 'services/links-metrics-collector/.gitlab-ci.yml' - local: 'services/lumin-billing-report/.gitlab-ci.yml' - local: 'services/new-bridge/.gitlab-ci.yml' <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! - local: 'services/notifier/.gitlab-ci.yml' - local: 'services/service-affecting-monitor/.gitlab-ci.yml' - local: 'services/service-outage-monitor/.gitlab-ci.yml' ... Now in the same file, let's define the \"desired_tasks\" variable for this new micro (do it respecting the alphabetical order): ... NATS_SERVER_DESIRED_TASKS: \"1\" NATS_SERVER_1_DESIRED_TASKS: \"1\" NATS_SERVER_2_DESIRED_TASKS: \"1\" NEW_BRIDGE_DESIRED_TASKS: \"1\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! NOTIFIER_DESIRED_TASKS: \"1\" SERVICE_AFFECTING_MONITOR_DESIRED_TASKS: \"1\" SERVICE_OUTAGE_MONITOR_1_DESIRED_TASKS: \"1\" ...","title":"3. Update CI-CD gitlab files with the proper values"},{"location":"CREATE_NEW_MICROSERVICE/#3-configure-semantic-release","text":"We need to edit two files, one in the new micro path and other in the root of the repo: * new-bridge/package.json update the name of the micro with our new working name (do it respecting the alphabetical order): { \"name\": \"new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"version\": \"0.0.1\", \"dependencies\": {}, \"devDependencies\": {} } * package.json (the file in the root of the repository) add to the semantic-release global config our new path to analyze version changes (do it respecting the alphabetical order): ... \"./services/links-metrics-api\", \"./services/links-metrics-collector\", \"./services/lumin-billing-report\", \"./services/new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"./services/notifier\", \"./services/service-affecting-monitor\", \"./services/service-outage-monitor\", ...","title":"3. Configure semantic-release"},{"location":"CREATE_NEW_MICROSERVICE/#3-configure-logs","text":"We use 2 systems to storage logs, papertrail for 3 days and cloudwath for 1 month. Let's add those config in: * ci-utils/papertrail-provisioning/config.py just copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... { \"query\": f\"lumin-billing-report AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[lumin-billing-report] - logs\", \"repository\": \"lumin-billing-report\", }, { \u00af\u2502 \"query\": f\"new-bridge AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \u2502 \"search_name\": f\"[new-bridge] - logs\", \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! \"repository\": \"new-bridge\", \u2502 }, _\u2502 { \"query\": f\"notifier AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[notifier] - logs\", \"repository\": \"notifier\", }, ... helm/charts/fluent-bit-custom/templates/configmap.yaml The same; copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... [OUTPUT] Name cloudwatch Match kube.var.log.containers.lumin-billing-report* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name lumin-billing-report auto_create_group true [OUTPUT] \u00af\u2502 Name cloudwatch \u2502 Match kube.var.log.containers.new-bridge* \u2502 region {{ .Values.config.region }} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! log_group_name {{ .Values.config.logGroupName }} \u2502 log_stream_name new-bridge \u2502 auto_create_group true _\u2502 [OUTPUT] Name cloudwatch Match kube.var.log.containers.notifier* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name notifier auto_create_group true ...","title":"3. Configure logs"},{"location":"CREATE_NEW_MICROSERVICE/#3-update-docker-compose-to-enable-local-deployments","text":"docker-compose.yml here we define our container along with the rest of the microservices. Just add the definition of our container respecting the alphabetical order: ... new-bridge: \u00af\u2502 build: \u2502 # Context must be the root of the monorepo \u2502 context: . \u2502 dockerfile: new-bridge/Dockerfile \u2502 args: \u2502 REPOSITORY_URL: 374050862540.dkr.ecr.us-east-1.amazonaws.com \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! DOCKER_BASE_IMAGE_VERSION: 2.2.0 \u2502 env_file: \u2502 - new-bridge/src/config/env \u2502 depends_on: \u2502 - \"nats-server\" \u2502 - redis \u2502 ports: \u2502 - 5006:5000 _\u2502 notifier: build: ...","title":"3. Update docker-compose to enable local deployments"},{"location":"CREATE_NEW_MICROSERVICE/#4-add-option-to-enable-or-disable-our-microservice","text":"helm/charts/automation-engine/Chart.yaml in this file we define our Automation-engine chart version and his dependencies. Let's add a condition for our microservice to have the possibility of disable or enable in our future deployments. ... - name: lumin-billing-report version: '*.*.*' condition: lumin-billing-report.enabled - name: new-bridge \u00af\u2502 version: '*.*.*' \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! condition: new-bridge.enabled _\u2502 - name: notifier version: '*.*.*' condition: notifier.enabled ...","title":"4. Add option to enable or disable our microservice"},{"location":"CREATE_NEW_MICROSERVICE/#5-helm-templates-and-variables","text":"Here we will define the infrastructure part of our microservice with a helm chart. Is very important to know that this \"how to\" is only to copy an existing microservice, therefore, we take the following statements for granted: 1. The microservice will not have a public endpoint (except email-tagger-monitor) 2. The microservice port will always be 5000 3. Depends on the base chart you use to copy & paste, you will have more or less kubernetes resources. although most microservices have: configmap, deployment, secret and service. Perfect, now let's copy and paste another chart to use as template, if we will develop a use-case, we must copy the most similar use-case. For this example we are creating a \"new-bridge\", so let's copy a \"bruin-bridge\" as a template: * BASE-FOLDER: copy this folder helm/charts/automation-engine/charts/bruin-bridge and paste here helm/charts/automation-engine/charts/ we will have something like bruin-bridge copy change the name to new-bridge . PREPARE BASE_FOLDER: now let's do a find and replace in our new folder new-bridge . find bruin-bridge and replace for new-bridge . many substitutions should appear (at the moment of write this, i can see 46 substitutions in 10 files but over time, this can change). Just remember to do this in the new-bridge folder context to evoid modify other resources. DEPENDENCIES and CHECKS: Now we have to customize our new microservice, first we must ask ourselves, what dependency does my new microservice have on other services? for example, bruin-bridge have a dependency with Nats and Redis, so it have a few checks to see if those services are available and if they are, it can be deployed. We can find this checks in the deployment.yaml file. Specifically in this part: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} ... We can add or remove all the init containers we want. Even, it is very possible that all the dependencies that we need already have the microservice that we use as a base or some other microservice already developed. So we can navigate through the folders of the rest of the microservices and copy any other dependency check and use it in ours. I will add a new dependency for notifier copied from other microservice, and my file will look like the following: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} {{- if .Values.config.capabilities_enabled.notifier }} \u00af\u2502 - name: wait-notifier \u2502 image: busybox:1.28 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! command: ['sh', '-c', 'until wget --spider -S http://notifier:5000/_health; do echo waiting for notifier; sleep 2; done'] \u2502 {{- end }} _\u2502 ... Note that we have a if condition. You will see this in some check, we use this because if we deploy only some microservices, we must contemplate this. If the notifier not exist, the check will not be created. Nats and Redis are always required, that's wy don't have the conditional. VARIABLES: time to update the variables that will use our microservice, this involves various files: helm/charts/automation-engine/charts/new-bridge/templates/configmap.yaml this file always will be part of the deployment, it contains the variables base and the variables with no sensitive information. let's add a new variable NEW_VAR: ... CURRENT_ENVIRONMENT: {{ .Values.global.current_environment }} ENVIRONMENT_NAME: \"{{ .Values.global.environment }}\" NATS_SERVER1: {{ .Values.global.nats_server }} NEW_VAR: \"{{ .Values.config.new_var }}\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! REDIS_HOSTNAME: {{ .Values.global.redis_hostname }} PAPERTRAIL_ACTIVE: \"{{ .Values.global.papertrail_active }}\" PAPERTRAIL_HOST: {{ .Values.global.papertrail_host }} PAPERTRAIL_PORT: \"{{ .Values.global.papertrail_port }}\" PAPERTRAIL_PREFIX: \"{{ .Values.config.papertrail_prefix }}\" ... You can see here two important things, 1. there are variables with quotes and without quotes: this depends on your needs, if you don't put quotes, YAML will interpret the best case for you.. example, if you put a number like 5 as a value, YAML will interpret this as an integer, but careful, this could be a danger if your application expects a string variable; if this is the case, use quotes to define your var. 2. Additionally, we have variables of two types: \"global\" and \"config\". The global ones are common for all microservices, and the \"config\" is specific for this microservice. All the additional variables that we add will be of the type \"config\" helm/charts/automation-engine/charts/new-bridge/templates/secret.yaml this file may or may not exist in the chart and contains variables that have sensitive information. This info will be encoded with base64 to no show in clear text. let's add a new variable NEW_SENSITIVE_VAR: apiVersion: v1 kind: Secret metadata: name: {{ include \"new-bridge.secretName\" . }} labels: {{- include \"new-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" data: NEW_SENSITIVE_VAR: {{ .Values.config.new_sensitive_var | b64enc }} <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! helm/charts/automation-engine/charts/new-bridge/values.yaml Now that we add our new variable in the configmap.yaml, we have to define it in our values file in order to use it. As you can see above, the definition of our variable points to the values file of our microservice; \"Values.config.new_var\" so let's go update it: ... config: <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 in config section!!! papertrail_prefix: \"\" # -- New useful variable with no sensitive information \u00af\u2502______________ here the configmap variable! new_var: \"\" _\u2502 # -- New useful variable with sensitive information \u00af\u2502______________ and here the secret variable! new_sensitive_var: \"\" _\u2502 ... Check that we only define the variable but no put any value, although we can also set a default value if we want. helm/charts/automation-engine/values.yaml This is the values template off the entire automation-engine application. This only have the structure of the values and no contain any real value. For this part we will copy the content of the values file that we just created and paste in the place that correspond (respecting the alphabetical order). It's important to note that we are pasting the values inside another Yaml, so we must adapt the indentation for the destiny file: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: # -- Field to indicate if the lumin-billing-report module is going to be deployed enabled: true # -- Number of replicas of lumin-billing-report module replicaCount: 1 config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"\" # -- URI of Lumin API lumin_uri: \"\" # -- Token credentials for Lumin API lumin_token: \"\" # -- Name of customer to generate lumin-billing-report customer_name: \"\" # -- Email address to send lumin-billing-report billing_recipient: \"\" image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: \"\" service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: 1 \u2502 enabled: true \u2502 config: \u2502 papertrail_prefix: \"\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: \"\" \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: \"\" \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: \"\" \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: # -- Field to indicate if the notifier module is going to be deployed enabled: true # -- Number of replicas of notifier module replicaCount: 1 # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: \"\" # -- notifier Service Configuration ... Things to check: first the indentation!!.. second, the \"global\" config is not set here; it is defined at the beginning of the values file and is common for all microservices. and finally, we remove blank and default configurations to get a shorter file (things removed: autoscaling, the default is false, so we can omit it. nodeSelector. tolerations and affinity). PD: You can keep autoscaling if you will enable it. helm/charts/automation-engine/values.yaml.tpl This is the most important file, it contains the values that will be parsed and used to deploy the Automation-Engine application. Basically it's the same file of values.yaml, but with the variables that will be replaced in the pipeline to deploy a production or develop environment. Let's add our new micro with the variables: ... # -- lumin-billing-report subchart specific configuration lumin-billing-report: enabled: ${LUMIN_BILLING_REPORT_ENABLED} replicaCount: ${LUMIN_BILLING_REPORT_DESIRED_TASKS} config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"lumin-billing-report-${LUMIN_BILLING_REPORT_BUILD_NUMBER}\" # -- URI of Lumin API lumin_uri: ${LUMIN_URI} # -- Token credentials for Lumin API lumin_token: ${LUMIN_TOKEN} # -- Name of customer to generate lumin-billing-report customer_name: ${CUSTOMER_NAME_BILLING_REPORT} # -- Email address to send lumin-billing-report billing_recipient: ${BILLING_RECIPIENT} image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: ${LUMIN_BILLING_REPORT_BUILD_NUMBER} service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi # -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: ${NEW_BRIDGE_DESIRED_TASKS} \u2502 enabled: ${NEW_BRIDGE_ENABLED} \u2502 config: \u2502 papertrail_prefix: \"new-bridge-${NEW_BRIDGE_BUILD_NUMBER}\" \u2502 # -- New useful variable with no sensitive information \u2502 new_var: ${NEW_BRIDGE_NEW_VAR} \u2502 # -- New useful variable with sensitive information \u2502 new_sensitive_var: ${NEW_BRIDGE_NEW_SENSITIVE_VAR} \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: ${NEW_BRIDGE_BUILD_NUMBER} \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 # -- notifier subchart specific configuration notifier: enabled: ${NOTIFIER_ENABLED} replicaCount: ${NOTIFIER_DESIRED_TASKS} # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: ${NOTIFIER_BUILD_NUMBER} # -- notifier Service Configuration service: type: ClusterIP port: 5000 ... With this, we have the entire template of our new microservice. Now we need to set in the pipeline the variables that we just created. ci-utils/environments/deploy_environment_vars.sh In this file, we define the variables that will be used in the values file. Most of the cases are variables that we create in GitLab with the value of dev and production environments. This file is a bash script that has multiple functions to define the variables, each function is for the microservice that requires those variables. If we are adding a new micro that requires variables, we need to define the function and in the bottom of the file execute that function. PD: no all microservices needs specific variables, so in some cases, we wouldn't need to touch this file or even create a secret.yaml. Rebember to respect the alphabetical order: ... function lumin_billing_report_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # lumin-billing-report environment variables for ephemeral environments export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_DEV} else # lumin-billing-report environment variables for production environment export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_PROD} fi } function new_bridge_variables() { \u00af\u2502 if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then \u2502 # new-bridge environment variables for ephemeral environments \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_DEV} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_DEV} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! else \u2502 # new-bridge environment variables for production environment \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_PRO} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_PRO} \u2502 fi \u2502 } _\u2502 function notifier_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # notifier environment variables for ephemeral environments export NOTIFIER_SLACK_URL=${SLACK_URL_DEV} else # notifier environment variables for production environment export NOTIFIER_SLACK_URL=${SLACK_URL_PRO} fi } _\u2502 ... function environments_assign() { # assign enabled variable for each subchart create_enabled_var_for_each_subchart # assign common environment variables for each environment common_variables_by_environment # assign specific environment variables for each subchart bruin_bridge_variables cts_bridge_variables digi_bridge_variables digi_reboot_report_variables email_tagger_monitor_variables hawkeye_bridge_variables links_metrics_api_variables lit_bridge_variables lumin_billing_report_variables new_bridge_variables <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 and here! notifier_variables t7_bridge_variables ticket_collector_variables velocloud_bridge_variables } ... add the variables in gitlab-ci finally, we have all the path until the real value in Gitlab. Let's go to the repository settings/ci-cd section and create the new variables: That's all, with this and the proper commit message the pipeline will run and deploy an ephemeral environment.","title":"5. Helm templates and variables"},{"location":"DOCUMENTATION/","text":"1. DOCS Organization Folder structure diagrams : In this folder you will encounter all the diagrams Intelygenz has done. decisions : Folder where Intelygenz store the decisions made by the team. metrics-definitions : Here is where all the metrics of the project are defined. images : Folder to store images to be use on other MD files. logging : 2. Rules This is the Main source of truth. Docs must be in the docs folder. Always make atomic commits. All the documentation must be reviewed and approved. Before start the code of a new metric, document it, and work on it only after approved by another team member. Organization of the documentation matters. Discuss with your team where is the best place to put new stuff. Always link a new MD file in an index README.md where makes sense. Diagrams are important, before start coding a new system/infrastructure, update the diagrams. 3. Tools Diagrams To create good diagrams centralized in the repository we use diagrams.net , this tool has plugins for the two main IDE that the company use: * IntelliJ * VSCode","title":"Organization"},{"location":"DOCUMENTATION/#1-docs-organization","text":"","title":"1. DOCS Organization"},{"location":"DOCUMENTATION/#folder-structure","text":"diagrams : In this folder you will encounter all the diagrams Intelygenz has done. decisions : Folder where Intelygenz store the decisions made by the team. metrics-definitions : Here is where all the metrics of the project are defined. images : Folder to store images to be use on other MD files. logging :","title":"Folder structure"},{"location":"DOCUMENTATION/#2-rules","text":"This is the Main source of truth. Docs must be in the docs folder. Always make atomic commits. All the documentation must be reviewed and approved. Before start the code of a new metric, document it, and work on it only after approved by another team member. Organization of the documentation matters. Discuss with your team where is the best place to put new stuff. Always link a new MD file in an index README.md where makes sense. Diagrams are important, before start coding a new system/infrastructure, update the diagrams.","title":"2. Rules"},{"location":"DOCUMENTATION/#3-tools","text":"","title":"3. Tools"},{"location":"DOCUMENTATION/#diagrams","text":"To create good diagrams centralized in the repository we use diagrams.net , this tool has plugins for the two main IDE that the company use: * IntelliJ * VSCode","title":"Diagrams"},{"location":"INFRASTRUCTURE_AS_CODE/","text":"INFRASTRUCTURE AS CODE Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. IaC is a key DevOps practice and is used in conjunction with continuous delivery. Azure Introduction Infrastructure as Code enables DevOps to test the deployment of environments before use it in production. IaC can deliver stable environments rapidly and at scale. Avoiding manual configuration of environments and enforce consistency by representing the desired state of their environments via code. This technique improves the automatic deployments in automation-engine, each time the pipelines launch the Continuous delivery will create, update or destroy the infrastructure if it's necessary. IaC in MetTel Automation Automation-engine runs IaC with terraform , this task is/will be included in the automation pipelines . Terraform save the state of the infrastructure in a storage, these files have the extension .tfstate . In MetTel Automation we saves these files in a protected Cloud storage to centralize the states and be accessible each time the pipeline needs to deploy/update the infrastructure. Folder structure infra-as-code/ \u251c\u2500\u2500 basic-infra # basic infrastructure in AWS \u2514\u2500\u2500 data-collector # data-collector infrastructre in AWS \u2514\u2500\u2500 dev # AWS resources for each environment (ECS Cluster, ElastiCache Cluster, etc.) \u2514\u2500\u2500 kre # kre infrastructure \u2514\u2500\u2500 0 -create-bucket # bucket to store EKS information \u2514\u2500\u2500 1 -eks-roles # IAM roles infrastructure for EKS cluster \u2514\u2500\u2500 2 -smtp # SES infrastructure folder \u2514\u2500\u2500 kre-runtimes # kre runtimes infrastructure \u2514\u2500\u2500 modules # custom terraform modules folders used for create KRE infrastructure \u2514\u2500\u2500 runtimes # KRE runtimes folders \u2514\u2500\u2500 network-resources # network resources infrastructure in AWS Al terraform files are located inside ./infra-as-code , in this folder there are four additional folders, basic-infra , dev , ecs-services and network-resources . basic-infra : there are the necessary terraform files to create the Docker images repositories in ECS, and the roles and policies necessary for use these. data-collector : there are the necessary terraform files to create a Lambda, a DocumentDB Cluster, as well as an API Gateway to call the necessary and all the necessary resources to perform the conexion between these elements. These resources will only be created for production environment dev : there are the necessary terraform files for create the resources used for each environment in AWS, these are as follows An ECS Cluster , the ECS Services and its Task Definition for all the microservices present in the project Three ElastiCache Redis Clusters An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A set of Security Groups for all the resources created by the terraform files present in this folder A set of null_resource Terraform type resources to execute the python script in charge of health checking the task instances created in the deployment of capabilities microservices. kre : there are the necessary terraform files for create the infrastructure for the kre component of konstellation , as well as all the components it needs at AWS. There is a series of folders with terraform code that have a number in their names, these will be used to deploy the components in a certain order and are detailed below: 0-create-bucket : In this folder the terraform code is available to create a bucket for each environment and save information about the cluster, such as the SSH key to connect to the worker nodes of the EKS cluster that is going to be created. 1-eks-roles : In this folder the terraform code is available to create different IAM roles to map with EKS users and assign specific permissions for each one, for this purpose, a cli will be used later. 2-create-eks-cluster : In this folder the terraform code is available to create the following resources An EKS cluster to be able to deploy the different kre components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A hosted zone on Route53 for the corresponding kre environment A SSH key to connect to any worker node of EKS 3-smtp : In this folder the terraform code to create a SMTP service through Amazon SES and all the necessary components of it. kre-runtimes : there are the necessary terraform files for create the infrastructure needed by a KRE runtime: modules : Contains the terraform code for custom modules created for provision a KRE runtimes. It will create the following for each KRE runtime: A Route53 Hosted Zone in mettel-automation.net domain. runtimes : Contains the terraform code files for deploy KRE runtimes used the custom module located in modules folder. network-resources : there are the necessary terraform files for create the VPC and all related resources in the environment used for deployment, these being the following: Internet Gateway Elastic IP Addresses NAT Gateways Subnets Route tables for the created subnets A set of Security Groups for all the resources created by the terraform files present in this folder","title":"INFRASTRUCTURE AS CODE"},{"location":"INFRASTRUCTURE_AS_CODE/#infrastructure-as-code","text":"Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. IaC is a key DevOps practice and is used in conjunction with continuous delivery. Azure","title":"INFRASTRUCTURE AS CODE"},{"location":"INFRASTRUCTURE_AS_CODE/#introduction","text":"Infrastructure as Code enables DevOps to test the deployment of environments before use it in production. IaC can deliver stable environments rapidly and at scale. Avoiding manual configuration of environments and enforce consistency by representing the desired state of their environments via code. This technique improves the automatic deployments in automation-engine, each time the pipelines launch the Continuous delivery will create, update or destroy the infrastructure if it's necessary.","title":"Introduction"},{"location":"INFRASTRUCTURE_AS_CODE/#iac-in-mettel-automation","text":"Automation-engine runs IaC with terraform , this task is/will be included in the automation pipelines . Terraform save the state of the infrastructure in a storage, these files have the extension .tfstate . In MetTel Automation we saves these files in a protected Cloud storage to centralize the states and be accessible each time the pipeline needs to deploy/update the infrastructure.","title":"IaC in MetTel Automation"},{"location":"INFRASTRUCTURE_AS_CODE/#folder-structure","text":"infra-as-code/ \u251c\u2500\u2500 basic-infra # basic infrastructure in AWS \u2514\u2500\u2500 data-collector # data-collector infrastructre in AWS \u2514\u2500\u2500 dev # AWS resources for each environment (ECS Cluster, ElastiCache Cluster, etc.) \u2514\u2500\u2500 kre # kre infrastructure \u2514\u2500\u2500 0 -create-bucket # bucket to store EKS information \u2514\u2500\u2500 1 -eks-roles # IAM roles infrastructure for EKS cluster \u2514\u2500\u2500 2 -smtp # SES infrastructure folder \u2514\u2500\u2500 kre-runtimes # kre runtimes infrastructure \u2514\u2500\u2500 modules # custom terraform modules folders used for create KRE infrastructure \u2514\u2500\u2500 runtimes # KRE runtimes folders \u2514\u2500\u2500 network-resources # network resources infrastructure in AWS Al terraform files are located inside ./infra-as-code , in this folder there are four additional folders, basic-infra , dev , ecs-services and network-resources . basic-infra : there are the necessary terraform files to create the Docker images repositories in ECS, and the roles and policies necessary for use these. data-collector : there are the necessary terraform files to create a Lambda, a DocumentDB Cluster, as well as an API Gateway to call the necessary and all the necessary resources to perform the conexion between these elements. These resources will only be created for production environment dev : there are the necessary terraform files for create the resources used for each environment in AWS, these are as follows An ECS Cluster , the ECS Services and its Task Definition for all the microservices present in the project Three ElastiCache Redis Clusters An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A set of Security Groups for all the resources created by the terraform files present in this folder A set of null_resource Terraform type resources to execute the python script in charge of health checking the task instances created in the deployment of capabilities microservices. kre : there are the necessary terraform files for create the infrastructure for the kre component of konstellation , as well as all the components it needs at AWS. There is a series of folders with terraform code that have a number in their names, these will be used to deploy the components in a certain order and are detailed below: 0-create-bucket : In this folder the terraform code is available to create a bucket for each environment and save information about the cluster, such as the SSH key to connect to the worker nodes of the EKS cluster that is going to be created. 1-eks-roles : In this folder the terraform code is available to create different IAM roles to map with EKS users and assign specific permissions for each one, for this purpose, a cli will be used later. 2-create-eks-cluster : In this folder the terraform code is available to create the following resources An EKS cluster to be able to deploy the different kre components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A hosted zone on Route53 for the corresponding kre environment A SSH key to connect to any worker node of EKS 3-smtp : In this folder the terraform code to create a SMTP service through Amazon SES and all the necessary components of it. kre-runtimes : there are the necessary terraform files for create the infrastructure needed by a KRE runtime: modules : Contains the terraform code for custom modules created for provision a KRE runtimes. It will create the following for each KRE runtime: A Route53 Hosted Zone in mettel-automation.net domain. runtimes : Contains the terraform code files for deploy KRE runtimes used the custom module located in modules folder. network-resources : there are the necessary terraform files for create the VPC and all related resources in the environment used for deployment, these being the following: Internet Gateway Elastic IP Addresses NAT Gateways Subnets Route tables for the created subnets A set of Security Groups for all the resources created by the terraform files present in this folder","title":"Folder structure"},{"location":"LOGGING_AND_MONITORING/","text":"Logging and Monitoring Cloudwatch Cloudwatch Log Groups A log group is created in Cloudwatch for the different environments deployed in AWS: Production environment : A log group will be created with the name automation-master in which the different logStreams will be created to store the logs of the different ECS services for the production environment. Ephemeral environments : A log group will be created with the name automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment, the logStreams required for the different ECS services deployed in that environment will be created using the mentioned log group . Cloudwatch Log Streams As mentioned in the previous section, the different logStreams of the deployed services will be stored in a specific logGroup for each environment. A logStream will be created for each of the ECS cluster services tasks created in each environment, which will follow the following scheme <environment_name>-<microservice_image_build_number>/<microservice_name>/<ecs_tasks_id> environment_name : The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. microservice_image_build_number : The pipeline number is used as build number to build the image of those microservices that need to build a new docker image. microservice_name : The name of the microservice deployed in ECS, e.g. bruin-bridge . ecs_task_id> : For each ECS service, one or several tasks are created, depending on the desired number in the service, these tasks are identified with an identifier formed by number and letters, e.g. 961ef51b61834a2e9dd804db564a9fe0 . Cloudwatch Retention Period All environments deployed on AWS have been configured to use Cloudwatch to record the logs of the microservices present in them, although it is important to note the following differences: Production environment : The retention period of the log group created for such an environment is 90 days. Ephemeral environments : The retention period of the log group created for such an environment is 14 days. This retention period is configured in the infra-as-code/dev/logs.tf file. Cloudwatch logs retrieval tool It is possible to obtain the events in logs of a logGroup through a tool designed for this purpose available in Github called log-stream-filter . Download and install This tool is available for Linux, MacOS and Windows, it is possible to download the latest binary for each of these OSs: Linux : It's possible download and install as a deb package curl -o log-stream-filter.deb -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_amd64.deb\")) | .browser_download_url' ) sudo dpkg -i log-stream-filter.deb It's also possible download and install as a simple binary curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_64-bit.tar.gz\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin MacOS : curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"macOS\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin Windows : curl -o log-stream-filter.zip -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"windows\")) | .browser_download_url' ) unzip log-stream-filter.zip Example of usage Example of search text Outage monitoring process finished between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 in log group with name automation-master for log streams that match with name service-outage-monitor-1 using AWS profile with name mettel-automation : $ log-stream-filter -n \"automation-master\" -l \"service-outage-monitor-1\" -a \"mettel-automation\" -s \"04/06/2021 12:00:00\" -e \"04/07/2021 12:00:00\" -T \"Outage monitoring process finished\" -t true Filtering logs for logGroup automation-master params: [ aws-profile mettel-automation ] [ log-stream-filter: service-outage-monitor-1 ] [ search-term-search: true ] [ search-term: Outage monitoring process finished ] [ path: /tmp ] [ start-date: 04 /06/2021 12 :00:00 ] [ end-date: 04 /07/2021 12 :00:00 ] Getting logStreams for logGroup automation-master applying filter service-outage-monitor-1 Getting the logEvents for those logStreams whose last event was inserted between 04 /06/2021 12 :00:00 and 04 /07/2021 12 :00:00 **************************************************************************************************** LogStreamName: automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 CreationTime: 04 /05/2021 23 :08:29 LastEventTime: 04 /06/2021 12 :43:49 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 from time 1617710400000 Event messages for stream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 in log group automation-master are going to be saved in the following files - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 CreationTime: 04 /06/2021 12 :44:40 LastEventTime: 04 /07/2021 10 :39:16 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 from time 1617710400000 Event messages for stream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 CreationTime: 04 /07/2021 10 :41:22 LastEventTime: 04 /07/2021 11 :04:33 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 from time 1617710400000 Event messages for stream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished **************************************************************************************************** 3 files generated for logs of logStreams filtered for logGroup automation-master Location of files where logs of logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 were stored are the following - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 were stored are the following - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 were stored are the following - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished To know all the options it is recommended to read the README of log-stream-filter in github. Papertrail There is a papertrail account used in the project for sending logs, but it will only be used for the production environment , this is because it is a high cost service and it is not feasible to have an account for all ephemeral environments due to its cost and the high volume of logs generated in these environments. Papertrail dashboards The system logs of the production environment are stored in papertrail, the account credentials are in the project's onepassword vault to which all people in the project have access. Below is a screenshot of the main screen of the system. In this system it's possible see three useful dashboards: [production] - master alarms : alarms are defined on different modules with notifications sent to slack through the mettel-alarms-papertrail-production channel, for example when the number of error messages in a module exceeds 100 times in an hour. Below is a screenshot where these alarms have been marked in a red rectangle. [production] - master logs : searches are defined to gather the logs of the replicas of each deployed microservice. Below is a screenshot where these searches have been marked in a red rectangle. [production] - master notifications : Searches on different modules are defined with their notification to slack through the mettel-notifications-papertrail-production channel. Below is a screenshot where these searches have been marked in a red rectangle. Papertrail logging configuration A certain configuration must be made for the microservices to use papertrail for sending logs in production, this configuration is made in the LOG_CONFIG section of the config.py file present in the src/config folder of each microservice, this configuration is shown below: LOG_CONFIG = { 'name' : '<microservice_name>' , 'level' : logging.DEBUG, 'stream_handler' : logging.StreamHandler ( sys.stdout ) , 'format' : f '%(asctime)s: {ENVIRONMENT_NAME}: %(hostname)s: %(module)s::%(lineno)d %(levelname)s: %(message)s' , 'papertrail' : { 'active' : True if os.getenv ( 'PAPERTRAIL_ACTIVE' ) == \"true\" else False, 'prefix' : os.getenv ( 'PAPERTRAIL_PREFIX' , f '{ENVIRONMENT_NAME}-<microservice_name>' ) , 'host' : os.getenv ( 'PAPERTRAIL_HOST' ) , 'port' : int ( os.getenv ( 'PAPERTRAIL_PORT' )) } , } It would be necessary to put the microservice name instead of microservice_name for each microservice in the project Papertrail searches configuration The papertrail-provisioning tool is available in the repository to configure the different groups of searches in papertrail, you must follow the procedure explained in the README of the same for its use. The above mentioned guide should be followed to add log searches for a specific microservice, it is also possible to configure alarms that will send notifications to slack.","title":"LOGGING AND MONITORING"},{"location":"LOGGING_AND_MONITORING/#logging-and-monitoring","text":"","title":"Logging and Monitoring"},{"location":"LOGGING_AND_MONITORING/#cloudwatch","text":"","title":"Cloudwatch"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-log-groups","text":"A log group is created in Cloudwatch for the different environments deployed in AWS: Production environment : A log group will be created with the name automation-master in which the different logStreams will be created to store the logs of the different ECS services for the production environment. Ephemeral environments : A log group will be created with the name automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment, the logStreams required for the different ECS services deployed in that environment will be created using the mentioned log group .","title":"Cloudwatch Log Groups"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-log-streams","text":"As mentioned in the previous section, the different logStreams of the deployed services will be stored in a specific logGroup for each environment. A logStream will be created for each of the ECS cluster services tasks created in each environment, which will follow the following scheme <environment_name>-<microservice_image_build_number>/<microservice_name>/<ecs_tasks_id> environment_name : The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. microservice_image_build_number : The pipeline number is used as build number to build the image of those microservices that need to build a new docker image. microservice_name : The name of the microservice deployed in ECS, e.g. bruin-bridge . ecs_task_id> : For each ECS service, one or several tasks are created, depending on the desired number in the service, these tasks are identified with an identifier formed by number and letters, e.g. 961ef51b61834a2e9dd804db564a9fe0 .","title":"Cloudwatch Log Streams"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-retention-period","text":"All environments deployed on AWS have been configured to use Cloudwatch to record the logs of the microservices present in them, although it is important to note the following differences: Production environment : The retention period of the log group created for such an environment is 90 days. Ephemeral environments : The retention period of the log group created for such an environment is 14 days. This retention period is configured in the infra-as-code/dev/logs.tf file.","title":"Cloudwatch Retention Period"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-logs-retrieval-tool","text":"It is possible to obtain the events in logs of a logGroup through a tool designed for this purpose available in Github called log-stream-filter .","title":"Cloudwatch logs retrieval tool"},{"location":"LOGGING_AND_MONITORING/#download-and-install","text":"This tool is available for Linux, MacOS and Windows, it is possible to download the latest binary for each of these OSs: Linux : It's possible download and install as a deb package curl -o log-stream-filter.deb -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_amd64.deb\")) | .browser_download_url' ) sudo dpkg -i log-stream-filter.deb It's also possible download and install as a simple binary curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_64-bit.tar.gz\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin MacOS : curl -o log-stream-filter.tgz -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"macOS\")) | .browser_download_url' ) tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin Windows : curl -o log-stream-filter.zip -L $( curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"windows\")) | .browser_download_url' ) unzip log-stream-filter.zip","title":"Download and install"},{"location":"LOGGING_AND_MONITORING/#example-of-usage","text":"Example of search text Outage monitoring process finished between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 in log group with name automation-master for log streams that match with name service-outage-monitor-1 using AWS profile with name mettel-automation : $ log-stream-filter -n \"automation-master\" -l \"service-outage-monitor-1\" -a \"mettel-automation\" -s \"04/06/2021 12:00:00\" -e \"04/07/2021 12:00:00\" -T \"Outage monitoring process finished\" -t true Filtering logs for logGroup automation-master params: [ aws-profile mettel-automation ] [ log-stream-filter: service-outage-monitor-1 ] [ search-term-search: true ] [ search-term: Outage monitoring process finished ] [ path: /tmp ] [ start-date: 04 /06/2021 12 :00:00 ] [ end-date: 04 /07/2021 12 :00:00 ] Getting logStreams for logGroup automation-master applying filter service-outage-monitor-1 Getting the logEvents for those logStreams whose last event was inserted between 04 /06/2021 12 :00:00 and 04 /07/2021 12 :00:00 **************************************************************************************************** LogStreamName: automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 CreationTime: 04 /05/2021 23 :08:29 LastEventTime: 04 /06/2021 12 :43:49 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 from time 1617710400000 Event messages for stream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 in log group automation-master are going to be saved in the following files - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 CreationTime: 04 /06/2021 12 :44:40 LastEventTime: 04 /07/2021 10 :39:16 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 from time 1617710400000 Event messages for stream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished **************************************************************************************************** **************************************************************************************************** LogStreamName: automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 CreationTime: 04 /07/2021 10 :41:22 LastEventTime: 04 /07/2021 11 :04:33 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 from time 1617710400000 Event messages for stream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished **************************************************************************************************** 3 files generated for logs of logStreams filtered for logGroup automation-master Location of files where logs of logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 were stored are the following - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 were stored are the following - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 were stored are the following - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished To know all the options it is recommended to read the README of log-stream-filter in github.","title":"Example of usage"},{"location":"LOGGING_AND_MONITORING/#papertrail","text":"There is a papertrail account used in the project for sending logs, but it will only be used for the production environment , this is because it is a high cost service and it is not feasible to have an account for all ephemeral environments due to its cost and the high volume of logs generated in these environments.","title":"Papertrail"},{"location":"LOGGING_AND_MONITORING/#papertrail-dashboards","text":"The system logs of the production environment are stored in papertrail, the account credentials are in the project's onepassword vault to which all people in the project have access. Below is a screenshot of the main screen of the system. In this system it's possible see three useful dashboards: [production] - master alarms : alarms are defined on different modules with notifications sent to slack through the mettel-alarms-papertrail-production channel, for example when the number of error messages in a module exceeds 100 times in an hour. Below is a screenshot where these alarms have been marked in a red rectangle. [production] - master logs : searches are defined to gather the logs of the replicas of each deployed microservice. Below is a screenshot where these searches have been marked in a red rectangle. [production] - master notifications : Searches on different modules are defined with their notification to slack through the mettel-notifications-papertrail-production channel. Below is a screenshot where these searches have been marked in a red rectangle.","title":"Papertrail dashboards"},{"location":"LOGGING_AND_MONITORING/#papertrail-logging-configuration","text":"A certain configuration must be made for the microservices to use papertrail for sending logs in production, this configuration is made in the LOG_CONFIG section of the config.py file present in the src/config folder of each microservice, this configuration is shown below: LOG_CONFIG = { 'name' : '<microservice_name>' , 'level' : logging.DEBUG, 'stream_handler' : logging.StreamHandler ( sys.stdout ) , 'format' : f '%(asctime)s: {ENVIRONMENT_NAME}: %(hostname)s: %(module)s::%(lineno)d %(levelname)s: %(message)s' , 'papertrail' : { 'active' : True if os.getenv ( 'PAPERTRAIL_ACTIVE' ) == \"true\" else False, 'prefix' : os.getenv ( 'PAPERTRAIL_PREFIX' , f '{ENVIRONMENT_NAME}-<microservice_name>' ) , 'host' : os.getenv ( 'PAPERTRAIL_HOST' ) , 'port' : int ( os.getenv ( 'PAPERTRAIL_PORT' )) } , } It would be necessary to put the microservice name instead of microservice_name for each microservice in the project","title":"Papertrail logging configuration"},{"location":"LOGGING_AND_MONITORING/#papertrail-searches-configuration","text":"The papertrail-provisioning tool is available in the repository to configure the different groups of searches in papertrail, you must follow the procedure explained in the README of the same for its use. The above mentioned guide should be followed to add log searches for a specific microservice, it is also possible to configure alarms that will send notifications to slack.","title":"Papertrail searches configuration"},{"location":"MONOREPO/","text":"Monorepo In revision control systems, a monorepo (syllabic abbreviation of monolithic repository) is a software development strategy where code for many projects are stored in the same repository. Wikipedia Advantages Simplified organization : The organization is simplified separating all the projects(called modules) in different folders that are stored in the root folder of the repository. Simplified automation : The automation gets easy with this approach, each time the repo has a commit in develop or master the automation will deploy all the necessary parts of the app to make it work correctly. Refactoring changes : When a project has a dependency with another in the monorepo, the changes are easier to be made. Atomic commits : When projects that work together are contained in separate repositories, releases need to determine which versions of one project are related to the other and then syncing them. Collaboration across team : The integration between projects will be easier thanks to the branches strategy Single source of truth : Like a developer you'll find all the available code, automation and documentation in the same place. What is not a monorepo about Monorepo is not the same that Monolith. Monolith is huge amount of coupled code of one application that is hell to maintain. This is not only a repository, it's a sum of good practices between automation, code and documentation.","title":"MONOREPO"},{"location":"MONOREPO/#monorepo","text":"In revision control systems, a monorepo (syllabic abbreviation of monolithic repository) is a software development strategy where code for many projects are stored in the same repository. Wikipedia","title":"Monorepo"},{"location":"MONOREPO/#advantages","text":"Simplified organization : The organization is simplified separating all the projects(called modules) in different folders that are stored in the root folder of the repository. Simplified automation : The automation gets easy with this approach, each time the repo has a commit in develop or master the automation will deploy all the necessary parts of the app to make it work correctly. Refactoring changes : When a project has a dependency with another in the monorepo, the changes are easier to be made. Atomic commits : When projects that work together are contained in separate repositories, releases need to determine which versions of one project are related to the other and then syncing them. Collaboration across team : The integration between projects will be easier thanks to the branches strategy Single source of truth : Like a developer you'll find all the available code, automation and documentation in the same place.","title":"Advantages"},{"location":"MONOREPO/#what-is-not-a-monorepo-about","text":"Monorepo is not the same that Monolith. Monolith is huge amount of coupled code of one application that is hell to maintain. This is not only a repository, it's a sum of good practices between automation, code and documentation.","title":"What is not a monorepo about"},{"location":"PIPELINES/","text":"Pipelines In this project is implemented Software delivery with total automation, thus avoiding manual intervention and therefore human errors in our product. Human error can and does occur when carrying out these boring and repetitive tasks manually and ultimately does affect the ability to meet deliverables. All of the automation is made with Gitlab CI technology, taking advantage of all the tools that Gitlab has. We separate the automatation in two parts, continuous integration and continuous delivery , that are explained in the next sections. To improve the speed and optimization of the pipelines, only the jobs and stages will be executed on those modules that change in each commit . Launch all jobs in a pipeline Exceptionally, it is possible to launch a pipeline with all the jobs and stages on a branch using the web interface, as shown in the following image . To do so, the following steps must be followed: From the project repository select the CI/CD option in the left sidebar and this Pipelines , as shown in the following image where these options are marked in red. Choose the Run Pipeline option, as shown in the image below in red. Indicate the branch where you want to run the pipeline in the Run for box and then click on Run pipeline . It's possible see an example in the following image, where the box Run for is shown in green and Run pipeline is shown in red. It is important to note that due to the extra time added by the tests of the dispacth-portal-frontend microservice, the tests of this one will only be executed when any of the files within it change or a pipeline with the Gitlab variable TEST_DISPATCH_PORTAL_FRONTEND with value true is executed. Environments Microservices Environments For the microservices there are the following environments Production : The environment is related to everything currently running in AWS related to the latest version of the master branch of the repository. Ephemerals : These environments are created from branches that start with name dev/feature or dev/fix . The name of any environment, regardless of the type, will identify all the resources created in the deployment process. The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. These names will identify all the resources created in AWS during the continuous delivery process, explained in the following sections. KRE Environments For KRE component there are the following environments: dev : This will be used for the various tests and calls made from the project's microservices in any ephemeral environment , ie from the microservices deployed in the ECS cluster with name automation-<environment_id> . production : This will be used for the different calls made from the project's microservices in the production environment , that is, from the microservices deployed in the ECS cluster with the name automation-master . Continuous integration (CI) Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day. Each integration can then be verified by an automated build and automated tests. While automated testing is not strictly part of CI it is typically implied. Codeship Validation steps This stage checks the following: All python microservices comply with the rules of PEP8 Terraform files used to configure the infrastructure are valid from a syntactic point of view. The frontend modules comply with the linter configured for them Unit tests steps All the available unit tests for each service should be run in this stage of the CI process. If the coverage obtained from these tests for a service is not greater than or equal to 80%, it will cause this phase to fail, this will mean that the steps of the next stage will not be executed and the process will fail. In cases in which a module does not reach the minimum coverage mentioned above, a message like the following will be seen in the step executed for that module. Continuous delivery (CD) Continuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements. Puppet.com Basic_infra steps This area covers the checking and creation, if necessary, of all the basic resources for the subsequent deployment, these being the specific image repositories in ECR Docker Container Registry , as well as the roles necessary in AWS to be able to display these images in ECS Container Orchestrator . In this stage there is also a job that must be executed manually if necessary, this is responsible for checking and creating if necessary network resources for the production environment or ephemeral environments. In this stage is also checked whether there are enough free resources in ECS to carry out the deployment with success or not. It's necessary run the basic-infra job the first time a new microservice is created in the project This has been done because ECR repositories are global resources and are stored in the same tfstate file, thus avoiding that when a microservice that creates a repository is created, it is not deleted by other branches that do not have it added. Basic_infra_kre steps In this stage will be the following jobs: * deploy-basic-infra-kre-dev for ephemeral environments and deploy-basic-infra-kre-production for the production environment, these are executed optionally manually . This job is responsible of checking and creation, if necessary, of the EKS cluster used by KRE in each environment and all the necessary resources related (RBAC configuration, helm charts needed for the KRE runtimes, etc) The process followed in this job is as follows: The necessary infrastructure is created in AWS for KRE, creating for them the following components: An S3 bucket for each environment and save information about the cluster, such as the SSH key to connect to the nodes. An EKS cluster to be able to deploy the different KRE components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A SMTP service through Amazon SES and all the necessary components of it A set of IAM roles, one for each user with access to the project. These will be used to assign subsequent permissions in the Kubernetes cluster according to the role they belong to. These are stored as terraform output values , saving the list of user roles belonging to each role in their corresponding variable. Below is an example of a pipeline execution where it's possible see the IAM roles of users generated for each role in the project: Outputs: eks_developer_ops_privileged_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xisco.capllonch\" , \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xoan.mallon.developer\" , ] eks_developer_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-brandon.samudio\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-daniel.fernandez\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-joseluis.vega\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-marc.vivancos\" , ] eks_devops_roles = [ \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-alberto.iglesias\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.costales\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.luis.piquero\" , ] A set of helm charts necessary for any KRE runtime: external-dns , using the helm chart from bitnami repository external-dns is a Kubernetes addon that configures public DNS servers with information about exposed Kubernetes services to make them discoverable. It allows in a simple way that through the creation of an ingress in AWS you can create an entry in Route53 of type alias so that the calls to that ingress redirect to the value configured for the alias, being the most typical the DNS of the balancer created by the ingress. cert-mananger , using the helm chart from jetstack repository . This component automate the management lifecycle of all required certificates used by the KRE component in each environment. nginx ingress controller , using the helm chart from ingress-nginx repository . A series of configurations are provided so that the IP of clients in Kubernetes services can be known, since by default it will always use the internal IP in EKS of the load balancer for requests made from the Internet. A list of allowed IPs is also provided in the chart configuration through a specific configuration key, thus restricting access to the cluster's microservices. This component will create a Classic Load Balancer in AWS to expose nginx ingress component. hostpath provisioner , using the helm chart from rimusz repository Using a Python cli , permissions are assigned in Kubernetes Cluster created for KRE for each of the IAM roles created in the previous step. Deploy_kre_runtimes steps In this stage the KRE runtimes will be deployed in the corresponding environment, creating the necessary infrastructure and resources: A Hosted Zone in Route53 for the runtime in the specified environment using the mettel-automation.net domain name The kre helm chart with the necessary values for the environment creating a specific namespace in the EKS cluster for deploy the helm chart Build steps This area will cover all build steps of all necessary modules to deploy the app to the selected environment. It's typical to build the docker images and push to the repository in this step. Deploy steps In this stage there are one job: deploy-branches for ephemeral environments and deploy-master for the production environment, these are executed automatically . In which MetTel Automation modules in the monorepo will be deployed to the selected environment, as well as all the resources associated to that environment in AWS. The deploy steps will deploy the following in AWS: An ECS Cluster will be created for the environment with a set of resources An ECS Service that will use the new Docker image uploaded for each service of the project, being these services the specified below: bruin-bridge cts-bridge customer-cache dispatch-portal-backend dispatch-portal-frontend last-contact-report lit-bridge lumin-billing-report metrics-prometheus nats-server, nats-server-1, nats-server-2 notifier service-affecting-monitor service-dispatch-monitor service-outage-monitor-1, service-outage-monitor-2, service-outage-monitor-3, service-outage-monitor-4, service-outage-monitor-triage t7-bridge tnba-feedback tnba-monitor velocloud-bridge A Task Definition for each of the above ECS Services In this process, a series of resources will also be created in AWS for the selected environment, as follows: Three ElastiCache Redis Clusters , which are detailed below: <environment> : used to save some data about dispatches, as well as to skip the limitation of messages of more than 1MB when passing them to NATS. <environment>-customer-cache-redis : used to save the mappings between Bruin clients and Velocloud edges, being able to use them between restarts if any occurs. <environment>-tnba-feedback-cache-redis : used to save ticket metrics sent to T7, so tnba-feedback can avoid sending them again afterwards. An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A S3 bucket to store the content of the metrics obtained by Thanos and displayed through Grafana . Also, resources of type null_resource are created to execute some Python scripts: The creation of ECS Services starts only if a Python script launched as a null_resource finishes with success. This script checks that the last ECS service created for NATS is running in HEALTHY state. If the previous step succeeded then ECS services related to capabilities microservices are created, with these being the following: bruin-bridge cts-bridge lit-bridge notifier prometheus t7-bridge velocloud-bridge hawkeye-bridge Once created, the script used for NATS is launched through null_resource to check that the task instances for each of these ECS services were created successfully and are in RUNNING and HEALTHY status. Once all the scripts for the capabilities microservices have finished successfully, ECS services for the use-cases microservices are all created, with these being the following: customer-cache dispatch-portal-backend hawkeye-customer-cache hawkeye-outage-monitor last-contact-report lumin-billing-report service-affecting-monitor service-dispatch-monitor service-outage-monitor-1 service-outage-monitor-2 service-outage-monitor-3 service-outage-monitor-4 service-outage-monitor-triage tnba-feedback tnba-monitor This is achieved by defining explicit dependencies between the ECS services for the capabilities microservices and the set of null resources that perform the healthcheck of the capabilities microservices.\u200b The following is an example of a definition for the use-case microservice service-affecting-monitor using Terraform . Here, the dependency between the corresponding null_resource type resources in charge of performing the health check of the different capabilities microservices in Terraform code for this microservice is established. resource \"aws_ecs_service\" \"automation-service-affecting-monitor\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck ] . . . } This procedure has been done to ensure that use case microservices are not created in ECS until new versions of the capability-type microservices are properly deployed, as use case microservices need to use capability-type microservices. Following the same procedure as in the previous step, a dependency is established between the microservice dispatch-portal-frontend and dispatch-portal-backend . The reason for this is that the dispatch-portal-frontend microservice needs to know the corresponding IP with the DNS entry in Route53 for the dispatch-portal-backend microservice, since if the previous deployment is saved, the new IP corresponding to the DNS entry is not updated. The following is the configuration in the terraform code of the service in ECS for the dispatch-portal-frontend microservice, where the necessary configuration to comply with this restriction can be seen. resource \"aws_ecs_service\" \"automation-dispatch-portal-frontend\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck , null_resource.dispatch-portal-backend-healthcheck , aws_lb.automation-alb ] } The provisioning of the different groups and the searches included in each one of them is done through a python utility , this makes calls to the util go-papertrail-cli who is in charge of the provisioning of the elements mentioned in Papertrail . Destroy steps In this stage a series of manual jobs are available to destroy what was created in the previous stage, both for KRE and for the microservices of the repository in AWS. These are detailed below: destroy-branches for ephemeral environments or destroy-master for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-branches or deploy-master depending on the environment. destroy-branches-aws-nuke : This job is only available for ephemeral environments, it generates a yml file using a specific script to be used by aws-nuke to destroy all the infrastructure created for an ephemeral environment in AWS. This job should only be used when the `destroy-branches' job fails. destroy-basic-infra-kre-dev for ephemeral environments or destroy-basic-infra-kre-production for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-kre-dev or deploy-kre-production depending on the environment.","title":"PIPELINES"},{"location":"PIPELINES/#pipelines","text":"In this project is implemented Software delivery with total automation, thus avoiding manual intervention and therefore human errors in our product. Human error can and does occur when carrying out these boring and repetitive tasks manually and ultimately does affect the ability to meet deliverables. All of the automation is made with Gitlab CI technology, taking advantage of all the tools that Gitlab has. We separate the automatation in two parts, continuous integration and continuous delivery , that are explained in the next sections. To improve the speed and optimization of the pipelines, only the jobs and stages will be executed on those modules that change in each commit .","title":"Pipelines"},{"location":"PIPELINES/#launch-all-jobs-in-a-pipeline","text":"Exceptionally, it is possible to launch a pipeline with all the jobs and stages on a branch using the web interface, as shown in the following image . To do so, the following steps must be followed: From the project repository select the CI/CD option in the left sidebar and this Pipelines , as shown in the following image where these options are marked in red. Choose the Run Pipeline option, as shown in the image below in red. Indicate the branch where you want to run the pipeline in the Run for box and then click on Run pipeline . It's possible see an example in the following image, where the box Run for is shown in green and Run pipeline is shown in red. It is important to note that due to the extra time added by the tests of the dispacth-portal-frontend microservice, the tests of this one will only be executed when any of the files within it change or a pipeline with the Gitlab variable TEST_DISPATCH_PORTAL_FRONTEND with value true is executed.","title":"Launch all jobs in a pipeline"},{"location":"PIPELINES/#environments","text":"","title":"Environments"},{"location":"PIPELINES/#microservices-environments","text":"For the microservices there are the following environments Production : The environment is related to everything currently running in AWS related to the latest version of the master branch of the repository. Ephemerals : These environments are created from branches that start with name dev/feature or dev/fix . The name of any environment, regardless of the type, will identify all the resources created in the deployment process. The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. These names will identify all the resources created in AWS during the continuous delivery process, explained in the following sections.","title":"Microservices Environments"},{"location":"PIPELINES/#kre-environments","text":"For KRE component there are the following environments: dev : This will be used for the various tests and calls made from the project's microservices in any ephemeral environment , ie from the microservices deployed in the ECS cluster with name automation-<environment_id> . production : This will be used for the different calls made from the project's microservices in the production environment , that is, from the microservices deployed in the ECS cluster with the name automation-master .","title":"KRE Environments"},{"location":"PIPELINES/#continuous-integration-ci","text":"Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day. Each integration can then be verified by an automated build and automated tests. While automated testing is not strictly part of CI it is typically implied. Codeship","title":"Continuous integration (CI)"},{"location":"PIPELINES/#validation-steps","text":"This stage checks the following: All python microservices comply with the rules of PEP8 Terraform files used to configure the infrastructure are valid from a syntactic point of view. The frontend modules comply with the linter configured for them","title":"Validation steps"},{"location":"PIPELINES/#unit-tests-steps","text":"All the available unit tests for each service should be run in this stage of the CI process. If the coverage obtained from these tests for a service is not greater than or equal to 80%, it will cause this phase to fail, this will mean that the steps of the next stage will not be executed and the process will fail. In cases in which a module does not reach the minimum coverage mentioned above, a message like the following will be seen in the step executed for that module.","title":"Unit tests steps"},{"location":"PIPELINES/#continuous-delivery-cd","text":"Continuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements. Puppet.com","title":"Continuous delivery (CD)"},{"location":"PIPELINES/#basic_infra-steps","text":"This area covers the checking and creation, if necessary, of all the basic resources for the subsequent deployment, these being the specific image repositories in ECR Docker Container Registry , as well as the roles necessary in AWS to be able to display these images in ECS Container Orchestrator . In this stage there is also a job that must be executed manually if necessary, this is responsible for checking and creating if necessary network resources for the production environment or ephemeral environments. In this stage is also checked whether there are enough free resources in ECS to carry out the deployment with success or not. It's necessary run the basic-infra job the first time a new microservice is created in the project This has been done because ECR repositories are global resources and are stored in the same tfstate file, thus avoiding that when a microservice that creates a repository is created, it is not deleted by other branches that do not have it added.","title":"Basic_infra steps"},{"location":"PIPELINES/#basic_infra_kre-steps","text":"In this stage will be the following jobs: * deploy-basic-infra-kre-dev for ephemeral environments and deploy-basic-infra-kre-production for the production environment, these are executed optionally manually . This job is responsible of checking and creation, if necessary, of the EKS cluster used by KRE in each environment and all the necessary resources related (RBAC configuration, helm charts needed for the KRE runtimes, etc) The process followed in this job is as follows: The necessary infrastructure is created in AWS for KRE, creating for them the following components: An S3 bucket for each environment and save information about the cluster, such as the SSH key to connect to the nodes. An EKS cluster to be able to deploy the different KRE components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A SMTP service through Amazon SES and all the necessary components of it A set of IAM roles, one for each user with access to the project. These will be used to assign subsequent permissions in the Kubernetes cluster according to the role they belong to. These are stored as terraform output values , saving the list of user roles belonging to each role in their corresponding variable. Below is an example of a pipeline execution where it's possible see the IAM roles of users generated for each role in the project: Outputs: eks_developer_ops_privileged_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xisco.capllonch\" , \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xoan.mallon.developer\" , ] eks_developer_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-brandon.samudio\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-daniel.fernandez\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-joseluis.vega\" , \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-marc.vivancos\" , ] eks_devops_roles = [ \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-alberto.iglesias\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.costales\" , \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.luis.piquero\" , ] A set of helm charts necessary for any KRE runtime: external-dns , using the helm chart from bitnami repository external-dns is a Kubernetes addon that configures public DNS servers with information about exposed Kubernetes services to make them discoverable. It allows in a simple way that through the creation of an ingress in AWS you can create an entry in Route53 of type alias so that the calls to that ingress redirect to the value configured for the alias, being the most typical the DNS of the balancer created by the ingress. cert-mananger , using the helm chart from jetstack repository . This component automate the management lifecycle of all required certificates used by the KRE component in each environment. nginx ingress controller , using the helm chart from ingress-nginx repository . A series of configurations are provided so that the IP of clients in Kubernetes services can be known, since by default it will always use the internal IP in EKS of the load balancer for requests made from the Internet. A list of allowed IPs is also provided in the chart configuration through a specific configuration key, thus restricting access to the cluster's microservices. This component will create a Classic Load Balancer in AWS to expose nginx ingress component. hostpath provisioner , using the helm chart from rimusz repository Using a Python cli , permissions are assigned in Kubernetes Cluster created for KRE for each of the IAM roles created in the previous step.","title":"Basic_infra_kre steps"},{"location":"PIPELINES/#deploy_kre_runtimes-steps","text":"In this stage the KRE runtimes will be deployed in the corresponding environment, creating the necessary infrastructure and resources: A Hosted Zone in Route53 for the runtime in the specified environment using the mettel-automation.net domain name The kre helm chart with the necessary values for the environment creating a specific namespace in the EKS cluster for deploy the helm chart","title":"Deploy_kre_runtimes steps"},{"location":"PIPELINES/#build-steps","text":"This area will cover all build steps of all necessary modules to deploy the app to the selected environment. It's typical to build the docker images and push to the repository in this step.","title":"Build steps"},{"location":"PIPELINES/#deploy-steps","text":"In this stage there are one job: deploy-branches for ephemeral environments and deploy-master for the production environment, these are executed automatically . In which MetTel Automation modules in the monorepo will be deployed to the selected environment, as well as all the resources associated to that environment in AWS. The deploy steps will deploy the following in AWS: An ECS Cluster will be created for the environment with a set of resources An ECS Service that will use the new Docker image uploaded for each service of the project, being these services the specified below: bruin-bridge cts-bridge customer-cache dispatch-portal-backend dispatch-portal-frontend last-contact-report lit-bridge lumin-billing-report metrics-prometheus nats-server, nats-server-1, nats-server-2 notifier service-affecting-monitor service-dispatch-monitor service-outage-monitor-1, service-outage-monitor-2, service-outage-monitor-3, service-outage-monitor-4, service-outage-monitor-triage t7-bridge tnba-feedback tnba-monitor velocloud-bridge A Task Definition for each of the above ECS Services In this process, a series of resources will also be created in AWS for the selected environment, as follows: Three ElastiCache Redis Clusters , which are detailed below: <environment> : used to save some data about dispatches, as well as to skip the limitation of messages of more than 1MB when passing them to NATS. <environment>-customer-cache-redis : used to save the mappings between Bruin clients and Velocloud edges, being able to use them between restarts if any occurs. <environment>-tnba-feedback-cache-redis : used to save ticket metrics sent to T7, so tnba-feedback can avoid sending them again afterwards. An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A S3 bucket to store the content of the metrics obtained by Thanos and displayed through Grafana . Also, resources of type null_resource are created to execute some Python scripts: The creation of ECS Services starts only if a Python script launched as a null_resource finishes with success. This script checks that the last ECS service created for NATS is running in HEALTHY state. If the previous step succeeded then ECS services related to capabilities microservices are created, with these being the following: bruin-bridge cts-bridge lit-bridge notifier prometheus t7-bridge velocloud-bridge hawkeye-bridge Once created, the script used for NATS is launched through null_resource to check that the task instances for each of these ECS services were created successfully and are in RUNNING and HEALTHY status. Once all the scripts for the capabilities microservices have finished successfully, ECS services for the use-cases microservices are all created, with these being the following: customer-cache dispatch-portal-backend hawkeye-customer-cache hawkeye-outage-monitor last-contact-report lumin-billing-report service-affecting-monitor service-dispatch-monitor service-outage-monitor-1 service-outage-monitor-2 service-outage-monitor-3 service-outage-monitor-4 service-outage-monitor-triage tnba-feedback tnba-monitor This is achieved by defining explicit dependencies between the ECS services for the capabilities microservices and the set of null resources that perform the healthcheck of the capabilities microservices.\u200b The following is an example of a definition for the use-case microservice service-affecting-monitor using Terraform . Here, the dependency between the corresponding null_resource type resources in charge of performing the health check of the different capabilities microservices in Terraform code for this microservice is established. resource \"aws_ecs_service\" \"automation-service-affecting-monitor\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck ] . . . } This procedure has been done to ensure that use case microservices are not created in ECS until new versions of the capability-type microservices are properly deployed, as use case microservices need to use capability-type microservices. Following the same procedure as in the previous step, a dependency is established between the microservice dispatch-portal-frontend and dispatch-portal-backend . The reason for this is that the dispatch-portal-frontend microservice needs to know the corresponding IP with the DNS entry in Route53 for the dispatch-portal-backend microservice, since if the previous deployment is saved, the new IP corresponding to the DNS entry is not updated. The following is the configuration in the terraform code of the service in ECS for the dispatch-portal-frontend microservice, where the necessary configuration to comply with this restriction can be seen. resource \"aws_ecs_service\" \"automation-dispatch-portal-frontend\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck , null_resource.cts-bridge-healthcheck , null_resource.lit-bridge-healthcheck , null_resource.velocloud-bridge-healthcheck , null_resource.hawkeye-bridge-healthcheck , null_resource.t7-bridge-healthcheck , null_resource.notifier-healthcheck , null_resource.metrics-prometheus-healthcheck , null_resource.dispatch-portal-backend-healthcheck , aws_lb.automation-alb ] } The provisioning of the different groups and the searches included in each one of them is done through a python utility , this makes calls to the util go-papertrail-cli who is in charge of the provisioning of the elements mentioned in Papertrail .","title":"Deploy steps"},{"location":"PIPELINES/#destroy-steps","text":"In this stage a series of manual jobs are available to destroy what was created in the previous stage, both for KRE and for the microservices of the repository in AWS. These are detailed below: destroy-branches for ephemeral environments or destroy-master for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-branches or deploy-master depending on the environment. destroy-branches-aws-nuke : This job is only available for ephemeral environments, it generates a yml file using a specific script to be used by aws-nuke to destroy all the infrastructure created for an ephemeral environment in AWS. This job should only be used when the `destroy-branches' job fails. destroy-basic-infra-kre-dev for ephemeral environments or destroy-basic-infra-kre-production for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-kre-dev or deploy-kre-production depending on the environment.","title":"Destroy steps"},{"location":"README_AUTOMATION/","text":"Documentation directory System overview System architecture Monorepo Pipelines Launch all jobs in a pipeline Environments CI CD Infrastructure as code Infrastructure Environment infrastructure Network infrastructure Logging and monitoring Cloudwatch Cloudwatch Log Groups Cloudwatch Log Streams Cloudwatch Retention Periord Cloudwatch logs retrieval tool Papertrail Papertrail Dashboards Papertrail Logging Configuration Papertrail Searches Configuration","title":"README AUTOMATION"},{"location":"README_AUTOMATION/#documentation-directory","text":"System overview System architecture Monorepo Pipelines Launch all jobs in a pipeline Environments CI CD Infrastructure as code Infrastructure Environment infrastructure Network infrastructure Logging and monitoring Cloudwatch Cloudwatch Log Groups Cloudwatch Log Streams Cloudwatch Retention Periord Cloudwatch logs retrieval tool Papertrail Papertrail Dashboards Papertrail Logging Configuration Papertrail Searches Configuration","title":"Documentation directory"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/","text":"Context Configurations are stored in AWS Systems Manager Parameter Store . Parameter Store is a capability of AWS Systems Manager that allows storing configuration values in hierarchical structures. It also provides the ability to encrypt these values through AWS KMS keys, and allows keeping track of configuration changes / versions for auditing purposes. From a pricing perspective, parameters can be defined either as simple or advanced : - Simple . This kind of parameters come with no additional charges, and can hold up to 4KB of data. Parameter Store allows defining up to 10,000 simple parameters per account and region. - Advanced . AWS charges $0.05 per advanced parameter per month. These can hold up to 8KB of data, and Parameter Store allows defining up to 100,000 of them per account and region. Attending to their type, parameters can be defined as: * String . These parameters consist of a block of text. Nothing else. * StringList . These parameters hold comma-separated values, but bear in mind that this is just a convention. AWS will not turn the value into an array before returning it through Parameter Store API; that is, it will be kept as a string. * SecureString . Essentially, these are the same as String parameters, but empowered with encryption features thanks to KMS keys. Conventions Please, take some time to go through the following conventions before publishing new parameters to Parameter Store. Parameter names Any new configuration published to Parameter Store must match the following pattern: /automation-engine/<environment>/<service-name>/<parameter> where: * environment refers to the Kubernetes context whose namespaces will take this parameter. Allowed values are dev , pro and common . * service-name refers to the name of the service this parameter will be loaded to. Examples of allowed values would be bruin-bridge , tnba-monitor , repair-tickets-monitor , and so on. * parameter refers to the name of the parameter that will be loaded to the service through an environment variable. Most configurations will follow the previous pattern, but if the business logic of a service carries out several tasks related to the same domain concept, an alternate form can be used: /automation-engine/<environment>/<domain>/<task>/<parameter> where: * domain refers to a domain concept that can represent multiple tasks accurately. An example would be service-affecting , which is a domain area that represents strongly related tasks (in this case, tasks related to Service Affecting troubles). * task refers to the underlying piece of logic that is independent of other tasks, but is still suitable to have under the subpath defined by domain . Considering that domain is set to service-affecting , acceptable values for task would be monitor , daily-bandwidth-report , or reoccurring-trouble-report . As a rule of thumb, choose the first pattern to name parameters if possible. However, if there is a strong reason to choose the second pattern over the first one, that is totally fine. If there are doubts about choosing one or the other, it can be brought up to discussion with the rest of the team. Values Before getting into how to define new configurations, bear these considerations in mind to keep certain consistency and avoid confusion / potential errors: * Values representing a time duration must always be expressed in seconds. * 24 should not be used to represent the hours in a period of 1 day. * 86400 should be used to represent the hours in a period of 1 day. Values representing percentages must always be expressed in their integer form. 0.75 should not be used to represent the value 75% . 75 should be used to represent the value 75% . JSON-like values must adhere to the JSON specification . The most relevant considerations are: All keys in an object-like JSON must be strings, even if they represent numbers. Example: // NOT valid { 1 : \"foo\" , 2 : \"bar\" } // Valid { \"1\" : \"foo\" , \"2\" : \"bar\" } // Valid { \"foo\" : \"bar\" , \"baz\" : \"hey\" } 2. An array-like JSON can hold a combination of values with different types, be them integers, floats, boolean, arrays, objects, and so on. Example: [ 1 , \"foo\" , 0.75 , \"2\" , \"bar\" , true , [ \"hello\" , \"world\" ], { \"hello\" : \"world\" } ] Consider prettifying JSON-like configurations before publishing to Parameter Store. This enhances readability when dealing with huge JSONs. Developers are responsible for making any necessary transformations related to data types, time conversions, etc. in the config files of a particular service . Encryption A parameter must be encrypted if it holds: * Personally Identifiable Information. This includes e-mail addresses, names, surnames, phone numbers, and so on. * Authentication credentials of any kind. * URLs from third party services. * Information related to application domains, such as: * IDs (even incremental ones) * Organization names * Domain-related values that potentially expose internal details about third party systems Publishing configurations to Parameter Store Adding new configurations to Parameter Store is pretty straightforward. The process should be as easy as: 1. Access the AWS Management Console. 2. Go to the AWS Systems Manager Parameter Store dashboard. 3. Hit Create parameter . 4. Give the parameter a Name that complies with any of the patterns under the Parameter names section. 5. Make sure to add a meaningful Description to the parameter. This is extremely important to give context to anyone in need of making changes to parameters, so take some time to think about a good, meaningful description. 6. Choose the tier that fits better for this parameter: 1. If the parameter is small and is not expected to grow much as time passes, choose Standard . 2. On the other hand, if the parameter is large enough and is expected to grow, choose Advanced . 7. Choose the type that fits better for this parameter: 1. If the parameter is safe to stay unencrypted in AWS: 1. Choose String . 2. Set the Data Type field to text . 2. On the other hand, if the parameter needs to be encrypted (see the Encryption section): 1. Choose SecureString . 2. Set the KMS Key Source field to My current account to pick a KMS key registered in the current account. 3. Set the KMS Key ID field to alias/aws/ssm to pick the default KMS key for Parameter Store. In general, avoid using StringList parameters. These are special cases of the String type, which essentially means that String can be used to create parameters based on comma-separated values as well. 8. Give the parameter a value that adheres to the conventions specified in the Values section. 9. Finally, hit Create parameter to save it. About the different environments When creating a new parameter in the Parameter Store, please decide if it will be different on dev and pro or if it will be same in both environments. If they're different, you should create a parameter for each environment. Otherwise, just create one under common . In general, most parameters will share the same value, unless there is a strong reason to keep them different. For example, parameters used to point to third party systems may differ if their app has not only a Production system, but also a Development / Test one. In that case, the pro version of the parameter should aim at the third party's Production system, and the dev version should aim at their Development / Test system. Hooking AWS Parameter Store with Kubernetes clusters Pre-requisites Before moving on, make sure to install k9s in your system. k9s is a powerful terminal UI that lets users manage Kubernetes clusters from their own machines, and even edit Kubernetes objects in place to reflect those changes immediately. After installing k9s , follow these steps to install a plugin that allows editing decoded Secret objects (more in the next section): 1. Install krew , following the instructions for the desired OS. 2. Install kubectl 's plugin kubectl-modify-secret , following the instructions for the desired OS. 3. Integrate kubectl-modify-secret into k9s 's plugin system: 1. In a terminal, run k9s info to check which folder holds k9s configurations. 2. Create file plugin.yml under that folder, if it does not exist yet. 3. Add the following snippet: plugin : edit-secret : shortCut : Ctrl-X confirm : false description : \"Edit Decoded Secret\" scopes : - secrets command : kubectl background : false args : - modify-secret - --namespace - $NAMESPACE - --context - $CONTEXT - $NAME > By default, the shortcut to edit decoded secrets is set to Ctrl + X . If needed, set a custom one instead. Kubernetes: Secrets and ConfigMaps Secret and ConfigMap objects are the Kubernetes objects used to inject environment variables to one or multiple pods. Like other Kubernetes objects, they are defined through .yaml files. These objects hold mappings between environment variables and their values, along with some metadata. Here is an example of a ConfigMap definition: apiVersion : v1 data : CURRENT_ENVIRONMENT : production ENVIRONMENT_NAME : production REDIS_HOSTNAME : redis.pro.somewhere.com kind : ConfigMap metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2021-08-30T15:08:12Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : case-of-use project : mettel-automation name : some-fancy-bridge-configmap namespace : automation-engine resourceVersion : \"83171937\" selfLink : /api/v1/namespaces/automation-engine/configmaps/some-fancy-bridge-configmap uid : ba625451-8ba4-4ac7-a8da-593cc938eae7 And here is an example of a Secret definition: apiVersion : v1 data : THIRD_PARTY_API_USERNAME : aGVsbG8K THIRD_PARTY_API_PASSWORD : d29ybGQK kind : Secret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reconcile.external-secrets.io/data-hash : 3162fd065a6777587e9a3a604e0c56e2 reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:31:36Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine ownerReferences : - apiVersion : external-secrets.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ExternalSecret name : some-fancy-bridge-secret uid : 42bdad1c-37c4-4890-b86b-d9623333df18 resourceVersion : \"82588568\" selfLink : /api/v1/namespaces/automation-engine/secrets/some-fancy-bridge-secret uid : 81ad3356-8696-406e-bba0-c4ec389676ee type : Opaque Both objects have a data field where the different configurations are stored. The main difference between both objects is that all fields under data remain clear in ConfigMap objects, but in Secret ones, these fields are base64-encoded. These objects lack of mechanisms to pull configurations from external sources, so an additional tool is needed to hook them with AWS Parameter Store. External secrets The (External Secrets Operator)[https://github.com/external-secrets/external-secrets] is a tool that allows setting up Secret objects based on external references through a new kind of object called ExternalSecret . ExternalSecret objects define the external source to pull configurations from, and the references that should be resolved. After these references have been resolved, the ExternalSecret will create a regular Secret object with a data section whose key-value pairs are based on the environment variables the pod expects to see, and the values gotten after resolving the references from the external source. Aside from that, ExternalSecret objects pull configuration values from the external source periodically to keep secrets up to date, also known as reconciling secrets . An example of ExternalSecret object would be this one: apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:11:02Z\" generation : 1 labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine resourceVersion : \"83201847\" selfLink : /apis/external-secrets.io/v1alpha1/namespaces/automation-engine/externalsecrets/some-fancy-bridge-secret uid : ca5c1faf-1b76-4f59-b57f-e10e43154ace spec : data : - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE status : conditions : - lastTransitionTime : \"2022-03-08T18:11:10Z\" message : Secret was synced reason : SecretSynced status : \"True\" type : Ready refreshTime : \"2022-03-10T13:29:12Z\" syncedResourceVersion : 1-3efb62db37d8b935be922ecc6f7ed99f In this example, there are two items under the data section. Each item refers to an external / remote reference (field remoteRef::key ), and the environment variable that the value behind that remote reference should be loaded to (field secretKey ). Manipulating external secrets in the Automation system To add, remove, or update external secrets for a particular service in the Automation system, head to helm/charts/automation-engine/<service>/templates/external-secret.yaml and make the appropriate changes under the data section. For example, consider the following external-secret.yaml : {{ - if and .Values.global.externalSecrets.enabled - }} apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : name : {{ include \"some-fancy-bridge.secretName\" . }} labels : {{ - include \"some-fancy-bridge.labels\" . | nindent 4 }} annotations : reloader.stakater.com/match : \"true\" spec : secretStoreRef : name : {{ .Values.global.environment }} -parameter-store kind : SecretStore target : creationPolicy : 'Owner' Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration) May be set to zero to fetch and create it {{ - if eq .Values.global.current_environment \"dev\" }} refreshInterval : \"0\" {{ else }} refreshInterval : \"5m\" {{ - end }} data : {{ - with .Values.global.externalSecrets.envPath }} - remoteRef : key : {{ .commonPath }} /some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : {{ .envPath }} /some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE {{ - end }} {{ - end }} If a new configuration called THIRD_PARTY_API_URL must be added to the underlying Secret created by this ExternalSecret , a new item should be placed under the data section, and it should look like this: - remoteRef : key : {{ . }} /some-fancy-bridge/third-party-api-url secretKey : THIRD_PARTY_API_URL The change can then be deployed to the Kubernetes cluster. A word of caution about ephemeral environments Although external-secrets is part of the EKS cluster for production and the cluster for ephemeral environments, the truth is it behaves differently across contexts. In the production cluster, ExternalSecret objects are configured to pull configurations from AWS Parameter Store every 5 minutes . That way, if a developer adds, removes, or updated a parameter in AWS, the cluster will realize about that event to update the regular Secret object created by the ExternalSecret with the most recent value. This will trigger another piece in the cluster called reloader , which ultimately will kill any pod that relies on the updated secret to spin up a new one with the configurations in the updated Secret . In ephemeral environments however, this is completely different. The ExternalSecret object will create a Secret object only when the ephemeral environment is deployed for the first time. After it has been deployed, control and management of Secret objects is delegated to developers; that is, external-secrets will never pull parameters from AWS again. The reasoning behind this behavior is that these parameters are shared by all ephemeral environments, so if one of them were to be updated in AWS and the polling rate was set to 5 minutes , all ephemeral environments would be updated because they all share the same reference. So essentially, the set of configurations for ephemeral environments that are stored in AWS are used as a template to populate ExternalSecret and Secret objects in ephemeral environments. If a secret needs to be updated, k9s should be used to edit the Secret in place. Using an AWS param key in our services Add the new variable to automation-engine/installation-utils/environment_files_generator.py SERVICE__DOMAIN__PARAM_KEY_NAME = parameters [ 'environment' ][ 'service' ][ 'domain' ][ 'param-key-name' ] And inside the env dictionary in the same file f 'DOMAIN__PARAM_KEY_NAME= { SERVICE__DOMAIN__PARAM_KEY_NAME } ' , Add the new variable as a template to services/<service>/src/config/.template.env DOMAIN__PARAM_KEY_NAME = Add the new variable retrieving the value from the environment to services/<service>/src/config/config.py 'param_key_name' : ( os . environ [ 'DOMAIN__PARAM_KEY_NAME' ] ), Any parameter that should be used with a primitive type other than str , should be cast here. We MUST use primitive types. DO NOT convert values to complex types like timedelta or datetime . Consumers of the service configuration should be responsible for doing that. Add the new variable to services/<service>/src/config/testconfig.py for being available when testing 'param_key_name' : 86400 After that you can use it in the service self . _config . MONITOR_CONFIG [ 'param_key_name' ] And while testing param_key_name = action . _config . MONITOR_CONFIG [ 'param_key_name' ]","title":"SYSTEM CONFIGURATIONS THROUGH AWS PARAM KEYS"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#context","text":"Configurations are stored in AWS Systems Manager Parameter Store . Parameter Store is a capability of AWS Systems Manager that allows storing configuration values in hierarchical structures. It also provides the ability to encrypt these values through AWS KMS keys, and allows keeping track of configuration changes / versions for auditing purposes. From a pricing perspective, parameters can be defined either as simple or advanced : - Simple . This kind of parameters come with no additional charges, and can hold up to 4KB of data. Parameter Store allows defining up to 10,000 simple parameters per account and region. - Advanced . AWS charges $0.05 per advanced parameter per month. These can hold up to 8KB of data, and Parameter Store allows defining up to 100,000 of them per account and region. Attending to their type, parameters can be defined as: * String . These parameters consist of a block of text. Nothing else. * StringList . These parameters hold comma-separated values, but bear in mind that this is just a convention. AWS will not turn the value into an array before returning it through Parameter Store API; that is, it will be kept as a string. * SecureString . Essentially, these are the same as String parameters, but empowered with encryption features thanks to KMS keys.","title":"Context"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#conventions","text":"Please, take some time to go through the following conventions before publishing new parameters to Parameter Store.","title":"Conventions"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#parameter-names","text":"Any new configuration published to Parameter Store must match the following pattern: /automation-engine/<environment>/<service-name>/<parameter> where: * environment refers to the Kubernetes context whose namespaces will take this parameter. Allowed values are dev , pro and common . * service-name refers to the name of the service this parameter will be loaded to. Examples of allowed values would be bruin-bridge , tnba-monitor , repair-tickets-monitor , and so on. * parameter refers to the name of the parameter that will be loaded to the service through an environment variable. Most configurations will follow the previous pattern, but if the business logic of a service carries out several tasks related to the same domain concept, an alternate form can be used: /automation-engine/<environment>/<domain>/<task>/<parameter> where: * domain refers to a domain concept that can represent multiple tasks accurately. An example would be service-affecting , which is a domain area that represents strongly related tasks (in this case, tasks related to Service Affecting troubles). * task refers to the underlying piece of logic that is independent of other tasks, but is still suitable to have under the subpath defined by domain . Considering that domain is set to service-affecting , acceptable values for task would be monitor , daily-bandwidth-report , or reoccurring-trouble-report . As a rule of thumb, choose the first pattern to name parameters if possible. However, if there is a strong reason to choose the second pattern over the first one, that is totally fine. If there are doubts about choosing one or the other, it can be brought up to discussion with the rest of the team.","title":"Parameter names"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#values","text":"Before getting into how to define new configurations, bear these considerations in mind to keep certain consistency and avoid confusion / potential errors: * Values representing a time duration must always be expressed in seconds. * 24 should not be used to represent the hours in a period of 1 day. * 86400 should be used to represent the hours in a period of 1 day. Values representing percentages must always be expressed in their integer form. 0.75 should not be used to represent the value 75% . 75 should be used to represent the value 75% . JSON-like values must adhere to the JSON specification . The most relevant considerations are: All keys in an object-like JSON must be strings, even if they represent numbers. Example: // NOT valid { 1 : \"foo\" , 2 : \"bar\" } // Valid { \"1\" : \"foo\" , \"2\" : \"bar\" } // Valid { \"foo\" : \"bar\" , \"baz\" : \"hey\" } 2. An array-like JSON can hold a combination of values with different types, be them integers, floats, boolean, arrays, objects, and so on. Example: [ 1 , \"foo\" , 0.75 , \"2\" , \"bar\" , true , [ \"hello\" , \"world\" ], { \"hello\" : \"world\" } ] Consider prettifying JSON-like configurations before publishing to Parameter Store. This enhances readability when dealing with huge JSONs. Developers are responsible for making any necessary transformations related to data types, time conversions, etc. in the config files of a particular service .","title":"Values"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#encryption","text":"A parameter must be encrypted if it holds: * Personally Identifiable Information. This includes e-mail addresses, names, surnames, phone numbers, and so on. * Authentication credentials of any kind. * URLs from third party services. * Information related to application domains, such as: * IDs (even incremental ones) * Organization names * Domain-related values that potentially expose internal details about third party systems","title":"Encryption"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#publishing-configurations-to-parameter-store","text":"Adding new configurations to Parameter Store is pretty straightforward. The process should be as easy as: 1. Access the AWS Management Console. 2. Go to the AWS Systems Manager Parameter Store dashboard. 3. Hit Create parameter . 4. Give the parameter a Name that complies with any of the patterns under the Parameter names section. 5. Make sure to add a meaningful Description to the parameter. This is extremely important to give context to anyone in need of making changes to parameters, so take some time to think about a good, meaningful description. 6. Choose the tier that fits better for this parameter: 1. If the parameter is small and is not expected to grow much as time passes, choose Standard . 2. On the other hand, if the parameter is large enough and is expected to grow, choose Advanced . 7. Choose the type that fits better for this parameter: 1. If the parameter is safe to stay unencrypted in AWS: 1. Choose String . 2. Set the Data Type field to text . 2. On the other hand, if the parameter needs to be encrypted (see the Encryption section): 1. Choose SecureString . 2. Set the KMS Key Source field to My current account to pick a KMS key registered in the current account. 3. Set the KMS Key ID field to alias/aws/ssm to pick the default KMS key for Parameter Store. In general, avoid using StringList parameters. These are special cases of the String type, which essentially means that String can be used to create parameters based on comma-separated values as well. 8. Give the parameter a value that adheres to the conventions specified in the Values section. 9. Finally, hit Create parameter to save it.","title":"Publishing configurations to Parameter Store"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#about-the-different-environments","text":"When creating a new parameter in the Parameter Store, please decide if it will be different on dev and pro or if it will be same in both environments. If they're different, you should create a parameter for each environment. Otherwise, just create one under common . In general, most parameters will share the same value, unless there is a strong reason to keep them different. For example, parameters used to point to third party systems may differ if their app has not only a Production system, but also a Development / Test one. In that case, the pro version of the parameter should aim at the third party's Production system, and the dev version should aim at their Development / Test system.","title":"About the different environments"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#hooking-aws-parameter-store-with-kubernetes-clusters","text":"","title":"Hooking AWS Parameter Store with Kubernetes clusters"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#pre-requisites","text":"Before moving on, make sure to install k9s in your system. k9s is a powerful terminal UI that lets users manage Kubernetes clusters from their own machines, and even edit Kubernetes objects in place to reflect those changes immediately. After installing k9s , follow these steps to install a plugin that allows editing decoded Secret objects (more in the next section): 1. Install krew , following the instructions for the desired OS. 2. Install kubectl 's plugin kubectl-modify-secret , following the instructions for the desired OS. 3. Integrate kubectl-modify-secret into k9s 's plugin system: 1. In a terminal, run k9s info to check which folder holds k9s configurations. 2. Create file plugin.yml under that folder, if it does not exist yet. 3. Add the following snippet: plugin : edit-secret : shortCut : Ctrl-X confirm : false description : \"Edit Decoded Secret\" scopes : - secrets command : kubectl background : false args : - modify-secret - --namespace - $NAMESPACE - --context - $CONTEXT - $NAME > By default, the shortcut to edit decoded secrets is set to Ctrl + X . If needed, set a custom one instead.","title":"Pre-requisites"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#kubernetes-secrets-and-configmaps","text":"Secret and ConfigMap objects are the Kubernetes objects used to inject environment variables to one or multiple pods. Like other Kubernetes objects, they are defined through .yaml files. These objects hold mappings between environment variables and their values, along with some metadata. Here is an example of a ConfigMap definition: apiVersion : v1 data : CURRENT_ENVIRONMENT : production ENVIRONMENT_NAME : production REDIS_HOSTNAME : redis.pro.somewhere.com kind : ConfigMap metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2021-08-30T15:08:12Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : case-of-use project : mettel-automation name : some-fancy-bridge-configmap namespace : automation-engine resourceVersion : \"83171937\" selfLink : /api/v1/namespaces/automation-engine/configmaps/some-fancy-bridge-configmap uid : ba625451-8ba4-4ac7-a8da-593cc938eae7 And here is an example of a Secret definition: apiVersion : v1 data : THIRD_PARTY_API_USERNAME : aGVsbG8K THIRD_PARTY_API_PASSWORD : d29ybGQK kind : Secret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reconcile.external-secrets.io/data-hash : 3162fd065a6777587e9a3a604e0c56e2 reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:31:36Z\" labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine ownerReferences : - apiVersion : external-secrets.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ExternalSecret name : some-fancy-bridge-secret uid : 42bdad1c-37c4-4890-b86b-d9623333df18 resourceVersion : \"82588568\" selfLink : /api/v1/namespaces/automation-engine/secrets/some-fancy-bridge-secret uid : 81ad3356-8696-406e-bba0-c4ec389676ee type : Opaque Both objects have a data field where the different configurations are stored. The main difference between both objects is that all fields under data remain clear in ConfigMap objects, but in Secret ones, these fields are base64-encoded. These objects lack of mechanisms to pull configurations from external sources, so an additional tool is needed to hook them with AWS Parameter Store.","title":"Kubernetes: Secrets and ConfigMaps"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#external-secrets","text":"The (External Secrets Operator)[https://github.com/external-secrets/external-secrets] is a tool that allows setting up Secret objects based on external references through a new kind of object called ExternalSecret . ExternalSecret objects define the external source to pull configurations from, and the references that should be resolved. After these references have been resolved, the ExternalSecret will create a regular Secret object with a data section whose key-value pairs are based on the environment variables the pod expects to see, and the values gotten after resolving the references from the external source. Aside from that, ExternalSecret objects pull configuration values from the external source periodically to keep secrets up to date, also known as reconciling secrets . An example of ExternalSecret object would be this one: apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : annotations : meta.helm.sh/release-name : automation-engine meta.helm.sh/release-namespace : automation-engine reloader.stakater.com/match : \"true\" creationTimestamp : \"2022-03-08T18:11:02Z\" generation : 1 labels : app.kubernetes.io/instance : automation-engine app.kubernetes.io/managed-by : Helm app.kubernetes.io/name : some-fancy-bridge app.kubernetes.io/version : 1.16.0 component : some-fancy-bridge current-environment : production environment-name : production helm.sh/chart : some-fancy-bridge-0.1.0 microservice-type : capability project : mettel-automation name : some-fancy-bridge-secret namespace : automation-engine resourceVersion : \"83201847\" selfLink : /apis/external-secrets.io/v1alpha1/namespaces/automation-engine/externalsecrets/some-fancy-bridge-secret uid : ca5c1faf-1b76-4f59-b57f-e10e43154ace spec : data : - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : /automation-engine/pro/some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE status : conditions : - lastTransitionTime : \"2022-03-08T18:11:10Z\" message : Secret was synced reason : SecretSynced status : \"True\" type : Ready refreshTime : \"2022-03-10T13:29:12Z\" syncedResourceVersion : 1-3efb62db37d8b935be922ecc6f7ed99f In this example, there are two items under the data section. Each item refers to an external / remote reference (field remoteRef::key ), and the environment variable that the value behind that remote reference should be loaded to (field secretKey ).","title":"External secrets"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#manipulating-external-secrets-in-the-automation-system","text":"To add, remove, or update external secrets for a particular service in the Automation system, head to helm/charts/automation-engine/<service>/templates/external-secret.yaml and make the appropriate changes under the data section. For example, consider the following external-secret.yaml : {{ - if and .Values.global.externalSecrets.enabled - }} apiVersion : external-secrets.io/v1alpha1 kind : ExternalSecret metadata : name : {{ include \"some-fancy-bridge.secretName\" . }} labels : {{ - include \"some-fancy-bridge.labels\" . | nindent 4 }} annotations : reloader.stakater.com/match : \"true\" spec : secretStoreRef : name : {{ .Values.global.environment }} -parameter-store kind : SecretStore target : creationPolicy : 'Owner' Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration) May be set to zero to fetch and create it {{ - if eq .Values.global.current_environment \"dev\" }} refreshInterval : \"0\" {{ else }} refreshInterval : \"5m\" {{ - end }} data : {{ - with .Values.global.externalSecrets.envPath }} - remoteRef : key : {{ .commonPath }} /some-fancy-bridge/some-common-value secretKey : SOME_COMMON_VALUE - remoteRef : key : {{ .envPath }} /some-fancy-bridge/some-env-specific-value secretKey : SOME_ENV_SPECIFIC_VALUE {{ - end }} {{ - end }} If a new configuration called THIRD_PARTY_API_URL must be added to the underlying Secret created by this ExternalSecret , a new item should be placed under the data section, and it should look like this: - remoteRef : key : {{ . }} /some-fancy-bridge/third-party-api-url secretKey : THIRD_PARTY_API_URL The change can then be deployed to the Kubernetes cluster.","title":"Manipulating external secrets in the Automation system"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#a-word-of-caution-about-ephemeral-environments","text":"Although external-secrets is part of the EKS cluster for production and the cluster for ephemeral environments, the truth is it behaves differently across contexts. In the production cluster, ExternalSecret objects are configured to pull configurations from AWS Parameter Store every 5 minutes . That way, if a developer adds, removes, or updated a parameter in AWS, the cluster will realize about that event to update the regular Secret object created by the ExternalSecret with the most recent value. This will trigger another piece in the cluster called reloader , which ultimately will kill any pod that relies on the updated secret to spin up a new one with the configurations in the updated Secret . In ephemeral environments however, this is completely different. The ExternalSecret object will create a Secret object only when the ephemeral environment is deployed for the first time. After it has been deployed, control and management of Secret objects is delegated to developers; that is, external-secrets will never pull parameters from AWS again. The reasoning behind this behavior is that these parameters are shared by all ephemeral environments, so if one of them were to be updated in AWS and the polling rate was set to 5 minutes , all ephemeral environments would be updated because they all share the same reference. So essentially, the set of configurations for ephemeral environments that are stored in AWS are used as a template to populate ExternalSecret and Secret objects in ephemeral environments. If a secret needs to be updated, k9s should be used to edit the Secret in place.","title":"A word of caution about ephemeral environments"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#using-an-aws-param-key-in-our-services","text":"Add the new variable to automation-engine/installation-utils/environment_files_generator.py SERVICE__DOMAIN__PARAM_KEY_NAME = parameters [ 'environment' ][ 'service' ][ 'domain' ][ 'param-key-name' ] And inside the env dictionary in the same file f 'DOMAIN__PARAM_KEY_NAME= { SERVICE__DOMAIN__PARAM_KEY_NAME } ' , Add the new variable as a template to services/<service>/src/config/.template.env DOMAIN__PARAM_KEY_NAME = Add the new variable retrieving the value from the environment to services/<service>/src/config/config.py 'param_key_name' : ( os . environ [ 'DOMAIN__PARAM_KEY_NAME' ] ), Any parameter that should be used with a primitive type other than str , should be cast here. We MUST use primitive types. DO NOT convert values to complex types like timedelta or datetime . Consumers of the service configuration should be responsible for doing that. Add the new variable to services/<service>/src/config/testconfig.py for being available when testing 'param_key_name' : 86400 After that you can use it in the service self . _config . MONITOR_CONFIG [ 'param_key_name' ] And while testing param_key_name = action . _config . MONITOR_CONFIG [ 'param_key_name' ]","title":"Using an AWS param key in our services"},{"location":"SYSTEM_OVERVIEW/","text":"System overview Architecture Basic concepts It's a project based on microservices, in which two types are distinguished: Capabilities : They are in charge of carrying out certain common actions for the business logic. I.e.: Collect information from SD-WAN routers For example: Collect information from SD-WAN routers. Use cases : They use capabilities as a base to make specific use cases. I.e.: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for the subsequent storage in the corresponding tickets. For example: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for subsequent storage in the corresponding tickets. It is important to emphasize on the architecture of the system the use of NATS in it. Messaging system It's important to emphasize in the architecture of the system the use of NATS in it. NATS is a simple, secure and performant communications system for digital system, services and devices NATS is used in the microservices system as a communication center for all of them. NATS it's used in cluster mode to safistify more work to be done by it due to the high number of events in the system to be processed Microservices communications There are two types of microservices depending on the connection between them and NATS: Microservices that communicates with NATS , divided into three types: Those that take the role of replier in the context of NATS, usually microservices that contains capabilities : bruin-bridge cts-bridge hawkeye-bridge lit-bridge notifier t7-bridge Those that take the role of requester in the context of NATS, usually microservices that contains use cases : dispatch-portal-backend grafana component, from metrics-prometheus microservice hawkeye-affecting-monitor hawkeye-outage-monitor last-contact-report service-affecting-monitor service-dispatch-monitor service-outage-monitor tnba-feedback tnba-monitor Those that take the role of both requester and replier in the context of NATS. These microservices can be considered a mixture between use use cases and capabilities : customer-cache hawkeye-customer-cache It's important take into account that all microservices that communicate with NATS can also communicate with the Redis Cluster. This is needed to bypass the limit size that NATS enforces for all messages it receives (1MB). Microservices that doesn't communicate with NATS : dispatch-portal-frontend lumin-billing-report prometheus and thanos components, from metrics-prometheus microservice redis cluster (Docker container in local environment or an Elasticache Redis Cluster in AWS environments) NATS is used in the microservice system as a communication center for all of them. It is used in cluster mode to satisfy more work to be done by it. In the following diagram it's possible see a graph with the relationships between the microservices explained previously in this section Relationships between microservices The services that are part of the previously explained architecture are related to each other, in the following diagram it's possible see the relationships between them. Capabilities microservices Bruin-bridge microservice This microservice is in charge of making requests to the bruin API, taking the role of replier in the context of NATS. When another microservice requests bruin data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. Bruin is a third-party system that allows creating and managing support tickets to deal with issues that appear in network devices, among other types of devices. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Cts-bridge microservice This microservice is in charge of making requests to the CTS API, taking the role of replier in the context of NATS. When another microservice requests CTS data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Digi-bridge microservice This microservice is in charge of making requests to the Digi Reboot API, taking the role of replier in the context of NATS. When another microservice asks to reboot a SD-WAN device, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Hawkeye-bridge microservice This microservice is in charge of making requests to the Hawkeye API, taking the role of replier in the context of NATS. When another microservice requests Hawkeye data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Lit-bridge microservice This microservice is in charge of making requests to the LIT API, taking the role of replier in the context of NATS. When another microservice requests LIT data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Notifier microservice This microservice is in charge of sending emails, Slack notifications and SMS. It is important to point out that it is not in charge of the composition of the messages to be sent, that is to say, of their content, but only of sending them. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. T7-bridge microservice The function of this microservice is to embed in the notes of a ticket the prediction calculated by T7, this prediction will store information on the recommendations actions for the ticket. In order to carry out the mentioned actions, it communicates with the API of T7 to obtain the information about the prediction, as it can be seen in the following diagram . Velocloud-bridge microservice This microservice is in charge of making requests to the velocloud API, taking the role of replier in the context of NATS. When another microservice requests velocloud data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Use cases microservices Dispatch-portal-backend microservice In conjunction with dispatch-portal-frontend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It also updates Bruin tickets to keep support people posted about the changes in the dispatch requests. It acts as an intermediary between dispatch-portal-frontend and CTS & LIT APIs by providing a REST API with multiple endpoints that, once they receive a payload from the frontend side, it modifies its fields with the help of some mappers to match the formats expected by CTS and LIT and then forward those customized payloads to their APIs. The following diagram shows the dependencies or interactions of this microservice with the others. Grafana microservice Although Grafana is a visualization tool for metrics, it needs to fetch some data from VeloCloud API to build dashboards for customer Titan America. The following diagram shows the dependencies or interactions of this microservice with the others. Hawkeye-outage-monitor microservice This service is responsible for resolving/unresolving outage tickets depending on the state of a Hawkeye device. It is triggered every 3 minutes. If a device is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the device is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the device was detected to be healthy, the system looks for an open outage ticket for this device and resolves it in case it exists. In the following diagram it's possible see the relationship of this microservice with the others. Last-contact-report microservice The function to be performed by this microservice is to send a monthly report with information about routers that were last contacted more than 30 days ago. The following flow is used to make this report: The last-contact-report microservice communicates with the velocloud-bridge microservice to obtain events from an edge. Once the events are obtained from an edge, it communicates with the notifier microservice to send an email with this information. It is possible to see the relations between the mentioned services for the flow in the following diagram . Service-affecting-monitor microservice In this microservice are defined a series of scales and thresholds, the function of this will be to check if there is loss of packages, latencies or jitter measurements that exceed the thresholds defined. In case the thresholds are exceeded, it will communicate with the notifier service to send a notification by email and slack, by means of which it will warn of the problems detected on a specific edge. This microservice also communicates with the bruin-bridge microservice to create tickets or add notes to an existing one, including in this information about the routers for which a problem is detected. In the following diagram it's possible see the relationships between this microservice and the others. Service-dispatch-monitor microservice This microservice monitor dispatches statuses for different vendors, at the time of writing this document LIT and CTS. Both processes are pretty much the same in concept but with differences in the implementation. A dispatch is general terms can have the following statuses: Requested Confirmed Tech on site Canceled Completed The main use is to monitor: Dispatch status changed Updates in the dispatch like the technician Send sms prior 2 and 12 hours before Send sms tech on site Cancel dispatch The basic algorithm behaves like this: Get all dispatches for a vendor Filter dispatches that are created through the dispatch-portal Discard invalid ticket ids or dispatches with not proper fields Split the dispatches by status and then send them to the function to process them, there are 3 general functions Confirmed dispatch: Send sms and append note to bruin when a dispatch is confirmed Send sms and append note to bruin 12 or 2 hours prior the dispatch Send sms and append note to bruin when a tech has changed Tech on site dispatch: Send sms and append note to bruin when tech on site Canceled dispatch: Append note to bruin when a dispatch is canceled Each vendor has it's own details like how to retrieve some fields or how we identify the tickets with the dispatches, all explained in the service-dispatch-monitor . In the following diagram it's possible see the relationships between this microservice and the others. Service-outage-monitor microservice This microservice orchestrates the execution of two different processes: Outage monitoring. This process is responsible for resolving/unresolving outage tickets depending on the state of an edge. It is triggered every 3 minutes. If an edge is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the edge is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the edge was detected to be healthy, the system looks for an open outage ticket for this edge and resolves it in case it exists. Triage. This process is aimed at updating Bruin tickets with information related to recent edge events. It is triggered every 10 minutes. At the beginning, the process gathers all the open tickets related with the companies that are under triage monitoring. Tickets not related with edges belonging to these companies are discarded before going on. The process starts dealing with every ticket in the set collected in the previous step: * If the outage ticket does not have any triage note from a previous execution of the triage process then a triage note is appended with information of the events related to the edge corresponding to this ticket. Events correspond to the period between 7 days ago and the current moment. If the current environment is DEV instead of PRODUCTION then no note is appended to the ticket; instead, a notification with a summary of the triage results is delivered to a Slack channel. If the outage ticket already has a triage note from a previous execution then the process attempts to append new triage notes to the ticket but only if the last triage note was not appended recently (30 minutes or less ago). In case there's no recent triage note, edge events from the period between the creation date of the last triage note and the current moment are claimed to Velocloud and then they are included in the triage notes, which are finally appended to the ticket. Note that due to Bruin limitations it is not feasible to have a triage note with 1500 characters or more; that is the reason why several triage notes are appended to the ticket (instead of just appending one). In the following diagram it's possible see the relationship of this microservice with the others. TNBA-feedback microservice This microservice is in charge of collecting closed tickets that had a TNBA note appended by tnba-monitor at some point. After collecting them, they are sent to t7-bridge to retrain predictive models and hence improve the accuracy of predictions claimed by tnba-monitor . The following diagram shows the relationship between this microservice and the others. TNBA-monitor microservice This microservice is in charge of appending notes to Bruin tickets indicating what is T he N ext B est A ction a member of the support team of Bruin can take to move forward on the resolution of the ticket. It mostly communicates with bruin-bridge and t7-bridge to embed predictions into tickets, but it also communicates with other capabilities as shown in the following diagram . The following diagram shows the relationship between this microservice and the others. Special microservices (NATS Requester and Replier) Customer-cache microservice This microservice is in charge of crossing Bruin and Velocloud data. More specifically, it focus on associating Bruin customers with Velocloud edges. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of customer-cache , it plays the role of a requester as it asks for data to Velocloud and Bruin to cross it. Hawkeye-customer-cache microservice This microservice is in charge of crossing Bruin and Hawkeye data. More specifically, it focus on associating Bruin customers with Hawkeye devices. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of hawkeye-customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of hawkeye-customer-cache , it plays the role of a requester as it asks for data to Hawkeye and Bruin to cross it. Microservices that don't communicate with NATS dispatch-portal-frontend In conjunction with dispatch-portal-backend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It exposes a UI that communicates directly with a REST API in dispatch-portal-backend to handle the visualization, creation and update of dispatch requests. The following diagram shows the relationship between this microservice and dispatch-portal-backend . lumin-billing-report This service automates requesting billing information for a given customer from the Lumin.AI service provider, generating a summary HTML email and attaching a csv with all data for the current billing period. This service is self-contained, i.e., it does not require access to NATS or Redis, or any other microservice within the Automation Engine. The following diagram shows the relationship between this service and the third-party services it uses. Prometheus & Thanos The purpose of Prometheus is to scrape metrics from HTTP servers placed in those services with the ability to write metrics, nothing else. Thanos is just another component that adds a layer of persistence to Prometheus, thus allowing to save metrics before they are lost when a service is re-deployed. These metrics can be restored after the deployment completes. Metrics are usually displayed in a Grafana instance with a few custom dashboards. The following diagram shows the relationship between Prometheus, the metrics servers it scrapes metrics, and Grafana. Redis Redis is an in-memory key-value store that, in this system, is used mostly for caching purposes, and also as a temporary storage for messages larger than 1 MB, which NATS cannot handle by itself. There are three Redis instances: * redis . Used to store NATS messages larger than 1 MB temporarily. All microservices that communicate with NATS in some way have the ability to store and retrieve messages from this Redis instance. redis-customer-cache . Used to turn customer-cache and hawkeye-customer-cache into fault-tolerant services, so if any of them fail caches will still be available to serve as soon as they come back. redis-tnba-feedback . Used to collect huge amounts of Bruin tickets' task histories before they are sent to T7 by the tnba-feedback service. Technologies and tools Code repository Intelygenz's Gitlab is used to store the project's code Gitlab CI is used as the CI/CD tool for the project Containerization The following containerization tools are used: Docker is used to create o container of this type by microservice > In the folder of each microservice there is a Dockerfile that allows to execute that microservice as a container Docker-compose is used for defining and running project microservices as a multi-container Docker application: > At the root of the repository there is a docker-compose.yml file that allows to run one or more microservices as docker containers Infrastructure Microservices Infrastructure For the microservices ECS is used to deploy a container for each microservice for all environments deployed, as each one has its own repository in the ECR registry used in the project. In the following diagram it's possible see how the microservices of the project are deployed, using the different images available in the registry created for the project in ECR. KRE Infrastructure In this project KRE is used, it has been deployed in an Kubernetes cluster using EKS for each of the necessary environments , as well as all the parts needed for this in AWS. In the following diagram it's possible see how is configured the KRE infrastructure in the project. Network infrastructure For the infrastructure of the network resources there is a distinction according to the microservice environments and also the kre-environmetns to deploy belongs to dev or production . In the following diagram it's possible see the infrastructure relative to the existing network resources in AWS created for the two type of environments. When deploying an environment it will use the resources belonging to the environment type. This approach has been implemented so that regardless of the number of ECS clusters being used, the same public IPs are always used to make requests outward from the different environments. KRE's clusters will also use the VPCs corresponding to each environment, i.e., dev or production .","title":"System overview"},{"location":"SYSTEM_OVERVIEW/#system-overview","text":"","title":"System overview"},{"location":"SYSTEM_OVERVIEW/#architecture","text":"","title":"Architecture"},{"location":"SYSTEM_OVERVIEW/#basic-concepts","text":"It's a project based on microservices, in which two types are distinguished: Capabilities : They are in charge of carrying out certain common actions for the business logic. I.e.: Collect information from SD-WAN routers For example: Collect information from SD-WAN routers. Use cases : They use capabilities as a base to make specific use cases. I.e.: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for the subsequent storage in the corresponding tickets. For example: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for subsequent storage in the corresponding tickets. It is important to emphasize on the architecture of the system the use of NATS in it.","title":"Basic concepts"},{"location":"SYSTEM_OVERVIEW/#messaging-system","text":"It's important to emphasize in the architecture of the system the use of NATS in it. NATS is a simple, secure and performant communications system for digital system, services and devices NATS is used in the microservices system as a communication center for all of them. NATS it's used in cluster mode to safistify more work to be done by it due to the high number of events in the system to be processed","title":"Messaging system"},{"location":"SYSTEM_OVERVIEW/#microservices-communications","text":"There are two types of microservices depending on the connection between them and NATS: Microservices that communicates with NATS , divided into three types: Those that take the role of replier in the context of NATS, usually microservices that contains capabilities : bruin-bridge cts-bridge hawkeye-bridge lit-bridge notifier t7-bridge Those that take the role of requester in the context of NATS, usually microservices that contains use cases : dispatch-portal-backend grafana component, from metrics-prometheus microservice hawkeye-affecting-monitor hawkeye-outage-monitor last-contact-report service-affecting-monitor service-dispatch-monitor service-outage-monitor tnba-feedback tnba-monitor Those that take the role of both requester and replier in the context of NATS. These microservices can be considered a mixture between use use cases and capabilities : customer-cache hawkeye-customer-cache It's important take into account that all microservices that communicate with NATS can also communicate with the Redis Cluster. This is needed to bypass the limit size that NATS enforces for all messages it receives (1MB). Microservices that doesn't communicate with NATS : dispatch-portal-frontend lumin-billing-report prometheus and thanos components, from metrics-prometheus microservice redis cluster (Docker container in local environment or an Elasticache Redis Cluster in AWS environments) NATS is used in the microservice system as a communication center for all of them. It is used in cluster mode to satisfy more work to be done by it. In the following diagram it's possible see a graph with the relationships between the microservices explained previously in this section","title":"Microservices communications"},{"location":"SYSTEM_OVERVIEW/#relationships-between-microservices","text":"The services that are part of the previously explained architecture are related to each other, in the following diagram it's possible see the relationships between them.","title":"Relationships between microservices"},{"location":"SYSTEM_OVERVIEW/#capabilities-microservices","text":"","title":"Capabilities microservices"},{"location":"SYSTEM_OVERVIEW/#bruin-bridge-microservice","text":"This microservice is in charge of making requests to the bruin API, taking the role of replier in the context of NATS. When another microservice requests bruin data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. Bruin is a third-party system that allows creating and managing support tickets to deal with issues that appear in network devices, among other types of devices. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Bruin-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#cts-bridge-microservice","text":"This microservice is in charge of making requests to the CTS API, taking the role of replier in the context of NATS. When another microservice requests CTS data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Cts-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#digi-bridge-microservice","text":"This microservice is in charge of making requests to the Digi Reboot API, taking the role of replier in the context of NATS. When another microservice asks to reboot a SD-WAN device, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Digi-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-bridge-microservice","text":"This microservice is in charge of making requests to the Hawkeye API, taking the role of replier in the context of NATS. When another microservice requests Hawkeye data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Hawkeye-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#lit-bridge-microservice","text":"This microservice is in charge of making requests to the LIT API, taking the role of replier in the context of NATS. When another microservice requests LIT data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Lit-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#notifier-microservice","text":"This microservice is in charge of sending emails, Slack notifications and SMS. It is important to point out that it is not in charge of the composition of the messages to be sent, that is to say, of their content, but only of sending them. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Notifier microservice"},{"location":"SYSTEM_OVERVIEW/#t7-bridge-microservice","text":"The function of this microservice is to embed in the notes of a ticket the prediction calculated by T7, this prediction will store information on the recommendations actions for the ticket. In order to carry out the mentioned actions, it communicates with the API of T7 to obtain the information about the prediction, as it can be seen in the following diagram .","title":"T7-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#velocloud-bridge-microservice","text":"This microservice is in charge of making requests to the velocloud API, taking the role of replier in the context of NATS. When another microservice requests velocloud data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Velocloud-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#use-cases-microservices","text":"","title":"Use cases microservices"},{"location":"SYSTEM_OVERVIEW/#dispatch-portal-backend-microservice","text":"In conjunction with dispatch-portal-frontend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It also updates Bruin tickets to keep support people posted about the changes in the dispatch requests. It acts as an intermediary between dispatch-portal-frontend and CTS & LIT APIs by providing a REST API with multiple endpoints that, once they receive a payload from the frontend side, it modifies its fields with the help of some mappers to match the formats expected by CTS and LIT and then forward those customized payloads to their APIs. The following diagram shows the dependencies or interactions of this microservice with the others.","title":"Dispatch-portal-backend microservice"},{"location":"SYSTEM_OVERVIEW/#grafana-microservice","text":"Although Grafana is a visualization tool for metrics, it needs to fetch some data from VeloCloud API to build dashboards for customer Titan America. The following diagram shows the dependencies or interactions of this microservice with the others.","title":"Grafana microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-outage-monitor-microservice","text":"This service is responsible for resolving/unresolving outage tickets depending on the state of a Hawkeye device. It is triggered every 3 minutes. If a device is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the device is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the device was detected to be healthy, the system looks for an open outage ticket for this device and resolves it in case it exists. In the following diagram it's possible see the relationship of this microservice with the others.","title":"Hawkeye-outage-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#last-contact-report-microservice","text":"The function to be performed by this microservice is to send a monthly report with information about routers that were last contacted more than 30 days ago. The following flow is used to make this report: The last-contact-report microservice communicates with the velocloud-bridge microservice to obtain events from an edge. Once the events are obtained from an edge, it communicates with the notifier microservice to send an email with this information. It is possible to see the relations between the mentioned services for the flow in the following diagram .","title":"Last-contact-report microservice"},{"location":"SYSTEM_OVERVIEW/#service-affecting-monitor-microservice","text":"In this microservice are defined a series of scales and thresholds, the function of this will be to check if there is loss of packages, latencies or jitter measurements that exceed the thresholds defined. In case the thresholds are exceeded, it will communicate with the notifier service to send a notification by email and slack, by means of which it will warn of the problems detected on a specific edge. This microservice also communicates with the bruin-bridge microservice to create tickets or add notes to an existing one, including in this information about the routers for which a problem is detected. In the following diagram it's possible see the relationships between this microservice and the others.","title":"Service-affecting-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#service-dispatch-monitor-microservice","text":"This microservice monitor dispatches statuses for different vendors, at the time of writing this document LIT and CTS. Both processes are pretty much the same in concept but with differences in the implementation. A dispatch is general terms can have the following statuses: Requested Confirmed Tech on site Canceled Completed The main use is to monitor: Dispatch status changed Updates in the dispatch like the technician Send sms prior 2 and 12 hours before Send sms tech on site Cancel dispatch The basic algorithm behaves like this: Get all dispatches for a vendor Filter dispatches that are created through the dispatch-portal Discard invalid ticket ids or dispatches with not proper fields Split the dispatches by status and then send them to the function to process them, there are 3 general functions Confirmed dispatch: Send sms and append note to bruin when a dispatch is confirmed Send sms and append note to bruin 12 or 2 hours prior the dispatch Send sms and append note to bruin when a tech has changed Tech on site dispatch: Send sms and append note to bruin when tech on site Canceled dispatch: Append note to bruin when a dispatch is canceled Each vendor has it's own details like how to retrieve some fields or how we identify the tickets with the dispatches, all explained in the service-dispatch-monitor . In the following diagram it's possible see the relationships between this microservice and the others.","title":"Service-dispatch-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#service-outage-monitor-microservice","text":"This microservice orchestrates the execution of two different processes: Outage monitoring. This process is responsible for resolving/unresolving outage tickets depending on the state of an edge. It is triggered every 3 minutes. If an edge is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the edge is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the edge was detected to be healthy, the system looks for an open outage ticket for this edge and resolves it in case it exists. Triage. This process is aimed at updating Bruin tickets with information related to recent edge events. It is triggered every 10 minutes. At the beginning, the process gathers all the open tickets related with the companies that are under triage monitoring. Tickets not related with edges belonging to these companies are discarded before going on. The process starts dealing with every ticket in the set collected in the previous step: * If the outage ticket does not have any triage note from a previous execution of the triage process then a triage note is appended with information of the events related to the edge corresponding to this ticket. Events correspond to the period between 7 days ago and the current moment. If the current environment is DEV instead of PRODUCTION then no note is appended to the ticket; instead, a notification with a summary of the triage results is delivered to a Slack channel. If the outage ticket already has a triage note from a previous execution then the process attempts to append new triage notes to the ticket but only if the last triage note was not appended recently (30 minutes or less ago). In case there's no recent triage note, edge events from the period between the creation date of the last triage note and the current moment are claimed to Velocloud and then they are included in the triage notes, which are finally appended to the ticket. Note that due to Bruin limitations it is not feasible to have a triage note with 1500 characters or more; that is the reason why several triage notes are appended to the ticket (instead of just appending one). In the following diagram it's possible see the relationship of this microservice with the others.","title":"Service-outage-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#tnba-feedback-microservice","text":"This microservice is in charge of collecting closed tickets that had a TNBA note appended by tnba-monitor at some point. After collecting them, they are sent to t7-bridge to retrain predictive models and hence improve the accuracy of predictions claimed by tnba-monitor . The following diagram shows the relationship between this microservice and the others.","title":"TNBA-feedback microservice"},{"location":"SYSTEM_OVERVIEW/#tnba-monitor-microservice","text":"This microservice is in charge of appending notes to Bruin tickets indicating what is T he N ext B est A ction a member of the support team of Bruin can take to move forward on the resolution of the ticket. It mostly communicates with bruin-bridge and t7-bridge to embed predictions into tickets, but it also communicates with other capabilities as shown in the following diagram . The following diagram shows the relationship between this microservice and the others.","title":"TNBA-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#special-microservices-nats-requester-and-replier","text":"","title":"Special microservices (NATS Requester and Replier)"},{"location":"SYSTEM_OVERVIEW/#customer-cache-microservice","text":"This microservice is in charge of crossing Bruin and Velocloud data. More specifically, it focus on associating Bruin customers with Velocloud edges. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of customer-cache , it plays the role of a requester as it asks for data to Velocloud and Bruin to cross it.","title":"Customer-cache microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-customer-cache-microservice","text":"This microservice is in charge of crossing Bruin and Hawkeye data. More specifically, it focus on associating Bruin customers with Hawkeye devices. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of hawkeye-customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of hawkeye-customer-cache , it plays the role of a requester as it asks for data to Hawkeye and Bruin to cross it.","title":"Hawkeye-customer-cache microservice"},{"location":"SYSTEM_OVERVIEW/#microservices-that-dont-communicate-with-nats","text":"","title":"Microservices that don't communicate with NATS"},{"location":"SYSTEM_OVERVIEW/#dispatch-portal-frontend","text":"In conjunction with dispatch-portal-backend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It exposes a UI that communicates directly with a REST API in dispatch-portal-backend to handle the visualization, creation and update of dispatch requests. The following diagram shows the relationship between this microservice and dispatch-portal-backend .","title":"dispatch-portal-frontend"},{"location":"SYSTEM_OVERVIEW/#lumin-billing-report","text":"This service automates requesting billing information for a given customer from the Lumin.AI service provider, generating a summary HTML email and attaching a csv with all data for the current billing period. This service is self-contained, i.e., it does not require access to NATS or Redis, or any other microservice within the Automation Engine. The following diagram shows the relationship between this service and the third-party services it uses.","title":"lumin-billing-report"},{"location":"SYSTEM_OVERVIEW/#prometheus-thanos","text":"The purpose of Prometheus is to scrape metrics from HTTP servers placed in those services with the ability to write metrics, nothing else. Thanos is just another component that adds a layer of persistence to Prometheus, thus allowing to save metrics before they are lost when a service is re-deployed. These metrics can be restored after the deployment completes. Metrics are usually displayed in a Grafana instance with a few custom dashboards. The following diagram shows the relationship between Prometheus, the metrics servers it scrapes metrics, and Grafana.","title":"Prometheus &amp; Thanos"},{"location":"SYSTEM_OVERVIEW/#redis","text":"Redis is an in-memory key-value store that, in this system, is used mostly for caching purposes, and also as a temporary storage for messages larger than 1 MB, which NATS cannot handle by itself. There are three Redis instances: * redis . Used to store NATS messages larger than 1 MB temporarily. All microservices that communicate with NATS in some way have the ability to store and retrieve messages from this Redis instance. redis-customer-cache . Used to turn customer-cache and hawkeye-customer-cache into fault-tolerant services, so if any of them fail caches will still be available to serve as soon as they come back. redis-tnba-feedback . Used to collect huge amounts of Bruin tickets' task histories before they are sent to T7 by the tnba-feedback service.","title":"Redis"},{"location":"SYSTEM_OVERVIEW/#technologies-and-tools","text":"","title":"Technologies and tools"},{"location":"SYSTEM_OVERVIEW/#code-repository","text":"Intelygenz's Gitlab is used to store the project's code Gitlab CI is used as the CI/CD tool for the project","title":"Code repository"},{"location":"SYSTEM_OVERVIEW/#containerization","text":"The following containerization tools are used: Docker is used to create o container of this type by microservice > In the folder of each microservice there is a Dockerfile that allows to execute that microservice as a container Docker-compose is used for defining and running project microservices as a multi-container Docker application: > At the root of the repository there is a docker-compose.yml file that allows to run one or more microservices as docker containers","title":"Containerization"},{"location":"SYSTEM_OVERVIEW/#infrastructure","text":"","title":"Infrastructure"},{"location":"SYSTEM_OVERVIEW/#microservices-infrastructure","text":"For the microservices ECS is used to deploy a container for each microservice for all environments deployed, as each one has its own repository in the ECR registry used in the project. In the following diagram it's possible see how the microservices of the project are deployed, using the different images available in the registry created for the project in ECR.","title":"Microservices Infrastructure"},{"location":"SYSTEM_OVERVIEW/#kre-infrastructure","text":"In this project KRE is used, it has been deployed in an Kubernetes cluster using EKS for each of the necessary environments , as well as all the parts needed for this in AWS. In the following diagram it's possible see how is configured the KRE infrastructure in the project.","title":"KRE Infrastructure"},{"location":"SYSTEM_OVERVIEW/#network-infrastructure","text":"For the infrastructure of the network resources there is a distinction according to the microservice environments and also the kre-environmetns to deploy belongs to dev or production . In the following diagram it's possible see the infrastructure relative to the existing network resources in AWS created for the two type of environments. When deploying an environment it will use the resources belonging to the environment type. This approach has been implemented so that regardless of the number of ECS clusters being used, the same public IPs are always used to make requests outward from the different environments. KRE's clusters will also use the VPCs corresponding to each environment, i.e., dev or production .","title":"Network infrastructure"},{"location":"WELCOME_PACK/","text":"Welcome Pack Team Composition Julia - Manager Dani - Tech Lead \u00c1ngel - Devops Brandon - Developer David - Developer Sergio - Developer Javier - Developer First steps Please request access to all the things listed below: Project repository https://gitlab.intelygenz.com/mettel/automation-engine/ , docker repository https://gitlab.intelygenz.com/mettel/docker_images/-/tree/master and One password to itcrowd@intelygenz.com through their ticketing system https://docs.google.com/document/d/1YLYdI9Dyq8tNlNy2iJ29InquKDz8r_Dw4XBsxI7pPiM/edit and CC your manager to allow the request Configure vpn https://docs.google.com/document/d/16_LFpkiBWN0mbfjAoqR4BaEB5kPNsuNHrUS7PtrWnEA/edit#heading=h.to49i8wu1vn3 AWS account creation to our devops Jira board to our manager Mettel's Slack channels to our tech lead Project overview Resources Check docs folder inside the mettel gitlab project and read carefully the readme for installing all the required programs and configure the project. After RRHH's on-boarding our team lead will give an overview of the project https://docs.google.com/presentation/d/1Y18vXn6lsSp-6pJVsB5swWg7UcTDWYrIYtXq8_brRMk/edit#slide=id.g6183830b53_0_5 Project guidelines This project uses Black and isort. You just need to install pip install pre-commit and then just run pre-commit run --all-files on the root folder. Please check pre-commit-config.yaml for more info about it. Another option (after adding the project poetry env as interpreter in pycharm) would be running poetry run black . and poetry run isort . on the root folder, the config options will be taken automatically from pyproject.toml . For adding this as autosave option please refer to https://black.readthedocs.io/en/stable/integrations/editors.html When updating a git branch please use rebase instead of merge. Tools k9s https://k9scli.io/ bash-completion","title":"Welcome Pack"},{"location":"WELCOME_PACK/#welcome-pack","text":"","title":"Welcome Pack"},{"location":"WELCOME_PACK/#team-composition","text":"Julia - Manager Dani - Tech Lead \u00c1ngel - Devops Brandon - Developer David - Developer Sergio - Developer Javier - Developer","title":"Team Composition"},{"location":"WELCOME_PACK/#first-steps","text":"Please request access to all the things listed below: Project repository https://gitlab.intelygenz.com/mettel/automation-engine/ , docker repository https://gitlab.intelygenz.com/mettel/docker_images/-/tree/master and One password to itcrowd@intelygenz.com through their ticketing system https://docs.google.com/document/d/1YLYdI9Dyq8tNlNy2iJ29InquKDz8r_Dw4XBsxI7pPiM/edit and CC your manager to allow the request Configure vpn https://docs.google.com/document/d/16_LFpkiBWN0mbfjAoqR4BaEB5kPNsuNHrUS7PtrWnEA/edit#heading=h.to49i8wu1vn3 AWS account creation to our devops Jira board to our manager Mettel's Slack channels to our tech lead","title":"First steps"},{"location":"WELCOME_PACK/#project-overview","text":"","title":"Project overview"},{"location":"WELCOME_PACK/#resources","text":"Check docs folder inside the mettel gitlab project and read carefully the readme for installing all the required programs and configure the project. After RRHH's on-boarding our team lead will give an overview of the project https://docs.google.com/presentation/d/1Y18vXn6lsSp-6pJVsB5swWg7UcTDWYrIYtXq8_brRMk/edit#slide=id.g6183830b53_0_5","title":"Resources"},{"location":"WELCOME_PACK/#project-guidelines","text":"This project uses Black and isort. You just need to install pip install pre-commit and then just run pre-commit run --all-files on the root folder. Please check pre-commit-config.yaml for more info about it. Another option (after adding the project poetry env as interpreter in pycharm) would be running poetry run black . and poetry run isort . on the root folder, the config options will be taken automatically from pyproject.toml . For adding this as autosave option please refer to https://black.readthedocs.io/en/stable/integrations/editors.html When updating a git branch please use rebase instead of merge.","title":"Project guidelines"},{"location":"WELCOME_PACK/#tools","text":"k9s https://k9scli.io/ bash-completion","title":"Tools"},{"location":"api/flows/1-authentication/","text":"Flow Diagram Requisites Okta account/user set up AWS login endpoint exposed to internet (Also whitelisted IP?) create a user on okta that has permisions to check the user and the token Description As a requirement we need to accomplish centralized users and groups in Okta. We must connect Okta with our authentication workflow. Flow User will launch a signin call to our API endpoint Our API endpoint will call to the auth enpoint of okta. create api token https://developer.okta.com/docs/guides/create-an-api-token/main/ https://developer.okta.com/docs/guides/create-an-api-token/main/ login with api token https://developer.okta.com/docs/reference/api/authn/#response-example-for-primary-authentication-with-public-application-factor-enroll token expiration https://developer.okta.com/docs/guides/create-an-api-token/main/ validate access token https://developer.okta.com/docs/guides/validate-access-tokens/python/main/ https://developer.okta.com/docs/guides/implement-grant-type/ropassword/main/#set-up-your-app","title":"1 authentication"},{"location":"api/flows/1-authentication/#flow-diagram","text":"","title":"Flow Diagram"},{"location":"api/flows/1-authentication/#requisites","text":"Okta account/user set up AWS login endpoint exposed to internet (Also whitelisted IP?) create a user on okta that has permisions to check the user and the token Description As a requirement we need to accomplish centralized users and groups in Okta. We must connect Okta with our authentication workflow.","title":"Requisites"},{"location":"api/flows/1-authentication/#flow","text":"User will launch a signin call to our API endpoint Our API endpoint will call to the auth enpoint of okta. create api token https://developer.okta.com/docs/guides/create-an-api-token/main/ https://developer.okta.com/docs/guides/create-an-api-token/main/ login with api token https://developer.okta.com/docs/reference/api/authn/#response-example-for-primary-authentication-with-public-application-factor-enroll token expiration https://developer.okta.com/docs/guides/create-an-api-token/main/ validate access token https://developer.okta.com/docs/guides/validate-access-tokens/python/main/ https://developer.okta.com/docs/guides/implement-grant-type/ropassword/main/#set-up-your-app","title":"Flow"},{"location":"decisions/","text":"GLOBAL DECISIONS Dashboard infrastructure and architecture","title":"MetTel decisions"},{"location":"decisions/01-dashboards-infrastructure-and-architecture/","text":"1: Design of an isolated an unique dashboard user experience Status: Approved Decission: Future is to have an external Grafana that connects to an external Prometheus & InfluxDB 2 Implementation plan as following: - Prometheus will be deployed as an AWS Managed Prometheus. - Grafana will be deployed independently in AWS pointing to AWS Prometheus. - Konstellation KRE will be merged to a single KRE instance as soon as KRE allows it. - KRE will use a dedicated InfluxDB2 in AWS. - Grafana will connect to InfluxDB2 as additional data source. - Chronograf dashboards will be migrated to the dedicated Grafana in AWS. Miro diagram of affected systems: https://miro.com/app/board/uXjVO6bg-zY=/ Alternatives considered: Justification: Current dashboarding solution has several different interfaces, this is confusing for all kind of users. Current stability of dashboarding is dependent in the stability of our K8s deployments, losing all visibility if anything goes wrong on K8s. License limitations on InfluxDB OSS1 could become a problem. Consequences:","title":"**1: Design of an isolated an unique dashboard user experience**"},{"location":"decisions/01-dashboards-infrastructure-and-architecture/#1-design-of-an-isolated-an-unique-dashboard-user-experience","text":"Status: Approved Decission: Future is to have an external Grafana that connects to an external Prometheus & InfluxDB 2 Implementation plan as following: - Prometheus will be deployed as an AWS Managed Prometheus. - Grafana will be deployed independently in AWS pointing to AWS Prometheus. - Konstellation KRE will be merged to a single KRE instance as soon as KRE allows it. - KRE will use a dedicated InfluxDB2 in AWS. - Grafana will connect to InfluxDB2 as additional data source. - Chronograf dashboards will be migrated to the dedicated Grafana in AWS. Miro diagram of affected systems: https://miro.com/app/board/uXjVO6bg-zY=/ Alternatives considered: Justification: Current dashboarding solution has several different interfaces, this is confusing for all kind of users. Current stability of dashboarding is dependent in the stability of our K8s deployments, losing all visibility if anything goes wrong on K8s. License limitations on InfluxDB OSS1 could become a problem. Consequences:","title":"1: Design of an isolated an unique dashboard user experience"},{"location":"diagrams/TOOLS/","text":"https://github.com/mingrammer/diagrams","title":"TOOLS"},{"location":"kafka/LAUNCH_DOCKER_COMPOSE/","text":"1. How to run In this section we explain all the useful information that we can use when working in our local environment. In order to raise our system we will have to go to the root folder of our system and launch the following command docker-compose up This will launch our system by creating a local docker image and deploying our fetcher. It is important to keep in mind that this process launches a multitude of requests against the production environment and it is important to limit them all so as not to overload the system. To facilitate the work in local we also have several local configurations that allow us to test step by step our application avoiding possible problems of launching too many requests. To launch this configuration in Intellij you will need to verify that the following steps were done: - Go to Settings >> Build, Execution, Deployment >> Docker - Select \"TCP socket\" - Enter 'unix:///var/run/docker.sock' under \"Engine API URL\"","title":"Launch docker compose"},{"location":"kafka/LAUNCH_DOCKER_COMPOSE/#1-how-to-run","text":"In this section we explain all the useful information that we can use when working in our local environment. In order to raise our system we will have to go to the root folder of our system and launch the following command docker-compose up This will launch our system by creating a local docker image and deploying our fetcher. It is important to keep in mind that this process launches a multitude of requests against the production environment and it is important to limit them all so as not to overload the system. To facilitate the work in local we also have several local configurations that allow us to test step by step our application avoiding possible problems of launching too many requests. To launch this configuration in Intellij you will need to verify that the following steps were done: - Go to Settings >> Build, Execution, Deployment >> Docker - Select \"TCP socket\" - Enter 'unix:///var/run/docker.sock' under \"Engine API URL\"","title":"1. How to run"},{"location":"lambda/PARAMETER_REPLICATOR/","text":"1. Summary Parameter Store is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. This service is only available in the region where is deployed. We use it to set configuration of Automation-Engine app. But if a desaster occurs in the main region we need to have the parameters replicated en the stand-by region to redeploy the app. Parameter-replicator is a phython lambda that replicate parameters from one region to another. If any parameter change, that change will be replicated in the other region, by this way we have a configuration ready to run the application in the mirror region. This lambda also run ones a day to create a parameter backup and store in S3. 1. Diagram parameter-replicator.drawio.svg","title":"PARAMETER REPLICATOR"},{"location":"lambda/PARAMETER_REPLICATOR/#1-summary","text":"Parameter Store is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. This service is only available in the region where is deployed. We use it to set configuration of Automation-Engine app. But if a desaster occurs in the main region we need to have the parameters replicated en the stand-by region to redeploy the app. Parameter-replicator is a phython lambda that replicate parameters from one region to another. If any parameter change, that change will be replicated in the other region, by this way we have a configuration ready to run the application in the mirror region. This lambda also run ones a day to create a parameter backup and store in S3.","title":"1. Summary"},{"location":"lambda/PARAMETER_REPLICATOR/#1-diagram","text":"parameter-replicator.drawio.svg","title":"1. Diagram"},{"location":"logging/","text":"","title":"Index"},{"location":"logging/events/1-service-outage/","text":"IPA Event Logging Process Workflows List of Decisions made by the IPA System Service Outage Start of Service Outage process 1. Checking edge & link status for outage Outage is detected Outage is not detected Autoresolution 2. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN 3. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 4. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 5. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 6. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times Ticket Creation 7. Checking the results of ticket creation attempt New ticket is created with a task for the SD-WAN and placed in an IPA queue Ticket already exists for the location 8. Determining what to do with an already existing ticket New task for SD-WAN is added to ticket SD-WAN task on existing ticket for is reopened In-progress tasks for SD-WAN already exists IPA queue 9. What type of Outage was caused Outage Detected is an EDGE DOWN Outage Detected is a LINK DOWN 10. Checking time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) DiGi Reboot 11. Are there any DiGi links Edge has at least one offline DiGi link Edge has no offline DiGi links 12. Has a DiGi reboot been attempted DiGi reboot has not been attempted yet DiGi reboot has been attempted 13. Has the DiGi reboot attempt occur after 30 mins 30 min has passed since reboot started 30 min has not passed since reboot started Event Descriptions Service Outage start_service_outage_monitoring","title":"IPA Event Logging"},{"location":"logging/events/1-service-outage/#ipa-event-logging","text":"","title":"IPA Event Logging"},{"location":"logging/events/1-service-outage/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/1-service-outage/#list-of-decisions-made-by-the-ipa-system","text":"","title":"List of Decisions made by the IPA System"},{"location":"logging/events/1-service-outage/#service-outage","text":"","title":"Service Outage"},{"location":"logging/events/1-service-outage/#start-of-service-outage-process","text":"1. Checking edge & link status for outage Outage is detected Outage is not detected","title":"Start of Service Outage process"},{"location":"logging/events/1-service-outage/#autoresolution","text":"2. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN 3. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 4. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 5. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 6. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times","title":"Autoresolution"},{"location":"logging/events/1-service-outage/#ticket-creation","text":"7. Checking the results of ticket creation attempt New ticket is created with a task for the SD-WAN and placed in an IPA queue Ticket already exists for the location 8. Determining what to do with an already existing ticket New task for SD-WAN is added to ticket SD-WAN task on existing ticket for is reopened In-progress tasks for SD-WAN already exists","title":"Ticket Creation"},{"location":"logging/events/1-service-outage/#ipa-queue","text":"9. What type of Outage was caused Outage Detected is an EDGE DOWN Outage Detected is a LINK DOWN 10. Checking time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY)","title":"IPA queue"},{"location":"logging/events/1-service-outage/#digi-reboot","text":"11. Are there any DiGi links Edge has at least one offline DiGi link Edge has no offline DiGi links 12. Has a DiGi reboot been attempted DiGi reboot has not been attempted yet DiGi reboot has been attempted 13. Has the DiGi reboot attempt occur after 30 mins 30 min has passed since reboot started 30 min has not passed since reboot started","title":"DiGi Reboot"},{"location":"logging/events/1-service-outage/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/1-service-outage/#service-outage_1","text":"start_service_outage_monitoring","title":"Service Outage"},{"location":"logging/events/10-ixia-outage-monitoring/","text":"Ixia Outage Monitoring Event Logging Process Workflows List of Decisions made by the Ixia outage monitoring System Ixia outage monitoring queue Start of Ixia outage monitoring workflow 1. Checking probe status The probe is not active The probe is active 2. Check Node to Node and Real service status Node to Node or Real service status is 0 (Offline) Node to Node AND Real status is 1 (Online) 3. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN on a Service Outage ticket 4. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 5. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 6. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 7. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times ## Event Descriptions ### Ixia outage monitoring queue * start_hawkeye_outage_monitoring","title":"Ixia Outage Monitoring Event Logging"},{"location":"logging/events/10-ixia-outage-monitoring/#ixia-outage-monitoring-event-logging","text":"","title":"Ixia Outage Monitoring Event Logging"},{"location":"logging/events/10-ixia-outage-monitoring/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/10-ixia-outage-monitoring/#list-of-decisions-made-by-the-ixia-outage-monitoring-system","text":"","title":"List of Decisions made by the Ixia outage monitoring System"},{"location":"logging/events/10-ixia-outage-monitoring/#ixia-outage-monitoring-queue","text":"","title":"Ixia outage monitoring queue"},{"location":"logging/events/10-ixia-outage-monitoring/#start-of-ixia-outage-monitoring-workflow","text":"1. Checking probe status The probe is not active The probe is active 2. Check Node to Node and Real service status Node to Node or Real service status is 0 (Offline) Node to Node AND Real status is 1 (Online) 3. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the SD-WAN on a Service Outage ticket No non-resolved task exists for the SD-WAN on a Service Outage ticket 4. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 5. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 6. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 7. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times ## Event Descriptions ### Ixia outage monitoring queue * start_hawkeye_outage_monitoring","title":"Start of Ixia outage monitoring workflow"},{"location":"logging/events/2-BYOB-IPA-queue/","text":"IPA Event Logging Process Workflows List of Decisions made by the IPA System BYOB IPA queue Start of BYOB workflow 1. Determining whether or not there is an ongoing trouble or not Trouble is detected Trouble stabilized Trouble is detected 2. Determining what kind of trouble occurred Trouble is on the link Trouble is on the Edge 3. Determining what VCO does the troubled device belong too VCO 1, 2, 3 VCO 4 4. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\" Trouble stabilized 5. Determining what kind of trouble last occurred Trouble is on the link Trouble is on the Edge 6. Determining what VCO does the device belong too VCO 1, 2, 3 VCO 4 7. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\" Event Descriptions BYOB IPA queue _attempt_ticket_creation","title":"IPA Event Logging"},{"location":"logging/events/2-BYOB-IPA-queue/#ipa-event-logging","text":"","title":"IPA Event Logging"},{"location":"logging/events/2-BYOB-IPA-queue/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/2-BYOB-IPA-queue/#list-of-decisions-made-by-the-ipa-system","text":"","title":"List of Decisions made by the IPA System"},{"location":"logging/events/2-BYOB-IPA-queue/#byob-ipa-queue","text":"","title":"BYOB IPA queue"},{"location":"logging/events/2-BYOB-IPA-queue/#start-of-byob-workflow","text":"1. Determining whether or not there is an ongoing trouble or not Trouble is detected Trouble stabilized","title":"Start of BYOB workflow"},{"location":"logging/events/2-BYOB-IPA-queue/#trouble-is-detected","text":"2. Determining what kind of trouble occurred Trouble is on the link Trouble is on the Edge 3. Determining what VCO does the troubled device belong too VCO 1, 2, 3 VCO 4 4. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\"","title":"Trouble is detected"},{"location":"logging/events/2-BYOB-IPA-queue/#trouble-stabilized","text":"5. Determining what kind of trouble last occurred Trouble is on the link Trouble is on the Edge 6. Determining what VCO does the device belong too VCO 1, 2, 3 VCO 4 7. Determining if link name includes \"BYOB\" or \"Customer Provided\" Link name includes \"BYOB\" or \"Customer Provided\" Link name doesn't include \"BYOB\" or \"Customer Provided\"","title":"Trouble stabilized"},{"location":"logging/events/2-BYOB-IPA-queue/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/2-BYOB-IPA-queue/#byob-ipa-queue_1","text":"_attempt_ticket_creation","title":"BYOB IPA queue"},{"location":"logging/events/3-HNOC-forwarding/","text":"HNOC Forwarding Event Logging Process Workflows List of Decisions made by the HNOC forwarding System HNOC forwarding queue Start of HNOC forwarding workflow (SO) 1. Service outage detected 2. Attempt to create ticket outage Create new SO ticket Reopen exist ticket 3. If exist ticket and have more than 60 min Forward HNOC END 4. New ticket created Append triage note 5. If edge outage Forward to HNOC END 6. If link outage Check if more than 60 minutes Start of HNOC forwarding workflow (SA) 1. Service affecting trouble detected Not SA ticket for device SA ticket for device 2. Not SA ticket Create new SA ticket and append note 3. SA ticket exist Reopen SA ticket and append note 4 Wait 60 seconds and forward to HNOC Event Descriptions HNOC forwarding outage queue _attempt_ticket_creation Event Descriptions HNOC forwarding affecting queue _attempt_ticket_creation","title":"HNOC Forwarding Event Logging"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-event-logging","text":"","title":"HNOC Forwarding Event Logging"},{"location":"logging/events/3-HNOC-forwarding/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/3-HNOC-forwarding/#list-of-decisions-made-by-the-hnoc-forwarding-system","text":"","title":"List of Decisions made by the HNOC forwarding System"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-queue","text":"","title":"HNOC forwarding queue"},{"location":"logging/events/3-HNOC-forwarding/#start-of-hnoc-forwarding-workflow-so","text":"1. Service outage detected 2. Attempt to create ticket outage Create new SO ticket Reopen exist ticket 3. If exist ticket and have more than 60 min Forward HNOC END 4. New ticket created Append triage note 5. If edge outage Forward to HNOC END 6. If link outage Check if more than 60 minutes","title":"Start of  HNOC forwarding workflow (SO)"},{"location":"logging/events/3-HNOC-forwarding/#start-of-hnoc-forwarding-workflow-sa","text":"1. Service affecting trouble detected Not SA ticket for device SA ticket for device 2. Not SA ticket Create new SA ticket and append note 3. SA ticket exist Reopen SA ticket and append note 4 Wait 60 seconds and forward to HNOC","title":"Start of  HNOC forwarding workflow (SA)"},{"location":"logging/events/3-HNOC-forwarding/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-outage-queue","text":"_attempt_ticket_creation","title":"HNOC forwarding outage queue"},{"location":"logging/events/3-HNOC-forwarding/#event-descriptions_1","text":"","title":"Event Descriptions"},{"location":"logging/events/3-HNOC-forwarding/#hnoc-forwarding-affecting-queue","text":"_attempt_ticket_creation","title":"HNOC forwarding affecting queue"},{"location":"logging/events/4-SA-forward-to-ASR/","text":"IPA Event Logging Process Workflows List of Decisions made by the IPA System Service Affecting Circuit Instability 1. Check for existing SA ticket for SD-WAN SA ticket does not exist for SD-WAN Resolved SA ticket exists for SD-WAN Open/In progress SA ticket exist for SD-WAN 2. Check for other troubles on the ticket Other trouble is documented on ticket. (Jitter, Latency, Packet Loss, Bandwidth) Only Circuit Instability is the only documented trouble on the ticket 3. Checking what kind of link the instability was detected on Instability was detected for Wireless Link Instability was detected for Wired Link 4. Checking wired link's name for BYOB or Customer Owned Wired Link name contains BYOB or Customer Owned Wired Link name does NOT contain BYOB or Customer Owned 5. Checking if the wired link name is an IP Wired link name is an IP Wired link name is NOT an IP Event Descriptions Service Affecting Bouncing check","title":"IPA Event Logging"},{"location":"logging/events/4-SA-forward-to-ASR/#ipa-event-logging","text":"","title":"IPA Event Logging"},{"location":"logging/events/4-SA-forward-to-ASR/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/4-SA-forward-to-ASR/#list-of-decisions-made-by-the-ipa-system","text":"","title":"List of Decisions made by the IPA System"},{"location":"logging/events/4-SA-forward-to-ASR/#service-affecting","text":"","title":"Service Affecting"},{"location":"logging/events/4-SA-forward-to-ASR/#circuit-instability","text":"1. Check for existing SA ticket for SD-WAN SA ticket does not exist for SD-WAN Resolved SA ticket exists for SD-WAN Open/In progress SA ticket exist for SD-WAN 2. Check for other troubles on the ticket Other trouble is documented on ticket. (Jitter, Latency, Packet Loss, Bandwidth) Only Circuit Instability is the only documented trouble on the ticket 3. Checking what kind of link the instability was detected on Instability was detected for Wireless Link Instability was detected for Wired Link 4. Checking wired link's name for BYOB or Customer Owned Wired Link name contains BYOB or Customer Owned Wired Link name does NOT contain BYOB or Customer Owned 5. Checking if the wired link name is an IP Wired link name is an IP Wired link name is NOT an IP","title":"Circuit Instability"},{"location":"logging/events/4-SA-forward-to-ASR/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/4-SA-forward-to-ASR/#service-affecting_1","text":"Bouncing check","title":"Service Affecting"},{"location":"logging/events/5-TNBA-monitor/","text":"1. <G ent Logging Process Workflows List of Decisions made by the TNBA Monitor 1. Get TNBA prediction for SD-WAN task Prediction not repair request complete Prediction repair request complete Ticket is not outage Ticket is outage Ticket is not created by IPA Ticket created by IPA Confidence level is < 80 Confidence level is > 80 2. Append prediction note SD-WAN - - 3. Check status SD-WAN Edge or link not online Edge and links online 4. TBD - - 5. Autoresolve SD-WAN task - - 6. Append A.I. resolve note - - Start of TNBA Monitor Event description Start TNBA automated process","title":"5 TNBA monitor"},{"location":"logging/events/5-TNBA-monitor/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/5-TNBA-monitor/#list-of-decisions-made-by-the-tnba-monitor","text":"1. Get TNBA prediction for SD-WAN task Prediction not repair request complete Prediction repair request complete Ticket is not outage Ticket is outage Ticket is not created by IPA Ticket created by IPA Confidence level is < 80 Confidence level is > 80 2. Append prediction note SD-WAN - - 3. Check status SD-WAN Edge or link not online Edge and links online 4. TBD - - 5. Autoresolve SD-WAN task - - 6. Append A.I. resolve note - -","title":"List of Decisions made by the TNBA Monitor"},{"location":"logging/events/5-TNBA-monitor/#start-of-tnba-monitor","text":"","title":"Start of TNBA Monitor"},{"location":"logging/events/5-TNBA-monitor/#event-description","text":"Start TNBA automated process","title":"Event description"},{"location":"logging/events/6-ticket-severity/","text":"Ticket severity Process Workflows List of Decisions made by ticket severity Ticket Severity Start of change ticket severity 1. Outage condition is detected Edge DOWN Link DOWN Edge Down 1. Attempt to create a service Outage bruin ticket Status 200, 409, 473, 472, 471 2. Set ticket severity 2 END Link Down 1. Attempt to create a service Outage bruin ticket Status 200 or 473 Status 409, 472 or 471 2. For status 200 or 473 Set ticket severity 3 - 3. For status 409, 472 or 471 Ticket with only 1 Task set severity 3 If ticket have more than 1 task end Autoresolution Event Descriptions Attempt ticket creation","title":"Ticket severity"},{"location":"logging/events/6-ticket-severity/#ticket-severity","text":"","title":"Ticket severity"},{"location":"logging/events/6-ticket-severity/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/6-ticket-severity/#list-of-decisions-made-by-ticket-severity","text":"","title":"List of Decisions made by ticket severity"},{"location":"logging/events/6-ticket-severity/#ticket-severity_1","text":"","title":"Ticket Severity"},{"location":"logging/events/6-ticket-severity/#start-of-change-ticket-severity","text":"1. Outage condition is detected Edge DOWN Link DOWN","title":"Start of change ticket severity"},{"location":"logging/events/6-ticket-severity/#edge-down","text":"1. Attempt to create a service Outage bruin ticket Status 200, 409, 473, 472, 471 2. Set ticket severity 2 END","title":"Edge Down"},{"location":"logging/events/6-ticket-severity/#link-down","text":"1. Attempt to create a service Outage bruin ticket Status 200 or 473 Status 409, 472 or 471 2. For status 200 or 473 Set ticket severity 3 - 3. For status 409, 472 or 471 Ticket with only 1 Task set severity 3 If ticket have more than 1 task end","title":"Link Down"},{"location":"logging/events/6-ticket-severity/#autoresolution","text":"","title":"Autoresolution"},{"location":"logging/events/6-ticket-severity/#event-descriptions","text":"Attempt ticket creation","title":"Event Descriptions"},{"location":"logging/events/7-ticket-creation-outcome/","text":"","title":"7 ticket creation outcome"},{"location":"logging/events/8-service-affecting/","text":"Service affecting Event Logging Process Workflows List of Decisions made by the service affecting System Service affecting queue Start of service affecting workflow 1. Detected service trouble SD-WAN 2. Check if exist ticket No new, in progress or resolved ticket created Exist a resolve ticket New or in progress exist 3. If no exist ticket Create ticket and append note for trouble 4. If resolved ticket exist Reopen ticket and append note for trouble 5. If exist in progress ticket Check if is the same problem, if is the same end process If not the same trouble append new note Event Descriptions Service affecting queue _attempt_ticket_creation","title":"Service affecting Event Logging"},{"location":"logging/events/8-service-affecting/#service-affecting-event-logging","text":"","title":"Service affecting Event Logging"},{"location":"logging/events/8-service-affecting/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/8-service-affecting/#list-of-decisions-made-by-the-service-affecting-system","text":"","title":"List of Decisions made by the service affecting System"},{"location":"logging/events/8-service-affecting/#service-affecting-queue","text":"","title":"Service affecting queue"},{"location":"logging/events/8-service-affecting/#start-of-service-affecting-workflow","text":"1. Detected service trouble SD-WAN 2. Check if exist ticket No new, in progress or resolved ticket created Exist a resolve ticket New or in progress exist 3. If no exist ticket Create ticket and append note for trouble 4. If resolved ticket exist Reopen ticket and append note for trouble 5. If exist in progress ticket Check if is the same problem, if is the same end process If not the same trouble append new note","title":"Start of  service affecting workflow"},{"location":"logging/events/8-service-affecting/#event-descriptions","text":"","title":"Event Descriptions"},{"location":"logging/events/8-service-affecting/#service-affecting-queue_1","text":"_attempt_ticket_creation","title":"Service affecting queue"},{"location":"logging/events/9-intermapper-monitor/","text":"Intermapper Event Logging Process Workflows List of Decisions made by the Intermapper System Intermapper queue Start of intermapper workflow 1. Check sender email Sender email is noreply@mettel.net Sender email is NOT noreply@mettel.net 2. Check for Alpha-numeric-value (Circuit id) from subject within parenthesis Alpha-numeric value (Circuit ID) can be parsed from subject between parenthsis to identify inventory Alpha-numeric value (Circuit ID) can NOT be parsed from email subject between parenthsis 3. Check for Warning event type and condition Event type is NOT Warning and Condition is NOT \"WAN Interface down - Device is running on LTE Interface\" Event type is Warning and Condition is \"WAN Interface down - Device is running on LTE Interface\" 4. Check for event type Event type is Alarm, Critical, Warning, or Down Event type is Up, OK 5. Check probe type Probe type is Data Remote Probe Type is NOT Data Remote 6. Check to see if we can retrieve data from DRI Successfully retrieved data from DRI from Inventory Could not retrieve any data for inventory from DRI 7. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the inventory on a Service Outage ticket No non-resolved task exists for the inventory on a Service Outage ticket 8. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 9. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 10. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 11. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times ## Event Descriptions ### Intermapper queue * start_intermapper_outage_monitoring","title":"Intermapper Event Logging"},{"location":"logging/events/9-intermapper-monitor/#intermapper-event-logging","text":"","title":"Intermapper Event Logging"},{"location":"logging/events/9-intermapper-monitor/#process-workflows","text":"","title":"Process Workflows"},{"location":"logging/events/9-intermapper-monitor/#list-of-decisions-made-by-the-intermapper-system","text":"","title":"List of Decisions made by the Intermapper System"},{"location":"logging/events/9-intermapper-monitor/#intermapper-queue","text":"","title":"Intermapper queue"},{"location":"logging/events/9-intermapper-monitor/#start-of-intermapper-workflow","text":"1. Check sender email Sender email is noreply@mettel.net Sender email is NOT noreply@mettel.net 2. Check for Alpha-numeric-value (Circuit id) from subject within parenthesis Alpha-numeric value (Circuit ID) can be parsed from subject between parenthsis to identify inventory Alpha-numeric value (Circuit ID) can NOT be parsed from email subject between parenthsis 3. Check for Warning event type and condition Event type is NOT Warning and Condition is NOT \"WAN Interface down - Device is running on LTE Interface\" Event type is Warning and Condition is \"WAN Interface down - Device is running on LTE Interface\" 4. Check for event type Event type is Alarm, Critical, Warning, or Down Event type is Up, OK 5. Check probe type Probe type is Data Remote Probe Type is NOT Data Remote 6. Check to see if we can retrieve data from DRI Successfully retrieved data from DRI from Inventory Could not retrieve any data for inventory from DRI 7. Checking for non-resolved task for Service Outage ticket A non-resolved task exists for the inventory on a Service Outage ticket No non-resolved task exists for the inventory on a Service Outage ticket 8. Checking for time at impacted site Time at impacted site is between 12am and 6am (NIGHT) Time at impacted site is between 6am and 12am (DAY) 9. Checking if time passed is more or less than 3 hrs More than 3 hours has passed since an outage was documented Less than 3 hours has passed since an outage was documented 10. Checking if time passed is more or less than 90 mins More than 90 mins has passed since an outage was documented Less than 90 mins has passed since an outage was documented 11. Checking how mamy time ticket has been autoresolved Ticket has been autoresolved less than 3 times Ticket has been autoresolved more than 3 times ## Event Descriptions ### Intermapper queue * start_intermapper_outage_monitoring","title":"Start of intermapper workflow"},{"location":"logging/services/TNBA-monitor/actions/_append_tnba_notes/","text":"Append tnba notes for ticket id in notes ticket id: self._logger.info(f\"Appending {len(notes)} TNBA notes to ticket {ticket_id}...\") append_multiple_notes_to_ticket If append multiple notes status is not OK: self._logger.warning(f\"Bad status calling append multiple notes to ticket id: {ticket_id}.\" f\" Skipping ...\")","title":" append tnba notes"},{"location":"logging/services/TNBA-monitor/actions/_append_tnba_notes/#append-tnba-notes","text":"for ticket id in notes ticket id: self._logger.info(f\"Appending {len(notes)} TNBA notes to ticket {ticket_id}...\") append_multiple_notes_to_ticket If append multiple notes status is not OK: self._logger.warning(f\"Bad status calling append multiple notes to ticket id: {ticket_id}.\" f\" Skipping ...\")","title":"Append tnba notes"},{"location":"logging/services/TNBA-monitor/actions/_autoresolve_ticket_detail/","text":"Autoresolve ticket detail self._logger.info(f\"Running autoresolve for serial {serial_number} of ticket {ticket_id}...\") * If ticket created by automation engine: self._logger.info( f\"Ticket {ticket_id}, where serial {serial_number} is, was not created by Automation Engine. \" \"Skipping autoresolve...\" ) * If is detail in outage ticket: self._logger.info( f\"Serial {serial_number} of ticket {ticket_id} is in outage state. Skipping autoresolve...\" ) * If is detail in affecting ticket and are all metrics within thresholds: self._logger.info( f\"At least one metric from serial {serial_number} of ticket {ticket_id} is not within the threshold.\" f\" Skipping autoresolve...\" ) * If prediction is not confident enough: self._logger.info( f\"The confidence of the best prediction found for ticket {ticket_id}, where serial {serial_number} is, \" f\"did not exceed the minimum threshold. Skipping autoresolve...\" ) * If environment is not PRODUCTION: self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} was about to be resolved, but the \" f\"current environment is {self._config.CURRENT_ENVIRONMENT.upper()}. Skipping autoresolve...\" ) * unpause_ticket_detail * resolve_ticket_detail * If resolve ticket detail status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket detail for ticket id: {ticket_id} \" f\"and ticket detail id: {ticket_detail_id} . Skipping resolve ticket detail\")","title":" autoresolve ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_autoresolve_ticket_detail/#autoresolve-ticket-detail","text":"self._logger.info(f\"Running autoresolve for serial {serial_number} of ticket {ticket_id}...\") * If ticket created by automation engine: self._logger.info( f\"Ticket {ticket_id}, where serial {serial_number} is, was not created by Automation Engine. \" \"Skipping autoresolve...\" ) * If is detail in outage ticket: self._logger.info( f\"Serial {serial_number} of ticket {ticket_id} is in outage state. Skipping autoresolve...\" ) * If is detail in affecting ticket and are all metrics within thresholds: self._logger.info( f\"At least one metric from serial {serial_number} of ticket {ticket_id} is not within the threshold.\" f\" Skipping autoresolve...\" ) * If prediction is not confident enough: self._logger.info( f\"The confidence of the best prediction found for ticket {ticket_id}, where serial {serial_number} is, \" f\"did not exceed the minimum threshold. Skipping autoresolve...\" ) * If environment is not PRODUCTION: self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} was about to be resolved, but the \" f\"current environment is {self._config.CURRENT_ENVIRONMENT.upper()}. Skipping autoresolve...\" ) * unpause_ticket_detail * resolve_ticket_detail * If resolve ticket detail status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket detail for ticket id: {ticket_id} \" f\"and ticket detail id: {ticket_detail_id} . Skipping resolve ticket detail\")","title":"Autoresolve ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_filter_outage_ticket_details_based_on_last_outage/","text":"Filter outage ticket details based on last outage for ticket detail in tickets details: If is outage ticket and last outage detected recently: self._logger.info( f\"Last outage detected for serial {serial_number} in Service Outage ticket {ticket_id} is \" \"too recent. Skipping...\" )","title":" filter outage ticket details based on last outage"},{"location":"logging/services/TNBA-monitor/actions/_filter_outage_ticket_details_based_on_last_outage/#filter-outage-ticket-details-based-on-last-outage","text":"for ticket detail in tickets details: If is outage ticket and last outage detected recently: self._logger.info( f\"Last outage detected for serial {serial_number} in Service Outage ticket {ticket_id} is \" \"too recent. Skipping...\" )","title":"Filter outage ticket details based on last outage"},{"location":"logging/services/TNBA-monitor/actions/_filter_tickets_and_details_related_to_edges_under_monitoring/","text":"Filter tickets and details related to edges under monitoring for ticket in tickets If not relevant ticket: self._logger.warning(f\"Don't found relevant tickets. Skipping ticket ...\")","title":" filter tickets and details related to edges under monitoring"},{"location":"logging/services/TNBA-monitor/actions/_filter_tickets_and_details_related_to_edges_under_monitoring/#filter-tickets-and-details-related-to-edges-under-monitoring","text":"for ticket in tickets If not relevant ticket: self._logger.warning(f\"Don't found relevant tickets. Skipping ticket ...\")","title":"Filter tickets and details related to edges under monitoring"},{"location":"logging/services/TNBA-monitor/actions/_get_all_open_tickets_with_details_for_monitored_companies/","text":"Get all open tickets with details for monitored companies _get_open_tickets_with_details_by_client_id","title":" get all open tickets with details for monitored companies"},{"location":"logging/services/TNBA-monitor/actions/_get_all_open_tickets_with_details_for_monitored_companies/#get-all-open-tickets-with-details-for-monitored-companies","text":"_get_open_tickets_with_details_by_client_id","title":"Get all open tickets with details for monitored companies"},{"location":"logging/services/TNBA-monitor/actions/_get_open_tickets_with_details_by_client_id/","text":"Get open tickets with details by client id get_open_outage_tickets If bad status calling to get outage tickets: self._logger.warning(f\"Bad status calling to get outage tickets. Return empty list ...\") get_open_affecting_tickets self._logger.warning(f\"Bad status calling to get affecting tickets. Return empty list ...\") self._logger.info(f\"Getting all opened tickets for Bruin customer {client_id}...\") For ticket in all tickets: get_ticket_details.md self._logger.warning(f\"Bad status calling to get tickets details with id: {ticket_id}.\" f\"Skipping ticket ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id} of Bruin customer {client_id}!\") self._logger.info(f\"Finished getting all opened tickets for Bruin customer {client_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get open tickets with details for Bruin client {client_id} -> {e}\" )","title":" get open tickets with details by client id"},{"location":"logging/services/TNBA-monitor/actions/_get_open_tickets_with_details_by_client_id/#get-open-tickets-with-details-by-client-id","text":"get_open_outage_tickets If bad status calling to get outage tickets: self._logger.warning(f\"Bad status calling to get outage tickets. Return empty list ...\") get_open_affecting_tickets self._logger.warning(f\"Bad status calling to get affecting tickets. Return empty list ...\") self._logger.info(f\"Getting all opened tickets for Bruin customer {client_id}...\") For ticket in all tickets: get_ticket_details.md self._logger.warning(f\"Bad status calling to get tickets details with id: {ticket_id}.\" f\"Skipping ticket ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id} of Bruin customer {client_id}!\") self._logger.info(f\"Finished getting all opened tickets for Bruin customer {client_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get open tickets with details for Bruin client {client_id} -> {e}\" )","title":"Get open tickets with details by client id"},{"location":"logging/services/TNBA-monitor/actions/_get_predictions_by_ticket_id/","text":"Get predictions by ticket id for ticket in tickets: self._logger.info(f\"Claiming T7 predictions for ticket {ticket_id}...\") get_ticket_task_history self._logger.warning(f\"Bad status calling to get ticket history. \" f\"Skipping for ticket id: {ticket_id} ...\") If any ticket row has asset: self._logger.info(f\"Task history of ticket {ticket_id} doesn't have any asset. Skipping...\") get_prediction self._logger.warning(f\"Bad status calling to t7 predictions. \" f\"Skipping predictions for ticket id: {ticket_id}\") If not predictions: self._logger.info(f\"There are no predictions for ticket {ticket_id}. Skipping...\") self._logger.info(f\"T7 predictions found for ticket {ticket_id}!\")","title":" get predictions by ticket id"},{"location":"logging/services/TNBA-monitor/actions/_get_predictions_by_ticket_id/#get-predictions-by-ticket-id","text":"for ticket in tickets: self._logger.info(f\"Claiming T7 predictions for ticket {ticket_id}...\") get_ticket_task_history self._logger.warning(f\"Bad status calling to get ticket history. \" f\"Skipping for ticket id: {ticket_id} ...\") If any ticket row has asset: self._logger.info(f\"Task history of ticket {ticket_id} doesn't have any asset. Skipping...\") get_prediction self._logger.warning(f\"Bad status calling to t7 predictions. \" f\"Skipping predictions for ticket id: {ticket_id}\") If not predictions: self._logger.info(f\"There are no predictions for ticket {ticket_id}. Skipping...\") self._logger.info(f\"T7 predictions found for ticket {ticket_id}!\")","title":"Get predictions by ticket id"},{"location":"logging/services/TNBA-monitor/actions/_map_ticket_details_with_predictions/","text":"Map ticket details with predictions for detail obj in detail ticket details: If not predictions for ticket: self._logger.info( f\"Ticket {ticket_id} does not have any prediction associated. Skipping serial \" f\"{serial_number}...\" ) If not prediction object for related serial: self._logger.info( f\"No predictions were found for ticket {ticket_id} and serial {serial_number}. Skipping...\" )","title":" map ticket details with predictions"},{"location":"logging/services/TNBA-monitor/actions/_map_ticket_details_with_predictions/#map-ticket-details-with-predictions","text":"for detail obj in detail ticket details: If not predictions for ticket: self._logger.info( f\"Ticket {ticket_id} does not have any prediction associated. Skipping serial \" f\"{serial_number}...\" ) If not prediction object for related serial: self._logger.info( f\"No predictions were found for ticket {ticket_id} and serial {serial_number}. Skipping...\" )","title":"Map ticket details with predictions"},{"location":"logging/services/TNBA-monitor/actions/_process_ticket_detail/","text":"Process ticket detail self._logger.info( f\"Processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}...\" ) If is a tnba note and tnba note is not old enough: self._logger.info( f\"TNBA note found for ticket {ticket_id} and detail {ticket_detail_id} is too recent. \" f\"Skipping detail...\" ) get_next_results_for_ticket_detail If get next result for ticket detail status is not ok: self._logger.warning(f\"Bad status calling get next result for ticket details.\" f\"Skipping process ticket details for ticket id: {ticket_id} and\" f\"ticket detail id: {ticket_detail_id}\") self._logger.info( f\"Filtering predictions available in next results for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If not relevant predictions: self._logger.info(f\"No predictions with name appearing in the next results were found for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}!\") self._logger.info( f\"Predictions available in next results found for ticket {ticket_id}, detail {ticket_detail_id} \" f\"and serial {serial_number}: {relevant_predictions}\" ) If newest note: If best prediction different that the ticket note: self._logger.info( f\"Best prediction for ticket {ticket_id}, detail {ticket_detail_id} and serial {serial_number} \" f\"didn't change since the last TNBA note was appended. Skipping detail...\" ) self._logger.info( f\"Building TNBA note from prediction {best_prediction['name']} for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If request or repair completed prediction: self._logger.info( f\"Best prediction found for serial {serial_number} of ticket {ticket_id} is \" f'{best_prediction[\"name\"]}. Running autoresolve...' ) _autoresolve_ticket_detail If autoresolve status is SUCCES: post_live_automation_metrics If autoresolve status is SKIPPED: self._logger.info( f\"Autoresolve was triggered because the best prediction found for serial {serial_number} of \" f'ticket {ticket_id} was {best_prediction[\"name\"]}, but the process failed. A TNBA note with ' \"this prediction will be built and appended to the ticket later on.\" ) IF autoresolve status is BAD_PREDICTION: post_live_automation_metrics self._logger.info( f\"The prediction for serial {serial_number} of ticket {ticket_id} is considered wrong.\" ) If not TNBA note: self._logger.info(f\"No TNBA note will be appended for serial {serial_number} of ticket {ticket_id}.\") If environment DEV: self._logger.info(f\"TNBA note would have been appended to ticket {ticket_id} and detail {ticket_detail_id} \" f\"(serial: {serial_number}). Note: {tnba_note}. Details at app.bruin.com/t/{ticket_id}\") self._logger.info( f\"Finished processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}!\" )","title":" process ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_process_ticket_detail/#process-ticket-detail","text":"self._logger.info( f\"Processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}...\" ) If is a tnba note and tnba note is not old enough: self._logger.info( f\"TNBA note found for ticket {ticket_id} and detail {ticket_detail_id} is too recent. \" f\"Skipping detail...\" ) get_next_results_for_ticket_detail If get next result for ticket detail status is not ok: self._logger.warning(f\"Bad status calling get next result for ticket details.\" f\"Skipping process ticket details for ticket id: {ticket_id} and\" f\"ticket detail id: {ticket_detail_id}\") self._logger.info( f\"Filtering predictions available in next results for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If not relevant predictions: self._logger.info(f\"No predictions with name appearing in the next results were found for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}!\") self._logger.info( f\"Predictions available in next results found for ticket {ticket_id}, detail {ticket_detail_id} \" f\"and serial {serial_number}: {relevant_predictions}\" ) If newest note: If best prediction different that the ticket note: self._logger.info( f\"Best prediction for ticket {ticket_id}, detail {ticket_detail_id} and serial {serial_number} \" f\"didn't change since the last TNBA note was appended. Skipping detail...\" ) self._logger.info( f\"Building TNBA note from prediction {best_prediction['name']} for ticket {ticket_id}, \" f\"detail {ticket_detail_id} and serial {serial_number}...\" ) If request or repair completed prediction: self._logger.info( f\"Best prediction found for serial {serial_number} of ticket {ticket_id} is \" f'{best_prediction[\"name\"]}. Running autoresolve...' ) _autoresolve_ticket_detail If autoresolve status is SUCCES: post_live_automation_metrics If autoresolve status is SKIPPED: self._logger.info( f\"Autoresolve was triggered because the best prediction found for serial {serial_number} of \" f'ticket {ticket_id} was {best_prediction[\"name\"]}, but the process failed. A TNBA note with ' \"this prediction will be built and appended to the ticket later on.\" ) IF autoresolve status is BAD_PREDICTION: post_live_automation_metrics self._logger.info( f\"The prediction for serial {serial_number} of ticket {ticket_id} is considered wrong.\" ) If not TNBA note: self._logger.info(f\"No TNBA note will be appended for serial {serial_number} of ticket {ticket_id}.\") If environment DEV: self._logger.info(f\"TNBA note would have been appended to ticket {ticket_id} and detail {ticket_detail_id} \" f\"(serial: {serial_number}). Note: {tnba_note}. Details at app.bruin.com/t/{ticket_id}\") self._logger.info( f\"Finished processing detail {ticket_detail_id} (serial: {serial_number}) of ticket {ticket_id}!\" )","title":"Process ticket detail"},{"location":"logging/services/TNBA-monitor/actions/_remove_erroneous_predictions/","text":"Remove erroneous predictions for ticket in prediction tickets: for prediction in predictions: If error in prediction: self._logger.info( f\"Prediction for serial {serial_number} in ticket {ticket_id} was found but it contains an \" f\"error from T7 API -> {prediction_obj['error']}\" ) If not valid prediction: self._logger.info(f\"All predictions in ticket {ticket_id} were erroneous. Skipping ticket...\")","title":" remove erroneous predictions"},{"location":"logging/services/TNBA-monitor/actions/_remove_erroneous_predictions/#remove-erroneous-predictions","text":"for ticket in prediction tickets: for prediction in predictions: If error in prediction: self._logger.info( f\"Prediction for serial {serial_number} in ticket {ticket_id} was found but it contains an \" f\"error from T7 API -> {prediction_obj['error']}\" ) If not valid prediction: self._logger.info(f\"All predictions in ticket {ticket_id} were erroneous. Skipping ticket...\")","title":"Remove erroneous predictions"},{"location":"logging/services/TNBA-monitor/actions/_run_tickets_polling/","text":"Run tickets polling self._logger.info(\"Starting TNBA process...\") * get_cache_for_tnba_monitoring * If get cache status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping run ticket polling ...\") * get_edges_for_tnba_monitoring * If not edges statuses: self._logger.error(\"No edges statuses were received from VeloCloud. Aborting TNBA monitoring...\") self._logger.info(\"Keeping serials that exist in both the customer cache and the set of edges statuses...\") * get_links_metrics_for_autoresolve * If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") * get_events_by_serial_and_interface self._logger.info(\"Loading customer cache and edges statuses by serial into the monitor instance...\") self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got {len(open_tickets)} open tickets for all customers. \" f\"Filtering them (and their details) to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) self._logger.info(\"Getting T7 predictions for all relevant open tickets...\") * _get_predictions_by_ticket_id self._logger.info(\"Removing erroneous T7 predictions...\") * _remove_erroneous_predictions self._logger.info(\"Creating detail objects based on all tickets...\") self._logger.info(\"Discarding resolved ticket details...\") self._logger.info(\"Discarding ticket details of outage tickets whose last outage happened too recently...\") * _filter_outage_ticket_details_based_on_last_outage self._logger.info(\"Mapping all ticket details with their predictions...\") * _map_ticket_details_with_predictions self._logger.info( f\"{len(ticket_detail_objects)} ticket details were successfully mapped to predictions. \" \"Processing all details...\" ) * _process_ticket_detail self._logger.info(\"All ticket details were processed.\") * If not append notes TNBA: self._logger.info(\"No TNBA notes for append were built for any detail processed.\") * Else: self._logger.info(f\"{len(self._tnba_notes_to_append)} TNBA notes were built for append.\") * _append_tnba_notes self._logger.info(f\"TNBA process finished! Took {round((end_time - start_time) / 60, 2)} minutes.\")","title":" run tickets polling"},{"location":"logging/services/TNBA-monitor/actions/_run_tickets_polling/#run-tickets-polling","text":"self._logger.info(\"Starting TNBA process...\") * get_cache_for_tnba_monitoring * If get cache status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping run ticket polling ...\") * get_edges_for_tnba_monitoring * If not edges statuses: self._logger.error(\"No edges statuses were received from VeloCloud. Aborting TNBA monitoring...\") self._logger.info(\"Keeping serials that exist in both the customer cache and the set of edges statuses...\") * get_links_metrics_for_autoresolve * If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") * get_events_by_serial_and_interface self._logger.info(\"Loading customer cache and edges statuses by serial into the monitor instance...\") self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got {len(open_tickets)} open tickets for all customers. \" f\"Filtering them (and their details) to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) self._logger.info(\"Getting T7 predictions for all relevant open tickets...\") * _get_predictions_by_ticket_id self._logger.info(\"Removing erroneous T7 predictions...\") * _remove_erroneous_predictions self._logger.info(\"Creating detail objects based on all tickets...\") self._logger.info(\"Discarding resolved ticket details...\") self._logger.info(\"Discarding ticket details of outage tickets whose last outage happened too recently...\") * _filter_outage_ticket_details_based_on_last_outage self._logger.info(\"Mapping all ticket details with their predictions...\") * _map_ticket_details_with_predictions self._logger.info( f\"{len(ticket_detail_objects)} ticket details were successfully mapped to predictions. \" \"Processing all details...\" ) * _process_ticket_detail self._logger.info(\"All ticket details were processed.\") * If not append notes TNBA: self._logger.info(\"No TNBA notes for append were built for any detail processed.\") * Else: self._logger.info(f\"{len(self._tnba_notes_to_append)} TNBA notes were built for append.\") * _append_tnba_notes self._logger.info(f\"TNBA process finished! Took {round((end_time - start_time) / 60, 2)} minutes.\")","title":"Run tickets polling"},{"location":"logging/services/TNBA-monitor/actions/start_tnba_automated_process/","text":"Start tnba automated process (Start of service) self._logger.info(\"Scheduling TNBA automated process job...\") * If exec on start: self._logger.info(\"TNBA automated process job is going to be executed immediately\") * _run_tickets_polling * If ConflictingIdError: self._logger.info(f\"Skipping start of TNBA automated process job. Reason: {conflict}\")","title":"Start tnba automated process"},{"location":"logging/services/TNBA-monitor/actions/start_tnba_automated_process/#start-tnba-automated-process-start-of-service","text":"self._logger.info(\"Scheduling TNBA automated process job...\") * If exec on start: self._logger.info(\"TNBA automated process job is going to be executed immediately\") * _run_tickets_polling * If ConflictingIdError: self._logger.info(f\"Skipping start of TNBA automated process job. Reason: {conflict}\")","title":"Start tnba automated process (Start of service)"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/","text":"Append multiple notes to ticket self._logger.info(f\"Posting multiple notes for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") self._logger.info(f\"Posted multiple notes for ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Notes were {notes}. \" f\"Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/#append-multiple-notes-to-ticket","text":"self._logger.info(f\"Posting multiple notes for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") self._logger.info(f\"Posted multiple notes for ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Notes were {notes}. \" f\"Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_next_results_for_ticket_detail/","text":"Get next results for ticket detail self._logger.info( f\"Claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number}...\" ) * If Exception: self._logger.error(f\"An error occurred when claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} -> {e}\") self._logger.info( f\"Got next results for ticket {ticket_id}, detail {detail_id} and service number {service_number}!\" ) * If status not 200: self._logger.error(f\"Error while claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get next results for ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_next_results_for_ticket_detail/#get-next-results-for-ticket-detail","text":"self._logger.info( f\"Claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number}...\" ) * If Exception: self._logger.error(f\"An error occurred when claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} -> {e}\") self._logger.info( f\"Got next results for ticket {ticket_id}, detail {detail_id} and service number {service_number}!\" ) * If status not 200: self._logger.error(f\"Error while claiming next results for ticket {ticket_id}, detail {detail_id} and \" f\"service number {service_number} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get next results for ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_affecting_tickets/","text":"Get open affecting tickets get_tickets","title":"Get open affecting tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_affecting_tickets/#get-open-affecting-tickets","text":"get_tickets","title":"Get open affecting tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_outage_tickets/","text":"Get open outage tickets get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_open_outage_tickets/#get-open-outage-tickets","text":"get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_task_history/","text":"Get ticket task history self._logger.info(f\"Getting task history of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting task history from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got task history of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving task history of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket task history"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_ticket_task_history/#get-ticket-task-history","text":"self._logger.info(f\"Getting task history of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting task history from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got task history of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving task history of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket task history"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_tickets/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get tickets"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/get_tickets/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/resolve_ticket_detail/","text":"Resolve ticket detail self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when resolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\")","title":"Resolve ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/resolve_ticket_detail/#resolve-ticket-detail","text":"self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when resolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\")","title":"Resolve ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache_for_tnba_monitoring/","text":"Get cache for tnba monitoring get_cache","title":"Get cache for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/customer_cache/get_cache_for_tnba_monitoring/#get-cache-for-tnba-monitoring","text":"get_cache","title":"Get cache for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/get_prediction/","text":"Get prediction self._logger.info(f\"Claiming T7 prediction for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when claiming T7 prediction for ticket {ticket_id}. Error: {e}\") self._logger.info(f\"Got T7 prediction for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error while claiming T7 prediction for ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Get prediction"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/get_prediction/#get-prediction","text":"self._logger.info(f\"Claiming T7 prediction for ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when claiming T7 prediction for ticket {ticket_id}. Error: {e}\") self._logger.info(f\"Got T7 prediction for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error while claiming T7 prediction for ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Get prediction"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/post_live_automation_metrics/","text":"Post live automation metrics self._logger.info(f\"Posting live metric for ticket {ticket_id} to T7...\") * If Exception: self._logger.error(f\"An error occurred when posting live metrics for ticket {ticket_id} to T7. Error: {e}\") * If status is OK: self._logger.info(f\"Live metrics posted for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error when posting live metrics for ticket {ticket_id} to T7 in \" f\"{self._config.ENVIRONMENT_NAME.upper()} \" f\"environment. Error: Error {response_status} - {response_body}\")","title":"Post live automation metrics"},{"location":"logging/services/TNBA-monitor/repositories/t7_repository/post_live_automation_metrics/#post-live-automation-metrics","text":"self._logger.info(f\"Posting live metric for ticket {ticket_id} to T7...\") * If Exception: self._logger.error(f\"An error occurred when posting live metrics for ticket {ticket_id} to T7. Error: {e}\") * If status is OK: self._logger.info(f\"Live metrics posted for ticket {ticket_id}!\") * If status not 200: self._logger.error(f\"Error when posting live metrics for ticket {ticket_id} to T7 in \" f\"{self._config.ENVIRONMENT_NAME.upper()} \" f\"environment. Error: Error {response_status} - {response_body}\")","title":"Post live automation metrics"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_all_links_metrics/","text":"Get all links metrics For host in hosts: get_links_metrics_by_host If status is not Ok: self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_all_links_metrics/#get-all-links-metrics","text":"For host in hosts: get_links_metrics_by_host If status is not Ok: self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_edges_for_tnba_monitoring/","text":"Get edges for tnba monitoring For host in host: get_links_with_edge_info If status is Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\") group_links_by_serial","title":"Get edges for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_edges_for_tnba_monitoring/#get-edges-for-tnba-monitoring","text":"For host in host: get_links_with_edge_info If status is Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\") group_links_by_serial","title":"Get edges for tnba monitoring"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_enterprise_events/","text":"Get enterprise events self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) If Exception self._logger.error(f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\") If status is Ok: self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) Else: self._logger.error(f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\")","title":"Get enterprise events"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_enterprise_events/#get-enterprise-events","text":"self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) If Exception self._logger.error(f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\") If status is Ok: self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) Else: self._logger.error(f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\")","title":"Get enterprise events"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/","text":"Get events by serial and interface for host in hosts: for enterprise id in enterprises ids: get_enterprise_events self._logger.warning(f\" Bad status calling get enterprise events for host: {host} and enterprise\" f\"{enterprise_id}. Skipping enterprise events ...\") For events in enterprise events: If not match edge: self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/#get-events-by-serial-and-interface","text":"for host in hosts: for enterprise id in enterprises ids: get_enterprise_events self._logger.warning(f\" Bad status calling get enterprise events for host: {host} and enterprise\" f\"{enterprise_id}. Skipping enterprise events ...\") For events in enterprise events: If not match edge: self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_by_host/","text":"Get links metrics by host self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) If Exception f\"An error occurred when requesting links metrics from Velocloud -> {e}\" self._logger.info(f\"Got links metrics from Velocloud host {host}!\") Else: self._logger.error(f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links metrics by host"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_by_host/#get-links-metrics-by-host","text":"self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) If Exception f\"An error occurred when requesting links metrics from Velocloud -> {e}\" self._logger.info(f\"Got links metrics from Velocloud host {host}!\") Else: self._logger.error(f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links metrics by host"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/","text":"Get links metrics for autoresolve get_all_links_metrics If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") get_events_by_serial_and_interface","title":"Get links metrics for autoresolve"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/#get-links-metrics-for-autoresolve","text":"get_all_links_metrics If not link metrics: self._logger.info(\"List of links metrics arrived empty. Skipping...\") get_events_by_serial_and_interface","title":"Get links metrics for autoresolve"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_with_edge_info/","text":"Get Links with edge info Documentation self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links with edge info"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/get_links_with_edge_info/#get-links-with-edge-info-documentation","text":"self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get Links with edge info Documentation"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/group_links_by_serial/","text":"Group links by serial For link in links: If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If never activated: self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Group links by serial"},{"location":"logging/services/TNBA-monitor/repositories/velocloud_repository/group_links_by_serial/#group-links-by-serial","text":"For link in links: If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If never activated: self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Group links by serial"},{"location":"logging/services/fraud-monitor/actions/_append_note_to_ticket/","text":"Append note to ticket self._logger.info(f\"Appending Fraud note to ticket {ticket_id}\") * If note existe: self._logger.info( f\"No Fraud trouble note will be appended to ticket {ticket_id}. \" f\"A note for this email was already appended to the ticket after the latest re-open or ticket creation.\" ) * If not PRODUCTION: self._logger.info( f\"No Fraud note will be appended to ticket {ticket_id} since the current environment is not production\" ) * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id}. \" f\"Skipping append note ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":" append note to ticket"},{"location":"logging/services/fraud-monitor/actions/_append_note_to_ticket/#append-note-to-ticket","text":"self._logger.info(f\"Appending Fraud note to ticket {ticket_id}\") * If note existe: self._logger.info( f\"No Fraud trouble note will be appended to ticket {ticket_id}. \" f\"A note for this email was already appended to the ticket after the latest re-open or ticket creation.\" ) * If not PRODUCTION: self._logger.info( f\"No Fraud note will be appended to ticket {ticket_id} since the current environment is not production\" ) * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id}. \" f\"Skipping append note ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":"Append note to ticket"},{"location":"logging/services/fraud-monitor/actions/_create_fraud_ticket/","text":"Create fraud ticket self._logger.info(f\"Creating Fraud ticket for client {client_id} and service number {service_number}\") * If not contacts: self._logger.warning(f\"Not found contacts to create the fraud ticket\") * If environment is not PRODUCTION: self._logger.info(f\"No Fraud ticket will be created since the current environment is not production\") * create_fraud_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to create fraud ticket with client id: {client_id} and\" f\"service number: {service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud ticket was successfully created! Ticket ID is {ticket_id}\") * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\") self._logger.info(f\"Forwarding ticket {ticket_id} to HNOC\") * change_detail_work_queue_to_hnoc","title":" create fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_create_fraud_ticket/#create-fraud-ticket","text":"self._logger.info(f\"Creating Fraud ticket for client {client_id} and service number {service_number}\") * If not contacts: self._logger.warning(f\"Not found contacts to create the fraud ticket\") * If environment is not PRODUCTION: self._logger.info(f\"No Fraud ticket will be created since the current environment is not production\") * create_fraud_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to create fraud ticket with client id: {client_id} and\" f\"service number: {service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud ticket was successfully created! Ticket ID is {ticket_id}\") * append_note_to_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Create fraud ticket return FALSE ...\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\") self._logger.info(f\"Forwarding ticket {ticket_id} to HNOC\") * change_detail_work_queue_to_hnoc","title":"Create fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_fraud_monitoring_process/","text":"Fraud monitoring process self._logger.info(f'Processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}') * get_unread_emails * If status is no Ok: self._logger.warning(f\"Bad status calling to get unread emails. Skipping fraud monitor process\") * for email in emails: * If message is none or uid -1: self._logger.error(f\"Invalid message: {email}\") * If not email regex: self._logger.info(f\"Email with msg_uid {msg_uid} is not a fraud warning. Skipping...\") * If not found body: self._logger.error(f\"Email with msg_uid {msg_uid} has an unexpected body\") self._logger.info(f\"Processing email with msg_uid {msg_uid}\") * _process_fraud * If processed and PRODUCTION: * If mark email as read status is Ok: self._logger.error(f\"Could not mark email with msg_uid {msg_uid} as read\") * If processed: self._logger.info(f\"Processed email with msg_uid {msg_uid}\") * Else: self._logger.info(f\"Failed to process email with msg_uid {msg_uid}\") self._logger.info( f'Finished processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}. ' f\"Elapsed time: {round((stop - start) / 60, 2)} minutes\" )","title":" fraud monitoring process"},{"location":"logging/services/fraud-monitor/actions/_fraud_monitoring_process/#fraud-monitoring-process","text":"self._logger.info(f'Processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}') * get_unread_emails * If status is no Ok: self._logger.warning(f\"Bad status calling to get unread emails. Skipping fraud monitor process\") * for email in emails: * If message is none or uid -1: self._logger.error(f\"Invalid message: {email}\") * If not email regex: self._logger.info(f\"Email with msg_uid {msg_uid} is not a fraud warning. Skipping...\") * If not found body: self._logger.error(f\"Email with msg_uid {msg_uid} has an unexpected body\") self._logger.info(f\"Processing email with msg_uid {msg_uid}\") * _process_fraud * If processed and PRODUCTION: * If mark email as read status is Ok: self._logger.error(f\"Could not mark email with msg_uid {msg_uid} as read\") * If processed: self._logger.info(f\"Processed email with msg_uid {msg_uid}\") * Else: self._logger.info(f\"Failed to process email with msg_uid {msg_uid}\") self._logger.info( f'Finished processing all unread email from {self._config.FRAUD_CONFIG[\"inbox_email\"]}. ' f\"Elapsed time: {round((stop - start) / 60, 2)} minutes\" )","title":"Fraud monitoring process"},{"location":"logging/services/fraud-monitor/actions/_get_oldest_fraud_ticket/","text":"Get oldest fraud ticket For ticket in tickets: get_ticket_details If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket details for ticket id: {ticket_id}.\" f\"Skipping get oldest fraud ticket ...\")","title":" get oldest fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_get_oldest_fraud_ticket/#get-oldest-fraud-ticket","text":"For ticket in tickets: get_ticket_details If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket details for ticket id: {ticket_id}.\" f\"Skipping get oldest fraud ticket ...\")","title":"Get oldest fraud ticket"},{"location":"logging/services/fraud-monitor/actions/_process_fraud/","text":"Process fraud get_client_info_by_did If status not Ok: self._logger.warning(f\"Failed to get client info by DID {did}, using default client info\") get_open_fraud_tickets self._logger.warning(f\"fBad status calling to get open fraud tickets for client id: {client_id} and \" f\"service number: {service_number}. Process fraud FALSE ...\") _get_oldest_fraud_ticket If open fraud ticket: self._logger.info(f\"An open Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") If is a task resolved: self._logger.info( f\"Fraud ticket with ID {ticket_id} is open, but the task related to {service_number} is resolved. \" f\"Therefore, the ticket will be considered as Resolved.\" ) Else: _append_note_to_ticket Else: self._logger.info(f\"No open Fraud ticket was found for {service_number}\") get_resolved_fraud_tickets If status is not Ok: self._logger.warning(f\"bad status calling to get resolved fraud tickets for client id: {client_id} \" f\"and service number: {service_number}. Skipping process fraud ...\") _get_oldest_fraud_ticket If resolved fraud ticket: self._logger.info(f\"A resolved Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") _unresolve_task_for_ticket self._logger.info(f\"No open or resolved Fraud ticket was found for {service_number}\") _create_fraud_ticket","title":" process fraud"},{"location":"logging/services/fraud-monitor/actions/_process_fraud/#process-fraud","text":"get_client_info_by_did If status not Ok: self._logger.warning(f\"Failed to get client info by DID {did}, using default client info\") get_open_fraud_tickets self._logger.warning(f\"fBad status calling to get open fraud tickets for client id: {client_id} and \" f\"service number: {service_number}. Process fraud FALSE ...\") _get_oldest_fraud_ticket If open fraud ticket: self._logger.info(f\"An open Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") If is a task resolved: self._logger.info( f\"Fraud ticket with ID {ticket_id} is open, but the task related to {service_number} is resolved. \" f\"Therefore, the ticket will be considered as Resolved.\" ) Else: _append_note_to_ticket Else: self._logger.info(f\"No open Fraud ticket was found for {service_number}\") get_resolved_fraud_tickets If status is not Ok: self._logger.warning(f\"bad status calling to get resolved fraud tickets for client id: {client_id} \" f\"and service number: {service_number}. Skipping process fraud ...\") _get_oldest_fraud_ticket If resolved fraud ticket: self._logger.info(f\"A resolved Fraud ticket was found for {service_number}. Ticket ID: {ticket_id}\") _unresolve_task_for_ticket self._logger.info(f\"No open or resolved Fraud ticket was found for {service_number}\") _create_fraud_ticket","title":"Process fraud"},{"location":"logging/services/fraud-monitor/actions/_unresolve_task_for_ticket/","text":"Unresolve task for ticket self._logger.info(f\"Unresolving task related to {service_number} of Fraud ticket {ticket_id}...\") * If environment not PRODUCTION: self._logger.info( f\"Task related to {service_number} of Fraud ticket {ticket_id} will not be unresolved \" f\"since the current environment is not production\" ) * open_ticket self._logger.warning(f\"Bad status calling to open ticket with ticket id: {ticket_id}. \" f\"Unresolve task for ticket return FALSE\") self._logger.info(f\"Task related to {service_number} of Fraud ticket {ticket_id} was successfully unresolved!\") * append_note_to_ticket self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Unresolve task for ticket return FALSE\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":" unresolve task for ticket"},{"location":"logging/services/fraud-monitor/actions/_unresolve_task_for_ticket/#unresolve-task-for-ticket","text":"self._logger.info(f\"Unresolving task related to {service_number} of Fraud ticket {ticket_id}...\") * If environment not PRODUCTION: self._logger.info( f\"Task related to {service_number} of Fraud ticket {ticket_id} will not be unresolved \" f\"since the current environment is not production\" ) * open_ticket self._logger.warning(f\"Bad status calling to open ticket with ticket id: {ticket_id}. \" f\"Unresolve task for ticket return FALSE\") self._logger.info(f\"Task related to {service_number} of Fraud ticket {ticket_id} was successfully unresolved!\") * append_note_to_ticket self._logger.warning(f\"Bad status calling to append note to ticket id: {ticket_id} and service number:\" f\"{service_number}. Unresolve task for ticket return FALSE\") self._logger.info(f\"Fraud note was successfully appended to ticket {ticket_id}!\")","title":"Unresolve task for ticket"},{"location":"logging/services/fraud-monitor/actions/start_fraud_monitoring/","text":"Start fraud monitoring self._logger.info(\"Scheduling Fraud Monitor job...\") * If exec on start: self._logger.info(\"Fraud Monitor job is going to be executed immediately\") * _fraud_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Fraud Monitoring job. Reason: {conflict}\")","title":"Start fraud monitoring"},{"location":"logging/services/fraud-monitor/actions/start_fraud_monitoring/#start-fraud-monitoring","text":"self._logger.info(\"Scheduling Fraud Monitor job...\") * If exec on start: self._logger.info(\"Fraud Monitor job is going to be executed immediately\") * _fraud_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Fraud Monitoring job. Reason: {conflict}\")","title":"Start fraud monitoring"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue/","text":"Change detail work queue self._logger.info( f\"Changing task result of serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) * If Exception: self._logger.error(f\"An error occurred when changing task result of serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\") * If status ok: self._logger.info(f\"Task result of detail serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\") * Else: self._logger.error(f\"Error while changing task result of serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue/#change-detail-work-queue","text":"self._logger.info( f\"Changing task result of serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) * If Exception: self._logger.error(f\"An error occurred when changing task result of serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\") * If status ok: self._logger.info(f\"Task result of detail serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\") * Else: self._logger.error(f\"Error while changing task result of serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/","text":"Change detail work queue to hnoc change_detail_work_queue","title":"Change detail work queue to hnoc"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/#change-detail-work-queue-to-hnoc","text":"change_detail_work_queue","title":"Change detail work queue to hnoc"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/create_fraud_ticket/","text":"Create fraud ticket self._logger.info( f\"Creating fraud ticket for service number {service_number} that belongs to client {client_id}...\" ) * If Exception: f\"An error occurred when creating fraud ticket for service number {service_number} \" f\"that belongs to client {client_id} -> {e}\" * If status ok: self._logger.info(f\"Fraud ticket for service number {service_number} that belongs to client {client_id} created!\") * Else: self._logger.error(f\"Error while creating fraud ticket for service number {service_number} that belongs to client \" f\"{client_id} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Create fraud ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/create_fraud_ticket/#create-fraud-ticket","text":"self._logger.info( f\"Creating fraud ticket for service number {service_number} that belongs to client {client_id}...\" ) * If Exception: f\"An error occurred when creating fraud ticket for service number {service_number} \" f\"that belongs to client {client_id} -> {e}\" * If status ok: self._logger.info(f\"Fraud ticket for service number {service_number} that belongs to client {client_id} created!\") * Else: self._logger.error(f\"Error while creating fraud ticket for service number {service_number} that belongs to client \" f\"{client_id} in {self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Create fraud ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_client_info_by_did/","text":"Get client info by did self._logger.info(f\"Getting client info by DID {did}...\") * If Exception: self._logger.error(f\"An error occurred when getting client info by DID {did} -> {e}\") * If status ok: self._logger.info(f\"Got client info by DID {did}!\") * Else: self._logger.error(f\"Error while getting client info by DID {did} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get client info by did"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_client_info_by_did/#get-client-info-by-did","text":"self._logger.info(f\"Getting client info by DID {did}...\") * If Exception: self._logger.error(f\"An error occurred when getting client info by DID {did} -> {e}\") * If status ok: self._logger.info(f\"Got client info by DID {did}!\") * Else: self._logger.error(f\"Error while getting client info by DID {did} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get client info by did"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_fraud_tickets/","text":"Get fraud tickets get_tickets","title":"Get fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_fraud_tickets/#get-fraud-tickets","text":"get_tickets","title":"Get fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_open_fraud_tickets/","text":"Get open fraud tickets get_fraud_tickets","title":"Get open fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_open_fraud_tickets/#get-open-fraud-tickets","text":"get_fraud_tickets","title":"Get open fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_resolved_fraud_tickets/","text":"Get resolved fraud tickets get_fraud_tickets","title":"Get resolved fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_resolved_fraud_tickets/#get-resolved-fraud-tickets","text":"get_fraud_tickets","title":"Get resolved fraud tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_tickets/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get tickets"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/get_tickets/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/fraud-monitor/repositories/bruin_repository/open_ticket/#open-ticket","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/fraud-monitor/repositories/notifications_repository/get_unread_emails/","text":"Get unread emails self._logger.info( f\"Getting the unread emails from the inbox of {email_account} sent from the users: \" f\"{email_filter}\" ) * If Exception: self._logger.error(f\"An error occurred while getting the unread emails from the inbox of {email_account} -> {e}\") * If status ok: self._logger.info(f\"Got the unread emails from the inbox of {email_account}\") * Else: self._logger.error(f\"Error getting the unread emails from the inbox of {email_account} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get unread emails"},{"location":"logging/services/fraud-monitor/repositories/notifications_repository/get_unread_emails/#get-unread-emails","text":"self._logger.info( f\"Getting the unread emails from the inbox of {email_account} sent from the users: \" f\"{email_filter}\" ) * If Exception: self._logger.error(f\"An error occurred while getting the unread emails from the inbox of {email_account} -> {e}\") * If status ok: self._logger.info(f\"Got the unread emails from the inbox of {email_account}\") * Else: self._logger.error(f\"Error getting the unread emails from the inbox of {email_account} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get unread emails"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_add_device_to_tickets_mapping/","text":"Add device to tickets mapping get_open_affecting_tickets If status is not Ok: self._logger.warning(f\"Bad status calling to get open affecting ticket to serial number \" f\"{serial_number}. Skipping add device to ticket mapping ...\") If not affecting ticket: self._logger.info( f\"No affecting tickets were found for device {serial_number} when building the mapping between \" f\"this serial and tickets.\" ) get_ticket_details self._logger.warning(f\"Bad status calling to get ticket details to ticket id: {ticket_id}.\" f\"Skipping add devices to ticket mapping ...\")","title":" add device to tickets mapping"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_add_device_to_tickets_mapping/#add-device-to-tickets-mapping","text":"get_open_affecting_tickets If status is not Ok: self._logger.warning(f\"Bad status calling to get open affecting ticket to serial number \" f\"{serial_number}. Skipping add device to ticket mapping ...\") If not affecting ticket: self._logger.info( f\"No affecting tickets were found for device {serial_number} when building the mapping between \" f\"this serial and tickets.\" ) get_ticket_details self._logger.warning(f\"Bad status calling to get ticket details to ticket id: {ticket_id}.\" f\"Skipping add devices to ticket mapping ...\")","title":"Add device to tickets mapping"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_affecting_monitoring_process/","text":"Affecting monitoring process self._logger.info(f\"Starting Hawkeye Affecting Monitor!\") * get_cache_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeye affecting monitor process ...\") * get_tests_results_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad request get test results for affectin monitor for probe uids: {probe_uids}.\" f\"Skipping hawkeye affecting monitor ...\") self._logger.info( f\"Looking for Service Affecting tickets for {len(cached_devices_mapped_to_tests_results)} devices...\" ) * _add_device_to_tickets_mapping self._logger.info(f\"Processing {len(cached_devices_mapped_to_tests_results)} devices...\") * _process_device self._logger.info(f\"Hawkeye Affecting Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":" affecting monitoring process"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_affecting_monitoring_process/#affecting-monitoring-process","text":"self._logger.info(f\"Starting Hawkeye Affecting Monitor!\") * get_cache_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeye affecting monitor process ...\") * get_tests_results_for_affecting_monitoring * If status is not Ok: self._logger.warning(f\"Bad request get test results for affectin monitor for probe uids: {probe_uids}.\" f\"Skipping hawkeye affecting monitor ...\") self._logger.info( f\"Looking for Service Affecting tickets for {len(cached_devices_mapped_to_tests_results)} devices...\" ) * _add_device_to_tickets_mapping self._logger.info(f\"Processing {len(cached_devices_mapped_to_tests_results)} devices...\") * _process_device self._logger.info(f\"Hawkeye Affecting Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":"Affecting monitoring process"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_append_new_notes_for_device/","text":"Append new notes for device If serial not in ticket: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so no notes can be posted to any ticket. Skipping...\" ) If not notes to append: self._logger.info(f\"No notes to append for serial {serial_number} were found. Skipping...\") If environment is PRODUCTION: self._logger.info( f\"{len(notes_to_append)} affecting notes to append to ticket {ticket_id} were found, but the current \" \"environment is not PRODUCTION. Skipping...\" ) self._logger.info( f\"Posting {len(notes_to_append)} affecting notes to ticket {ticket_id} (serial: {serial_number})...\" )","title":" append new notes for device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_append_new_notes_for_device/#append-new-notes-for-device","text":"If serial not in ticket: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so no notes can be posted to any ticket. Skipping...\" ) If not notes to append: self._logger.info(f\"No notes to append for serial {serial_number} were found. Skipping...\") If environment is PRODUCTION: self._logger.info( f\"{len(notes_to_append)} affecting notes to append to ticket {ticket_id} were found, but the current \" \"environment is not PRODUCTION. Skipping...\" ) self._logger.info( f\"Posting {len(notes_to_append)} affecting notes to ticket {ticket_id} (serial: {serial_number})...\" )","title":"Append new notes for device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_device/","text":"Process device self._logger.info(f\"Processing device {serial_number}...\") * for test result in test results: * If test result passed: * _process_passed_test_result * If test result failed: * _process_failed_test_result * Else: self._logger.info( f'Test result {test_result[\"summary\"][\"id\"]} has state {test_result[\"summary\"][\"status\"].upper()}. ' \"Skipping...\" ) * _append_new_notes_for_device self._logger.info(f\"Finished processing device {serial_number}!\")","title":" process device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_device/#process-device","text":"self._logger.info(f\"Processing device {serial_number}...\") * for test result in test results: * If test result passed: * _process_passed_test_result * If test result failed: * _process_failed_test_result * Else: self._logger.info( f'Test result {test_result[\"summary\"][\"id\"]} has state {test_result[\"summary\"][\"status\"].upper()}. ' \"Skipping...\" ) * _append_new_notes_for_device self._logger.info(f\"Finished processing device {serial_number}!\")","title":"Process device"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_failed_test_result/","text":"Process failed test result self._logger.info( f\"Processing FAILED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current FAILED state for test type {test_type} will be ignored. Skipping...\" ) * If not affecting ticket: * If working environment is not PRODUCTION: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}, but the current environment is not PRODUCTION. Skipping ticket creation...\" ) self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}. Creating affecting ticket..\" ) * create_affecting_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling create affecting ticket to serial: {serial_number}.\" f\"Skipping process test failed ...\") self._logger.info( f\"Affecting ticket created for serial {serial_number} (ID: {ticket_id}). A new note reporting the \" f\"current FAILED state for test type {test_type} will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Serial {serial_number} is under affecting ticket {ticket_id} and some troubles were spotted for \" f\"test type {test_type}.\" ) * If detail resolved ticket: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} is \" f\"currently unresolved and a FAILED state was spotted. Unresolving detail...\" ) * unresolve_ticket_detail * If status is not Ok: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"could not be unresolved. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * Else: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"was unresolved successfully. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"A new note reporting the current FAILED state for this test type will be built and appended \" \"to the ticket later on.\" ) * Else: * If passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. A new note reporting the current FAILED state for this test \" \"type will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a previous FAILED state. No new notes will be built to report the current \" \"FAILED state.\" ) self._logger.info(f\"Finished processing FAILED test result {test_result_id}!\")","title":" process failed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_failed_test_result/#process-failed-test-result","text":"self._logger.info( f\"Processing FAILED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current FAILED state for test type {test_type} will be ignored. Skipping...\" ) * If not affecting ticket: * If working environment is not PRODUCTION: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}, but the current environment is not PRODUCTION. Skipping ticket creation...\" ) self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and some troubles were spotted for \" f\"test type {test_type}. Creating affecting ticket..\" ) * create_affecting_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling create affecting ticket to serial: {serial_number}.\" f\"Skipping process test failed ...\") self._logger.info( f\"Affecting ticket created for serial {serial_number} (ID: {ticket_id}). A new note reporting the \" f\"current FAILED state for test type {test_type} will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Serial {serial_number} is under affecting ticket {ticket_id} and some troubles were spotted for \" f\"test type {test_type}.\" ) * If detail resolved ticket: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} is \" f\"currently unresolved and a FAILED state was spotted. Unresolving detail...\" ) * unresolve_ticket_detail * If status is not Ok: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"could not be unresolved. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * Else: self._logger.info( f\"Ticket detail of affecting ticket {ticket_id} that is related to serial {serial_number} \" \"was unresolved successfully. A note reporting the spotted FAILED state will be built and \" \"appended to the ticket later on.\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"A new note reporting the current FAILED state for this test type will be built and appended \" \"to the ticket later on.\" ) * Else: * If passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. A new note reporting the current FAILED state for this test \" \"type will be built and appended to the ticket later on.\" ) * Else: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a previous FAILED state. No new notes will be built to report the current \" \"FAILED state.\" ) self._logger.info(f\"Finished processing FAILED test result {test_result_id}!\")","title":"Process failed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_passed_test_result/","text":"Process passed test result self._logger.info( f\"Processing PASSED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current PASSED state for test type {test_type} will be ignored. Skipping...\" ) * If affecting tickets: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and all thresholds are normal for \" f\"test type {test_type}. Skipping...\" ) * If affecting ticket detail is solved: self._logger.info( f\"Serial {serial_number} is under an affecting ticket (ID {ticket_id}) whose ticket detail is resolved \" f\"and all thresholds are normal for test type {test_type}, so the current PASSED state will not be \" \"reported. Skipping...\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"Skipping...\" ) * If is passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. Skipping...\" ) self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a FAILED state. A new note reporting the current PASSED state will be built and appended \" \"to the ticket later on.\" ) self._logger.info(f\"Finished processing PASSED test result {test_result_id}!\")","title":" process passed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/_process_passed_test_result/#process-passed-test-result","text":"self._logger.info( f\"Processing PASSED test result {test_result_id} (type: {test_type}) that was run for serial \" f\"{serial_number}...\" ) * If serial not in tickets: self._logger.info( f\"Serial {serial_number} could not be added to the tickets mapping at the beginning of the \" f\"process, so the current PASSED state for test type {test_type} will be ignored. Skipping...\" ) * If affecting tickets: self._logger.info( f\"Serial {serial_number} is not under any affecting ticket and all thresholds are normal for \" f\"test type {test_type}. Skipping...\" ) * If affecting ticket detail is solved: self._logger.info( f\"Serial {serial_number} is under an affecting ticket (ID {ticket_id}) whose ticket detail is resolved \" f\"and all thresholds are normal for test type {test_type}, so the current PASSED state will not be \" \"reported. Skipping...\" ) * If not last note: self._logger.info( f\"No note was found for serial {serial_number} and test type {test_type} in ticket {ticket_id}. \" \"Skipping...\" ) * If is passed note: self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" f\"corresponds to a PASSED state. Skipping...\" ) self._logger.info( f\"Last note found for serial {serial_number} and test type {test_type} in ticket {ticket_id} \" \"corresponds to a FAILED state. A new note reporting the current PASSED state will be built and appended \" \"to the ticket later on.\" ) self._logger.info(f\"Finished processing PASSED test result {test_result_id}!\")","title":"Process passed test result"},{"location":"logging/services/hawkeye-affecting-monitor/actions/start_hawkeye_affecting_monitoring/","text":"Start hawkeye affecting monitoring (Start service) self._logger.info(\"Scheduling Hawkeye Affecting Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Affecting Monitor job is going to be executed immediately\") * _affecting_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Affecting Monitoring job. Reason: {conflict}\")","title":"Start hawkeye affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/actions/start_hawkeye_affecting_monitoring/#start-hawkeye-affecting-monitoring-start-service","text":"self._logger.info(\"Scheduling Hawkeye Affecting Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Affecting Monitor job is going to be executed immediately\") * _affecting_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Affecting Monitoring job. Reason: {conflict}\")","title":"Start hawkeye affecting monitoring (Start service)"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/","text":"Append multiple notes to ticket self._logger.info(f\"Posting multiple notes to ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") * If status ok: self._logger.info(f\"Posted multiple notes to ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Notes were {notes}. Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/append_multiple_notes_to_ticket/#append-multiple-notes-to-ticket","text":"self._logger.info(f\"Posting multiple notes to ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when appending multiple ticket notes to ticket {ticket_id}. \" f\"Notes: {notes}. Error: {e}\") * If status ok: self._logger.info(f\"Posted multiple notes to ticket {ticket_id}!\") * Else: self._logger.error(f\"Error while appending multiple notes to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Notes were {notes}. Error: Error {response_status} - {response_body}\")","title":"Append multiple notes to ticket"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/","text":"Create Affecting ticket Documentation self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create affecting ticket"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/#create-affecting-ticket-documentation","text":"self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create Affecting ticket Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/","text":"Get affecting tickets Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/#get-affecting-tickets","text":"Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/","text":"Get open affecting tickets Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/#get-open-affecting-tickets","text":"Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get Ticket details Documentation self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get ticket details"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details-documentation","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get Ticket details Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_tickets/","text":"Get tickets Documentation if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/get_tickets/#get-tickets-documentation","text":"if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/unresolve_ticket_detail/","text":"Unesolve ticket detail self._logger.info(f\"Unresolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unresolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} unresolved successfully!\") * Else: self._logger.error(f\"Error while unresolving detail {detail_id} of affecting ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Unresolve ticket detail"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/bruin_repository/unresolve_ticket_detail/#unesolve-ticket-detail","text":"self._logger.info(f\"Unresolving detail {detail_id} of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unresolving detail {detail_id} of affecting ticket {ticket_id} -> {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} unresolved successfully!\") * Else: self._logger.error(f\"Error while unresolving detail {detail_id} of affecting ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Unesolve ticket detail"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/","text":"Get cache for affecting monitoring get_cache","title":"Get cache for affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/#get-cache-for-affecting-monitoring","text":"get_cache","title":"Get cache for affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results/","text":"Get tests results Get probes self._logger.info(f\"Getting tests results for {len(probe_uids)} probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting tests results from Hawkeye -> {e}\") * If status ok: self._logger.info(f\"Error while retrieving tests results: Error {response_status} - {response_body}\") * Else: self._logger.error(f\"Got all tests results for {len(probe_uids)} probes from Hawkeye!\")","title":"Get tests results"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results/#get-tests-results","text":"","title":"Get tests results"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results/#get-probes","text":"self._logger.info(f\"Getting tests results for {len(probe_uids)} probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting tests results from Hawkeye -> {e}\") * If status ok: self._logger.info(f\"Error while retrieving tests results: Error {response_status} - {response_body}\") * Else: self._logger.error(f\"Got all tests results for {len(probe_uids)} probes from Hawkeye!\")","title":"Get probes"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results_for_affecting_monitoring/","text":"Get tests results for affecting monitoring get_tests_results","title":"Get tests results for affecting monitoring"},{"location":"logging/services/hawkeye-affecting-monitor/repositories/hawkeye_repository/get_tests_results_for_affecting_monitoring/#get-tests-results-for-affecting-monitoring","text":"get_tests_results","title":"Get tests results for affecting monitoring"},{"location":"logging/services/hawkeye-outage-monitor/actions/_append_triage_note_if_needed/","text":"Append triage note if needed self._logger.info(f\"Checking ticket {ticket_id} to see if device {serial_number} has a triage note already...\") * get_ticket_details * If status is nor Ok: self._logger.warning(f\"Bad status calling to get ticket details. Skipping append triage note ...\") * If triage note: self._logger.info( f\"Triage note already exists in ticket {ticket_id} for serial {serial_number}, so no triage \" f\"note will be appended.\" ) self._logger.info( f\"No triage note was found for serial {serial_number} in ticket {ticket_id}. Appending triage note...\" ) * append_triage_note_to_ticket self._logger.info(f\"Triage note for device {serial_number} appended to ticket {ticket_id}!\")","title":" append triage note if needed"},{"location":"logging/services/hawkeye-outage-monitor/actions/_append_triage_note_if_needed/#append-triage-note-if-needed","text":"self._logger.info(f\"Checking ticket {ticket_id} to see if device {serial_number} has a triage note already...\") * get_ticket_details * If status is nor Ok: self._logger.warning(f\"Bad status calling to get ticket details. Skipping append triage note ...\") * If triage note: self._logger.info( f\"Triage note already exists in ticket {ticket_id} for serial {serial_number}, so no triage \" f\"note will be appended.\" ) self._logger.info( f\"No triage note was found for serial {serial_number} in ticket {ticket_id}. Appending triage note...\" ) * append_triage_note_to_ticket self._logger.info(f\"Triage note for device {serial_number} appended to ticket {ticket_id}!\")","title":"Append triage note if needed"},{"location":"logging/services/hawkeye-outage-monitor/actions/_map_probes_info_with_customer_cache/","text":"Map probes info with customer cache for probe in probes: If not cached info: self._logger.info(f\"No cached info was found for device {serial_number}. Skipping...\")","title":" map probes info with customer cache"},{"location":"logging/services/hawkeye-outage-monitor/actions/_map_probes_info_with_customer_cache/#map-probes-info-with-customer-cache","text":"for probe in probes: If not cached info: self._logger.info(f\"No cached info was found for device {serial_number}. Skipping...\")","title":"Map probes info with customer cache"},{"location":"logging/services/hawkeye-outage-monitor/actions/_outage_monitoring_process/","text":"Outage monitoring process self._logger.info(f\"Starting Hawkeye Outage Monitor!\") * get_cache_for_outage_monitoring * If status is not ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeyey outage monitoring process ...\") * get_probes * If status is not ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If outage devices: self._logger.info( f\"{len(outage_devices)} devices were detected in outage state. \" \"Scheduling re-check job for all of them...\" ) * _schedule_recheck_job_for_devices * Else: self._logger.info(\"No devices were detected in outage state. Re-check job won't be scheduled\") * If healthy edges: self._logger.info( f\"{len(healthy_devices)} devices were detected in healthy state. Running autoresolve for all of them\" ) * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state. Autoresolve won't be triggered\") self._logger.info(f\"Hawkeye Outage Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":" outage monitoring process"},{"location":"logging/services/hawkeye-outage-monitor/actions/_outage_monitoring_process/#outage-monitoring-process","text":"self._logger.info(f\"Starting Hawkeye Outage Monitor!\") * get_cache_for_outage_monitoring * If status is not ok: self._logger.warning(f\"Bad status calling to get cache. Skipping hawkeyey outage monitoring process ...\") * get_probes * If status is not ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If outage devices: self._logger.info( f\"{len(outage_devices)} devices were detected in outage state. \" \"Scheduling re-check job for all of them...\" ) * _schedule_recheck_job_for_devices * Else: self._logger.info(\"No devices were detected in outage state. Re-check job won't be scheduled\") * If healthy edges: self._logger.info( f\"{len(healthy_devices)} devices were detected in healthy state. Running autoresolve for all of them\" ) * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state. Autoresolve won't be triggered\") self._logger.info(f\"Hawkeye Outage Monitor process finished! Took {round((stop - start) / 60, 2)} minutes\")","title":"Outage monitoring process"},{"location":"logging/services/hawkeye-outage-monitor/actions/_recheck_devices_for_ticket_creation/","text":"Recheck devices for ticket creation self._logger.info(f\"Re-checking {len(devices)} devices in outage state prior to ticket creation...\") * get_probes * If status not Ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If environment not PRODUCTION: self._logger.info( f\"Process cannot keep going as the current environment is {working_environment.upper()}. \" f\"Healthy devices: {len(healthy_devices)} | Outage devices: {len(devices_still_in_outage)}\" ) * If devices still outage: self._logger.info( f\"{len(devices_still_in_outage)} devices were detected as still in outage state after re-check.\" ) * for device in devices: self._logger.info(f\"Attempting outage ticket creation for faulty device {serial_number}...\") * create_outage_ticket * If create outage ticket status is Ok: self._logger.info(f\"Outage ticket created for device {serial_number}! Ticket ID: {ticket_id}\") self._logger.info(f\"Appending triage note to outage ticket {ticket_id}...\") * append_triage_note_to_ticket * If create outage ticket status is 409: self._logger.info( f\"Faulty device {serial_number} already has an outage ticket in progress (ID = {ticket_id}).\" ) * _append_triage_note_if_needed * If create outage ticket status is 471: self._logger.info( f\"Faulty device {serial_number} has a resolved outage ticket (ID = {ticket_id}). \" \"Re-opening ticket...\" ) * _reopen_outage_ticket * If create outage ticket status is 472: self._logger.info( f\"[outage-recheck] Faulty device {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\" ) * append_note_to_ticket * If create outage ticket status is 473: self._logger.info( f\"[outage-recheck] There is a resolve outage ticket for the same location of faulty device \" f\"{serial_number} (ticket ID = {ticket_id}). The ticket was\" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\" ) * append_triage_note * Else: self._logger.info( \"No devices were detected in outage state after re-check. Outage tickets won't be created\" ) * If healthy devices: self._logger.info(f\"{len(healthy_devices)} devices were detected in healthy state after re-check.\") * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state after re-check.\")","title":" recheck devices for ticket creation"},{"location":"logging/services/hawkeye-outage-monitor/actions/_recheck_devices_for_ticket_creation/#recheck-devices-for-ticket-creation","text":"self._logger.info(f\"Re-checking {len(devices)} devices in outage state prior to ticket creation...\") * get_probes * If status not Ok: self._logger.warning(f\"Bad status calling to get probes. Skipping hawkeye outage monitoring process ...\") * If not probes: self._logger.info(\"The list of probes arrived empty. Skipping monitoring process...\") * If not active probes: self._logger.info(\"All probes were detected as inactive. Skipping monitoring process...\") * _map_probes_info_with_customer_cache * If environment not PRODUCTION: self._logger.info( f\"Process cannot keep going as the current environment is {working_environment.upper()}. \" f\"Healthy devices: {len(healthy_devices)} | Outage devices: {len(devices_still_in_outage)}\" ) * If devices still outage: self._logger.info( f\"{len(devices_still_in_outage)} devices were detected as still in outage state after re-check.\" ) * for device in devices: self._logger.info(f\"Attempting outage ticket creation for faulty device {serial_number}...\") * create_outage_ticket * If create outage ticket status is Ok: self._logger.info(f\"Outage ticket created for device {serial_number}! Ticket ID: {ticket_id}\") self._logger.info(f\"Appending triage note to outage ticket {ticket_id}...\") * append_triage_note_to_ticket * If create outage ticket status is 409: self._logger.info( f\"Faulty device {serial_number} already has an outage ticket in progress (ID = {ticket_id}).\" ) * _append_triage_note_if_needed * If create outage ticket status is 471: self._logger.info( f\"Faulty device {serial_number} has a resolved outage ticket (ID = {ticket_id}). \" \"Re-opening ticket...\" ) * _reopen_outage_ticket * If create outage ticket status is 472: self._logger.info( f\"[outage-recheck] Faulty device {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\" ) * append_note_to_ticket * If create outage ticket status is 473: self._logger.info( f\"[outage-recheck] There is a resolve outage ticket for the same location of faulty device \" f\"{serial_number} (ticket ID = {ticket_id}). The ticket was\" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\" ) * append_triage_note * Else: self._logger.info( \"No devices were detected in outage state after re-check. Outage tickets won't be created\" ) * If healthy devices: self._logger.info(f\"{len(healthy_devices)} devices were detected in healthy state after re-check.\") * _run_ticket_autoresolve * Else: self._logger.info(\"No devices were detected in healthy state after re-check.\")","title":"Recheck devices for ticket creation"},{"location":"logging/services/hawkeye-outage-monitor/actions/_reopen_outage_ticket/","text":"Reopen outage ticket self._logger.info(f\"Reopening Hawkeye outage ticket {ticket_id}...\") * get_ticket_details * If status is not ok: self._logger.warning(f\"Bad status calling get ticket details. Skipping reopen outage ticket ...\") * open_ticket * If status is not Ok: self._logger.error(f\"[outage-ticket-creation] Outage ticket {ticket_id} reopening failed.\") * Else: self._logger.info(f\"Hawkeye outage ticket {ticket_id} reopening succeeded.\") * append_note_to_ticket","title":" reopen outage ticket"},{"location":"logging/services/hawkeye-outage-monitor/actions/_reopen_outage_ticket/#reopen-outage-ticket","text":"self._logger.info(f\"Reopening Hawkeye outage ticket {ticket_id}...\") * get_ticket_details * If status is not ok: self._logger.warning(f\"Bad status calling get ticket details. Skipping reopen outage ticket ...\") * open_ticket * If status is not Ok: self._logger.error(f\"[outage-ticket-creation] Outage ticket {ticket_id} reopening failed.\") * Else: self._logger.info(f\"Hawkeye outage ticket {ticket_id} reopening succeeded.\") * append_note_to_ticket","title":"Reopen outage ticket"},{"location":"logging/services/hawkeye-outage-monitor/actions/_run_ticket_autoresolve/","text":"Run ticket autoresolve self._logger.info(f\"Starting autoresolve for device {serial_number}...\") * get_open_outage_tickets * If status is not Ok: self._logger.warning(f\"Bad status calling to get open outage tickets. \" f\"Skipping run ticket autoresolve ...\") * If not outage tickets: self._logger.info( f\"No open outage ticket found for device {serial_number}. \" f\"Skipping autoresolve...\" ) * If ticket created by automation: self._logger.info( f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\" ) * get_ticket_details * If status not Ok: self._logger.warning(f\"Bad status calling to get ticket details. \" f\"Skipping run ticket autoresolve ...\") * If was last outage detected recently: self._logger.info( f\"Device {device} has been in outage state for a long time, so detail {client_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\" ) * If can't ticket be autoresolve one more time: self._logger.info( f\"Limit to autoresolve ticket {outage_ticket_id} linked to device \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) * If detail is resolved: self._logger.info( f\"Detail {ticket_detail_id} of ticket {outage_ticket_id} is already resolved. \" f\"Skipping autoresolve...\" ) * If working environment PRODUCTION: self._logger.info( f\"Skipping autoresolve for device {serial_number} since the \" f\"current environment is {working_environment.upper()}.\" ) self._logger.info( f\"Autoresolving detail {ticket_detail_id} (serial: {serial_number}) of ticket {outage_ticket_id}...\" ) * unpause_ticket_detail * resolve_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket. Skipping autoresolve ...\") * append_autoresolve_note_to_ticket.md self._logger.info(f\"Ticket {outage_ticket_id} linked to device {serial_number} was autoresolved!\")","title":" run ticket autoresolve"},{"location":"logging/services/hawkeye-outage-monitor/actions/_run_ticket_autoresolve/#run-ticket-autoresolve","text":"self._logger.info(f\"Starting autoresolve for device {serial_number}...\") * get_open_outage_tickets * If status is not Ok: self._logger.warning(f\"Bad status calling to get open outage tickets. \" f\"Skipping run ticket autoresolve ...\") * If not outage tickets: self._logger.info( f\"No open outage ticket found for device {serial_number}. \" f\"Skipping autoresolve...\" ) * If ticket created by automation: self._logger.info( f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\" ) * get_ticket_details * If status not Ok: self._logger.warning(f\"Bad status calling to get ticket details. \" f\"Skipping run ticket autoresolve ...\") * If was last outage detected recently: self._logger.info( f\"Device {device} has been in outage state for a long time, so detail {client_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\" ) * If can't ticket be autoresolve one more time: self._logger.info( f\"Limit to autoresolve ticket {outage_ticket_id} linked to device \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) * If detail is resolved: self._logger.info( f\"Detail {ticket_detail_id} of ticket {outage_ticket_id} is already resolved. \" f\"Skipping autoresolve...\" ) * If working environment PRODUCTION: self._logger.info( f\"Skipping autoresolve for device {serial_number} since the \" f\"current environment is {working_environment.upper()}.\" ) self._logger.info( f\"Autoresolving detail {ticket_detail_id} (serial: {serial_number}) of ticket {outage_ticket_id}...\" ) * unpause_ticket_detail * resolve_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling resolve ticket. Skipping autoresolve ...\") * append_autoresolve_note_to_ticket.md self._logger.info(f\"Ticket {outage_ticket_id} linked to device {serial_number} was autoresolved!\")","title":"Run ticket autoresolve"},{"location":"logging/services/hawkeye-outage-monitor/actions/_schedule_recheck_job_for_devices/","text":"Schedule recheck job for devices self._logger.info(f\"Scheduling recheck job for {len(devices)} devices in outage state...\") * _recheck_devices_for_ticket_creation self._logger.info(f\"Devices scheduled for recheck successfully\")","title":" schedule recheck job for devices"},{"location":"logging/services/hawkeye-outage-monitor/actions/_schedule_recheck_job_for_devices/#schedule-recheck-job-for-devices","text":"self._logger.info(f\"Scheduling recheck job for {len(devices)} devices in outage state...\") * _recheck_devices_for_ticket_creation self._logger.info(f\"Devices scheduled for recheck successfully\")","title":"Schedule recheck job for devices"},{"location":"logging/services/hawkeye-outage-monitor/actions/start_hawkeye_outage_monitoring/","text":"Start hawkeye outage monitoring (Start of service) self._logger.info(\"Scheduling Hawkeye Outage Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Outage Monitoring job. Reason: {conflict}\")","title":"Start hawkeye outage monitoring"},{"location":"logging/services/hawkeye-outage-monitor/actions/start_hawkeye_outage_monitoring/#start-hawkeye-outage-monitoring-start-of-service","text":"self._logger.info(\"Scheduling Hawkeye Outage Monitor job...\") * If exec on start: self._logger.info(\"Hawkeye Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process If ConflictingIdError: self._logger.info(f\"Skipping start of Hawkeye Outage Monitoring job. Reason: {conflict}\")","title":"Start hawkeye outage monitoring (Start of service)"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket","text":"append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_triage_note/","text":"Append triage note If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/append_triage_note/#append-triage-note","text":"If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/create_outage_ticket/","text":"Create outage ticket Documentation self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/create_outage_ticket/#create-outage-ticket-documentation","text":"self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket Documentation"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/","text":"Get open outage tickets get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/#get-open-outage-tickets","text":"get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_outage_tickets/","text":"Get outage tickets * get_ticket","title":"Get outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_outage_tickets/#get-outage-tickets","text":"* get_ticket","title":"Get outage tickets"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/open_ticket/#open-ticket","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket detail self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket-detail","text":"self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket detail"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/hawkeye-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/","text":"Get cache for outage monitoring get_cache","title":"Get cache for outage monitoring"},{"location":"logging/services/hawkeye-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/#get-cache-for-outage-monitoring","text":"get_cache","title":"Get cache for outage monitoring"},{"location":"logging/services/hawkeye-outage-monitor/repositories/hawkeye_repository/get_probes/","text":"Get probes self._logger.info(f\"Getting all probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting all probes from Hawkeye -> {e}\") * If status ok: self._logger.info(\"Got all probes from Hawkeye!\") * Else: self._logger.error(f\"Error while retrieving probes: Error {response_status} - {response_body}\")","title":"Get probes"},{"location":"logging/services/hawkeye-outage-monitor/repositories/hawkeye_repository/get_probes/#get-probes","text":"self._logger.info(f\"Getting all probes from Hawkeye...\") * If Exception: self._logger.error(f\"An error occurred when requesting all probes from Hawkeye -> {e}\") * If status ok: self._logger.info(\"Got all probes from Hawkeye!\") * Else: self._logger.error(f\"Error while retrieving probes: Error {response_status} - {response_body}\")","title":"Get probes"},{"location":"logging/services/intermapper-outage-monitor/actions/_autoresolve_ticket/","text":"Autoresolve ticket self._logger.info(\"Starting the autoresolve process\") * get_ticket_basic_info * If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket basic info for client id: {client_id}.\" f\"Skipping autoresolve ticket ...\") self._logger.info(f\"Found {len(tickets_body)} tickets for circuit ID {circuit_id} from bruin:\") self._logger.info(tickets_body) * For ticket in tickets: self._logger.info( f\"Posting InterMapper UP note to task of ticket id {ticket_id} \" f\"related to circuit ID {circuit_id}...\" ) * _append_intermapper_up_note * If status is not Ok: self._logger.warning(f\"Bad status calling to append intermapper note to ticket id: {ticket_id}.\" f\"Skipping autoresolve ticket ....\") * get_tickets * If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket for client id: {client_id} and \" f\"ticket id: {ticket_id}. Skipping autoresolve ticket ...\") * If no product category: self._logger.info(f\"Ticket {ticket_id} couldn't be found in Bruin. Skipping autoresolve...\") self._logger.info(f\"Product category of ticket {ticket_id} from bruin is {product_category}\") * If product category whitelisted: self._logger.info( f\"At least one product category of ticket {ticket_id} from the \" f\"following list is not one of the whitelisted categories for \" f\"auto-resolve: {product_category}. Skipping autoresolve ...\" ) self._logger.info(f\"Checking to see if ticket {ticket_id} can be autoresolved\") * get_ticket_details * If status is not Ok: self._logger.warning(f\"Bad status calling get ticket details to ticket id: {ticket_id}.\" f\"Skipping autoresolve ...\") * If not detected outage recently: self._logger.info( f\"Edge has been in outage state for a long time, so detail {ticket_detail_id} \" f\"(circuit ID {circuit_id}) of ticket {ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\" ) * If can't autoresolve one more time: self._logger.info( f\"Limit to autoresolve detail {ticket_detail_id} (circuit ID {circuit_id}) \" f\"of ticket {ticket_id} has been maxed out already. \" \"Skipping autoresolve...\" ) * If detail is resolved: self._logger.info( f\"Detail {ticket_detail_id} (circuit ID {circuit_id}) of ticket {ticket_id} is already \" \"resolved. Skipping autoresolve...\" ) * If curren environment is not production: self._logger.info( f\"Skipping autoresolve for circuit ID {circuit_id} \" f\"since the current environment is not production\" ) * unpause_ticket_detail * resolve_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to resolve ticket: {ticket_id}. Skipping autoresolve ...\") self._logger.info( f\"Autoresolve was successful for task of ticket {ticket_id} related to \" f\"circuit ID {circuit_id}. Posting autoresolve note...\" ) * append_autoresolve_note_to_ticket self._logger.info( f\"Detail {ticket_detail_id} (circuit ID {circuit_id}) of ticket {ticket_id} \" f\"was autoresolved!\" )","title":" autoresolve ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_autoresolve_ticket/#autoresolve-ticket","text":"self._logger.info(\"Starting the autoresolve process\") * get_ticket_basic_info * If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket basic info for client id: {client_id}.\" f\"Skipping autoresolve ticket ...\") self._logger.info(f\"Found {len(tickets_body)} tickets for circuit ID {circuit_id} from bruin:\") self._logger.info(tickets_body) * For ticket in tickets: self._logger.info( f\"Posting InterMapper UP note to task of ticket id {ticket_id} \" f\"related to circuit ID {circuit_id}...\" ) * _append_intermapper_up_note * If status is not Ok: self._logger.warning(f\"Bad status calling to append intermapper note to ticket id: {ticket_id}.\" f\"Skipping autoresolve ticket ....\") * get_tickets * If status is not Ok: self._logger.warning(f\"Bad status calling to get ticket for client id: {client_id} and \" f\"ticket id: {ticket_id}. Skipping autoresolve ticket ...\") * If no product category: self._logger.info(f\"Ticket {ticket_id} couldn't be found in Bruin. Skipping autoresolve...\") self._logger.info(f\"Product category of ticket {ticket_id} from bruin is {product_category}\") * If product category whitelisted: self._logger.info( f\"At least one product category of ticket {ticket_id} from the \" f\"following list is not one of the whitelisted categories for \" f\"auto-resolve: {product_category}. Skipping autoresolve ...\" ) self._logger.info(f\"Checking to see if ticket {ticket_id} can be autoresolved\") * get_ticket_details * If status is not Ok: self._logger.warning(f\"Bad status calling get ticket details to ticket id: {ticket_id}.\" f\"Skipping autoresolve ...\") * If not detected outage recently: self._logger.info( f\"Edge has been in outage state for a long time, so detail {ticket_detail_id} \" f\"(circuit ID {circuit_id}) of ticket {ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\" ) * If can't autoresolve one more time: self._logger.info( f\"Limit to autoresolve detail {ticket_detail_id} (circuit ID {circuit_id}) \" f\"of ticket {ticket_id} has been maxed out already. \" \"Skipping autoresolve...\" ) * If detail is resolved: self._logger.info( f\"Detail {ticket_detail_id} (circuit ID {circuit_id}) of ticket {ticket_id} is already \" \"resolved. Skipping autoresolve...\" ) * If curren environment is not production: self._logger.info( f\"Skipping autoresolve for circuit ID {circuit_id} \" f\"since the current environment is not production\" ) * unpause_ticket_detail * resolve_ticket * If status is not Ok: self._logger.warning(f\"Bad status calling to resolve ticket: {ticket_id}. Skipping autoresolve ...\") self._logger.info( f\"Autoresolve was successful for task of ticket {ticket_id} related to \" f\"circuit ID {circuit_id}. Posting autoresolve note...\" ) * append_autoresolve_note_to_ticket self._logger.info( f\"Detail {ticket_detail_id} (circuit ID {circuit_id}) of ticket {ticket_id} \" f\"was autoresolved!\" )","title":"Autoresolve ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_create_outage_ticket/","text":"Create outage ticket self._logger.info( f\"Attempting outage ticket creation for client_id {client_id}, \" f\"and circuit_id {circuit_id}\" ) * If not PRODUCTION: self._logger.info( f\"No outage ticket will be created for client_id {client_id} and circuit_id {circuit_id} \" f\"since the current environment is not production\" ) * create_outage_ticket self._logger.info( f\"Bruin response for ticket creation for edge with circuit id {circuit_id}: \" f\"{outage_ticket_response}\" ) * If status is Ok: self._logger.info(f\"Successfully created outage ticket with ticket_id {outage_ticket_body}\") * If is custom status: self._logger.info( f\"Ticket for circuit id {circuit_id} already exists with ticket_id {outage_ticket_body}.\" f\"Status returned was {outage_ticket_status}\" ) * If status = 409: self._logger.info(f\"In Progress ticket exists for location of circuit id {circuit_id}\") * If status = 472: self._logger.info(f\"Resolved ticket exists for circuit id {circuit_id}\") * If status = 473: self._logger.info(f\"Resolved ticket exists for location of circuit id {circuit_id}\") * If dri paramas: self._logger.info( f\"Appending InterMapper note to ticket id {outage_ticket_body} with dri parameters: \" f\"{dri_parameters}\" ) * append_dri_note * If status is not Ok: self._logger.warning(f\"Bad status calling append dri note. Skipping create outage ticket ...\") self._logger.info(f\"Appending InterMapper note to ticket id {outage_ticket_body}\") * append_intermapper_note * If status is not Ok: self._logger.warning(f\"Bad status calling append intermapper note. Skipping create outage ticket ...\")","title":" create outage ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_create_outage_ticket/#create-outage-ticket","text":"self._logger.info( f\"Attempting outage ticket creation for client_id {client_id}, \" f\"and circuit_id {circuit_id}\" ) * If not PRODUCTION: self._logger.info( f\"No outage ticket will be created for client_id {client_id} and circuit_id {circuit_id} \" f\"since the current environment is not production\" ) * create_outage_ticket self._logger.info( f\"Bruin response for ticket creation for edge with circuit id {circuit_id}: \" f\"{outage_ticket_response}\" ) * If status is Ok: self._logger.info(f\"Successfully created outage ticket with ticket_id {outage_ticket_body}\") * If is custom status: self._logger.info( f\"Ticket for circuit id {circuit_id} already exists with ticket_id {outage_ticket_body}.\" f\"Status returned was {outage_ticket_status}\" ) * If status = 409: self._logger.info(f\"In Progress ticket exists for location of circuit id {circuit_id}\") * If status = 472: self._logger.info(f\"Resolved ticket exists for circuit id {circuit_id}\") * If status = 473: self._logger.info(f\"Resolved ticket exists for location of circuit id {circuit_id}\") * If dri paramas: self._logger.info( f\"Appending InterMapper note to ticket id {outage_ticket_body} with dri parameters: \" f\"{dri_parameters}\" ) * append_dri_note * If status is not Ok: self._logger.warning(f\"Bad status calling append dri note. Skipping create outage ticket ...\") self._logger.info(f\"Appending InterMapper note to ticket id {outage_ticket_body}\") * append_intermapper_note * If status is not Ok: self._logger.warning(f\"Bad status calling append intermapper note. Skipping create outage ticket ...\")","title":"Create outage ticket"},{"location":"logging/services/intermapper-outage-monitor/actions/_get_dri_parameters/","text":"Get dri parameters get_attributes_serial","title":" get dri parameters"},{"location":"logging/services/intermapper-outage-monitor/actions/_get_dri_parameters/#get-dri-parameters","text":"get_attributes_serial","title":"Get dri parameters"},{"location":"logging/services/intermapper-outage-monitor/actions/_intermapper_monitoring_process/","text":"Intermapper monitoring process self._logger.info(f'Processing all unread email from {self._config.INTERMAPPER_CONFIG[\"inbox_email\"]}') * If status is not Ok: self._logger.warning(f\"Bad status calling to get unread emails. \" f\"Skipping intermapper monitoring process ...\") * _process_email_batch self._logger.info( f'Finished processing unread emails from {self._config.INTERMAPPER_CONFIG[\"inbox_email\"]}. ' f\"Elapsed time: {round((stop - start) / 60, 2)} minutes\" )","title":" intermapper monitoring process"},{"location":"logging/services/intermapper-outage-monitor/actions/_intermapper_monitoring_process/#intermapper-monitoring-process","text":"self._logger.info(f'Processing all unread email from {self._config.INTERMAPPER_CONFIG[\"inbox_email\"]}') * If status is not Ok: self._logger.warning(f\"Bad status calling to get unread emails. \" f\"Skipping intermapper monitoring process ...\") * _process_email_batch self._logger.info( f'Finished processing unread emails from {self._config.INTERMAPPER_CONFIG[\"inbox_email\"]}. ' f\"Elapsed time: {round((stop - start) / 60, 2)} minutes\" )","title":"Intermapper monitoring process"},{"location":"logging/services/intermapper-outage-monitor/actions/_mark_email_as_read/","text":"Mark email as read mark_email_as_read If status is not Ok: self._logger.error(f\"Could not mark email with msg_uid: {msg_uid} as read\")","title":" mark email as read"},{"location":"logging/services/intermapper-outage-monitor/actions/_mark_email_as_read/#mark-email-as-read","text":"mark_email_as_read If status is not Ok: self._logger.error(f\"Could not mark email with msg_uid: {msg_uid} as read\")","title":"Mark email as read"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email/","text":"Process email If not message or msg_uid = -1: self._logger.error(f\"Invalid message: {email}\") self._logger.info(f\"Processing email with msg_uid: {msg_uid} and subject: {subject}\") If event in intermaper config up events: self._logger.info( f'Event from InterMapper was {parsed_email_dict[\"event\"]}, there is no need to create ' f\"a new ticket. Checking for autoresolve ...\" ) _autoresolve_ticket If event in intermaper config down events: self._logger.info( f'Event from InterMapper was {parsed_email_dict[\"event\"]}, ' f'condition was {parsed_email_dict[\"condition\"]}. Checking for ticket creation ...' ) If is piab device; self._logger.info( f\"The probe type from Intermapper is {parsed_email_dict['probe_type']}.\" f\"Attempting to get additional parameters from DRI...\" ) _get_dri_parameters _create_outage_ticket Else: self._logger.info( f'Event from InterMapper was {parsed_email_dict[\"event\"]}, ' f\"so no further action is needs to be taken\" ) If event process successfully and environment is production: _mark_email_as_read If even process successfully: self._logger.info(f\"Processed email: {msg_uid}\") Else: self._logger.error( f\"Email with msg_uid: {msg_uid} and subject: {subject} \" f\"related to circuit ID: {circuit_id} could not be processed\" )","title":" process email"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email/#process-email","text":"If not message or msg_uid = -1: self._logger.error(f\"Invalid message: {email}\") self._logger.info(f\"Processing email with msg_uid: {msg_uid} and subject: {subject}\") If event in intermaper config up events: self._logger.info( f'Event from InterMapper was {parsed_email_dict[\"event\"]}, there is no need to create ' f\"a new ticket. Checking for autoresolve ...\" ) _autoresolve_ticket If event in intermaper config down events: self._logger.info( f'Event from InterMapper was {parsed_email_dict[\"event\"]}, ' f'condition was {parsed_email_dict[\"condition\"]}. Checking for ticket creation ...' ) If is piab device; self._logger.info( f\"The probe type from Intermapper is {parsed_email_dict['probe_type']}.\" f\"Attempting to get additional parameters from DRI...\" ) _get_dri_parameters _create_outage_ticket Else: self._logger.info( f'Event from InterMapper was {parsed_email_dict[\"event\"]}, ' f\"so no further action is needs to be taken\" ) If event process successfully and environment is production: _mark_email_as_read If even process successfully: self._logger.info(f\"Processed email: {msg_uid}\") Else: self._logger.error( f\"Email with msg_uid: {msg_uid} and subject: {subject} \" f\"related to circuit ID: {circuit_id} could not be processed\" )","title":"Process email"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email_batch/","text":"Process email batch self._logger.info(f\"Processing {len(emails)} email(s) with asset_id {asset_id}...\") * If not asset id or asset id = SD-WAN: * for email in emails: * mark email as read self._logger.info(f\"Invalid asset_id. Skipping emails with asset_id {asset_id}...\") * get_circuit_id * If status not Ok: self._logger.error(f\"Failed to get circuit id. Skipping emails with asset_id {asset_id}...\") * If status = 204: self._logger.error( f\"Bruin returned a 204 when getting the circuit id of asset_id {asset_id}. \" f\"Marking all emails with this asset_id as read\" ) * _process_email self._logger.info(f\"Finished processing all emails with asset_id {asset_id}!\")","title":" process email batch"},{"location":"logging/services/intermapper-outage-monitor/actions/_process_email_batch/#process-email-batch","text":"self._logger.info(f\"Processing {len(emails)} email(s) with asset_id {asset_id}...\") * If not asset id or asset id = SD-WAN: * for email in emails: * mark email as read self._logger.info(f\"Invalid asset_id. Skipping emails with asset_id {asset_id}...\") * get_circuit_id * If status not Ok: self._logger.error(f\"Failed to get circuit id. Skipping emails with asset_id {asset_id}...\") * If status = 204: self._logger.error( f\"Bruin returned a 204 when getting the circuit id of asset_id {asset_id}. \" f\"Marking all emails with this asset_id as read\" ) * _process_email self._logger.info(f\"Finished processing all emails with asset_id {asset_id}!\")","title":"Process email batch"},{"location":"logging/services/intermapper-outage-monitor/actions/start_intermapper_outage_monitoring/","text":"Start intermapper outage monitoring self._logger.info(\"Scheduling InterMapper Monitor job...\") * If exec on start: self._logger.info(\"InterMapper Monitor job is going to be executed immediately\") * _intermapper_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of InterMapper Monitoring job. Reason: {conflict}\")","title":"Start intermapper outage monitoring"},{"location":"logging/services/intermapper-outage-monitor/actions/start_intermapper_outage_monitoring/#start-intermapper-outage-monitoring","text":"self._logger.info(\"Scheduling InterMapper Monitor job...\") * If exec on start: self._logger.info(\"InterMapper Monitor job is going to be executed immediately\") * _intermapper_monitoring_process * If ConflictingIdError: self._logger.info(f\"Skipping start of InterMapper Monitoring job. Reason: {conflict}\")","title":"Start intermapper outage monitoring"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket","text":"append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_dri_note/","text":"Append dri note append_note_to_ticket","title":"Append dri note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_dri_note/#append-dri-note","text":"append_note_to_ticket","title":"Append dri note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_note/","text":"Append intermapper note append_note_to_ticket","title":"Append intermapper note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_note/#append-intermapper-note","text":"append_note_to_ticket","title":"Append intermapper note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_up_note/","text":"Append intermapper up note append_note_to_ticket","title":"Append intermapper up note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_intermapper_up_note/#append-intermapper-up-note","text":"append_note_to_ticket","title":"Append intermapper up note"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/create_outage_ticket/","text":"Create outage ticket Documentation self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/create_outage_ticket/#create-outage-ticket-documentation","text":"self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket Documentation"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_attributes_serial/","text":"Get attributes serial self._logger.info(f\"Getting the attribute's serial number of serial number {service_number}\") * If Exception: self._logger.error(f\"Getting the attribute's serial number of serial number {service_number} Error: {e}\") * If status not Ok: self._logger.error(f\"'Getting the attribute's serial number of serial number {service_number}'\" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Error: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Got the attribute's serial number of serial number {service_number}!\")","title":"Get attributes serial"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_attributes_serial/#get-attributes-serial","text":"self._logger.info(f\"Getting the attribute's serial number of serial number {service_number}\") * If Exception: self._logger.error(f\"Getting the attribute's serial number of serial number {service_number} Error: {e}\") * If status not Ok: self._logger.error(f\"'Getting the attribute's serial number of serial number {service_number}'\" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Error: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Got the attribute's serial number of serial number {service_number}!\")","title":"Get attributes serial"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_circuit_id/","text":"Get circuit id self._logger.info(f\"Getting the translation of circuit_id {circuit_id}\") * If Exception: self._logger.error(f\"Getting the translation of circuit_id {circuit_id} Error: {e}\") * If status not Ok: self._logger.error(f\"Getting the translation of circuit_id {circuit_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Error: \" f\"Error {response_status} - {response_body}\")","title":"Get circuit id"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_circuit_id/#get-circuit-id","text":"self._logger.info(f\"Getting the translation of circuit_id {circuit_id}\") * If Exception: self._logger.error(f\"Getting the translation of circuit_id {circuit_id} Error: {e}\") * If status not Ok: self._logger.error(f\"Getting the translation of circuit_id {circuit_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Error: \" f\"Error {response_status} - {response_body}\")","title":"Get circuit id"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_basic_info/","text":"Get ticket basic info self._logger.info( f\"Getting all tickets basic info with any status of {ticket_statuses}, with ticket topic \" f\"VOO, service number {service_number} and belonging to client {client_id} from Bruin...\" ) * If Exception: self._logger.error(f\"An error occurred when requesting tickets basic info from Bruin API with any status\" f\" of {ticket_statuses}, with ticket topic VOO and belonging to client {client_id} -> {e}\") * If status not Ok: self._logger.error(f\"Error while retrieving tickets basic info with any status of {ticket_statuses}, \" f\"with ticket topic VOO, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info( f\"Got all tickets basic info with any status of {ticket_statuses}, with ticket topic \" f\"VOO, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" )","title":"Get ticket basic info"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_basic_info/#get-ticket-basic-info","text":"self._logger.info( f\"Getting all tickets basic info with any status of {ticket_statuses}, with ticket topic \" f\"VOO, service number {service_number} and belonging to client {client_id} from Bruin...\" ) * If Exception: self._logger.error(f\"An error occurred when requesting tickets basic info from Bruin API with any status\" f\" of {ticket_statuses}, with ticket topic VOO and belonging to client {client_id} -> {e}\") * If status not Ok: self._logger.error(f\"Error while retrieving tickets basic info with any status of {ticket_statuses}, \" f\"with ticket topic VOO, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info( f\"Got all tickets basic info with any status of {ticket_statuses}, with ticket topic \" f\"VOO, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" )","title":"Get ticket basic info"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_tickets/","text":"Get tickets self._logger.info(f\"Getting all tickets of ticket id {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting all tickets of ticket id {ticket_id} from Bruin API -> {e}\") * If status not Ok: self._logger.error(f\"Error while retrieving all tickets of ticket id {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Got all tickets of ticket id {ticket_id} from Bruin\")","title":"Get tickets"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/get_tickets/#get-tickets","text":"self._logger.info(f\"Getting all tickets of ticket id {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting all tickets of ticket id {ticket_id} from Bruin API -> {e}\") * If status not Ok: self._logger.error(f\"Error while retrieving all tickets of ticket id {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\") * Else: self._logger.info(f\"Got all tickets of ticket id {ticket_id} from Bruin\")","title":"Get tickets"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket detail self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket-detail","text":"self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket detail"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/intermapper-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/get_unread_emails/","text":"Get unread emails self._logger.info( f\"Getting the unread emails from the inbox of {email_account} sent from the users: \" f\"{email_filter}\" ) * If Exception: self._logger.error(f\"An error occurred while getting the unread emails from the inbox of {email_account} -> {e}\") * If status ok: self._logger.info(f\"Got the unread emails from the inbox of {email_account}\") * Else: self._logger.error(f\"Error getting the unread emails from the inbox of {email_account} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get unread emails"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/get_unread_emails/#get-unread-emails","text":"self._logger.info( f\"Getting the unread emails from the inbox of {email_account} sent from the users: \" f\"{email_filter}\" ) * If Exception: self._logger.error(f\"An error occurred while getting the unread emails from the inbox of {email_account} -> {e}\") * If status ok: self._logger.info(f\"Got the unread emails from the inbox of {email_account}\") * Else: self._logger.error(f\"Error getting the unread emails from the inbox of {email_account} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get unread emails"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/mark_email_as_read/","text":"Mark email as read self._logger.info(f\"Marking message {msg_uid} from the inbox of {email_account} as read\") * If Exception: self._logger.error(f\"An error occurred while marking message {msg_uid} as read -> {e}\") * If status ok: self._logger.info(f\"Marked message {msg_uid} as read\") * Else: self._logger.error(f\"Error marking message {msg_uid} as read in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Mark email as read"},{"location":"logging/services/intermapper-outage-monitor/repositories/notifications_repository/mark_email_as_read/#mark-email-as-read","text":"self._logger.info(f\"Marking message {msg_uid} from the inbox of {email_account} as read\") * If Exception: self._logger.error(f\"An error occurred while marking message {msg_uid} as read -> {e}\") * If status ok: self._logger.info(f\"Marked message {msg_uid} as read\") * Else: self._logger.error(f\"Error marking message {msg_uid} as read in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Mark email as read"},{"location":"logging/services/service-affecting-monitor/actions/_append_latest_trouble_to_ticket/","text":"Append latest trouble to ticket Documentation self._logger.info( f\"Appending Service Affecting trouble note to ticket {ticket_id} for {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}...\" ) if is_there_any_note_for_trouble self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}. A note for this trouble was already \" f\"appended to the ticket after the latest re-open (or ticket creation)\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Append note to ticket self._logger.info( f\"Service Affecting trouble note for {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully appended to ticket {ticket_id}!\" )","title":" append latest trouble to ticket"},{"location":"logging/services/service-affecting-monitor/actions/_append_latest_trouble_to_ticket/#append-latest-trouble-to-ticket-documentation","text":"self._logger.info( f\"Appending Service Affecting trouble note to ticket {ticket_id} for {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}...\" ) if is_there_any_note_for_trouble self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}. A note for this trouble was already \" f\"appended to the ticket after the latest re-open (or ticket creation)\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting trouble note will be appended to ticket {ticket_id} for {trouble.value} trouble \" f\"detected in interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Append note to ticket self._logger.info( f\"Service Affecting trouble note for {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully appended to ticket {ticket_id}!\" )","title":"Append latest trouble to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_append_reminder_note/","text":"Append Reminder Note Documentation Launch append_note_to_ticket","title":" append reminder note"},{"location":"logging/services/service-affecting-monitor/actions/_append_reminder_note/#append-reminder-note-documentation","text":"Launch append_note_to_ticket","title":"Append Reminder Note Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_attempt_forward_to_asr/","text":"Attempt forward to asr Documentation self._logger.info( f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\" ) if link_interface_type != \"WIRED\" self._logger.info( f\"Link {interface} is of type {link_interface_type} and not WIRED. Attempting to forward \" f\"to HNOC...\" ) Forward ticket to hnoc queue self._logger.info( f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f\"following: \" f'{self._config.ASR_CONFIG[\"link_labels_blacklist\"]} ' f\"in the link label\" ) if not _should_be_forwarded_to_asr self._logger.info( f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to {target_queue}.\" ) Get ticket details if other_troubles_in_ticket self._logger.info( f\"Other service affecting troubles were found in ticket id {ticket_id}. Skipping forward\" f\"to asr...\" ) if task_result_note is not None self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{task_result}\"' ) Change detail work queue Append asr forwarding note","title":" attempt forward to asr"},{"location":"logging/services/service-affecting-monitor/actions/_attempt_forward_to_asr/#attempt-forward-to-asr-documentation","text":"self._logger.info( f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\" ) if link_interface_type != \"WIRED\" self._logger.info( f\"Link {interface} is of type {link_interface_type} and not WIRED. Attempting to forward \" f\"to HNOC...\" ) Forward ticket to hnoc queue self._logger.info( f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f\"following: \" f'{self._config.ASR_CONFIG[\"link_labels_blacklist\"]} ' f\"in the link label\" ) if not _should_be_forwarded_to_asr self._logger.info( f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to {target_queue}.\" ) Get ticket details if other_troubles_in_ticket self._logger.info( f\"Other service affecting troubles were found in ticket id {ticket_id}. Skipping forward\" f\"to asr...\" ) if task_result_note is not None self._logger.info( f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{task_result}\"' ) Change detail work queue Append asr forwarding note","title":"Attempt forward to asr Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_bandwidth_check/","text":"Bandwidth Check Documentation self._logger.info(\"Looking for bandwidth issues...\") Get links metrics for bandwidth checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bandwidth issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed any bandwidth thresholds\" ) * Process bandwidth trouble self._logger.info(\"Finished looking for bandwidth issues!\")","title":" bandwidth check"},{"location":"logging/services/service-affecting-monitor/actions/_bandwidth_check/#bandwidth-check-documentation","text":"self._logger.info(\"Looking for bandwidth issues...\") Get links metrics for bandwidth checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bandwidth issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed any bandwidth thresholds\" ) * Process bandwidth trouble self._logger.info(\"Finished looking for bandwidth issues!\")","title":"Bandwidth Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_bouncing_check/","text":"Bouncing Check Documentation self._logger.info(\"Looking for bouncing issues...\") Get links metrics for bouncing checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bouncing issues. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if not events self._logger.info( f\"No events were found for {link_status['interface']} from {serial_number} \" f\"while looking for bouncing troubles\" ) if are_bouncing_events_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed bouncing thresholds\" ) * Process bouncing trouble self._logger.info(\"Finished looking for bouncing issues!\")","title":" bouncing check"},{"location":"logging/services/service-affecting-monitor/actions/_bouncing_check/#bouncing-check-documentation","text":"self._logger.info(\"Looking for bouncing issues...\") Get links metrics for bouncing checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking bouncing issues. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if not events self._logger.info( f\"No events were found for {link_status['interface']} from {serial_number} \" f\"while looking for bouncing troubles\" ) if are_bouncing_events_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed bouncing thresholds\" ) * Process bouncing trouble self._logger.info(\"Finished looking for bouncing issues!\")","title":"Bouncing Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_create_affecting_ticket/","text":"Create affecting ticket Documentation self._logger.info( f\"Creating Service Affecting ticket to report a {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number}...\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting ticket will be created to report a {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Create affecting ticket self._logger.info( f\"Service Affecting ticket to report {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully created! Ticket ID is {ticket_id}\" ) Append note to ticket if trouble is not AffectingTroubles.BOUNCING if should_schedule_hnoc_forwarding Schedule forward to hnoc queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":" create affecting ticket"},{"location":"logging/services/service-affecting-monitor/actions/_create_affecting_ticket/#create-affecting-ticket-documentation","text":"self._logger.info( f\"Creating Service Affecting ticket to report a {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number}...\" ) if not working_environment == \"production\" self._logger.info( f\"No Service Affecting ticket will be created to report a {trouble.value} trouble detected in \" f\"interface {interface} of edge {serial_number}, since the current environment is \" f\"{working_environment.upper()}\" ) Create affecting ticket self._logger.info( f\"Service Affecting ticket to report {trouble.value} trouble detected in interface {interface} \" f\"of edge {serial_number} was successfully created! Ticket ID is {ticket_id}\" ) Append note to ticket if trouble is not AffectingTroubles.BOUNCING if should_schedule_hnoc_forwarding Schedule forward to hnoc queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":"Create affecting ticket Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_forward_ticket_to_hnoc_queue/","text":"Forward ticket to HNOC Documentation self._logger.info( f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\" ) Get ticket details if ticket_details_response[\"status\"] not in range(200, 300) self._logger.error( f\"Getting ticket details of ticket_id: {ticket_id} and serial: {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\" ) if is_task_resolved self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is resolved. \" f\"Skipping forward to HNOC...\" ) self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is not resolved. \" f\"Forwarding to HNOC...\" ) Change detail work queue to hnoc if change_work_queue_response[\"status\"] not in range(200, 300) self._logger.error( f\"Failed to forward ticket_id: {ticket_id} and \" f\"serial: {serial_number} to HNOC Investigate due to bruin \" f\"returning {change_work_queue_response} when attempting to forward to HNOC.\" ) if Exception self._logger.error( f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\" )","title":" forward ticket to hnoc queue"},{"location":"logging/services/service-affecting-monitor/actions/_forward_ticket_to_hnoc_queue/#forward-ticket-to-hnoc-documentation","text":"self._logger.info( f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\" ) Get ticket details if ticket_details_response[\"status\"] not in range(200, 300) self._logger.error( f\"Getting ticket details of ticket_id: {ticket_id} and serial: {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\" ) if is_task_resolved self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is resolved. \" f\"Skipping forward to HNOC...\" ) self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} is not resolved. \" f\"Forwarding to HNOC...\" ) Change detail work queue to hnoc if change_work_queue_response[\"status\"] not in range(200, 300) self._logger.error( f\"Failed to forward ticket_id: {ticket_id} and \" f\"serial: {serial_number} to HNOC Investigate due to bruin \" f\"returning {change_work_queue_response} when attempting to forward to HNOC.\" ) if Exception self._logger.error( f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\" )","title":"Forward ticket to HNOC Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_jitter_check/","text":"Jitter Check Documentation self._logger.info(\"Looking for jitter issues...\") Get links metrics for jitter checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking jitter issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_jitter_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed jitter thresholds\" ) * Process jitter trouble self._logger.info(\"Finished looking for jitter issues!\")","title":" jitter check"},{"location":"logging/services/service-affecting-monitor/actions/_jitter_check/#jitter-check-documentation","text":"self._logger.info(\"Looking for jitter issues...\") Get links metrics for jitter checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking jitter issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_jitter_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed jitter thresholds\" ) * Process jitter trouble self._logger.info(\"Finished looking for jitter issues!\")","title":"Jitter Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_latency_check/","text":"Latency Check Documentation self._logger.info(\"Looking for latency issues...\") Get links metrics for latency checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking latency issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_latency_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed latency thresholds\" ) * Process latency trouble self._logger.info(\"Finished looking for latency issues!\")","title":" latency check"},{"location":"logging/services/service-affecting-monitor/actions/_latency_check/#latency-check-documentation","text":"self._logger.info(\"Looking for latency issues...\") Get links metrics for latency checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking latency issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_latency_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed latency thresholds\" ) * Process latency trouble self._logger.info(\"Finished looking for latency issues!\")","title":"Latency Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_map_cached_edges_with_links_metrics_and_contact_info/","text":"Map cached edges with links metrics and contact info documentation for elem in links_metrics if not cached_edge self._logger.info(f\"No cached info was found for edge {serial_number}. Skipping...\")","title":" map cached edges with links metrics and contact info"},{"location":"logging/services/service-affecting-monitor/actions/_map_cached_edges_with_links_metrics_and_contact_info/#map-cached-edges-with-links-metrics-and-contact-info-documentation","text":"for elem in links_metrics if not cached_edge self._logger.info(f\"No cached info was found for edge {serial_number}. Skipping...\")","title":"Map cached edges with links metrics and contact info documentation"},{"location":"logging/services/service-affecting-monitor/actions/_packet_loss_check/","text":"Packet loss Check Documentation self._logger.info(\"Looking for packet loss issues...\") Get links metrics for packet loss checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking packet loss issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_packet_loss_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed packet loss thresholds\" ) * Process packet loss trouble self._logger.info(\"Finished looking for packet loss issues!\")","title":" packet loss check"},{"location":"logging/services/service-affecting-monitor/actions/_packet_loss_check/#packet-loss-check-documentation","text":"self._logger.info(\"Looking for packet loss issues...\") Get links metrics for packet loss checks Check if link metrics is empty self._logger.info(\"List of links arrived empty while checking packet loss issues. Skipping...\") Structure link metrics Map cached edges with links metrics and contact info for elem in metrics_with_cache_and_contact_info: if are_packet_loss_metrics_within_threshold self._logger.info( f\"Link {link_status['interface']} from {serial_number} didn't exceed packet loss thresholds\" ) * Process packet loss trouble self._logger.info(\"Finished looking for packet loss issues!\")","title":"Packet loss Check Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_affecting_trouble/","text":"Process affecting trouble Documentation self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number}\" ) Get open affecting tickets if open_affecting_ticket self._logger.info( f\"An open Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) if task_resolved self._logger.info( f\"Service Affecting ticket with ID {ticket_id} is open, but the task related to edge \" f\"{serial_number} is Resolved. Therefore, the ticket will be considered as Resolved.\" ) else Append latest trouble to ticket else no open_affecting_ticket self._logger.info(f\"No open Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not resolved_affecting_ticket Get resolved affecting tickets if not trouble_processed and resolved_affecting_ticket self._logger.info( f\"A resolved Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) Unresolve task for affecting_ticket else self._logger.info(f\"No resolved Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not open_affecting_ticket and not resolved_affecting_ticket: self._logger.info(f\"No open or resolved Service Affecting ticket was found for edge {serial_number}\") Create affecting ticket self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number} has been processed\" )","title":" process affecting trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_affecting_trouble/#process-affecting-trouble-documentation","text":"self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number}\" ) Get open affecting tickets if open_affecting_ticket self._logger.info( f\"An open Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) if task_resolved self._logger.info( f\"Service Affecting ticket with ID {ticket_id} is open, but the task related to edge \" f\"{serial_number} is Resolved. Therefore, the ticket will be considered as Resolved.\" ) else Append latest trouble to ticket else no open_affecting_ticket self._logger.info(f\"No open Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not resolved_affecting_ticket Get resolved affecting tickets if not trouble_processed and resolved_affecting_ticket self._logger.info( f\"A resolved Service Affecting ticket was found for edge {serial_number}. Ticket ID: {ticket_id}\" ) Unresolve task for affecting_ticket else self._logger.info(f\"No resolved Service Affecting ticket was found for edge {serial_number}\") if not trouble_processed and not open_affecting_ticket and not resolved_affecting_ticket: self._logger.info(f\"No open or resolved Service Affecting ticket was found for edge {serial_number}\") Create affecting ticket self._logger.info( f\"Service Affecting trouble of type {trouble.value} detected in interface {interface} of edge \" f\"{serial_number} has been processed\" )","title":"Process affecting trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_bandwidth_trouble/","text":"Process bandwidth trouble Documentation Launch _process_affecting_trouble","title":" process bandwidth trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_bandwidth_trouble/#process-bandwidth-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process bandwidth trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_bouncing_trouble/","text":"Process bouncing trouble Documentation Launch _process_affecting_trouble Launch _attempt_forward_to_asr","title":" process bouncing trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_bouncing_trouble/#process-bouncing-trouble-documentation","text":"Launch _process_affecting_trouble Launch _attempt_forward_to_asr","title":"Process bouncing trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_jitter_trouble/","text":"Process jitter trouble Documentation Launch _process_affecting_trouble","title":" process jitter trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_jitter_trouble/#process-jitter-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process jitter trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_latency_trouble/","text":"Process latency trouble Documentation Launch _process_affecting_trouble","title":" process latency trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_latency_trouble/#process-latency-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process latency trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_process_packet_loss_trouble/","text":"Process packet loss trouble Documentation Launch _process_affecting_trouble","title":" process packet loss trouble"},{"location":"logging/services/service-affecting-monitor/actions/_process_packet_loss_trouble/#process-packet-loss-trouble-documentation","text":"Launch _process_affecting_trouble","title":"Process packet loss trouble Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_for_edge/","text":"Run autoresolve for edge Documentation self._logger.info(f\"Starting autoresolve for edge {serial_number}...\") if all_metrics_within_thresholds is empty self._logger.info( f\"At least one metric of edge {serial_number} is not within the threshold. Skipping autoresolve...\" ) Get open affecting tickets if affecting tickets is empty self._logger.info( f\"No affecting ticket found for edge with serial number {serial_number}. Skipping autoresolve...\" ) for affecting_ticket in affecting_tickets if not was_ticket_created_by_automation_engine self._logger.info(f\"Ticket {affecting_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") Get ticket details if remove_auto_resolution_restrictions_for_byob self._logger.info( f\"Task for serial {serial_number} in ticket {affecting_ticket_id} is in the IPA Investigate\" f\" queue. Skipping checks for max auto-resolves and grace period to auto-resolve after last\" f\" documented trouble...\" ) else if not last_trouble_was_detected_recently self._logger.info( f\"Edge with serial number {serial_number} has been under an affecting trouble for a long \" f\"time, so the detail of ticket {affecting_ticket_id} related to it will not be \" f\"autoresolved. Skipping autoresolve...\" ) if is_autoresolve_threshold_maxed_out self._logger.info( f\"Limit to autoresolve detail of ticket {affecting_ticket_id} related to serial \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) if is_task_resolved self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial {serial_number} is already \" \"resolved. Skipping autoresolve...\" ) if working_environment != \"production\" self._logger.info( f\"Skipping autoresolve for detail of ticket {affecting_ticket_id} related to serial number \" f\"{serial_number} since the current environment is {working_environment.upper()}\" ) self._logger.info( f\"Autoresolving detail of ticket {affecting_ticket_id} related to serial number {serial_number}...\" ) * Unpause ticket detail * Resolve ticket * Append autoresolve note to ticket self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial number {serial_number} was autoresolved!\" ) self._logger.info(f\"Finished autoresolve for edge {serial_number}!\")","title":" run autoresolve for edge"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_for_edge/#run-autoresolve-for-edge-documentation","text":"self._logger.info(f\"Starting autoresolve for edge {serial_number}...\") if all_metrics_within_thresholds is empty self._logger.info( f\"At least one metric of edge {serial_number} is not within the threshold. Skipping autoresolve...\" ) Get open affecting tickets if affecting tickets is empty self._logger.info( f\"No affecting ticket found for edge with serial number {serial_number}. Skipping autoresolve...\" ) for affecting_ticket in affecting_tickets if not was_ticket_created_by_automation_engine self._logger.info(f\"Ticket {affecting_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") Get ticket details if remove_auto_resolution_restrictions_for_byob self._logger.info( f\"Task for serial {serial_number} in ticket {affecting_ticket_id} is in the IPA Investigate\" f\" queue. Skipping checks for max auto-resolves and grace period to auto-resolve after last\" f\" documented trouble...\" ) else if not last_trouble_was_detected_recently self._logger.info( f\"Edge with serial number {serial_number} has been under an affecting trouble for a long \" f\"time, so the detail of ticket {affecting_ticket_id} related to it will not be \" f\"autoresolved. Skipping autoresolve...\" ) if is_autoresolve_threshold_maxed_out self._logger.info( f\"Limit to autoresolve detail of ticket {affecting_ticket_id} related to serial \" f\"{serial_number} has been maxed out already. Skipping autoresolve...\" ) if is_task_resolved self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial {serial_number} is already \" \"resolved. Skipping autoresolve...\" ) if working_environment != \"production\" self._logger.info( f\"Skipping autoresolve for detail of ticket {affecting_ticket_id} related to serial number \" f\"{serial_number} since the current environment is {working_environment.upper()}\" ) self._logger.info( f\"Autoresolving detail of ticket {affecting_ticket_id} related to serial number {serial_number}...\" ) * Unpause ticket detail * Resolve ticket * Append autoresolve note to ticket self._logger.info( f\"Detail of ticket {affecting_ticket_id} related to serial number {serial_number} was autoresolved!\" ) self._logger.info(f\"Finished autoresolve for edge {serial_number}!\")","title":"Run autoresolve for edge Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_process/","text":"Run autoresolve process Documentation self._logger.info(\"Starting auto-resolve process...\") * Get links metrics for autoresolve if link metrics is empty self._logger.info(\"List of links metrics arrived empty while running auto-resolve process. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info self._logger.info(f\"Running auto-resolve for {len(edges_with_links_info)} edges\") autoresolve_tasks = [ Run autoresolve for edge for edge in edges_with_links_info] self._logger.info(\"Auto-resolve process finished!\")","title":" run autoresolve process"},{"location":"logging/services/service-affecting-monitor/actions/_run_autoresolve_process/#run-autoresolve-process-documentation","text":"self._logger.info(\"Starting auto-resolve process...\") * Get links metrics for autoresolve if link metrics is empty self._logger.info(\"List of links metrics arrived empty while running auto-resolve process. Skipping...\") Get events by serial and interface Structure link metrics Map cached edges with links metrics and contact info self._logger.info(f\"Running auto-resolve for {len(edges_with_links_info)} edges\") autoresolve_tasks = [ Run autoresolve for edge for edge in edges_with_links_info] self._logger.info(\"Auto-resolve process finished!\")","title":"Run autoresolve process Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_schedule_forward_to_hnoc_queue/","text":"Schedule forward to hnoc queue Documentation self._logger.info( f\"Scheduling HNOC forwarding for ticket_id: {ticket_id} and serial: {serial_number} \" f\"to happen at timestamp: {forward_task_run_date}\" ) Schedule Forward ticket to hnoc queue","title":" schedule forward to hnoc queue"},{"location":"logging/services/service-affecting-monitor/actions/_schedule_forward_to_hnoc_queue/#schedule-forward-to-hnoc-queue-documentation","text":"self._logger.info( f\"Scheduling HNOC forwarding for ticket_id: {ticket_id} and serial: {serial_number} \" f\"to happen at timestamp: {forward_task_run_date}\" ) Schedule Forward ticket to hnoc queue","title":"Schedule forward to hnoc queue Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_service_affecting_monitor_process/","text":"Service Affecting process Documentation self._logger.info(f\"Starting Service Affecting Monitor process now...\") Get cache for affecting monitoring Check if customer cache is empty self._logger.info(\"Got an empty customer cache. Process cannot keep going.\") Latency Check Packet Loss Check Jitter Check Bandwidth Check Bouncing Check Run Autoresolve process self._logger.info(f\"Finished processing all links! Took {round((time.time() - start_time) / 60, 2)} minutes\")","title":" service affecting monitor process"},{"location":"logging/services/service-affecting-monitor/actions/_service_affecting_monitor_process/#service-affecting-process-documentation","text":"self._logger.info(f\"Starting Service Affecting Monitor process now...\") Get cache for affecting monitoring Check if customer cache is empty self._logger.info(\"Got an empty customer cache. Process cannot keep going.\") Latency Check Packet Loss Check Jitter Check Bandwidth Check Bouncing Check Run Autoresolve process self._logger.info(f\"Finished processing all links! Took {round((time.time() - start_time) / 60, 2)} minutes\")","title":"Service Affecting process Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_structure_links_metrics/","text":"Structure links metrics Documentation for link_info in links_metrics if edge_state is None self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) if edge_state == \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":" structure links metrics"},{"location":"logging/services/service-affecting-monitor/actions/_structure_links_metrics/#structure-links-metrics-documentation","text":"for link_info in links_metrics if edge_state is None self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) if edge_state == \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Structure links metrics Documentation"},{"location":"logging/services/service-affecting-monitor/actions/_unresolve_task_for_affecting_ticket/","text":"Unresolve task for affecting ticket Documentation self._logger.info( f\"Unresolving task related to edge {serial_number} of Service Affecting ticket {ticket_id} due to a \" f\"{trouble.value} trouble detected in interface {interface}...\" ) if not working_environment == \"production\" self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} will not be unresolved \" f\"because of the {trouble.value} trouble detected in interface {interface}, since the current \" f\"environment is {working_environment.upper()}\" ) Open ticket self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} was successfully \" f\"unresolved! The cause was a {trouble.value} trouble detected in interface {interface}\" ) Append note to ticket if should_schedule_hnoc_forwarding self._logger.info( f\"Forwarding reopened task for serial {serial_number} of ticket {ticket_id} to the HNOC queue...\" ) Schedule forward to HNOC queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for the reopened task of ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":" unresolve task for affecting ticket"},{"location":"logging/services/service-affecting-monitor/actions/_unresolve_task_for_affecting_ticket/#unresolve-task-for-affecting-ticket-documentation","text":"self._logger.info( f\"Unresolving task related to edge {serial_number} of Service Affecting ticket {ticket_id} due to a \" f\"{trouble.value} trouble detected in interface {interface}...\" ) if not working_environment == \"production\" self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} will not be unresolved \" f\"because of the {trouble.value} trouble detected in interface {interface}, since the current \" f\"environment is {working_environment.upper()}\" ) Open ticket self._logger.info( f\"Task related to edge {serial_number} of Service Affecting ticket {ticket_id} was successfully \" f\"unresolved! The cause was a {trouble.value} trouble detected in interface {interface}\" ) Append note to ticket if should_schedule_hnoc_forwarding self._logger.info( f\"Forwarding reopened task for serial {serial_number} of ticket {ticket_id} to the HNOC queue...\" ) Schedule forward to HNOC queue else self._logger.info( f\"Ticket_id: {ticket_id} for serial: {serial_number} with link_label: \" f\"{link_data['link_status']['displayName']} is a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\" ) self._logger.info( f\"Sending an email for the reopened task of ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) Send initial email milestone notification if email_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Append reminder note if append_note_response[\"status\"] not in range(200, 300) self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" )","title":"Unresolve task for affecting ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_asr_forwarding_note_to_ticket/","text":"Append asr forwarding note to ticket Documentation Launch append_note_to_ticket","title":"Append asr forwarding note to ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_asr_forwarding_note_to_ticket/#append-asr-forwarding-note-to-ticket-documentation","text":"Launch append_note_to_ticket","title":"Append asr forwarding note to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket Documentation Launch append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket-documentation","text":"Launch append_note_to_ticket","title":"Append autoresolve note to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket Documentation self._logger.info(f\"Appending note to ticket {ticket_id}... Note contents: {note}\") self._logger.info(f\"Note appended to ticket {ticket_id}!\") if Exception self._logger.error( f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\" ) if response_status not in range(200, 300) self._logger.error( f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\" )","title":"Append note to ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket-documentation","text":"self._logger.info(f\"Appending note to ticket {ticket_id}... Note contents: {note}\") self._logger.info(f\"Note appended to ticket {ticket_id}!\") if Exception self._logger.error( f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\" ) if response_status not in range(200, 300) self._logger.error( f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\" )","title":"Append note to ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue/","text":"Change detail work queue Documentation self._logger.info( f\"Changing task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) if Exception self._logger.error( f\"An error occurred when changing task result of detail {detail_id} / serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\" ) else self._logger.error( f\"Error while changing task result of detail {detail_id} / serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Change detail work queue"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue/#change-detail-work-queue-documentation","text":"self._logger.info( f\"Changing task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"to {task_result}...\" ) if Exception self._logger.error( f\"An error occurred when changing task result of detail {detail_id} / serial {service_number} \" f\"in ticket {ticket_id} to {task_result} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Task result of detail {detail_id} / serial {service_number} in ticket {ticket_id} \" f\"changed to {task_result} successfully!\" ) else self._logger.error( f\"Error while changing task result of detail {detail_id} / serial {service_number} in ticket \" f\"{ticket_id} to {task_result} in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Change detail work queue Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/","text":"Change detail work queue to hnoc Documentation Launch change_detail_work_queue","title":"Change detail work queue to hnoc"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/change_detail_work_queue_to_hnoc/#change-detail-work-queue-to-hnoc-documentation","text":"Launch change_detail_work_queue","title":"Change detail work queue to hnoc Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/","text":"Create Affecting ticket Documentation self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create affecting ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/create_affecting_ticket/#create-affecting-ticket-documentation","text":"self._logger.info( f\"Creating affecting ticket for serial {service_number} belonging to client {client_id}...\" ) if Exception self._logger.error( f\"An error occurred while creating affecting ticket for client id {client_id} and serial \" f\"{service_number} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Affecting ticket for client {client_id} and serial {service_number} created successfully!\" ) else self._logger.error( f\"Error while opening affecting ticket for client {client_id} and serial {service_number} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Create Affecting ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/","text":"Get affecting tickets Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_affecting_tickets/#get-affecting-tickets","text":"Launch Get tickets","title":"Get affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/","text":"Get open affecting tickets Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_open_affecting_tickets/#get-open-affecting-tickets","text":"Launch Get affecting tickets","title":"Get open affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_resolved_affecting_tickets/","text":"Get resolved affecting tickets Launch Get affecting tickets","title":"Get resolved affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_resolved_affecting_tickets/#get-resolved-affecting-tickets","text":"Launch Get affecting tickets","title":"Get resolved affecting tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get Ticket details Documentation self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get ticket details"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details-documentation","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\" if Exception self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while retrieving details of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\")","title":"Get Ticket details Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_tickets/","text":"Get tickets Documentation if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/get_tickets/#get-tickets-documentation","text":"if not service_number self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin...\" ) else self._logger.info( f\"Getting all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} from Bruin...\" ) if Exception self._logger.error( f\"An error occurred when requesting tickets from Bruin API with any status of {ticket_statuses}, \" f\"with ticket topic {ticket_topic} and belonging to client {client_id} -> {e}\" ) if response_status in range(200, 300): if not service_number: self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} from Bruin!\" ) else self._logger.info( f\"Got all tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client \" f\"{client_id} from Bruin!\" ) else if not service_number self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" ) else self._logger.error( f\"Error while retrieving tickets with any status of {ticket_statuses}, with ticket topic \" f\"{ticket_topic}, service number {service_number} and belonging to client {client_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Get tickets Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket Documentation self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") self._logger.info(f\"Ticket {ticket_id} opened!\") if Exception self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Open ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/open_ticket/#open-ticket-documentation","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") self._logger.info(f\"Ticket {ticket_id} opened!\") if Exception self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") if response_status not in range(200, 300) self._logger.error( f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Open ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/post_notification_email_milestone/","text":"Post notification email milestone Documentation self._logger.info( f\"Sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...\" ) if Exception self._logger.error( f\"An error occurred when sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...-> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Email sent for ticket {ticket_id}, service number {service_number} \" f\"and notification type {notification_type}!\" ) else self._logger.error( f\"Error while sending email for ticket {ticket_id}, \" f\"service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Post notification email milestone"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/post_notification_email_milestone/#post-notification-email-milestone-documentation","text":"self._logger.info( f\"Sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...\" ) if Exception self._logger.error( f\"An error occurred when sending email for ticket id {ticket_id}, \" f\"service_number {service_number} \" f\"and notification type {notification_type}...-> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Email sent for ticket {ticket_id}, service number {service_number} \" f\"and notification type {notification_type}!\" ) else self._logger.error( f\"Error while sending email for ticket {ticket_id}, \" f\"service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Post notification email milestone Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket Documentation self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") if Exception self._logger.erorr(f\"An error occurred while resolving detail {detail_id} of ticket {ticket_id} -> {e}\") if response_status in range(200, 300) self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\") else self._logger.error( f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Resolve ticket"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket-documentation","text":"self._logger.info(f\"Resolving detail {detail_id} of ticket {ticket_id}...\") if Exception self._logger.erorr(f\"An error occurred while resolving detail {detail_id} of ticket {ticket_id} -> {e}\") if response_status in range(200, 300) self._logger.info(f\"Detail {detail_id} of ticket {ticket_id} resolved successfully!\") else self._logger.error( f\"Error while resolving detail {detail_id} of ticket {ticket_id} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: \" f\"Error {response_status} - {response_body}\" )","title":"Resolve ticket Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/","text":"Send initial email milestone notification Documentation Launch post_notification_email_milestone","title":"Send initial email milestone notification"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/#send-initial-email-milestone-notification-documentation","text":"Launch post_notification_email_milestone","title":"Send initial email milestone notification Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail Documentation self._logger.info(f\"Unpausing detail of ticket {ticket_id} related to serial number {service_number}...\") if Exception self._logger.error( f\"An error occurred when unpausing detail of ticket {ticket_id} related to serial number \" f\"{service_number}. Error: {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Detail of ticket {ticket_id} related to serial number {service_number}) was unpaused!\" ) else self._logger.error( f\"Error while unpausing detail of ticket {ticket_id} related to serial number {service_number}) in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\" )","title":"Unpause ticket detail"},{"location":"logging/services/service-affecting-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail-documentation","text":"self._logger.info(f\"Unpausing detail of ticket {ticket_id} related to serial number {service_number}...\") if Exception self._logger.error( f\"An error occurred when unpausing detail of ticket {ticket_id} related to serial number \" f\"{service_number}. Error: {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Detail of ticket {ticket_id} related to serial number {service_number}) was unpaused!\" ) else self._logger.error( f\"Error while unpausing detail of ticket {ticket_id} related to serial number {service_number}) in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\" )","title":"Unpause ticket detail Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache/","text":"Get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/","text":"Get cache for affecting Documentation Launch get_cache","title":"Get cache for affecting monitoring"},{"location":"logging/services/service-affecting-monitor/repositories/customer_cache_repository/get_cache_for_affecting_monitoring/#get-cache-for-affecting-documentation","text":"Launch get_cache","title":"Get cache for affecting Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_all_links_metrics/","text":"Get all links metrics Documentation for host in self._config.MONITOR_CONFIG[\"velo_filter\"] * Gets links metrics by host * if status from get_links_metrics_by_host return is not in range(200, 300) self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_all_links_metrics/#get-all-links-metrics-documentation","text":"for host in self._config.MONITOR_CONFIG[\"velo_filter\"] * Gets links metrics by host * if status from get_links_metrics_by_host return is not in range(200, 300) self._logger.info(f\"Error: could not retrieve links metrics from Velocloud host {host}\")","title":"Get all links metrics Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_enterprise_events/","text":"Get enterprise events Documentation self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) if Exception : self._logger.error( f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) else self._logger.error( f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\" )","title":"Get enterprise events"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_enterprise_events/#get-enterprise-events-documentation","text":"self._logger.info( f\"Getting events of host {host} and enterprise id {enterprise_id} having any type of {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud...\" ) if Exception : self._logger.error( f\"An error occurred when requesting edge events from Velocloud for host {host} \" f\"and enterprise id {enterprise_id} -> {e}\" ) if response_status in range(200, 300) self._logger.info( f\"Got events of host {host} and enterprise id {enterprise_id} having any type in {event_types} \" f\"that took place between {past_moment} and {now} from Velocloud!\" ) else self._logger.error( f\"Error while retrieving events of host {host} and enterprise id {enterprise_id} having any type \" f\"in {event_types} that took place between {past_moment} and {now} \" f\"in {self._config.ENVIRONMENT_NAME.upper()}\" f\"environment: Error {response_status} - {response_body}\" )","title":"Get enterprise events Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/","text":"Get events by serial and interface Documentation for host in edges_by_host_and_enterprise for enterprise_id in edges_by_enterprise Get enterprise events for event in enterprise_events if not matching_edge self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_events_by_serial_and_interface/#get-events-by-serial-and-interface-documentation","text":"for host in edges_by_host_and_enterprise for enterprise_id in edges_by_enterprise Get enterprise events for event in enterprise_events if not matching_edge self._logger.info( f'No edge in the customer cache had edge name {event[\"edgeName\"]}. Skipping...' ) self._logger.info( f'Event with edge name {event[\"edgeName\"]} matches edge from customer cache with' f\"serial number {serial}\" )","title":"Get events by serial and interface Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_by_host/","text":"Get links metrics by host Documentation self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) self._logger.info(f\"Got links metrics from Velocloud host {host}!\") if Exception : self._logger.error(f\"An error occurred when requesting links metrics from Velocloud -> {e}\") if response status not in range(200, 300) self._logger.error( f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Get links metrics by host"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_by_host/#get-links-metrics-by-host-documentation","text":"self._logger.info( f\"Getting links metrics between {interval['start']} and {interval['end']} \" f\"from Velocloud host {host}...\" ) self._logger.info(f\"Got links metrics from Velocloud host {host}!\") if Exception : self._logger.error(f\"An error occurred when requesting links metrics from Velocloud -> {e}\") if response status not in range(200, 300) self._logger.error( f\"Error while retrieving links metrics in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\" )","title":"Get links metrics by host Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/","text":"Get links metrics for autoresolve Documentation Launch get_all_links_metrics","title":"Get links metrics for autoresolve"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_autoresolve/#get-links-metrics-for-autoresolve-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for autoresolve Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bandwidth_checks/","text":"Get links metrics for bandwidth checks Documentation Launch get_all_links_metrics","title":"Get links metrics for bandwidth checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bandwidth_checks/#get-links-metrics-for-bandwidth-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for bandwidth checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bouncing_checks/","text":"Get links metrics for bouncing checks Documentation Launch get_all_links_metrics","title":"Get links metrics for bouncing checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_bouncing_checks/#get-links-metrics-for-bouncing-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for bouncing checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_jitter_checks/","text":"Get links metrics for jitter checks Documentation Launch get_all_links_metrics","title":"Get links metrics for jitter checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_jitter_checks/#get-links-metrics-for-jitter-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for jitter checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_latency_checks/","text":"Get links metrics for latency checks Documentation Launch get_all_links_metrics","title":"Get links metrics for latency checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_latency_checks/#get-links-metrics-for-latency-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for latency checks Documentation"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_packet_loss_checks/","text":"Get links metrics for packet loss checks Documentation Launch get_all_links_metrics","title":"Get links metrics for packet loss checks"},{"location":"logging/services/service-affecting-monitor/repositories/velocloud_repository/get_links_metrics_for_packet_loss_checks/#get-links-metrics-for-packet-loss-checks-documentation","text":"Launch get_all_links_metrics","title":"Get links metrics for packet loss checks Documentation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_reminder_note/","text":"Append reminder note append_note_to_ticket","title":" append reminder note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_reminder_note/#append-reminder-note","text":"append_note_to_ticket","title":"Append reminder note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_triage_note/","text":"Append triage note get_last_edge_events If status not OK: self._logger.warning(f\"Don't found last edge events for edge id: {edge_full_id}. Skipping append triage \" f\"note ...\") get_ticket_details If status not OK: self._logger.warning(f\"Don't found ticket details for ticket id: {ticket_id}. Skipping append triage \" f\"note ...\") build_triage_note self._logger.info(f\"Appending triage note to detail {ticket_detail_id} (serial {serial_number}) of ticket {ticket_id}...\") append_triage_note","title":" append triage note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_append_triage_note/#append-triage-note","text":"get_last_edge_events If status not OK: self._logger.warning(f\"Don't found last edge events for edge id: {edge_full_id}. Skipping append triage \" f\"note ...\") get_ticket_details If status not OK: self._logger.warning(f\"Don't found ticket details for ticket id: {ticket_id}. Skipping append triage \" f\"note ...\") build_triage_note self._logger.info(f\"Appending triage note to detail {ticket_detail_id} (serial {serial_number}) of ticket {ticket_id}...\") append_triage_note","title":"Append triage note"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_forward_to_asr/","text":"Attempt forward to asr self._logger.info(f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\") * If faulty edge: self._logger.info(f\"Outage of serial {serial_number} is caused by a faulty edge. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Searching serial {serial_number} for any wired links\") * If not wired links: self._logger.info(f\"No wired links are disconnected for serial {serial_number}. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f'following: {self._config.MONITOR_CONFIG[\"blacklisted_link_labels_for_asr_forwards\"]} ' f\"in the link label\") * If not whitelist links: self._logger.info(f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to ASR Investigate.\") * get_ticket_details * If status not Ok: self._logger.info(f\"Bad status calling get ticket details. Skipping forward asr ...\") self._logger.info(f\"Notes of ticket {ticket_id}: {notes_from_outage_ticket}\") * If task result note: self._logger.info(f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{target_queue}\"') * change_detail_work_queue * If status of change detail work queue in Ok: * append_asr_forwarding_note","title":" attempt forward to asr"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_forward_to_asr/#attempt-forward-to-asr","text":"self._logger.info(f\"Attempting to forward task of ticket {ticket_id} related to serial {serial_number} to ASR Investigate...\") * If faulty edge: self._logger.info(f\"Outage of serial {serial_number} is caused by a faulty edge. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Searching serial {serial_number} for any wired links\") * If not wired links: self._logger.info(f\"No wired links are disconnected for serial {serial_number}. Related detail of ticket {ticket_id} \" \"will not be forwarded to ASR Investigate.\") self._logger.info(f\"Filtering out any of the wired links of serial {serial_number} that contains any of the \" f'following: {self._config.MONITOR_CONFIG[\"blacklisted_link_labels_for_asr_forwards\"]} ' f\"in the link label\") * If not whitelist links: self._logger.info(f\"No links with whitelisted labels were found for serial {serial_number}. \" f\"Related detail of ticket {ticket_id} will not be forwarded to ASR Investigate.\") * get_ticket_details * If status not Ok: self._logger.info(f\"Bad status calling get ticket details. Skipping forward asr ...\") self._logger.info(f\"Notes of ticket {ticket_id}: {notes_from_outage_ticket}\") * If task result note: self._logger.info(f\"Detail related to serial {serial_number} of ticket {ticket_id} has already been forwarded to \" f'\"{target_queue}\"') * change_detail_work_queue * If status of change detail work queue in Ok: * append_asr_forwarding_note","title":"Attempt forward to asr"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_ticket_creation/","text":"Attempt ticket creation self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") create_outage_ticket self._logger.info(f\"[{outage_type.value}] Bruin response for ticket creation for edge {edge}: \" f\"{ticket_creation_response}\") If status is OK: self._logger.info(f\"[{outage_type.value}] Successfully created outage ticket for edge {edge}.\") _append_triage_note _change_severity If should schedule forward to hnoc queue: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email milestone status not ok: self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Else: _append_reminder_note If append reminder note status is not ok: self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" ) _check_for_digi_reboot If status 409: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} already has an outage ticket in \" f\"progress (ID = {ticket_id}). Skipping outage ticket creation for \" \"this edge...\") _change_ticket_severity If change severity is different to NOT_CHANGED: If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") get_ticket_details _send_reminder * * * * * * * * * * * * ** MIRAR ESTO _check_for_failed_digi_reboot _attempt_forward_to_asr If status 471: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Re-opening ticket...\") _reopen_outage_ticket _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append reminder is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 492 self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 473: self._logger.info(f\"[{outage_type.value}] There is a resolve outage ticket for the same location of faulty \" f\"edge {serial_number} (ticket ID = {ticket_id}). The ticket was \" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email status is no Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\")","title":" attempt ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_attempt_ticket_creation/#attempt-ticket-creation","text":"self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") create_outage_ticket self._logger.info(f\"[{outage_type.value}] Bruin response for ticket creation for edge {edge}: \" f\"{ticket_creation_response}\") If status is OK: self._logger.info(f\"[{outage_type.value}] Successfully created outage ticket for edge {edge}.\") _append_triage_note _change_severity If should schedule forward to hnoc queue: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email milestone status not ok: self._logger.error( f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\" ) Else: _append_reminder_note If append reminder note status is not ok: self._logger.error( f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\" ) _check_for_digi_reboot If status 409: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} already has an outage ticket in \" f\"progress (ID = {ticket_id}). Skipping outage ticket creation for \" \"this edge...\") _change_ticket_severity If change severity is different to NOT_CHANGED: If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") get_ticket_details _send_reminder * * * * * * * * * * * * ** MIRAR ESTO _check_for_failed_digi_reboot _attempt_forward_to_asr If status 471: self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Re-opening ticket...\") _reopen_outage_ticket _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info( f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\" ) send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append reminder is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 492 self._logger.info(f\"[{outage_type.value}] Faulty edge {serial_number} has a resolved outage ticket \" f\"(ID = {ticket_id}). Its ticket detail was automatically unresolved \" f\"by Bruin. Appending reopen note to ticket...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email is not Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\") If status 473: self._logger.info(f\"[{outage_type.value}] There is a resolve outage ticket for the same location of faulty \" f\"edge {serial_number} (ticket ID = {ticket_id}). The ticket was \" f\"automatically unresolved by Bruin and a new ticket detail for serial {serial_number} was \" f\"appended to it. Appending initial triage note for this service number...\") _append_triage_note _change_ticket_severity If should schedule hnoc forwarding: schedule_forward_to_hnoc_queue Else: self._logger.info(f\"Ticket_id: {ticket_id} for serial: {serial_number} \" f\"with link_data: {edge_links} has a blacklisted link and \" f\"should not be forwarded to HNOC. Skipping forward to HNOC...\") self._logger.info(f\"Sending an email for ticket_id: {ticket_id} \" f\"with serial: {serial_number} instead of scheduling forward to HNOC...\") send_initial_email_milestone_notification If send email status is no Ok: self._logger.error(f\"Reminder email of edge {serial_number} could not be sent for ticket\" f\" {ticket_id}!\") Else: _append_reminder_note If append note is not Ok: self._logger.error(f\"Reminder note of edge {serial_number} could not be appended to ticket\" f\" {ticket_id}!\")","title":"Attempt ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_change_ticket_severity/","text":"Change ticket severity self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") If is a faulty edge: self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that edge {serial_number} is offline.\") change_ticket_severity_for_offline_edge Else: If check ticket tasks get_ticket_details If response status is not OK: self._logger.warning(f\"Bad response calling get ticket details for ticket id: {ticket_id}. \" f\"The ticket severity don't change\") If ticket have multiple unresolved task self._logger.info(f\"Severity level of ticket {ticket_id} will remain the same, as the root cause of the outage \" f\"issue is that at least one link of edge {serial_number} is disconnected, and this ticket \" f\"has multiple unresolved tasks.\") self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that at least one link of edge {serial_number} is disconnected, and this ticket has a single \" \"unresolved task.\") get_ticket_details self._logger.warning( f\"Bad response calling get ticket for ticket id: {ticket_id}. The ticket severity don't change!\") If ticket already in severity level: self._logger.info( f\"Ticket {ticket_id} is already in severity level {target_severity}, so there is no need \" \"to change it.\") If change severity task response is not ok self._logger.info( f\"Bad response for change severity task. The ticket severity don't change\") self._logger.info( f\"Finished changing severity level of ticket {ticket_id} to {target_severity}!\" )","title":" change ticket severity"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_change_ticket_severity/#change-ticket-severity","text":"self._logger.info(f\"[{outage_type.value}] Attempting outage ticket creation for serial {serial_number}...\") If is a faulty edge: self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that edge {serial_number} is offline.\") change_ticket_severity_for_offline_edge Else: If check ticket tasks get_ticket_details If response status is not OK: self._logger.warning(f\"Bad response calling get ticket details for ticket id: {ticket_id}. \" f\"The ticket severity don't change\") If ticket have multiple unresolved task self._logger.info(f\"Severity level of ticket {ticket_id} will remain the same, as the root cause of the outage \" f\"issue is that at least one link of edge {serial_number} is disconnected, and this ticket \" f\"has multiple unresolved tasks.\") self._logger.info(f\"Severity level of ticket {ticket_id} is about to be changed, as the root cause of the outage issue \" f\"is that at least one link of edge {serial_number} is disconnected, and this ticket has a single \" \"unresolved task.\") get_ticket_details self._logger.warning( f\"Bad response calling get ticket for ticket id: {ticket_id}. The ticket severity don't change!\") If ticket already in severity level: self._logger.info( f\"Ticket {ticket_id} is already in severity level {target_severity}, so there is no need \" \"to change it.\") If change severity task response is not ok self._logger.info( f\"Bad response for change severity task. The ticket severity don't change\") self._logger.info( f\"Finished changing severity level of ticket {ticket_id} to {target_severity}!\" )","title":"Change ticket severity"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_digi_reboot/","text":"Check for digi reboot self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * reboot_link * If status of reboot link is not Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note * If status not Ok: self._logger.warning(f\" Bad status calling to append digi reboot note. Can't append the note\")","title":" check for digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_digi_reboot/#check-for-digi-reboot","text":"self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * reboot_link * If status of reboot link is not Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note * If status not Ok: self._logger.warning(f\" Bad status calling to append digi reboot note. Can't append the note\")","title":"Check for digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_failed_digi_reboot/","text":"Check for failed digi reboot self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * for link in digi_links: * get_ticket_details * If status is not Ok: self._logger.info(f\"Bad status calling to get ticket details checking failed digi reboot.\" f\" Skipping link ...\") * If not find digi note: self._logger.info(f\"No DiGi note was found for ticket {ticket_id}\") * If rebooted recently: self._logger.info(f\"The last DiGi reboot attempt for Edge {serial_number} did not occur \" f'{self._config.MONITOR_CONFIG[\"last_digi_reboot_seconds\"] / 60} or more mins ago.') * If interface note is same that link: * If not find wireless: self._logger.info(f'Task results has already been changed to \"{target_queue}\"') * change_detail_work_queue * If status Ok: * append_task_result_change_note * Else: * reboot_link * If reboot link status Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note","title":" check for failed digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_check_for_failed_digi_reboot/#check-for-failed-digi-reboot","text":"self._logger.info(f\"Checking edge {serial_number} for DiGi Links\") * for link in digi_links: * get_ticket_details * If status is not Ok: self._logger.info(f\"Bad status calling to get ticket details checking failed digi reboot.\" f\" Skipping link ...\") * If not find digi note: self._logger.info(f\"No DiGi note was found for ticket {ticket_id}\") * If rebooted recently: self._logger.info(f\"The last DiGi reboot attempt for Edge {serial_number} did not occur \" f'{self._config.MONITOR_CONFIG[\"last_digi_reboot_seconds\"] / 60} or more mins ago.') * If interface note is same that link: * If not find wireless: self._logger.info(f'Task results has already been changed to \"{target_queue}\"') * change_detail_work_queue * If status Ok: * append_task_result_change_note * Else: * reboot_link * If reboot link status Ok: self._logger.info(f'Attempting DiGi reboot of link with MAC address of {digi_link[\"logical_id\"]}' f\"in edge {serial_number}\") * append_digi_reboot_note","title":"Check for failed digi reboot"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_map_cached_edges_with_edges_status/","text":"Map cached edges with edges status Documentation for edge in edges: If not edge status: self._logger.info(f'No edge status was found for cached edge {cached_edge[\"serial_number\"]}. ' \"Skipping...\") If host == metvco03.mettel.net and enterprise id == 124: self._logger.info(f\"Edge {edge} was appended to the list of edges that have no status but\" f\"are in the customer cache.\")","title":" map cached edges with edges status"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_map_cached_edges_with_edges_status/#map-cached-edges-with-edges-status-documentation","text":"for edge in edges: If not edge status: self._logger.info(f'No edge status was found for cached edge {cached_edge[\"serial_number\"]}. ' \"Skipping...\") If host == metvco03.mettel.net and enterprise id == 124: self._logger.info(f\"Edge {edge} was appended to the list of edges that have no status but\" f\"are in the customer cache.\")","title":"Map cached edges with edges status Documentation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_outage_monitoring_process/","text":"Outage monitor process self._logger.info(f\"[outage_monitoring_process] Start with map cache!\") Get cache for outage monitor Check if cache status is not 200 self._logger.warning(\"Not found cache for service outage. Stop the outage monitoring process\" self._logger.info(\"[outage_monitoring_process] Ignoring blacklisted edges...\") self._logger.info(f\"List of serials from customer cache: {serials_for_monitoring}\") self._logger.info(\"[outage_monitoring_process] Creating list of whitelisted serials for autoresolve...\") self._logger.info(\"[outage_monitoring_process] Splitting cache by host\") self._logger.info(\"[outage_monitoring_process] Cache split\") _process_velocloud_host self._logger.info( f\"[outage_monitoring_process] Outage monitoring process finished! Elapsed time:\" f\"{round((stop - start) / 60, 2)} minutes\" )","title":" outage monitoring process"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_outage_monitoring_process/#outage-monitor-process","text":"self._logger.info(f\"[outage_monitoring_process] Start with map cache!\") Get cache for outage monitor Check if cache status is not 200 self._logger.warning(\"Not found cache for service outage. Stop the outage monitoring process\" self._logger.info(\"[outage_monitoring_process] Ignoring blacklisted edges...\") self._logger.info(f\"List of serials from customer cache: {serials_for_monitoring}\") self._logger.info(\"[outage_monitoring_process] Creating list of whitelisted serials for autoresolve...\") self._logger.info(\"[outage_monitoring_process] Splitting cache by host\") self._logger.info(\"[outage_monitoring_process] Cache split\") _process_velocloud_host self._logger.info( f\"[outage_monitoring_process] Outage monitoring process finished! Elapsed time:\" f\"{round((stop - start) / 60, 2)} minutes\" )","title":"Outage monitor process"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_process_velocloud_host/","text":"Process velocloud host Documentation self._logger.info(f\"Processing {len(host_cache)} edges in Velocloud {host}...\") get_link_with_edge_info If status get link with edge info not OK: self._logger.warning(f\"Not found links with edge info for host: {host}. Stop process velocloud host\") get_network_enterprises If status get network enterprises not OK: self._logger.warning(f\"Not found network enterprises for host: {host}. Stop process velocloud host\") self._logger.warning(f\"Link status with edge info from Velocloud: {links_with_edge_info}\") grouped_links_by_edge self._logger.info( \"Adding HA info to existing edges, and putting standby edges under monitoring as if they were \" \"standalone edges...\" ) map_edges_with_ha_info self._logger.info(f\"Service Outage monitoring is about to check {len(all_edges)} edges\") self._logger.info(f\"{len(serials_with_ha_disabled)} edges have HA disabled: {serials_with_ha_disabled}\") self._logger.info(f\"{len(serials_with_ha_enabled)} edges have HA enabled: {serials_with_ha_enabled}\") self._logger.info(f\"{len(primary_serials)} edges are the primary of a HA pair: {primary_serials}\") self._logger.info(f\"{len(standby_serials)} edges are the standby of a HA pair: {standby_serials}\") map_cached_edges_with_edges_status self._logger.info(f\"Mapped cache serials with status: {mapped_serials_w_status}\") For outage in outages: self._logger.info(f'{outage_type.value} serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in down_edges]}') self._logger.info( f\"{outage_type.value} serials that should be documented: \" f'{[e[\"status\"][\"edgeSerialNumber\"] for e in relevant_down_edges]}' ) If relevant down edges: self._logger.info(f\"{len(relevant_down_edges)} edges were detected in {outage_type.value} state.\") attempt_ticket_creation If ticket creation None: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge \" f\"with Business Grade Link(s): {ex}\") _schedule_recheck_job_for_edges Else: self._logger.info( f\"No edges were detected in {outage_type.value} state. \" f\"No ticket creations will trigger for this outage type\" ) self._logger.info(f'Healthy serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in healthy_edges]}') IF healthy edges: self._logger.info( f\"{len(healthy_edges)} edges were detected in healthy state. Running autoresolve for all of them...\" ) _run_ticket_autoresolve_for_edge Else: self._logger.info( \"No edges were detected in healthy state. Autoresolve won't be triggered\" ) self._logger.info(f\"Elapsed time processing edges in host {host}: {round((stop - start) / 60, 2)} minutes\")","title":" process velocloud host"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_process_velocloud_host/#process-velocloud-host-documentation","text":"self._logger.info(f\"Processing {len(host_cache)} edges in Velocloud {host}...\") get_link_with_edge_info If status get link with edge info not OK: self._logger.warning(f\"Not found links with edge info for host: {host}. Stop process velocloud host\") get_network_enterprises If status get network enterprises not OK: self._logger.warning(f\"Not found network enterprises for host: {host}. Stop process velocloud host\") self._logger.warning(f\"Link status with edge info from Velocloud: {links_with_edge_info}\") grouped_links_by_edge self._logger.info( \"Adding HA info to existing edges, and putting standby edges under monitoring as if they were \" \"standalone edges...\" ) map_edges_with_ha_info self._logger.info(f\"Service Outage monitoring is about to check {len(all_edges)} edges\") self._logger.info(f\"{len(serials_with_ha_disabled)} edges have HA disabled: {serials_with_ha_disabled}\") self._logger.info(f\"{len(serials_with_ha_enabled)} edges have HA enabled: {serials_with_ha_enabled}\") self._logger.info(f\"{len(primary_serials)} edges are the primary of a HA pair: {primary_serials}\") self._logger.info(f\"{len(standby_serials)} edges are the standby of a HA pair: {standby_serials}\") map_cached_edges_with_edges_status self._logger.info(f\"Mapped cache serials with status: {mapped_serials_w_status}\") For outage in outages: self._logger.info(f'{outage_type.value} serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in down_edges]}') self._logger.info( f\"{outage_type.value} serials that should be documented: \" f'{[e[\"status\"][\"edgeSerialNumber\"] for e in relevant_down_edges]}' ) If relevant down edges: self._logger.info(f\"{len(relevant_down_edges)} edges were detected in {outage_type.value} state.\") attempt_ticket_creation If ticket creation None: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge \" f\"with Business Grade Link(s): {ex}\") _schedule_recheck_job_for_edges Else: self._logger.info( f\"No edges were detected in {outage_type.value} state. \" f\"No ticket creations will trigger for this outage type\" ) self._logger.info(f'Healthy serials: {[e[\"status\"][\"edgeSerialNumber\"] for e in healthy_edges]}') IF healthy edges: self._logger.info( f\"{len(healthy_edges)} edges were detected in healthy state. Running autoresolve for all of them...\" ) _run_ticket_autoresolve_for_edge Else: self._logger.info( \"No edges were detected in healthy state. Autoresolve won't be triggered\" ) self._logger.info(f\"Elapsed time processing edges in host {host}: {round((stop - start) / 60, 2)} minutes\")","title":"Process velocloud host Documentation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_recheck_edges_for_ticket_creation/","text":"Recheck edges for ticket creation self._logger.info(f\"[{outage_type.value}] Re-checking {len(outage_edges)} edges in outage state prior to ticket creation...\") self._logger.info(f\"[{outage_type.value}] Edges in outage before quarantine recheck: {outage_edges}\") * get_links_with_edge_info * If get links with edge info status not Ok: self._logger.warning(f\"Bad status calling to get links with edge info for host: {host}. Skipping recheck ...\") * get_network_enterprises * If get network enterprises tatus not Ok: self._logger.warning(f\"Bad status calling to get network enterprises info for host: {host}. Skipping recheck ...\") self._logger.info(f\"[{outage_type.value}] Velocloud edge status response in quarantine recheck: \" f\"{links_with_edge_info_response}\") * group_links_by_edge self._logger.info(f\"[{outage_type.value}] Adding HA info to existing edges, and putting standby edges under monitoring as if \" \"they were standalone edges...\") * map_edges_with_ha_info * get_edges_with_standbys_as_standalone_edges * _map_cached_edges_with_edges_status self._logger.info(f\"[{outage_type.value}] Current status of edges that were in outage state: {edges_full_info}\") self._logger.info(f\"[{outage_type.value}] Edges still in outage state after recheck: {edges_still_down}\") self._logger.info(f\"[{outage_type.value}] Serials still in outage state after recheck: {serials_still_down}\") self._logger.info(f\"[{outage_type.value}] Edges that are healthy after recheck: {healthy_edges}\") self._logger.info(f\"[{outage_type.value}] Serials that are healthy after recheck: {healthy_serials}\") * If edges still down: self._logger.info(f\"[{outage_type.value}] {len(edges_still_down)} edges are still in outage state after re-check. \" \"Attempting outage ticket creation for all of them...\") * If environment PRODUCTION: * _attempt_ticket_creation * If error in attempt ticket creation: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge in \" f\"the quarantine: {ex}\") * Else: self._logger.info(f\"[{outage_type.value}] Not starting outage ticket creation for {len(edges_still_down)} faulty \" f\"edges because the current working environment is {working_environment.upper()}.\") * Else: self._logger.info(f\"[{outage_type.value}] No edges were detected in outage state after re-check. \" \"Outage tickets won't be created\") * If healthy edges: self._logger.info( f\"[{outage_type.value}] {len(healthy_edges)} edges were detected in healthy state after re-check. '\" \"Running autoresolve for all of them...\" ) self._logger.info( f\"[{outage_type.value}] Edges that are going to be attempted to autoresolve: {healthy_edges}\" ) * _run_ticket_autoresolve_for_edge * Else: self._logger.info( f\"[{outage_type.value}] No edges were detected in healthy state. \" \"Autoresolve won't be triggered\" ) self._logger.info(f\"[{outage_type.value}] Re-check process finished for {len(outage_edges)} edges\")","title":" recheck edges for ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_recheck_edges_for_ticket_creation/#recheck-edges-for-ticket-creation","text":"self._logger.info(f\"[{outage_type.value}] Re-checking {len(outage_edges)} edges in outage state prior to ticket creation...\") self._logger.info(f\"[{outage_type.value}] Edges in outage before quarantine recheck: {outage_edges}\") * get_links_with_edge_info * If get links with edge info status not Ok: self._logger.warning(f\"Bad status calling to get links with edge info for host: {host}. Skipping recheck ...\") * get_network_enterprises * If get network enterprises tatus not Ok: self._logger.warning(f\"Bad status calling to get network enterprises info for host: {host}. Skipping recheck ...\") self._logger.info(f\"[{outage_type.value}] Velocloud edge status response in quarantine recheck: \" f\"{links_with_edge_info_response}\") * group_links_by_edge self._logger.info(f\"[{outage_type.value}] Adding HA info to existing edges, and putting standby edges under monitoring as if \" \"they were standalone edges...\") * map_edges_with_ha_info * get_edges_with_standbys_as_standalone_edges * _map_cached_edges_with_edges_status self._logger.info(f\"[{outage_type.value}] Current status of edges that were in outage state: {edges_full_info}\") self._logger.info(f\"[{outage_type.value}] Edges still in outage state after recheck: {edges_still_down}\") self._logger.info(f\"[{outage_type.value}] Serials still in outage state after recheck: {serials_still_down}\") self._logger.info(f\"[{outage_type.value}] Edges that are healthy after recheck: {healthy_edges}\") self._logger.info(f\"[{outage_type.value}] Serials that are healthy after recheck: {healthy_serials}\") * If edges still down: self._logger.info(f\"[{outage_type.value}] {len(edges_still_down)} edges are still in outage state after re-check. \" \"Attempting outage ticket creation for all of them...\") * If environment PRODUCTION: * _attempt_ticket_creation * If error in attempt ticket creation: self._logger.error(f\"[{outage_type.value}] Error while attempting ticket creation(s) for edge in \" f\"the quarantine: {ex}\") * Else: self._logger.info(f\"[{outage_type.value}] Not starting outage ticket creation for {len(edges_still_down)} faulty \" f\"edges because the current working environment is {working_environment.upper()}.\") * Else: self._logger.info(f\"[{outage_type.value}] No edges were detected in outage state after re-check. \" \"Outage tickets won't be created\") * If healthy edges: self._logger.info( f\"[{outage_type.value}] {len(healthy_edges)} edges were detected in healthy state after re-check. '\" \"Running autoresolve for all of them...\" ) self._logger.info( f\"[{outage_type.value}] Edges that are going to be attempted to autoresolve: {healthy_edges}\" ) * _run_ticket_autoresolve_for_edge * Else: self._logger.info( f\"[{outage_type.value}] No edges were detected in healthy state. \" \"Autoresolve won't be triggered\" ) self._logger.info(f\"[{outage_type.value}] Re-check process finished for {len(outage_edges)} edges\")","title":"Recheck edges for ticket creation"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_reopen_outage_ticket/","text":"Reopen outage ticket self._logger.info(f\"Reopening outage ticket {ticket_id} for serial {serial_number}...\") * get_ticket_details * If get ticket details status is not Ok: self._logger.info(f\"Bad status calling to get ticket details. Skipping reopen ticket ...\") * open_ticket * If open ticket status is Ok: self._logger.info(f\"Detail {detail_id_for_reopening} of outage ticket {ticket_id} reopened successfully.\") * _append_triage_note * Else: self._logger.error(f\"Reopening for detail {detail_id_for_reopening} of outage ticket {ticket_id} failed.\")","title":" reopen outage ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_reopen_outage_ticket/#reopen-outage-ticket","text":"self._logger.info(f\"Reopening outage ticket {ticket_id} for serial {serial_number}...\") * get_ticket_details * If get ticket details status is not Ok: self._logger.info(f\"Bad status calling to get ticket details. Skipping reopen ticket ...\") * open_ticket * If open ticket status is Ok: self._logger.info(f\"Detail {detail_id_for_reopening} of outage ticket {ticket_id} reopened successfully.\") * _append_triage_note * Else: self._logger.error(f\"Reopening for detail {detail_id_for_reopening} of outage ticket {ticket_id} failed.\")","title":"Reopen outage ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_run_ticket_autoresolve_for_edge/","text":"Run ticket autoresolve for edge self._logger.info(f\"[ticket-autoresolve] Starting autoresolve for edge {serial_number}...\") If serial number not in autoresolve serial whitelist: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} because its \" f\"serial ({serial_number}) is not whitelisted.\") get_open_outage_tickets self._logger.info(f\"Bad status calling for outage tickets for client id: {client_id} and serial: {serial_number}. \" f\"Skipping autoresolve ...\") If not found outage tickets: self._logger.info(f\"[ticket-autoresolve] No outage ticket found for edge {serial_number}. \" f\"Skipping autoresolve...\") If ticket not created by automation: self._logger.info(f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") get_ticket_details self._logger.info(f\"Bad status calling get ticket details for outage ticket: {outage_ticket_id}. \" f\"Skipping autoresolve ...\") If is ticket task in ipa queue: self._logger.info(f\"Task for serial {serial_number} in ticket {outage_ticket_id} is in the IPA Investigate queue. \" f\"Skipping checks for max auto-resolves and grace period to auto-resolve after last documented \" f\"outage...\") Else: If was last outage detect recently: self._logger.info(f\"Edge {serial_number} has been in outage state for a long time, so detail {ticket_detail_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\") If can't autoresolve one time more: self._logger.info(f\"[ticket-autoresolve] Limit to autoresolve detail {ticket_detail_id} (serial {serial_number}) \" f\"of ticket {outage_ticket_id} linked to edge {serial_number} has been maxed out already. \" \"Skipping autoresolve...\") If is detail resolved: self._logger.info(f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} is already \" \"resolved. Skipping autoresolve...\") If not PRODUCTION: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} since the \" f\"current environment is {working_environment.upper()}.\") self._logger.info(f\"Autoresolving detail {ticket_detail_id} of ticket {outage_ticket_id} linked to edge \" f\"{serial_number} with serial number {serial_number}...\") unpause_ticket_detail resolve_ticket self._logger.warning(f\"Bad status calling resolve ticket for outage ticket_id: {outage_ticket_id} and\" f\"ticket detail: {ticket_detail_id}. Skipping autoresolve ...\") append_autoresolve_note_to_ticket self._logger.info( f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} linked to \" f\"edge {serial_number} was autoresolved!\" )","title":" run ticket autoresolve for edge"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_run_ticket_autoresolve_for_edge/#run-ticket-autoresolve-for-edge","text":"self._logger.info(f\"[ticket-autoresolve] Starting autoresolve for edge {serial_number}...\") If serial number not in autoresolve serial whitelist: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} because its \" f\"serial ({serial_number}) is not whitelisted.\") get_open_outage_tickets self._logger.info(f\"Bad status calling for outage tickets for client id: {client_id} and serial: {serial_number}. \" f\"Skipping autoresolve ...\") If not found outage tickets: self._logger.info(f\"[ticket-autoresolve] No outage ticket found for edge {serial_number}. \" f\"Skipping autoresolve...\") If ticket not created by automation: self._logger.info(f\"Ticket {outage_ticket_id} was not created by Automation Engine. Skipping autoresolve...\") get_ticket_details self._logger.info(f\"Bad status calling get ticket details for outage ticket: {outage_ticket_id}. \" f\"Skipping autoresolve ...\") If is ticket task in ipa queue: self._logger.info(f\"Task for serial {serial_number} in ticket {outage_ticket_id} is in the IPA Investigate queue. \" f\"Skipping checks for max auto-resolves and grace period to auto-resolve after last documented \" f\"outage...\") Else: If was last outage detect recently: self._logger.info(f\"Edge {serial_number} has been in outage state for a long time, so detail {ticket_detail_id} \" f\"(serial {serial_number}) of ticket {outage_ticket_id} will not be autoresolved. Skipping \" f\"autoresolve...\") If can't autoresolve one time more: self._logger.info(f\"[ticket-autoresolve] Limit to autoresolve detail {ticket_detail_id} (serial {serial_number}) \" f\"of ticket {outage_ticket_id} linked to edge {serial_number} has been maxed out already. \" \"Skipping autoresolve...\") If is detail resolved: self._logger.info(f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} is already \" \"resolved. Skipping autoresolve...\") If not PRODUCTION: self._logger.info(f\"[ticket-autoresolve] Skipping autoresolve for edge {serial_number} since the \" f\"current environment is {working_environment.upper()}.\") self._logger.info(f\"Autoresolving detail {ticket_detail_id} of ticket {outage_ticket_id} linked to edge \" f\"{serial_number} with serial number {serial_number}...\") unpause_ticket_detail resolve_ticket self._logger.warning(f\"Bad status calling resolve ticket for outage ticket_id: {outage_ticket_id} and\" f\"ticket detail: {ticket_detail_id}. Skipping autoresolve ...\") append_autoresolve_note_to_ticket self._logger.info( f\"Detail {ticket_detail_id} (serial {serial_number}) of ticket {outage_ticket_id} linked to \" f\"edge {serial_number} was autoresolved!\" )","title":"Run ticket autoresolve for edge"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_schedule_recheck_job_for_edges/","text":"Schedule recheck job for edges self._logger.info(f\"Scheduling recheck job for {len(edges)} edges in {outage_type.value} state...\") * _recheck_edges_for_ticket_creation self._logger.info(f\"Edges in {outage_type.value} state scheduled for recheck successfully\")","title":" schedule recheck job for edges"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_schedule_recheck_job_for_edges/#schedule-recheck-job-for-edges","text":"self._logger.info(f\"Scheduling recheck job for {len(edges)} edges in {outage_type.value} state...\") * _recheck_edges_for_ticket_creation self._logger.info(f\"Edges in {outage_type.value} state scheduled for recheck successfully\")","title":"Schedule recheck job for edges"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_send_reminder/","text":"Send reminder self._logger.info(f\"Attempting to send reminder for service number {service_number} to ticket {ticket_id}\") * If not should reminder notification: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id},\" f\" since either the last documentation cycle started or the last reminder\" f\" was sent too recently\") * If working environment not PRODUCTION: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id} since \" f\"the current environment is {working_environment.upper()}\") * send_reminder_email_milestone_notification * If status not OK: self._logger.error(f\"Reminder email of edge {service_number} could not be sent for ticket\" f\" {ticket_id}!\") * _append_reminder_note * If status to append reminder note not Ok: self._logger.error(f\"Reminder note of edge {service_number} could not be appended to ticket\" f\" {ticket_id}!\") self._logger.error(f\"Reminder note of edge {service_number} was successfully appended to ticket\" f\" {ticket_id}!\")","title":" send reminder"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/_send_reminder/#send-reminder","text":"self._logger.info(f\"Attempting to send reminder for service number {service_number} to ticket {ticket_id}\") * If not should reminder notification: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id},\" f\" since either the last documentation cycle started or the last reminder\" f\" was sent too recently\") * If working environment not PRODUCTION: self._logger.info(f\"No Reminder note will be appended for service number {service_number} to ticket {ticket_id} since \" f\"the current environment is {working_environment.upper()}\") * send_reminder_email_milestone_notification * If status not OK: self._logger.error(f\"Reminder email of edge {service_number} could not be sent for ticket\" f\" {ticket_id}!\") * _append_reminder_note * If status to append reminder note not Ok: self._logger.error(f\"Reminder note of edge {service_number} could not be appended to ticket\" f\" {ticket_id}!\") self._logger.error(f\"Reminder note of edge {service_number} was successfully appended to ticket\" f\" {ticket_id}!\")","title":"Send reminder"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/append_note_to_ticket/","text":"Append note to ticket","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/append_note_to_ticket/#append-note-to-ticket","text":"","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/change_detail_work_queue_to_hnoc/","text":"Change detail work queue to hnoc change_detail_work_queue If change detail work queue status is ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") Else: self._logger.error( f\"Failed to forward ticket_id {ticket_id} and \" f\"serial {serial_number} to {target_queue} due to bruin \" f\"returning {change_detail_work_queue_response} when attempting to forward to HNOC.\" )","title":"Change detail work queue to hnoc"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/change_detail_work_queue_to_hnoc/#change-detail-work-queue-to-hnoc","text":"change_detail_work_queue If change detail work queue status is ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") Else: self._logger.error( f\"Failed to forward ticket_id {ticket_id} and \" f\"serial {serial_number} to {target_queue} due to bruin \" f\"returning {change_detail_work_queue_response} when attempting to forward to HNOC.\" )","title":"Change detail work queue to hnoc"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/forward_ticket_to_hnoc_queue/","text":"Forward_ticket_to_hnoc_queue self._logger.info(f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\") * get_ticket_details * If ticket details status is not OK self._logger.info(f\"Getting ticket details of ticket_id {ticket_id} and serial {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\") * If detail is resolved: self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is resolved. \" f\"Skipping forward to HNOC...\") self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is not resolved. \" f\"Forwarding to HNOC...\") * change_detail_work_queue_to_hnoc * If Exception: self._logger.error(f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\")","title":"Forward ticket to hnoc queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/forward_ticket_to_hnoc_queue/#forward_ticket_to_hnoc_queue","text":"self._logger.info(f\"Checking if ticket_id {ticket_id} for serial {serial_number} is resolved before \" f\"attempting to forward to HNOC...\") * get_ticket_details * If ticket details status is not OK self._logger.info(f\"Getting ticket details of ticket_id {ticket_id} and serial {serial_number} \" f\"from Bruin failed: {ticket_details_response}. \" f\"Retrying forward to HNOC...\") * If detail is resolved: self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is resolved. \" f\"Skipping forward to HNOC...\") self._logger.info(f\"Ticket id {ticket_id} for serial {serial_number} is not resolved. \" f\"Forwarding to HNOC...\") * change_detail_work_queue_to_hnoc * If Exception: self._logger.error(f\"An error occurred while trying to forward ticket_id {ticket_id} for serial {serial_number} to HNOC\" f\" -> {e}\")","title":"Forward_ticket_to_hnoc_queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/schedule_forward_to_hnoc_queue/","text":"Schedule forward to hnoc queue self._logger.info(f\"Scheduling HNOC forwarding for ticket_id {ticket_id} and serial {serial_number}\" f\" to happen at timestamp: {forward_task_run_date}\") * Add job * forward_ticket_to_hnoc_queue","title":"Schedule forward to hnoc queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/schedule_forward_to_hnoc_queue/#schedule-forward-to-hnoc-queue","text":"self._logger.info(f\"Scheduling HNOC forwarding for ticket_id {ticket_id} and serial {serial_number}\" f\" to happen at timestamp: {forward_task_run_date}\") * Add job * forward_ticket_to_hnoc_queue","title":"Schedule forward to hnoc queue"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/start_service_outage_monitoring/","text":"Start service outage monitoring ( start the process of outage ) self._logger.info(\"Scheduling Service Outage Monitor job...\") * If exe on start: self._logger.info(\"Service Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process * If Exception: self._logger.error(f\"Skipping start of Service Outage Monitoring job. Reason: {conflict}\")","title":"Start service outage monitoring"},{"location":"logging/services/service-outage-monitor/actions/outage_monitoring/start_service_outage_monitoring/#start-service-outage-monitoring-start-the-process-of-outage","text":"self._logger.info(\"Scheduling Service Outage Monitor job...\") * If exe on start: self._logger.info(\"Service Outage Monitor job is going to be executed immediately\") * _outage_monitoring_process * If Exception: self._logger.error(f\"Skipping start of Service Outage Monitoring job. Reason: {conflict}\")","title":"Start service outage monitoring (start the process of outage)"},{"location":"logging/services/service-outage-monitor/actions/triage/_append_new_triage_notes_based_on_recent_events/","text":"Append new triage notes based on recent events self._logger.info(f\"Appending new triage note to detail {ticket_detail_id} of ticket {ticket_id}...\") self._logger.info( f\"Getting events for serial {service_number} (detail {ticket_detail_id}) in ticket \" f\"{ticket_id} before applying triage...\" ) * get_last_edge_events * If get last events status is not Ok: self._logger.warning(f\"Bad status calling get last edge events for edge: {edge_full_id}. \" f\"Skipping append triage notes based in recent events ...\") * If not recent events: self._logger.info( f\"No events were found for edge {service_number} starting from {events_lookup_timestamp}. \" f\"Not appending any new triage notes to detail {ticket_detail_id} of ticket {ticket_id}.\" ) * For chunk in event chunked: * If environment is PRODUCTION: * append_note_to_ticket * If append note status is not Ok: self._logger.warning(f\"Bad status apeending note to ticket: {ticket_id}. Skipping append note ...\") self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * Else: self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * If note appended: * _notify_triage_note_was_appended_to_ticket","title":" append new triage notes based on recent events"},{"location":"logging/services/service-outage-monitor/actions/triage/_append_new_triage_notes_based_on_recent_events/#append-new-triage-notes-based-on-recent-events","text":"self._logger.info(f\"Appending new triage note to detail {ticket_detail_id} of ticket {ticket_id}...\") self._logger.info( f\"Getting events for serial {service_number} (detail {ticket_detail_id}) in ticket \" f\"{ticket_id} before applying triage...\" ) * get_last_edge_events * If get last events status is not Ok: self._logger.warning(f\"Bad status calling get last edge events for edge: {edge_full_id}. \" f\"Skipping append triage notes based in recent events ...\") * If not recent events: self._logger.info( f\"No events were found for edge {service_number} starting from {events_lookup_timestamp}. \" f\"Not appending any new triage notes to detail {ticket_detail_id} of ticket {ticket_id}.\" ) * For chunk in event chunked: * If environment is PRODUCTION: * append_note_to_ticket * If append note status is not Ok: self._logger.warning(f\"Bad status apeending note to ticket: {ticket_id}. Skipping append note ...\") self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * Else: self._logger.info(f\"Triage appended to detail {ticket_detail_id} of ticket {ticket_id}!\") * If note appended: * _notify_triage_note_was_appended_to_ticket","title":"Append new triage notes based on recent events"},{"location":"logging/services/service-outage-monitor/actions/triage/_build_edges_status_by_serial/","text":"Build edges status by serial get_edges_for_triage get_network_enterprises_for_triage map_edges_with_ha_info","title":" build edges status by serial"},{"location":"logging/services/service-outage-monitor/actions/triage/_build_edges_status_by_serial/#build-edges-status-by-serial","text":"get_edges_for_triage get_network_enterprises_for_triage map_edges_with_ha_info","title":"Build edges status by serial"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_irrelevant_notes_in_tickets/","text":"Filter irrelevant notes in tickets For ticket in tickets: self._logger.info(f'Filtering notes for ticket_id: {ticket[\"ticket_id\"]} to contain relevant notes')","title":" filter irrelevant notes in tickets"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_irrelevant_notes_in_tickets/#filter-irrelevant-notes-in-tickets","text":"For ticket in tickets: self._logger.info(f'Filtering notes for ticket_id: {ticket[\"ticket_id\"]} to contain relevant notes')","title":"Filter irrelevant notes in tickets"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_tickets_and_details_related_to_edges_under_monitoring/","text":"Filter tickets and details related to edges under monitoring For ticket in tickets: self._logger.info(f'Checking ticket_id: {ticket[\"ticket_id\"]} for relevant details') If not relevant details: self._logger.info(f'Ticket with ticket_id: {ticket[\"ticket_id\"]} has no relevant details') self._logger.info( f'Ticket with ticket_id: {ticket[\"ticket_id\"]} contains relevant details.' f\"Appending to relevant_tickets list ...\" )","title":" filter tickets and details related to edges under monitoring"},{"location":"logging/services/service-outage-monitor/actions/triage/_filter_tickets_and_details_related_to_edges_under_monitoring/#filter-tickets-and-details-related-to-edges-under-monitoring","text":"For ticket in tickets: self._logger.info(f'Checking ticket_id: {ticket[\"ticket_id\"]} for relevant details') If not relevant details: self._logger.info(f'Ticket with ticket_id: {ticket[\"ticket_id\"]} has no relevant details') self._logger.info( f'Ticket with ticket_id: {ticket[\"ticket_id\"]} contains relevant details.' f\"Appending to relevant_tickets list ...\" )","title":"Filter tickets and details related to edges under monitoring"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_all_open_tickets_with_details_for_monitored_companies/","text":"Get all open tickets with details for monitored companies get_open_outage_tickets If get open outage status is not Ok: self._logger.warning(f\"Bad status calling to open tickets. Return an empty list ...\") self._logger.info(\"Getting all opened tickets details for each open ticket ...\") _get_open_tickets_with_details_by_ticket_id self._logger.info(\"Finished getting all opened ticket details!\")","title":" get all open tickets with details for monitored companies"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_all_open_tickets_with_details_for_monitored_companies/#get-all-open-tickets-with-details-for-monitored-companies","text":"get_open_outage_tickets If get open outage status is not Ok: self._logger.warning(f\"Bad status calling to open tickets. Return an empty list ...\") self._logger.info(\"Getting all opened tickets details for each open ticket ...\") _get_open_tickets_with_details_by_ticket_id self._logger.info(\"Finished getting all opened ticket details!\")","title":"Get all open tickets with details for monitored companies"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_open_tickets_with_details_by_ticket_id/","text":"Get open tickets with details by ticket id get_ticket_details If get ticket detail status is not Ok: self._logger.warning( f\"Bad status calling get ticket details for ticket id: {ticket_id}. \" f\"Skipping get ticket details ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get tickets details for ticket_id {ticket_id} -> {e}\" )","title":" get open tickets with details by ticket id"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_open_tickets_with_details_by_ticket_id/#get-open-tickets-with-details-by-ticket-id","text":"get_ticket_details If get ticket detail status is not Ok: self._logger.warning( f\"Bad status calling get ticket details for ticket id: {ticket_id}. \" f\"Skipping get ticket details ...\") If not ticket details: self._logger.info( f\"Ticket {ticket_id} doesn't have any detail under ticketDetails key. \" f\"Skipping...\" ) self._logger.info(f\"Got details for ticket {ticket_id}!\") If Exception: self._logger.error( f\"An error occurred while trying to get tickets details for ticket_id {ticket_id} -> {e}\" )","title":"Get open tickets with details by ticket id"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_ticket_details_with_and_without_triage/","text":"Get ticket details with and without triage For ticket in tickets: self._logger.info(f\"Checking details of ticket_id: {ticket_id}\") For detail in ticket details: self._logger.info( f\"Checking for triage notes in ticket_id: {ticket_id} \" f\"relating to serial number: {serial_number}\" ) If notes related to serial: self._logger.info( f\"No triage notes found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_without_triage list...\" ) Else: sself._logger.info( f\"Triage note found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_with_triage list...\" )","title":" get ticket details with and without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_get_ticket_details_with_and_without_triage/#get-ticket-details-with-and-without-triage","text":"For ticket in tickets: self._logger.info(f\"Checking details of ticket_id: {ticket_id}\") For detail in ticket details: self._logger.info( f\"Checking for triage notes in ticket_id: {ticket_id} \" f\"relating to serial number: {serial_number}\" ) If notes related to serial: self._logger.info( f\"No triage notes found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_without_triage list...\" ) Else: sself._logger.info( f\"Triage note found in ticket_id: {ticket_id} \" f\"for serial number {serial_number}. \" f\"Adding to ticket_details_with_triage list...\" )","title":"Get ticket details with and without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_notify_triage_note_was_appended_to_ticket/","text":"Notify triage note was appended to ticket self._logger.info(f\"Triage appended to detail {ticket_detail_id} (serial: {service_number}) of ticket {ticket_id}. \" f\"Details at https://app.bruin.com/t/{ticket_id}\")","title":" notify triage note was appended to ticket"},{"location":"logging/services/service-outage-monitor/actions/triage/_notify_triage_note_was_appended_to_ticket/#notify-triage-note-was-appended-to-ticket","text":"self._logger.info(f\"Triage appended to detail {ticket_detail_id} (serial: {service_number}) of ticket {ticket_id}. \" f\"Details at https://app.bruin.com/t/{ticket_id}\")","title":"Notify triage note was appended to ticket"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_with_triage/","text":"Process ticket details with triage self._logger.info(\"Processing ticket details with triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} with triage of ticket {ticket_id}...\") self._logger.info( f\"Checking if events need to be appended to detail {ticket_detail_id} of ticket {ticket_id}...\" ) * If ticket note append recently: self._logger.info( f\"The last triage note was appended to detail {ticket_detail_id} of ticket \" f\"{ticket_id} not long ago so no new triage note will be appended for now\" ) self._logger.info(f\"Appending events to detail {ticket_detail_id} of ticket {ticket_id}...\") * _append_new_triage_notes_based_on_recent_events self._logger.info(f\"Events appended to detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details with triage!\")","title":" process ticket details with triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_with_triage/#process-ticket-details-with-triage","text":"self._logger.info(\"Processing ticket details with triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} with triage of ticket {ticket_id}...\") self._logger.info( f\"Checking if events need to be appended to detail {ticket_detail_id} of ticket {ticket_id}...\" ) * If ticket note append recently: self._logger.info( f\"The last triage note was appended to detail {ticket_detail_id} of ticket \" f\"{ticket_id} not long ago so no new triage note will be appended for now\" ) self._logger.info(f\"Appending events to detail {ticket_detail_id} of ticket {ticket_id}...\") * _append_new_triage_notes_based_on_recent_events self._logger.info(f\"Events appended to detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details with triage!\")","title":"Process ticket details with triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_without_triage/","text":"Process ticket details without triage self._logger.info(\"Processing ticket details without triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} without triage of ticket {ticket_id}...\") * If not outage type: self._logger.info( f\"Edge {serial_number} is no longer down, so the initial triage note won't be posted to ticket \" f\"{ticket_id}. Posting events of the last 24 hours to the ticket so it's not blank...\" ) * _append_new_triage_notes_based_on_recent_events * Else: self._logger.info( f\"Edge {serial_number} is in {outage_type.value} state. Posting initial triage note to ticket \" f\"{ticket_id}...\" ) * If not document outage: self._logger.info( f\"Edge {serial_number} is down, but it doesn't qualify to be documented as a Service Outage in \" f\"ticket {ticket_id}. Most probable thing is that the edge is the standby of a HA pair, and \" \"standbys in outage state are only documented in the event of a Soft Down. Skipping...\" ) * get_last_edge_events * If get last edge events not Ok: self._logger.warning(f\"Bad status calling to get last edge events. \" f\"Skipping process details without details ...\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details without triage!\")","title":" process ticket details without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_process_ticket_details_without_triage/#process-ticket-details-without-triage","text":"self._logger.info(\"Processing ticket details without triage...\") * For detail in details: self._logger.info(f\"Processing detail {ticket_detail_id} without triage of ticket {ticket_id}...\") * If not outage type: self._logger.info( f\"Edge {serial_number} is no longer down, so the initial triage note won't be posted to ticket \" f\"{ticket_id}. Posting events of the last 24 hours to the ticket so it's not blank...\" ) * _append_new_triage_notes_based_on_recent_events * Else: self._logger.info( f\"Edge {serial_number} is in {outage_type.value} state. Posting initial triage note to ticket \" f\"{ticket_id}...\" ) * If not document outage: self._logger.info( f\"Edge {serial_number} is down, but it doesn't qualify to be documented as a Service Outage in \" f\"ticket {ticket_id}. Most probable thing is that the edge is the standby of a HA pair, and \" \"standbys in outage state are only documented in the event of a Soft Down. Skipping...\" ) * get_last_edge_events * If get last edge events not Ok: self._logger.warning(f\"Bad status calling to get last edge events. \" f\"Skipping process details without details ...\") self._logger.info(f\"Finished processing detail {ticket_detail_id} of ticket {ticket_id}!\") self._logger.info(\"Finished processing ticket details without triage!\")","title":"Process ticket details without triage"},{"location":"logging/services/service-outage-monitor/actions/triage/_run_tickets_polling/","text":"Run tickets polling self._logger.info(f\"Starting triage process...\") * get_cache_for_triage_monitoring self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got all {len(open_tickets)} open tickets for all customers. \" f\"Filtering them to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) * _filter_irrelevant_notes_in_tickets self._logger.info(f\"Splitting relevant tickets in tickets with and without triage...\") * _get_ticket_details_with_and_without_triage self._logger.info( f\"Ticket details split successfully. \" f\"Ticket details with triage: {len(details_with_triage)}. \" f\"Ticket details without triage: {len(details_without_triage)}. \" \"Processing both sets...\" ) * _build_edges_status_by_serial * _process_ticket_details_with_triage self._logger.info(f\"Triage process finished! took {time.time() - total_start_time} seconds\")","title":" run tickets polling"},{"location":"logging/services/service-outage-monitor/actions/triage/_run_tickets_polling/#run-tickets-polling","text":"self._logger.info(f\"Starting triage process...\") * get_cache_for_triage_monitoring self._logger.info(\"Getting all open tickets for all customers...\") * _get_all_open_tickets_with_details_for_monitored_companies self._logger.info( f\"Got all {len(open_tickets)} open tickets for all customers. \" f\"Filtering them to get only the ones under the device list\" ) * _filter_tickets_and_details_related_to_edges_under_monitoring self._logger.info( f\"Got {len(relevant_open_tickets)} relevant tickets for all customers. \" f\"Cleaning them up to exclude all invalid notes...\" ) * _filter_irrelevant_notes_in_tickets self._logger.info(f\"Splitting relevant tickets in tickets with and without triage...\") * _get_ticket_details_with_and_without_triage self._logger.info( f\"Ticket details split successfully. \" f\"Ticket details with triage: {len(details_with_triage)}. \" f\"Ticket details without triage: {len(details_without_triage)}. \" \"Processing both sets...\" ) * _build_edges_status_by_serial * _process_ticket_details_with_triage self._logger.info(f\"Triage process finished! took {time.time() - total_start_time} seconds\")","title":"Run tickets polling"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_asr_forwarding_note/","text":"Append asr forwarding note append_note_to_ticket","title":"Append asr forwarding note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_asr_forwarding_note/#append-asr-forwarding-note","text":"append_note_to_ticket","title":"Append asr forwarding note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/","text":"Append autoresolve note to ticket append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_autoresolve_note_to_ticket/#append-autoresolve-note-to-ticket","text":"append_note_to_ticket","title":"Append autoresolve note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_digi_reboot_note/","text":"Append digi reboot note append_note_to_ticket","title":"Append digi reboot note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_digi_reboot_note/#append-digi-reboot-note","text":"append_note_to_ticket","title":"Append digi reboot note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_note_to_ticket/","text":"Append note to ticket If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_note_to_ticket/#append-note-to-ticket","text":"If service number: self._logger.info(f'Appending note for service number(s) {\", \".join(service_numbers)} in ticket {ticket_id}...') Else: self._logger.info(f\"Appending note for all service number(s) in ticket {ticket_id}...\") If Exception: self._logger.error(f\"An error occurred when appending a ticket note to ticket {ticket_id}. \" f\"Ticket note: {note}. Error: {e}\") If status not ok: self._logger.error(f\"Error while appending note to ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. Note was {note}. Error: \" f\"Error {response_status} - {response_body}\")","title":"Append note to ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_task_result_change_note/","text":"Append task result change note append_note_to_ticket","title":"Append task result change note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_task_result_change_note/#append-task-result-change-note","text":"append_note_to_ticket","title":"Append task result change note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_triage_note/","text":"Append triage note If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/append_triage_note/#append-triage-note","text":"If environment is DEV: self._logger.info(f\"Triage note would have been appended to detail {ticket_detail_id} of ticket {ticket_id}\" f\"(serial: {service_number}). Note: {ticket_note}. Details at app.bruin.com/t/{ticket_id}\") Elif environment is PRODUCTION: If len of ticket note is lower than 1500: append_note_to_ticket Else: Split lines and send in blocks: append_note_to_ticket","title":"Append triage note"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_detail_work_queue/","text":"Change detail work queue self._logger.info(f\"Changing task result for ticket {ticket_id} and detail id {detail_id} for device {serial_number} to {task_result}...\") * If Exception: self._logger.error(f\"An error occurred when changing task result for ticket {ticket_id} and serial{serial_number}\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.info(f\"Error while changing task result for ticket {ticket_id} and serial {serial_number} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_detail_work_queue/#change-detail-work-queue","text":"self._logger.info(f\"Changing task result for ticket {ticket_id} and detail id {detail_id} for device {serial_number} to {task_result}...\") * If Exception: self._logger.error(f\"An error occurred when changing task result for ticket {ticket_id} and serial{serial_number}\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.info(f\"Error while changing task result for ticket {ticket_id} and serial {serial_number} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change detail work queue"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity/","text":"Change severity self._logger.info(f\"Changing severity level of ticket {ticket_id} to {severity_level}...\") If Exception: self._logger.error(f\"An error occurred when changing the severity level of ticket {ticket_id} to {severity_level} -> {e}\") If status is 200: self._logger.info(f\"Severity level of ticket {ticket_id} successfully changed to {severity_level}!\") Else: self._logger.error(f\"Error while changing severity of ticket {ticket_id} to {severity_level} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change severity"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity/#change-severity","text":"self._logger.info(f\"Changing severity level of ticket {ticket_id} to {severity_level}...\") If Exception: self._logger.error(f\"An error occurred when changing the severity level of ticket {ticket_id} to {severity_level} -> {e}\") If status is 200: self._logger.info(f\"Severity level of ticket {ticket_id} successfully changed to {severity_level}!\") Else: self._logger.error(f\"Error while changing severity of ticket {ticket_id} to {severity_level} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Change severity"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_disconnected_links/","text":"Change ticket severity for disconnected links change_ticket_severity","title":"Change ticket severity for disconnected links"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_disconnected_links/#change-ticket-severity-for-disconnected-links","text":"change_ticket_severity","title":"Change ticket severity for disconnected links"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_offline_edge/","text":"Change ticket severity for offline edges change_ticket_severity","title":"Change ticket severity for offline edge"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/change_ticket_severity_for_offline_edge/#change-ticket-severity-for-offline-edges","text":"change_ticket_severity","title":"Change ticket severity for offline edges"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/create_outage_ticket/","text":"Create outage ticket Documentation self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/create_outage_ticket/#create-outage-ticket-documentation","text":"self._logger.info(f\"Creating outage ticket for device {service_number} that belongs to client {client_id}...\") ``` * If Exception: ``` self._logger.error(f\"An error occurred when creating outage ticket for device {service_number} belong to client {client_id} -> {e}\") ``` self._logger.info(f\"Outage ticket for device {service_number} that belongs to client {client_id} created!\") * If not a correct status self._logger.error(f\"Error while creating outage ticket for device {service_number} that belongs to client \" f\"{client_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\") ```","title":"Create outage ticket Documentation"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/","text":"Get open outage tickets get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_open_outage_tickets/#get-open-outage-tickets","text":"get_outage_tickets","title":"Get open outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_outage_tickets/","text":"Get outage tickets * get_ticket","title":"Get outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_outage_tickets/#get-outage-tickets","text":"* get_ticket","title":"Get outage tickets"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket/","text":"Get ticket self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket/#get-ticket","text":"self._logger.info(f'Getting all tickets with parameters of {request[\"body\"]} from Bruin...') * If Exception: self._logger.error(f'An error occurred when requesting tickets from Bruin API with parameters of {request[\"body\"]} -> {e}') * If status ok: self._logger.info(f'Got all tickets with parameters of {request[\"body\"]} from Bruin!') * Else: self._logger.error(f'Error while retrieving tickets with parameters of {request[\"body\"]} in ' f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket_details/","text":"Get ticket details self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/get_ticket_details/#get-ticket-details","text":"self._logger.info(f\"Getting details of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when requesting ticket details from Bruin API for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not 200: self._logger.error(f\"Error while retrieving details of ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Get ticket details"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/open_ticket/","text":"Open ticket self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/open_ticket/#open-ticket","text":"self._logger.info(f\"Opening ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when opening outage ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} opened!\") * If status ok: self._logger.info(f\"Ticket {ticket_id} and serial {serial_number} task result changed to {task_result}\") * Else: self._logger.error(f\"Error while opening outage ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Open ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/post_notification_email_milestone/","text":"post notification email milestone self._logger.info(f\"Sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...\") * If Exception: self._logger.info(f\"An error occurred when sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...-> {e}\") * If status ok: self._logger.info(f\"Email sent for ticket {ticket_id}, service number {service_number} and notification type {notification_type}!\") * Else: self._logger.info(f\"Error while sending email for ticket {ticket_id}, service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Post notification email milestone"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/post_notification_email_milestone/#post-notification-email-milestone","text":"self._logger.info(f\"Sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...\") * If Exception: self._logger.info(f\"An error occurred when sending email for ticket id {ticket_id}, service_number {service_number} and notification type {notification_type}...-> {e}\") * If status ok: self._logger.info(f\"Email sent for ticket {ticket_id}, service number {service_number} and notification type {notification_type}!\") * Else: self._logger.info(f\"Error while sending email for ticket {ticket_id}, service_number {service_number} and notification type {notification_type} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"post notification email milestone"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/resolve_ticket/","text":"Resolve ticket detail self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/resolve_ticket/#resolve-ticket-detail","text":"self._logger.info(f\"Resolving ticket {ticket_id} (affected detail ID: {detail_id})...\") * If Exception: self._logger.error(f\"An error occurred when resolving ticket {ticket_id} -> {e}\") self._logger.info(f\"Ticket {ticket_id} resolved!\") * If status not ok: self._logger.error(f\"Error while resolving ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Resolve ticket detail"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/","text":"send initial email milestone notification post_notification_email_milestone","title":"Send initial email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_initial_email_milestone_notification/#send-initial-email-milestone-notification","text":"post_notification_email_milestone","title":"send initial email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_reminder_email_milestone_notification/","text":"Send reminder email milestone notification post_notification_email_milestone","title":"Send reminder email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/send_reminder_email_milestone_notification/#send-reminder-email-milestone-notification","text":"post_notification_email_milestone","title":"Send reminder email milestone notification"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/","text":"Unpause ticket detail self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/service-outage-monitor/repositories/bruin_repository/unpause_ticket_detail/#unpause-ticket-detail","text":"self._logger.info(f\"Unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}...\") * If Exception: self._logger.error(f\"An error occurred when unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id}. \" f\"Error: {e}\") * If status ok: self._logger.info(f\"Detail {detail_id} (serial {service_number}) of ticket {ticket_id} was unpaused!\") * Else: self._logger.error(f\"Error while unpausing detail {detail_id} (serial {service_number}) of ticket {ticket_id} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment. \" f\"Error: Error {response_status} - {response_body}\")","title":"Unpause ticket detail"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache/","text":"get cache Documentation If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"Get cache"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache/#get-cache-documentation","text":"If velo_filter self._logger.info(f\"Getting customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}...\") 2. Else: self._logger.info(f\"Getting customer cache for all Velocloud hosts...\") 3. If Exception : self._logger.error(f\"An error occurred when requesting customer cache -> {e}\") If response status == 202: self._logger.error(response_body) Else: If velo_filter: self._logger.info(f\"Got customer cache for Velocloud host(s) {', '.join(velo_filter.keys())}!\") * Else self._logger.info(f\"Got customer cache for all Velocloud hosts!\")","title":"get cache Documentation"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/","text":"get cache for outage Documentation Launch get_cache","title":"Get cache for outage monitoring"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_outage_monitoring/#get-cache-for-outage-documentation","text":"Launch get_cache","title":"get cache for outage Documentation"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_triage_monitoring/","text":"Get cache for triage monitoring get_cache","title":"Get cache for triage monitoring"},{"location":"logging/services/service-outage-monitor/repositories/customer_cache_repository/get_cache_for_triage_monitoring/#get-cache-for-triage-monitoring","text":"get_cache","title":"Get cache for triage monitoring"},{"location":"logging/services/service-outage-monitor/repositories/digi_repository/reboot_link/","text":"Reboot link self._logger.info(f\"Rebooting DiGi link of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when attempting a DiGi reboot for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not ok: self._logger.error(f\"Error while attempting a DiGi reboot for ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Reboot link"},{"location":"logging/services/service-outage-monitor/repositories/digi_repository/reboot_link/#reboot-link","text":"self._logger.info(f\"Rebooting DiGi link of ticket {ticket_id} from Bruin...\") * If Exception: self._logger.error(f\"An error occurred when attempting a DiGi reboot for ticket {ticket_id} -> {e}\") self._logger.info(f\"Got details of ticket {ticket_id} from Bruin!\") * If status not ok: self._logger.error(f\"Error while attempting a DiGi reboot for ticket {ticket_id} in {self._config.CURRENT_ENVIRONMENT.upper()} environment: Error {response_status} - {response_body}\")","title":"Reboot link"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/get_edges_with_standbys_as_standalone_edges/","text":"Map edges with HA info Documentation","title":"Get edges with standbys as standalone edges"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/get_edges_with_standbys_as_standalone_edges/#map-edges-with-ha-info-documentation","text":"","title":"Map edges with HA info Documentation"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/map_edges_with_ha_info/","text":"Map edges with HA info Documentation for edge in edges If not edge ha info: self._logger.warning(f\"No HA info was found for edge {serial_number}. Skipping...\") If is not a raw ha state under monitoring self._logger.info( f\"HA partner for {serial_number} is in state {ha_state}, so HA will be considered as disabled for \" \"this edge\" )","title":"Map edges with ha info"},{"location":"logging/services/service-outage-monitor/repositories/ha_repository/map_edges_with_ha_info/#map-edges-with-ha-info-documentation","text":"for edge in edges If not edge ha info: self._logger.warning(f\"No HA info was found for edge {serial_number}. Skipping...\") If is not a raw ha state under monitoring self._logger.info( f\"HA partner for {serial_number} is in state {ha_state}, so HA will be considered as disabled for \" \"this edge\" )","title":"Map edges with HA info Documentation"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/build_triage_note/","text":"Build triage note This function don't have logs, only generate a note as string","title":"Build triage note"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/build_triage_note/#build-triage-note","text":"This function don't have logs, only generate a note as string","title":"Build triage note"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/start_triage_job/","text":"Start triage job (star process of triage) self._logger.info( f\"Scheduled task: service outage triage configured to run every \" f'{self._config.TRIAGE_CONFIG[\"polling_minutes\"]} minutes' ) * If exec on start: self._logger.info(f\"It will be executed now\") * _run_tickets_polling","title":"Start triage job"},{"location":"logging/services/service-outage-monitor/repositories/triage_repository/start_triage_job/#start-triage-job-star-process-of-triage","text":"self._logger.info( f\"Scheduled task: service outage triage configured to run every \" f'{self._config.TRIAGE_CONFIG[\"polling_minutes\"]} minutes' ) * If exec on start: self._logger.info(f\"It will be executed now\") * _run_tickets_polling","title":"Start triage job (star process of triage)"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edge_events/","text":"Get edge events self._logger.info(f\"Getting events of edge {json.dumps(edge_full_id)} having any type of {event_types} that took place \" f\"between {from_} and {to} from Velocloud...\") * If Exception: self._logger.error(f\"An error occurred when requesting edge events from Velocloud for edge \" f\"{json.dumps(edge_full_id)} -> {e}\") self._logger.info(f\"Got events of edge {json.dumps(edge_full_id)} having any type in {event_types} that took place \" f\"between {from_} and {to} from Velocloud!\") * If status not ok: self._logger.error(f\"Error while retrieving events of edge {json.dumps(edge_full_id)} having any type in \" f\"{event_types} that took place between {from_} and {to} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edge_events/#get-edge-events","text":"self._logger.info(f\"Getting events of edge {json.dumps(edge_full_id)} having any type of {event_types} that took place \" f\"between {from_} and {to} from Velocloud...\") * If Exception: self._logger.error(f\"An error occurred when requesting edge events from Velocloud for edge \" f\"{json.dumps(edge_full_id)} -> {e}\") self._logger.info(f\"Got events of edge {json.dumps(edge_full_id)} having any type in {event_types} that took place \" f\"between {from_} and {to} from Velocloud!\") * If status not ok: self._logger.error(f\"Error while retrieving events of edge {json.dumps(edge_full_id)} having any type in \" f\"{event_types} that took place between {from_} and {to} in \" f\"{self._config.CURRENT_ENVIRONMENT.upper()} environment: \" f\"Error {response_status} - {response_body}\")","title":"Get edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edges_for_triage/","text":"Get edges for triage For host in triage host: get_links_with_edge_info Is get links with edge info status is not Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\")","title":"Get edges for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_edges_for_triage/#get-edges-for-triage","text":"For host in triage host: get_links_with_edge_info Is get links with edge info status is not Ok: self._logger.info(f\"Error: could not retrieve edges links by host: {host}\")","title":"Get edges for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_last_edge_events/","text":"Get last edge events get_edge_events","title":"Get last edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_last_edge_events/#get-last-edge-events","text":"get_edge_events","title":"Get last edge events"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_links_with_edge_info/","text":"Get Links with edge info Documentation self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get links with edge info"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_links_with_edge_info/#get-links-with-edge-info-documentation","text":"self._logger.info(f\"Getting links with edge info from Velocloud for host {velocloud_host}...\") If Exception self._logger.error(f\"An error occurred when requesting edge list from Velocloud -> {e}\") * If status OK: self._logger.info(f\"Got links with edge info from Velocloud for host {velocloud_host}!\") * Else: self._logger.error(f\"Error while retrieving links with edge info in {self._config.ENVIRONMENT_NAME.upper()} \" f\"environment: Error {response_status} - {response_body}\")","title":"Get Links with edge info Documentation"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises/","text":"Get network enterprises Documentation If enterprises ids: self._logger.info( f\"Getting network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}...\" ) * Else: self._logger.info( \"Getting network information for all edges belonging to all enterprises in host \" f\"{velocloud_host}...\" ) If Exception self._logger.error(f\"An error occurred when requesting network info from Velocloud host {velocloud_host} -> {e}\") * If status OK: If enterprises ids: self._logger.info( f\"Got network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}!\" ) * Else: self._logger.info( f\"Got network information for all edges belonging to all enterprises in host {velocloud_host}!\" ) Else: self._logger.error(f\"Error while retrieving network info from Velocloud host {velocloud_host} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get network enterprises"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises/#get-network-enterprises-documentation","text":"If enterprises ids: self._logger.info( f\"Getting network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}...\" ) * Else: self._logger.info( \"Getting network information for all edges belonging to all enterprises in host \" f\"{velocloud_host}...\" ) If Exception self._logger.error(f\"An error occurred when requesting network info from Velocloud host {velocloud_host} -> {e}\") * If status OK: If enterprises ids: self._logger.info( f\"Got network information for all edges belonging to enterprises \" f\"{', '.join(map(str, enterprise_ids))} in host {velocloud_host}!\" ) * Else: self._logger.info( f\"Got network information for all edges belonging to all enterprises in host {velocloud_host}!\" ) Else: self._logger.error(f\"Error while retrieving network info from Velocloud host {velocloud_host} in \" f\"{self._config.ENVIRONMENT_NAME.upper()} environment: Error {response_status} - {response_body}\")","title":"Get network enterprises Documentation"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises_for_triage/","text":"Get network enterprises for triage For host in triage host: get_network_enterprises If get network enterprises status is not ok: self._logger.error(f\"Could not retrieve network enterprises for triage using host {host}\")","title":"Get network enterprises for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/get_network_enterprises_for_triage/#get-network-enterprises-for-triage","text":"For host in triage host: get_network_enterprises If get network enterprises status is not ok: self._logger.error(f\"Could not retrieve network enterprises for triage using host {host}\")","title":"Get network enterprises for triage"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/group_links_by_edge/","text":"Process velocloud host Documentation for link in links If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If edge state is \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Group links by edge"},{"location":"logging/services/service-outage-monitor/repositories/velocloud_repository/group_links_by_edge/#process-velocloud-host-documentation","text":"for link in links If not edge state: self._logger.info( f\"Edge in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has an invalid state. Skipping...\" ) If edge state is \"NEVER_ACTIVATED\" self._logger.info( f\"Edge {edge_name} in host {velocloud_host} and enterprise {enterprise_name} (ID: {enterprise_id}) \" f\"has never been activated. Skipping...\" )","title":"Process velocloud host Documentation"},{"location":"manual_configurations/GITLAB_MAINTENANCE/","text":"1. Summary Gitlab is the tool selected to manage CI-CD process of Automation-Engine APP. The source code is in the project repository , under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . This nevironment only require the modules 1-parameters , 2-network and 4-services . Gitlab is deployed in a separated environemt (workspace) of automation-engine application, so the changes applied in this workspace not afect the main app. Gitlab is the basis for CI-CD, so it cannot manage its own lifecycle (it could be the snake biting its own tail). So the integration and maintenance of this environment is managed locally with Terraform by the infrastructure administrators, keeping the code versions in the repository of the project itself. 2. Modules 1-parameters , define the parameter required by the workspace \"builder\", like password of the database or gitlab root account. The parameter creation are managed by terraform, but the value is updated by operators in AWS parameter store. 2-network , manage the networking of the environemt, this terraform configuration have all the VPC configuration required to deploy gitlab. 4-services , deploy AWS services required like RDS, EKS, or S3 among others. Also deploy the Chart of the gitlab application. 3. Deploy All terraform modules have a common Makefile that contain the required configurati\u00f3n to apply terraform in a specifig workspace. For example, to deploy the complete gitlab environment you need to: cd infra-as-code/basic-infra/1-parameters make terraform_apply env=builder // note: after parameter creation, you need to update the value of that paramters in AWS web console. cd ../2-network make terraform_apply env=builder cd ../4-services make terraform_apply env=builder 4. Updates Gitlab receives constant security patches and updates, it is recommended to keep gitlab up to date. To do this, we go to the gitlab-ci.yml root file of the repo and update the version, for example: ... TF_VAR_GITLAB_CHART_VERSION: \"6.0.3\" ... Then just execute in 4-services folder the command: make terraform apply env=builder verify the changes, and accepting them by write \"yes\" to applied it. 5. References https://docs.gitlab.com/ee/raketasks/backup_restore.html https://gitlab.com/gitlab-org/charts/gitlab/-/blob/master/doc/backup-restore/index.md https://gitlab.com/gitlab-org/charts/gitlab/blob/master/doc/backup-restore/restore.md","title":"GITLAB MAINTENANCE"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#1-summary","text":"Gitlab is the tool selected to manage CI-CD process of Automation-Engine APP. The source code is in the project repository , under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . This nevironment only require the modules 1-parameters , 2-network and 4-services . Gitlab is deployed in a separated environemt (workspace) of automation-engine application, so the changes applied in this workspace not afect the main app. Gitlab is the basis for CI-CD, so it cannot manage its own lifecycle (it could be the snake biting its own tail). So the integration and maintenance of this environment is managed locally with Terraform by the infrastructure administrators, keeping the code versions in the repository of the project itself.","title":"1. Summary"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#2-modules","text":"1-parameters , define the parameter required by the workspace \"builder\", like password of the database or gitlab root account. The parameter creation are managed by terraform, but the value is updated by operators in AWS parameter store. 2-network , manage the networking of the environemt, this terraform configuration have all the VPC configuration required to deploy gitlab. 4-services , deploy AWS services required like RDS, EKS, or S3 among others. Also deploy the Chart of the gitlab application.","title":"2. Modules"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#3-deploy","text":"All terraform modules have a common Makefile that contain the required configurati\u00f3n to apply terraform in a specifig workspace. For example, to deploy the complete gitlab environment you need to: cd infra-as-code/basic-infra/1-parameters make terraform_apply env=builder // note: after parameter creation, you need to update the value of that paramters in AWS web console. cd ../2-network make terraform_apply env=builder cd ../4-services make terraform_apply env=builder","title":"3. Deploy"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#4-updates","text":"Gitlab receives constant security patches and updates, it is recommended to keep gitlab up to date. To do this, we go to the gitlab-ci.yml root file of the repo and update the version, for example: ... TF_VAR_GITLAB_CHART_VERSION: \"6.0.3\" ... Then just execute in 4-services folder the command: make terraform apply env=builder verify the changes, and accepting them by write \"yes\" to applied it.","title":"4. Updates"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#5-references","text":"https://docs.gitlab.com/ee/raketasks/backup_restore.html https://gitlab.com/gitlab-org/charts/gitlab/-/blob/master/doc/backup-restore/index.md https://gitlab.com/gitlab-org/charts/gitlab/blob/master/doc/backup-restore/restore.md","title":"5. References"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/","text":"Pre requisites Okta Account AWS Account Description Because of FEDRAMP we need to implement as a team a IdP that control all users and permissions related to MetTel projects that are made by Intelygenz. For automatic synchronization we are going to create SCIM sync between OKTA and AWS SSO, because of that, groups and users are going to be synced if someone deletes/create a group/user in Okta. Considerations Remove a user/group don't revoke the session tokens in AWS, the minimum duration of these tokens are of 1h. Info here Using the same Okta group for both assignments and group push is not currently supported. To maintain consistent group memberships between Okta and AWS SSO, you need to create a separate group and configure it to push groups to AWS SSO. Info here If you update a user\u2019s address you must have streetAddress, city, state, zipCode and the countryCode value specified. If any of these values are not specified for the Okta user at the time of synchronization, the user or changes to the user will not be provisioned. Info here Entitlements and role attributes are not supported and cannot be synced to AWS SSO. Info here Steps Configure IdP with Okta, this is the guide . Create the following groups: OKTA-IPA-FED-INT-PRIVILEGED: Internal users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-INT-NON-PRIVILEGED: Internal users Federated non privileged group on the federal account. OKTA-IPA-COM-INT-PRIVILEGED: Internal users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-INT-NON-PRIVILEGED: Internal users Federated privileged group on the commercial account. Administration accounts. OKTA-IPA-FED-EXT-PRIVILEGED: External users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-EXT-NON-PRIVILEGED: External users Federated non privileged group on the federal account. OKTA-IPA-COM-EXT-PRIVILEGED: External users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-EXT-NON-PRIVILEGED: External users Federated privileged group on the commercial account. Administration accounts. Associate permissions to groups. Guide Privileged accounts will have general administrator permissions Non privileged accounts will only have access to logs on cloud watch and grafana Revoke permissions Because of the problem of token duration of 1h that can not be revoked from okta, there is a manual procedure to delete the access from AWS SSO. For revoking access follow this guide","title":"OKTA CONFIGURATIONS"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#pre-requisites","text":"Okta Account AWS Account","title":"Pre requisites"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#description","text":"Because of FEDRAMP we need to implement as a team a IdP that control all users and permissions related to MetTel projects that are made by Intelygenz. For automatic synchronization we are going to create SCIM sync between OKTA and AWS SSO, because of that, groups and users are going to be synced if someone deletes/create a group/user in Okta.","title":"Description"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#considerations","text":"Remove a user/group don't revoke the session tokens in AWS, the minimum duration of these tokens are of 1h. Info here Using the same Okta group for both assignments and group push is not currently supported. To maintain consistent group memberships between Okta and AWS SSO, you need to create a separate group and configure it to push groups to AWS SSO. Info here If you update a user\u2019s address you must have streetAddress, city, state, zipCode and the countryCode value specified. If any of these values are not specified for the Okta user at the time of synchronization, the user or changes to the user will not be provisioned. Info here Entitlements and role attributes are not supported and cannot be synced to AWS SSO. Info here","title":"Considerations"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#steps","text":"Configure IdP with Okta, this is the guide . Create the following groups: OKTA-IPA-FED-INT-PRIVILEGED: Internal users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-INT-NON-PRIVILEGED: Internal users Federated non privileged group on the federal account. OKTA-IPA-COM-INT-PRIVILEGED: Internal users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-INT-NON-PRIVILEGED: Internal users Federated privileged group on the commercial account. Administration accounts. OKTA-IPA-FED-EXT-PRIVILEGED: External users Federated privileged group on the federal account. Administration accounts. OKTA-IPA-FED-EXT-NON-PRIVILEGED: External users Federated non privileged group on the federal account. OKTA-IPA-COM-EXT-PRIVILEGED: External users Commercial privileged group on the commercial account. Administration accounts. OKTA-IPA-COM-EXT-NON-PRIVILEGED: External users Federated privileged group on the commercial account. Administration accounts. Associate permissions to groups. Guide Privileged accounts will have general administrator permissions Non privileged accounts will only have access to logs on cloud watch and grafana","title":"Steps"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#revoke-permissions","text":"Because of the problem of token duration of 1h that can not be revoked from okta, there is a manual procedure to delete the access from AWS SSO. For revoking access follow this guide","title":"Revoke permissions"},{"location":"manual_configurations/OKTA_JWT/","text":"","title":"OKTA JWT"},{"location":"metrics-definitions/","text":"Metrics Definitions This folder will contain all the metrics created to track functional and business values that improve the overall observability of the system. There will be one markdown file per metric in this folder. The filename will be the metric name and it will contain all the descriptions and possible label combinations. Naming conventions must follow the Prometheus Best Practices for naming and units . List of metrics Metric Description tasks_created Task Creations tasks_reopened Task Re-Opens tasks_forwarded Task Forwards tasks_autoresolved Task Auto-Resolves velocloud_fetcher_to_kafka_messages_attempts VeloCloud fetcher attempts to kafka velocloud_fetcher_to_kafka_messages_status VeloCloud fetcher errors when pushing to kafka","title":"Metrics definitions"},{"location":"metrics-definitions/#metrics-definitions","text":"This folder will contain all the metrics created to track functional and business values that improve the overall observability of the system. There will be one markdown file per metric in this folder. The filename will be the metric name and it will contain all the descriptions and possible label combinations. Naming conventions must follow the Prometheus Best Practices for naming and units .","title":"Metrics Definitions"},{"location":"metrics-definitions/#list-of-metrics","text":"Metric Description tasks_created Task Creations tasks_reopened Task Re-Opens tasks_forwarded Task Forwards tasks_autoresolved Task Auto-Resolves velocloud_fetcher_to_kafka_messages_attempts VeloCloud fetcher attempts to kafka velocloud_fetcher_to_kafka_messages_status VeloCloud fetcher errors when pushing to kafka","title":"List of metrics"},{"location":"metrics-definitions/tasks_autoresolved/","text":"Task Auto-Resolves Metric name: tasks_autoresolved Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been auto-resolved by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA) | Unknown] severity: [2 | 3] has_digi: [True | False | Unknown] has_byob: [True | False | Unknown] link_types: [Wired | Wireless | Both | None | Unknown] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been auto-resolved by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability | Multiple | Unknown] has_byob: [True | False] link_types: [Wired | Wireless | Unknown] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been auto-resolved by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] TNBA Monitor Description: Number of tasks related to VeloCloud edges that have been auto-resolved by the tnba-monitor . Labels: feature: TNBA Monitor system: VeloCloud topic: [VOO | VAS] client: [<client> | FIS | Other] host: <host> severity: [2 | 3] Hawkeye Outage Monitor Description: Number of service outage tasks related to Ixia probes that have been auto-resolved by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None | Unknown] severity: 2","title":"Task Auto-Resolves #"},{"location":"metrics-definitions/tasks_autoresolved/#task-auto-resolves","text":"Metric name: tasks_autoresolved Metric type: Counter Data store: Prometheus","title":"Task Auto-Resolves"},{"location":"metrics-definitions/tasks_autoresolved/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been auto-resolved by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA) | Unknown] severity: [2 | 3] has_digi: [True | False | Unknown] has_byob: [True | False | Unknown] link_types: [Wired | Wireless | Both | None | Unknown]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been auto-resolved by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability | Multiple | Unknown] has_byob: [True | False] link_types: [Wired | Wireless | Unknown]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been auto-resolved by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#tnba-monitor","text":"Description: Number of tasks related to VeloCloud edges that have been auto-resolved by the tnba-monitor . Labels: feature: TNBA Monitor system: VeloCloud topic: [VOO | VAS] client: [<client> | FIS | Other] host: <host> severity: [2 | 3]","title":"TNBA Monitor"},{"location":"metrics-definitions/tasks_autoresolved/#hawkeye-outage-monitor","text":"Description: Number of service outage tasks related to Ixia probes that have been auto-resolved by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None | Unknown] severity: 2","title":"Hawkeye Outage Monitor"},{"location":"metrics-definitions/tasks_created/","text":"Task Creations Metric name: tasks_created Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been created by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been created by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been created by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] Gateway Monitor Description: Number of ServiceNow incidents related to VeloCloud gateways that have been created by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count Hawkeye Outage Monitor Description: Number of service outage tasks related to Ixia probes that have been created by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2 Fraud Monitor Description: Number of service affecting tasks related to Fraud alerts that have been created by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Task Creations #"},{"location":"metrics-definitions/tasks_created/#task-creations","text":"Metric name: tasks_created Metric type: Counter Data store: Prometheus","title":"Task Creations"},{"location":"metrics-definitions/tasks_created/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been created by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_created/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been created by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_created/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been created by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_created/#gateway-monitor","text":"Description: Number of ServiceNow incidents related to VeloCloud gateways that have been created by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count","title":"Gateway Monitor"},{"location":"metrics-definitions/tasks_created/#hawkeye-outage-monitor","text":"Description: Number of service outage tasks related to Ixia probes that have been created by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2","title":"Hawkeye Outage Monitor"},{"location":"metrics-definitions/tasks_created/#fraud-monitor","text":"Description: Number of service affecting tasks related to Fraud alerts that have been created by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Fraud Monitor"},{"location":"metrics-definitions/tasks_forwarded/","text":"Task Forwards Metric name: tasks_forwarded Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been forwarded by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] target_queue: [HNOC Investigate | ASR Investigate | Wireless Repair Intervention Needed] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been forwarded by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] target_queue: [HNOC Investigate | ASR Investigate] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been forwarded by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] target_queue: [HNOC Investigate | IPA Investigate] Fraud Monitor Description: Number of service affecting tasks related to Fraud alerts that have been forwarded by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation] target_queue: HNOC Investigate","title":"Task Forwards #"},{"location":"metrics-definitions/tasks_forwarded/#task-forwards","text":"Metric name: tasks_forwarded Metric type: Counter Data store: Prometheus","title":"Task Forwards"},{"location":"metrics-definitions/tasks_forwarded/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been forwarded by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] target_queue: [HNOC Investigate | ASR Investigate | Wireless Repair Intervention Needed]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_forwarded/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been forwarded by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] target_queue: [HNOC Investigate | ASR Investigate]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_forwarded/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been forwarded by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] target_queue: [HNOC Investigate | IPA Investigate]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_forwarded/#fraud-monitor","text":"Description: Number of service affecting tasks related to Fraud alerts that have been forwarded by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation] target_queue: HNOC Investigate","title":"Fraud Monitor"},{"location":"metrics-definitions/tasks_reopened/","text":"Task Re-Opens Metric name: tasks_reopened Metric type: Counter Data store: Prometheus Service Outage Monitor Description: Number of service outage tasks related to VeloCloud edges that have been re-opened by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None] Service Affecting Monitor Description: Number of service affecting tasks related to VeloCloud edges that have been re-opened by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless] InterMapper Outage Monitor Description: Number of service outage tasks related to InterMapper devices that have been re-opened by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False] Gateway Monitor Description: Number of ServiceNow incidents related to VeloCloud gateways that have been re-opened by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count Hawkeye Outage Monitor Description: Number of service outage tasks related to Ixia probes that have been re-opened by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2 Fraud Monitor Description: Number of service affecting tasks related to Fraud alerts that have been re-opened by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Task Re-Opens #"},{"location":"metrics-definitions/tasks_reopened/#task-re-opens","text":"Metric name: tasks_reopened Metric type: Counter Data store: Prometheus","title":"Task Re-Opens"},{"location":"metrics-definitions/tasks_reopened/#service-outage-monitor","text":"Description: Number of service outage tasks related to VeloCloud edges that have been re-opened by the service-outage-monitor . Labels: feature: Service Outage Monitor system: VeloCloud topic: VOO client: [<client> | FIS | Other] host: <host> outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] severity: [2 | 3] has_digi: [True | False] has_byob: [True | False] link_types: [Wired | Wireless | Both | None]","title":"Service Outage Monitor"},{"location":"metrics-definitions/tasks_reopened/#service-affecting-monitor","text":"Description: Number of service affecting tasks related to VeloCloud edges that have been re-opened by the service-affecting-monitor . Labels: feature: Service Affecting Monitor system: VeloCloud topic: VAS client: [<client> | FIS | Other] host: <host> severity: 3 trouble: [Latency | Packet Loss | Jitter | Bandwidth Over Utilization | Circuit Instability] has_byob: [True | False] link_types: [Wired | Wireless]","title":"Service Affecting Monitor"},{"location":"metrics-definitions/tasks_reopened/#intermapper-outage-monitor","text":"Description: Number of service outage tasks related to InterMapper devices that have been re-opened by the intermapper-outage-monitor . Labels: feature: InterMapper Outage Monitor system: InterMapper topic: VOO severity: 2 event: <event> is_piab: [True | False]","title":"InterMapper Outage Monitor"},{"location":"metrics-definitions/tasks_reopened/#gateway-monitor","text":"Description: Number of ServiceNow incidents related to VeloCloud gateways that have been re-opened by the gateway-monitor . Labels: feature: Gateway Monitor system: VeloCloud host: <host> trouble: Tunnel Count","title":"Gateway Monitor"},{"location":"metrics-definitions/tasks_reopened/#hawkeye-outage-monitor","text":"Description: Number of service outage tasks related to Ixia probes that have been re-opened by the hawkeye-outage-monitor . Labels: feature: Hawkeye Outage Monitor system: Ixia topic: VOO client: [<client> | FIS | Other] outage_type: [Node To Node | Real Service | Both | None] severity: 2","title":"Hawkeye Outage Monitor"},{"location":"metrics-definitions/tasks_reopened/#fraud-monitor","text":"Description: Number of service affecting tasks related to Fraud alerts that have been re-opened by the fraud-monitor . Labels: feature: Fraud Monitor system: MetTel Fraud Alerts topic: VAS severity: 3 trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Fraud Monitor"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/","text":"VeloCloud fetcher attempts to kafka Metric name: velocloud_fetcher_to_kafka_messages_attempts Metric type: Counter Data store: Prometheus VeloCloud - Attempts total messages to Kafka Description: Number of attempts sending messages to kafka. Labels: schema_name: <schema_name> environment: [develop | master]","title":"VeloCloud fetcher attempts to kafka #"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/#velocloud-fetcher-attempts-to-kafka","text":"Metric name: velocloud_fetcher_to_kafka_messages_attempts Metric type: Counter Data store: Prometheus","title":"VeloCloud fetcher attempts to kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/#velocloud-attempts-total-messages-to-kafka","text":"Description: Number of attempts sending messages to kafka. Labels: schema_name: <schema_name> environment: [develop | master]","title":"VeloCloud - Attempts total messages to Kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/","text":"VeloCloud fetcher errors when pushing to kafka Metric name: velocloud_fetcher_to_kafka_messages_status Metric type: Counter Data store: Prometheus VeloCloud - Status of Messages sent to Kafka Description: Number of OK calls or Errors when pushing data to the kafka server. Labels: schema_name: <schema_name> status: [OK | ERROR] environment: [develop | master]","title":"VeloCloud fetcher errors when pushing to kafka #"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/#velocloud-fetcher-errors-when-pushing-to-kafka","text":"Metric name: velocloud_fetcher_to_kafka_messages_status Metric type: Counter Data store: Prometheus","title":"VeloCloud fetcher errors when pushing to kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/#velocloud-status-of-messages-sent-to-kafka","text":"Description: Number of OK calls or Errors when pushing data to the kafka server. Labels: schema_name: <schema_name> status: [OK | ERROR] environment: [develop | master]","title":"VeloCloud - Status of Messages sent to Kafka"},{"location":"parameters/parameters/","text":"Parameter Store parameters /automation-engine/common/.. Name Description Format Units Range Default /automation-engine/common/autoresolve-day-end-hour Defines the hour at which the day ends and the night starts for dynamic auto-resolution times int hour 0-24 0 /automation-engine/common/autoresolve-day-start-hour Defines the hour at which the night ends and the day starts for dynamic auto-resolution times int hour 0-24 8 /automation-engine/common/bruin-ipa-system-username Name of the user that performs operations in Bruin on behalf of the IPA system string /automation-engine/common/customer-cache/refresh-check-interval Defines how often the next refresh flag is checked to decide if it's time to refresh the cache or not int seconds 0-inf 300 /automation-engine/common/customer-cache/refresh-job-interval Defines how often the cache is refreshed int seconds 0-inf 14400 /automation-engine/common/customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process list management statues [management status,...] [\"Pending\", \"Active \u2013 Gold Monitoring\", \"Active \u2013 Platinum Monitoring\"] /automation-engine/common/digi-bridge/digi-headers List of possible headers included in all DiGi links list headers [digi header,...] [\"00:04:2d\", \"00:27:04\"] /automation-engine/common/digi-bridge/digi-reboot-api-token-ttl Authentication tokens TTL(time to live) int seconds 0-inf 3600 /automation-engine/common/digi-reboot-report/logs-lookup-interval Defines how much time back to look for DiGi Reboot logs int seconds 0-inf 259200 /automation-engine/common/digi-reboot-report/report-job-interval Defines how often the report is built and sent int seconds 0-inf 86400 /automation-engine/common/dri-bridge/base-url Base URL for DRI API string https://api.dataremote.com /automation-engine/common/dri-bridge/dri-data-redis-ttl Defines how much time the data retrieved from DRI for a specific device can be stored and served from Redis int seconds 0-inf 300 /automation-engine/common/dri-bridge/password Password to log into DRI API string /automation-engine/common/dri-bridge/username Username to log into DRI API string /automation-engine/common/email-tagger-monitor/api-endpoint-prefix API server endpoint prefix for incoming requests string /api/email-tagger-webhook /automation-engine/common/email-tagger-monitor/max-concurrent-emails Defines how many simultaneous emails are processed int emails 0-inf 10 /automation-engine/common/email-tagger-monitor/new-emails-job-interval Defines how often new emails received from Bruin are processed int emails 0-inf 10 /automation-engine/common/email-tagger-monitor/new-tickets-job-interval Defines how often new tickets received from Bruin are sent to the KRE(konstellation) to train the AI model int tickets 0-inf 10 /automation-engine/common/email-tagger-monitor/reply-email-ttl Reply emails time to live (in milliseconds) when storing them to Redis int milliseconds 0-inf 300 /automation-engine/common/fraud-monitor/alerts-lookup-days How many days to look back for Fraud alerts in the desired e-mail inbox int days 0-inf 1 /automation-engine/common/fraud-monitor/default-client-info-for-did-without-inventory Default client info used when the DID device in the Fraud alert does not have an inventory assigned in Bruin dictionary {\"client_id\": ..., \"service_number\": ...} {\"client_id\": 9994, \"service_number\": \"2126070002\"} /automation-engine/common/fraud-monitor/default-contact-for-new-tickets Default contact details used when a Fraud is reported as a Service Affecting ticket dictionary {\"name\": ..., \"email\": ...} {\"name\": \"Holmdel NOC\", \"email\": \"holmdelnoc@mettel.net\"} /automation-engine/common/fraud-monitor/monitoring-job-interval Defines how often Fraud e-mails are checked to report them as Service Affecting tickets int seconds 0-inf 300 /automation-engine/common/fraud-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent Fraud alerts list emails [email address, ...] [\"svirdiya@mettel.net\"] /automation-engine/common/gateway-monitor/monitoring-job-interval Defines how often gateways are checked to find and report incidents int seconds 0-inf 300 /automation-engine/common/gateway-monitor/tunnel-count-threshold Threshold for tunnel count incidents int tunnels 0-inf 20 /automation-engine/common/hawkeye-affecting-monitor/monitored-product-category Bruin's product category under monitoring string product category Network Scout /automation-engine/common/hawkeye-affecting-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int seconds 0-inf 900 /automation-engine/common/hawkeye-affecting-monitor/probes-tests-results-lookup-interval Defines how much time back to look for probes' tests results int seconds 0-inf 900 /automation-engine/common/hawkeye-bridge/base-url Base URL to access Hawkeye API string https://ixia.metconnect.net/api /automation-engine/common/hawkeye-bridge/client-password Client password to log into Hawkeye API string /automation-engine/common/hawkeye-bridge/client-username Client username to log into Hawkeye API string /automation-engine/common/hawkeye-customer-cache/refresh-job-interval Defines how often the cache is refreshed int seconds 0-inf 14400 /automation-engine/common/hawkeye-customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process list management statues [management status,...] [\"Pending\", \"Active \u2013 Gold Monitoring\", \"Active \u2013 Platinum Monitoring\"] /automation-engine/common/hawkeye-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage Defines for how long a ticket can be auto-resolved after the last documented outage int seconds 0-inf 5400 /automation-engine/common/hawkeye-outage-monitor/monitored-product-category Bruin's product category under monitoring string product category Network Scout /automation-engine/common/hawkeye-outage-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int seconds 0-inf 180 /automation-engine/common/hawkeye-outage-monitor/quarantine-for-devices-in-outage Defines how much time to wait before checking if a particular device is still in outage state int seconds 0-inf 5 /automation-engine/common/intermapper-outage-monitor/dri-parameters-for-piab-notes Parameters to fetch from DRI to include them in InterMapper notes for PIAB devices list dri parameters [dri parameter, ...] [\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.SimInsert\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.Providers\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.SimIccid\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.Subscribernum\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.ModemImei\",\"InternetGatewayDevice.WANDevice1.WANConnectionDevice.1.WANIPConnection.1.MACAddress\"] /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day int seconds 0-inf 5400 /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night int seconds 0-inf 10800 /automation-engine/common/intermapper-outage-monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved int 0-inf 3 /automation-engine/common/intermapper-outage-monitor/max-concurrent-email-batches Defines how many simultaneous email batches related to the same InterMapper asset are processed int email 0-inf 10 /automation-engine/common/intermapper-outage-monitor/monitored-up-events InterMapper events considered as UP list events [edge event,...] [\"Up\", \"OK\"] /automation-engine/common/intermapper-outage-monitor/monitoring-job-interval Defines how often InterMapper events are checked to find and report issues int seconds 0-inf 30 /automation-engine/common/intermapper-outage-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent InterMapper events list email address [email address,...] [\"noreply@mettel.net\"] /automation-engine/common/timezone Timezone used for periodic jobs, timestamps, etc string Timezone US/Eastern /automation-engine/common/intermapper-outage-monitor/events-lookup-days How many days to look back for InterMapper events in the desired e-mail inbox int days 0-inf 1 /automation-engine/common/intermapper-outage-monitor/monitored-down-events InterMapper events considered as DOWN list events [edge event,...] [\"Down\", \"Critical\", \"Alarm\", \"Warning\", \"Link Warning\"] /automation-engine/common/intermapper-outage-monitor/whitelisted-product-categories-for-autoresolve Defines which Bruin product categories are taken into account when auto-resolving tickets list product categories [product category,...] [\"Cloud Connect\", \"Cloud Firewall\", \"POTS in a Box\", \"Premise Firewall\", \"Routers\", \"SIP Trunking\", \"Switches\", \"VPNS\", \"Wi-Fi\", \"SD-WAN\"] /automation-engine/common/link-labels-blacklisted-from-asr-forwards List of link labels that are excluded from forwards to the ASR queue list link labels [link label, ...] [\"byob\", \"customer owned\", \"client owned\", \"piab\"] /automation-engine/common/link-labels-blacklisted-from-hnoc-forwards List of link labels that are excluded from forwards to the HNOC queue list link labels [link label, ...] [\"byob\", \"customer provided\"] /automation-engine/common/lumin-billing-report/access-token Access token for Lumin's Billing API string /automation-engine/common/lumin-billing-report/customer-name Name of the customer for which the Billing Report will be generated string Customer name FCI /automation-engine/common/lumin-billing-report/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) string /automation-engine/common/lumin-billing-report/lumin-billing-api-base-url Base URL for Lumin's Billing API string https://fci.api.lumin.ai/api/billing /automation-engine/common/lumin-billing-report/recipient Email address to send the report to string email address mettel@intelygenz.com /automation-engine/common/metrics/relevant-clients List of relevant client names to use on Prometheus metrics labels list client name [client name,...] [\"RSI\", \"OReilly Auto Parts\", \"Sterling Jewelers, Inc.\"] /automation-engine/common/nats/endpoint-url Nats endpoint URL for messaging string nats://automation-engine-nats:4222 /automation-engine/common/notifier/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) string /automation-engine/common/notifier/email-account-for-message-delivery-username Email account used to send messages to other accounts (username) string /automation-engine/common/papertrail/host Papertrail host to send logs string /automation-engine/common/papertrail/port Papertrail port to send logs string /automation-engine/common/repair-tickets-monitor/max-concurrent-closed-tickets-for-feedback Defines how many simultaneous new closed tickets are sent to the KRE(konstellation) to train the AI model int tickets 0-inf 1 /automation-engine/common/repair-tickets-monitor/max-concurrent-created-tickets-for-feedback Defines how many simultaneous new created tickets are sent to the KRE(konstellation) to train the AI model int tickets 0-inf 10 /automation-engine/common/repair-tickets-monitor/max-concurrent-emails-for-monitoring Defines how many simultaneous tagged emails are processed int emails 0-inf 10 /automation-engine/common/repair-tickets-monitor/new-closed-tickets-feedback-job-interval Defines how often new closed tickets fetched from Bruin are sent to the KRE(konstellation) to train the AI model int seconds 0-inf 86400 /automation-engine/common/repair-tickets-monitor/new-created-tickets-feedback-job-interval Defines how often new created tickets fetched from Bruin are sent to the KRE(konstellation) to train the AI model int seconds 0-inf 10 /automation-engine/common/repair-tickets-monitor/rta-monitor-job-interval Defines how often new emails tagged by the E-mail Tagger are processed int seconds 0-inf 10 /automation-engine/common/repair-tickets-monitor/tag-ids-mapping Mapping of tag names and their corresponding numeric ID, as defined in the AI model dictionary {tag name: corresponding number id} {\"Repair\": 1, \"New Order\": 2, \"Change\": 3, \"Billing\": 4, \"Other\": 5} /automation-engine/common/service-affecting/daily-bandwidth-report/enabled-customers-per-host Mapping of VeloCloud hosts and Bruin customer IDs for whom this report will trigger periodically dictionary {host: [client id,...]} { \"metvco02.mettel.net\": [86937] } /automation-engine/common/service-affecting/daily-bandwidth-report/execution-cron-expression Cron expression that determines when to build and deliver this report string 0 4 * * * /automation-engine/common/service-affecting/daily-bandwidth-report/lookup-interval Defines how much time back to look for bandwidth metrics and Bruin tickets int seconds 0-inf 86400 /automation-engine/common/service-affecting/daily-bandwidth-report/recipients List of recipients that will get these reports list emails [email address, ...] [ \"bsullivan@mettel.net\",\"efox@mettel.net\",\"mettel.automation@intelygenz.com\" ] /automation-engine/common/service-affecting/monitor/autoresolve-lookup-interval Defines how much time back to look for all kinds of metrics while running auto-resolves int seconds 0-inf 1200 /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-lookup-interval Defines how much time back to look for Bandwidth metrics in Bandwidth Over Utilization checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-threshold Threshold for Bandwidth Over Utilization troubles int 0-inf 80 /automation-engine/common/service-affecting/monitor/circuit-instability-autoresolve-threshold Max DOWN events allowed in Circuit Instability checks while auto-resolving tickets int events 0-inf 4 /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-lookup-interval Defines how much time back to look for DOWN events in Circuit Instability checks int seconds 0-inf 3600 /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-threshold Threshold for Circuit Instability troubles int 0-inf 10 /automation-engine/common/service-affecting/monitor/customers-to-always-use-default-contact-info [Monitoring] List Bruin customers that should always use the default contact info list client ids [client id, ...] [83109, \"ALL_FIS_CLIENTS\"] /automation-engine/common/service-affecting/monitor/customers-with-bandwidth-over-utilization-monitoring List of client IDs for which Bandwidth Over Utilization checks are enabled list client ids [client id, ...] [83109, 85940, 86937] /automation-engine/common/service-affecting/monitor/default-contact-info-per-customer Mapping of VeloCloud hosts, Bruin customers and default contact info dictionary {host:{client id:[default client info]}} {\"mettel.velocloud.net\":{\"72959\":[{\"email\":\"DL_Tenet_Telecom@nttdata.com\",\"name\":\"TenetTelecom\",\"type\":\"ticket\"},{\"email\":\"DL_Tenet_Telecom@nttdata.com\",\"name\":\"TenetTelecom\",\"type\":\"site\"}],\"83959\":[{\"email\":\"MetTelTicket@oreillyauto.com\",\"phone\":\"4178622647EXT8305\",\"name\":\"O'ReillyHelpDesk\",\"type\":\"ticket\"},{\"email\":\"MetTelTicket@oreillyauto.com\",\"phone\":\"4178622647EXT8305\",\"name\":\"O'ReillyHelpDesk\",\"type\":\"site\"}],\"85940\":[{\"email\":\"mettel_alerts@titanamerica.com\",\"phone\":\"757-858-6600\",\"name\":\"TitanAmericaServiceDesk\",\"type\":\"ticket\"},{\"email\":\"mettel_alerts@titanamerica.com\",\"phone\":\"757-858-6600\",\"name\":\"TitanAmericaServiceDesk\",\"type\":\"site\"}],\"85134\":[{\"email\":\"service@carouselindustries.com\",\"phone\":\"800-401-0760\",\"name\":\"CarouselNOC\",\"type\":\"ticket\"},{\"email\":\"service@carouselindustries.com\",\"phone\":\"800-401-0760\",\"name\":\"CarouselNOC\",\"type\":\"site\"}]},\"metvco02.mettel.net\":{\"86937\":[{\"email\":\"itstslevel2_3@signetjewelers.com\",\"phone\":\"8008778825\",\"name\":\"ItstsLevel2_3\",\"type\":\"ticket\"},{\"email\":\"itstslevel2_3@signetjewelers.com\",\"phone\":\"8008778825\",\"name\":\"ItstsLevel2_3\",\"type\":\"site\"}]},\"metvco03.mettel.net\":{\"87671\":[{\"email\":\"Bruin@confie.com\",\"phone\":\"7142522752\",\"name\":\"ITSupportHelpdesk\",\"type\":\"ticket\"},{\"email\":\"Bruin@confie.com\",\"phone\":\"7142522752\",\"name\":\"ITSupportHelpdesk\",\"type\":\"site\"}],\"85719\":[{\"email\":\"Telecom@rotech.com\",\"name\":\"HelpDesk\",\"type\":\"ticket\"},{\"email\":\"Telecom@rotech.com\",\"name\":\"HelpDesk\",\"type\":\"site\"}],\"88480\":[{\"email\":\"helpdesk@royalbrassandhose.com\",\"phone\":\"865-251-9161\",\"name\":\"RoyalBrassHelpDesk\",\"type\":\"ticket\"},{\"email\":\"helpdesk@royalbrassandhose.com\",\"phone\":\"865-251-9161\",\"name\":\"RoyalBrassHelpDesk\",\"type\":\"site\"}],\"87827\":[{\"email\":\"tixfix@blackfinsquare.com\",\"phone\":\"770-992-9199\",\"name\":\"BlackfinSquareEscalationsSupportTeam\",\"type\":\"ticket\"},{\"email\":\"tixfix@blackfinsquare.com\",\"phone\":\"770-992-9199\",\"name\":\"BlackfinSquareEscalationsSupportTeam\",\"type\":\"site\"}],\"83109\":[{\"email\":\"DLITCoreNetwork@republicservices.com\",\"name\":\"DlitCoreNetwork\",\"type\":\"ticket\"},{\"email\":\"DLITCoreNetwork@republicservices.com\",\"name\":\"DlitCoreNetwork\",\"type\":\"site\"}],\"84782\":[{\"email\":\"plant_netops_dl@nrg.com\",\"phone\":\"7134885660\",\"name\":\"PlantnetopsPlantsd-wansites\",\"type\":\"ticket\"},{\"email\":\"plant_netops_dl@nrg.com\",\"phone\":\"7134885660\",\"name\":\"PlantnetopsPlantsd-wansites\",\"type\":\"site\"}]},\"metvco04.mettel.net\":{\"ALL_FIS_CLIENTS\":[{\"email\":\"az_phx_team-sdwan-support@fisglobal.com\",\"phone\":\"6023875757\",\"name\":\"PronetspocSupportteam\",\"type\":\"ticket\"},{\"email\":\"az_phx_team-sdwan-support@fisglobal.com\",\"phone\":\"6023875757\",\"name\":\"PronetspocSupportteam\",\"type\":\"site\"}]}} /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day int seconds 0-inf 5400 /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night int seconds 0-inf 10800 /automation-engine/common/service-affecting/monitor/jitter-monitoring-lookup-interval Defines how much time back to look for Jitter metrics in Jitter checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/jitter-monitoring-threshold Threshold for Jitter troubles int 0-inf 50 /automation-engine/common/service-affecting/monitor/latency-monitoring-lookup-interval Defines how much time back to look for Latency metrics in Latency checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/latency-monitoring-threshold Threshold for Latency troubles int 0-inf 140 /automation-engine/common/service-affecting/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved int 0-inf 3 /automation-engine/common/service-affecting/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int seconds 0-inf 600 /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-lookup-interval Defines how much time back to look for Packet Loss metrics in Packet Loss checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-threshold Threshold for Packet Loss troubles int 0-inf 8 /automation-engine/common/service-affecting/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent int seconds 0-inf 86400 /automation-engine/common/service-affecting/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/service-affecting/reoccurring-trouble-report/recipients-per-host-and-customer Mapping of VeloCloud hosts, Bruin customer IDs and recipients of these reports dictionary {host:{client id:[list of customer emails]}} {\"mettel.velocloud.net\":{\"83959\":[\"clmillsap@oreillyauto.com\",\"mgallion2@oreillyauto.com\",\"rbodenhamer@oreillyauto.com\",\"tkaufmann@oreillyauto.com\",\"mgoldstein@mettel.net\",\"dshim@mettel.net\"],\"85940\":[\"ta-infrastructure@titanamerica.com\"]},\"metvco02.mettel.net\":{\"86937\":[\"networkservices@signetjewelers.com\",\"pallen@mettel.net\"]},\"metvco03.mettel.net\":{\"72959\":[\"DL_Tenet_Telecom@nttdata.com\",\"Jake.Salas@tenethealth.com\",\"dshim@mettel.net\",\"mgoldstein@mettel.net\"],\"83109\":[\"JIngwersen@republicservices.com\",\"LRozendal@republicservices.com\",\"bsherman@mettel.net\"],\"89267\":[\"_DL_IT_Admin@benevis.com\"]}} /automation-engine/common/service-affecting/reoccurring-trouble-report/reported-troubles Troubles that will be reported list troubles [trouble, ...] [\"Jitter\", \"Latency\", \"Packet Loss\", \"Bandwidth Over Utilization\"] /automation-engine/common/service-affecting/reoccurring-trouble-report/default-contacts List of default contacts to whom this report will always be delivered to list emails [email, ...] [\"bsullivan@mettel.net\",\"jtaylor@mettel.net\",\"HNOCleaderteam@mettel.net\",\"mettel.automation@intelygenz.com\"] /automation-engine/common/service-affecting/reoccurring-trouble-report/execution-cron-expression Cron expression that determines when to build and deliver this report int 0 3 * * 0 /automation-engine/common/service-affecting/reoccurring-trouble-report/reoccurring-trouble-tickets-threshold Number of different tickets a trouble must appear in for a particular edge and interface to include it in the report int tickets 0-inf 3 /automation-engine/common/service-affecting/reoccurring-trouble-report/tickets-lookup-interval Defines how much time back to look for Bruin tickets int seconds 0-inf 1209600 /automation-engine/common/service-outage/monitor/business-grade-link-labels List of labels that define a link as business grade list labels [label, ...] [\"DIA\"] /automation-engine/common/service-outage/monitor/grace-period-before-attempting-new-digi-reboots Defines for how long the monitor will wait before attempting a new DiGi Reboot on an edge int seconds 0-inf 1800 /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day int seconds 0-inf 5400 /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night int seconds 0-inf 10800 /automation-engine/common/service-outage/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved int 0-inf 3 /automation-engine/common/service-outage/monitor/missing-edges-from-cache-report-recipient E-mail address that will receive a tiny report showing which edges from VeloCloud responses are not in the cache of customers string email address mettel@intelygenz.com /automation-engine/common/service-outage/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int 0-inf 600 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down (HA) state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down (HA) state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-soft-down-outage Defines how much time to wait before re-checking an edge currently in Soft Down (HA) state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/severity-for-edge-down-outages Severity level for Edge Down outages int [1 High, 2 MediumHigh , 3 MediumLow , 4Low] 2 /automation-engine/common/service-outage/monitor/severity-for-link-down-outages Severity level for Link Down outages int [1 High, 2 MediumHigh , 3 MediumLow , 4Low] 3 /automation-engine/common/service-outage/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent int seconds 0-inf 86400 /automation-engine/common/service-outage/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/service-outage/triage/last-note-interval Defines how long the last note on a ticket is considered recent int seconds 0-inf 3600 /automation-engine/common/service-outage/triage/max-events-per-event-note Defines how many events will be included in events notes int events 0-inf 15 /automation-engine/common/service-outage/triage/monitoring-job-interval Defines how often tickets are checked to see if it needs an initial triage or events note int seconds 0-inf 600 /automation-engine/common/sites-monitor/monitoring-job-interval Defines how often to look for links and edges, and write their data to the metrics server int seconds 0-inf 1200 /automation-engine/common/tnba-feedback/feedback-job-interval Defines how often tickets are pulled from Bruin and sent to the KRE(konstellation) to train the predictive model int seconds 0-inf 3600 /automation-engine/common/tnba-feedback/grace-period-before-resending-tickets Defines for how long a ticket needs to wait before being re-sent to the KRE(konstellation) int seconds 0-inf 604800 /automation-engine/common/tnba-feedback/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/tnba-monitor/grace-period-before-appending-new-tnba-notes Defines for how long a ticket needs to wait since it was opened before appending a new TNBA note int seconds 0-inf 1800 /automation-engine/common/tnba-monitor/grace-period-before-monitoring-tickets-based-on-last-documented-outage Defines for how long a Service Outage ticket needs to wait after the last documented outage to get a new TNBA note appended int seconds 0-inf 3600 /automation-engine/common/tnba-monitor/min-required-confidence-for-request-and-repair-completed-predictions Defines the minimum confidence level required to consider a Request Completed / Repair Completed prediction accurate in TNBA auto-resolves int 0-inf 75 /automation-engine/common/tnba-monitor/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/tnba-monitor/monitoring-job-interval Defines how often tickets are checked to see if they need a new TNBA note int seconds 0-inf 300 /automation-engine/pro/.. Name Description Format Units Range Default /automation-engine/pro/bruin-bridge/base-url Base URL for Bruin's production API string https://api.bruin.com /automation-engine/pro/bruin-bridge/client-id Client ID credential to authenticate against Bruin's production API string /automation-engine/pro/bruin-bridge/client-secret Client secret credential to authenticate against Bruin's production API string /automation-engine/pro/bruin-bridge/login-url Login URL for Bruin's production API string https://id.bruin.com /automation-engine/pro/bruin-bridge/test-base-url Base URL for Bruin's TEST API string https://bruinapi.mettel.net /automation-engine/pro/bruin-bridge/test-client-id Client ID credential to authenticate against Bruin's TEST API string /automation-engine/pro/bruin-bridge/test-client-secret Client secret credential to authenticate against Bruin's TEST API string /automation-engine/pro/bruin-bridge/test-login-url Login URL for Bruin's TEST API string https://id.bruin.com /automation-engine/pro/customer-cache/blacklisted-clients-with-pending-status Client IDs whose edges have Pending management status that should be ignored in the caching process list clients [83109, 87671, 88377, 87915, 88854, 87903, 88012, 89242, 89044, 88912, 89180, 89317, 88748, 89401, 88792, 87916, 89309, 89544, 89268, 88434, 88873, 89332, 89416, 89235, 88550, 89160, 89162, 88345, 88803, 89336, 83763, 89024, 88883, 88848, 89322, 89261, 89191, 89190, 88286, 88272, 88509, 88859, 88110, 88926, 89164, 89233, 88417, 88270, 88698, 89134, 88839, 87957, 89279, 87978, 89342, 88987, 89441, 88989, 89195, 82368, 89353, 89305, 89548, 89080, 88271, 89023, 87955, 88715, 89139, 89077, 89341, 88015, 89521, 89665, 88293, 89321, 89501, 89072, 89107, 88187, 89556, 89323, 88416, 89326, 79939] /automation-engine/pro/customer-cache/blacklisted-edges VeloCloud edges that should be ignored in the caching process list [{\"host\": ..., \"enterprise_id\": ..., \"edge_id\": ...}, ...] [{\"host\": \"mettel.velocloud.net\", \"enterprise_id\": 170, \"edge_id\": 3195}] /automation-engine/pro/customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories string mettel.alerts@intelygenz.com /automation-engine/pro/customer-cache/velocloud-hosts VeloCloud hosts whose edges will be stored to the cache list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/digi-bridge/digi-reboot-api-base-url Base URL for Digi API string https://luci.mettel.net /automation-engine/pro/digi-bridge/digi-reboot-api-client-id Client ID credentials for Digi API string /automation-engine/pro/digi-bridge/digi-reboot-api-client-secret Client Secret credentials for Digi API string /automation-engine/pro/digi-reboot-report/report-recipient Email address to send the report to string mettel.reports@intelygenz.com /automation-engine/pro/email-tagger-kre-bridge/kre-base-url Base URL for E-mail Tagger's KRE(konstellation) string entrypoint.kre-email-tagger.mettel-automation.net:443 /automation-engine/pro/email-tagger-monitor/api-request-key API request key for incoming requests string /automation-engine/pro/email-tagger-monitor/api-request-signature-secret-key API signature secret key for incoming requests string /automation-engine/pro/external-secrets/iam-role-arn The ARN(Amazon Resource Name) of the AWS(Amazon Web Services) IAM(Identity and Access Management) role necessary to manage parameter store and secret manager string /automation-engine/pro/fraud-monitor/observed-inbox-email-address E-mail account that receives Fraud e-mails for later analysis string mettel.automation@intelygenz.com /automation-engine/pro/gateway-monitor/monitored-velocloud-hosts VeloCloud hosts whose gateways will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/hawkeye-customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories string mettel.alerts@intelygenz.com /automation-engine/pro/intermapper-outage-monitor/observed-inbox-email-address E-mail account that receives InterMapper events for later analysis string mettel.automation@intelygenz.com /automation-engine/pro/last-contact-report/monitored-velocloud-hosts VeloCloud hosts whose edges will be used to build the report list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/last-contact-report/report-recipient Email address to send the report to string mettel.reports@intelygenz.com /automation-engine/pro/notifier/monitorable-email-accounts Mapping of e-mail addresses and passwords whose inboxes can be read for later analysis dictionary /automation-engine/pro/notifier/slack-webhook-url Slack webhook to send messages string /automation-engine/pro/papertrail/enabled Enable/Disable Papertrail logs boolean True/False True /automation-engine/pro/redis/customer-cache-hostname Customer Cache Redis hostname string redis-mettel-automation-customer-cache.pro.mettel-automation.net /automation-engine/pro/redis/email-tagger-hostname Email Tagger Redis Hostname string redis-mettel-automation-email-tagger.pro.mettel-automation.net /automation-engine/pro/redis/main-hostname Main Redis server for Automation-Engine string redis-mettel-automation.pro.mettel-automation.net /automation-engine/pro/redis/tnba-feedback-hostname TNBA Feedback hostname string redis-mettel-automation-tnba-feedback.pro.mettel-automation.net /automation-engine/pro/repair-tickets-kre-bridge/kre-base-url Base URL for RTA's KRE(konstellation) dns entrypoint.kre-rta.mettel-automation.net:443 /automation-engine/pro/service-affecting/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list host [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/service-outage/monitor/blacklisted-edges List of edges that are excluded from Service Outage monitoring list edges [{\"host\": ..., \"enterprise_id\": ..., \"edge_id\": ...}, ...] [{\"host\": \"mettel.velocloud.net\", \"enterprise_id\": 170, \"edge_id\": 3195}] /automation-engine/pro/service-outage/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/service-outage/triage/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/sites-monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/t7-bridge/kre-base-url Base URL for TNBA's KRE(konstellation) dns entrypoint.kre-tnba.mettel-automation.net:443 /automation-engine/pro/tnba-feedback/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/tnba-monitor/blacklisted-edges List of edges that are excluded from TNBA monitoring list edges [{\"host\": ..., \"enterprise_id\": ..., \"edge_id\": ...}, ...] [ {\"host\": \"mettel.velocloud.net\", \"enterprise_id\": 170, \"edge_id\": 3195} ] /automation-engine/pro/tnba-monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/velocloud-bridge/velocloud-credentials Velocloud credentials string /data-highway/develop/.. Name Description Format Units Range Default /data-highway/develop/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins string https://develop.mettel-data.net /data-highway/develop/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries string SERVICE_ACCOUNTS /data-highway/develop/shared/SECRET_JWT Velocloud hosts to fetch data from string /data-highway/develop/velocloud-fetcher/BLACKLISTED_VENDORS Blacklisted vendors string invented:0 /data-highway/develop/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers string /data-highway/develop/velocloud-fetcher/KAFKA_PASSWORD Kafka develop password to publish velocloud data on Kafka string /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential string /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL string /data-highway/develop/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka develop ssl ca path of Kafka string /data-highway/develop/velocloud-fetcher/KAFKA_USERNAME Kafka develop username to publish velocloud data on kafka string /data-highway/develop/velocloud-fetcher/METRICS_LIST Metrics list string metrics;... bytesRx;bytesTx;totalBytes;totalPackets;p1BytesRx;p1BytesTx;p1PacketsRx;p1PacketsTx;p2BytesRx;p2BytesTx;p2PacketsRx;p2PacketsTx;p3BytesRx;p3BytesTx;p3PacketsRx;p3PacketsTx;packetsRx;packetsTx;controlBytesRx;controlBytesTx;controlPacketsRx;controlPacketsTx;bestJitterMsRx;bestJitterMsTx;bestLatencyMsRx;bestLatencyMsTx;bestLossPctRx;bestLossPctTx;bpsOfBestPathRx;bpsOfBestPathTx;signalStrength;scoreTx;scoreRx /data-highway/develop/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials string /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas boolean True/False True /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list boolean True/False True /data-highway/develop/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from string host;... mettel.velocloud.net,metvco02.mettel.net,metvco03.mettel.net,metvco04.mettel.net /data-highway/develop/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl string yes/no yes /data-highway/master/.. Name Description Format Units Range Default /data-highway/master/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins string https://mettel-data.net /data-highway/master/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries string SERVICE_ACCOUNTS /data-highway/master/shared/SECRET_JWT Velocloud hosts to fetch data from string /data-highway/master/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers string /data-highway/master/velocloud-fetcher/KAFKA_PASSWORD Kafka master password to publish velocloud data on Kafka string /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential string /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL string /data-highway/master/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka master ssl ca path of Kafka string /data-highway/master/velocloud-fetcher/KAFKA_USERNAME Kafka master username to publish velocloud data on kafka string /data-highway/master/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials string /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas boolean True/False True /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list boolean True/False True /data-highway/master/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from string host;... mettel.velocloud.net,metvco02.mettel.net,metvco03.mettel.net,metvco04.mettel.net /data-highway/master/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl string yes/no yes","title":"Parameter Store parameters"},{"location":"parameters/parameters/#parameter-store-parameters","text":"","title":"Parameter Store parameters"},{"location":"parameters/parameters/#automation-enginecommon","text":"Name Description Format Units Range Default /automation-engine/common/autoresolve-day-end-hour Defines the hour at which the day ends and the night starts for dynamic auto-resolution times int hour 0-24 0 /automation-engine/common/autoresolve-day-start-hour Defines the hour at which the night ends and the day starts for dynamic auto-resolution times int hour 0-24 8 /automation-engine/common/bruin-ipa-system-username Name of the user that performs operations in Bruin on behalf of the IPA system string /automation-engine/common/customer-cache/refresh-check-interval Defines how often the next refresh flag is checked to decide if it's time to refresh the cache or not int seconds 0-inf 300 /automation-engine/common/customer-cache/refresh-job-interval Defines how often the cache is refreshed int seconds 0-inf 14400 /automation-engine/common/customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process list management statues [management status,...] [\"Pending\", \"Active \u2013 Gold Monitoring\", \"Active \u2013 Platinum Monitoring\"] /automation-engine/common/digi-bridge/digi-headers List of possible headers included in all DiGi links list headers [digi header,...] [\"00:04:2d\", \"00:27:04\"] /automation-engine/common/digi-bridge/digi-reboot-api-token-ttl Authentication tokens TTL(time to live) int seconds 0-inf 3600 /automation-engine/common/digi-reboot-report/logs-lookup-interval Defines how much time back to look for DiGi Reboot logs int seconds 0-inf 259200 /automation-engine/common/digi-reboot-report/report-job-interval Defines how often the report is built and sent int seconds 0-inf 86400 /automation-engine/common/dri-bridge/base-url Base URL for DRI API string https://api.dataremote.com /automation-engine/common/dri-bridge/dri-data-redis-ttl Defines how much time the data retrieved from DRI for a specific device can be stored and served from Redis int seconds 0-inf 300 /automation-engine/common/dri-bridge/password Password to log into DRI API string /automation-engine/common/dri-bridge/username Username to log into DRI API string /automation-engine/common/email-tagger-monitor/api-endpoint-prefix API server endpoint prefix for incoming requests string /api/email-tagger-webhook /automation-engine/common/email-tagger-monitor/max-concurrent-emails Defines how many simultaneous emails are processed int emails 0-inf 10 /automation-engine/common/email-tagger-monitor/new-emails-job-interval Defines how often new emails received from Bruin are processed int emails 0-inf 10 /automation-engine/common/email-tagger-monitor/new-tickets-job-interval Defines how often new tickets received from Bruin are sent to the KRE(konstellation) to train the AI model int tickets 0-inf 10 /automation-engine/common/email-tagger-monitor/reply-email-ttl Reply emails time to live (in milliseconds) when storing them to Redis int milliseconds 0-inf 300 /automation-engine/common/fraud-monitor/alerts-lookup-days How many days to look back for Fraud alerts in the desired e-mail inbox int days 0-inf 1 /automation-engine/common/fraud-monitor/default-client-info-for-did-without-inventory Default client info used when the DID device in the Fraud alert does not have an inventory assigned in Bruin dictionary {\"client_id\": ..., \"service_number\": ...} {\"client_id\": 9994, \"service_number\": \"2126070002\"} /automation-engine/common/fraud-monitor/default-contact-for-new-tickets Default contact details used when a Fraud is reported as a Service Affecting ticket dictionary {\"name\": ..., \"email\": ...} {\"name\": \"Holmdel NOC\", \"email\": \"holmdelnoc@mettel.net\"} /automation-engine/common/fraud-monitor/monitoring-job-interval Defines how often Fraud e-mails are checked to report them as Service Affecting tickets int seconds 0-inf 300 /automation-engine/common/fraud-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent Fraud alerts list emails [email address, ...] [\"svirdiya@mettel.net\"] /automation-engine/common/gateway-monitor/monitoring-job-interval Defines how often gateways are checked to find and report incidents int seconds 0-inf 300 /automation-engine/common/gateway-monitor/tunnel-count-threshold Threshold for tunnel count incidents int tunnels 0-inf 20 /automation-engine/common/hawkeye-affecting-monitor/monitored-product-category Bruin's product category under monitoring string product category Network Scout /automation-engine/common/hawkeye-affecting-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int seconds 0-inf 900 /automation-engine/common/hawkeye-affecting-monitor/probes-tests-results-lookup-interval Defines how much time back to look for probes' tests results int seconds 0-inf 900 /automation-engine/common/hawkeye-bridge/base-url Base URL to access Hawkeye API string https://ixia.metconnect.net/api /automation-engine/common/hawkeye-bridge/client-password Client password to log into Hawkeye API string /automation-engine/common/hawkeye-bridge/client-username Client username to log into Hawkeye API string /automation-engine/common/hawkeye-customer-cache/refresh-job-interval Defines how often the cache is refreshed int seconds 0-inf 14400 /automation-engine/common/hawkeye-customer-cache/whitelisted-management-statuses Management statuses that should be considered in the caching process list management statues [management status,...] [\"Pending\", \"Active \u2013 Gold Monitoring\", \"Active \u2013 Platinum Monitoring\"] /automation-engine/common/hawkeye-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage Defines for how long a ticket can be auto-resolved after the last documented outage int seconds 0-inf 5400 /automation-engine/common/hawkeye-outage-monitor/monitored-product-category Bruin's product category under monitoring string product category Network Scout /automation-engine/common/hawkeye-outage-monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int seconds 0-inf 180 /automation-engine/common/hawkeye-outage-monitor/quarantine-for-devices-in-outage Defines how much time to wait before checking if a particular device is still in outage state int seconds 0-inf 5 /automation-engine/common/intermapper-outage-monitor/dri-parameters-for-piab-notes Parameters to fetch from DRI to include them in InterMapper notes for PIAB devices list dri parameters [dri parameter, ...] [\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.SimInsert\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.Providers\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.SimIccid\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.Subscribernum\",\"InternetGatewayDevice.DeviceInfo.X_8C192D_lte_info.ModemImei\",\"InternetGatewayDevice.WANDevice1.WANConnectionDevice.1.WANIPConnection.1.MACAddress\"] /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day int seconds 0-inf 5400 /automation-engine/common/intermapper-outage-monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night int seconds 0-inf 10800 /automation-engine/common/intermapper-outage-monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved int 0-inf 3 /automation-engine/common/intermapper-outage-monitor/max-concurrent-email-batches Defines how many simultaneous email batches related to the same InterMapper asset are processed int email 0-inf 10 /automation-engine/common/intermapper-outage-monitor/monitored-up-events InterMapper events considered as UP list events [edge event,...] [\"Up\", \"OK\"] /automation-engine/common/intermapper-outage-monitor/monitoring-job-interval Defines how often InterMapper events are checked to find and report issues int seconds 0-inf 30 /automation-engine/common/intermapper-outage-monitor/observed-inbox-senders Senders addresses whose e-mail messages represent InterMapper events list email address [email address,...] [\"noreply@mettel.net\"] /automation-engine/common/timezone Timezone used for periodic jobs, timestamps, etc string Timezone US/Eastern /automation-engine/common/intermapper-outage-monitor/events-lookup-days How many days to look back for InterMapper events in the desired e-mail inbox int days 0-inf 1 /automation-engine/common/intermapper-outage-monitor/monitored-down-events InterMapper events considered as DOWN list events [edge event,...] [\"Down\", \"Critical\", \"Alarm\", \"Warning\", \"Link Warning\"] /automation-engine/common/intermapper-outage-monitor/whitelisted-product-categories-for-autoresolve Defines which Bruin product categories are taken into account when auto-resolving tickets list product categories [product category,...] [\"Cloud Connect\", \"Cloud Firewall\", \"POTS in a Box\", \"Premise Firewall\", \"Routers\", \"SIP Trunking\", \"Switches\", \"VPNS\", \"Wi-Fi\", \"SD-WAN\"] /automation-engine/common/link-labels-blacklisted-from-asr-forwards List of link labels that are excluded from forwards to the ASR queue list link labels [link label, ...] [\"byob\", \"customer owned\", \"client owned\", \"piab\"] /automation-engine/common/link-labels-blacklisted-from-hnoc-forwards List of link labels that are excluded from forwards to the HNOC queue list link labels [link label, ...] [\"byob\", \"customer provided\"] /automation-engine/common/lumin-billing-report/access-token Access token for Lumin's Billing API string /automation-engine/common/lumin-billing-report/customer-name Name of the customer for which the Billing Report will be generated string Customer name FCI /automation-engine/common/lumin-billing-report/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) string /automation-engine/common/lumin-billing-report/lumin-billing-api-base-url Base URL for Lumin's Billing API string https://fci.api.lumin.ai/api/billing /automation-engine/common/lumin-billing-report/recipient Email address to send the report to string email address mettel@intelygenz.com /automation-engine/common/metrics/relevant-clients List of relevant client names to use on Prometheus metrics labels list client name [client name,...] [\"RSI\", \"OReilly Auto Parts\", \"Sterling Jewelers, Inc.\"] /automation-engine/common/nats/endpoint-url Nats endpoint URL for messaging string nats://automation-engine-nats:4222 /automation-engine/common/notifier/email-account-for-message-delivery-password Email account used to send messages to other accounts (password) string /automation-engine/common/notifier/email-account-for-message-delivery-username Email account used to send messages to other accounts (username) string /automation-engine/common/papertrail/host Papertrail host to send logs string /automation-engine/common/papertrail/port Papertrail port to send logs string /automation-engine/common/repair-tickets-monitor/max-concurrent-closed-tickets-for-feedback Defines how many simultaneous new closed tickets are sent to the KRE(konstellation) to train the AI model int tickets 0-inf 1 /automation-engine/common/repair-tickets-monitor/max-concurrent-created-tickets-for-feedback Defines how many simultaneous new created tickets are sent to the KRE(konstellation) to train the AI model int tickets 0-inf 10 /automation-engine/common/repair-tickets-monitor/max-concurrent-emails-for-monitoring Defines how many simultaneous tagged emails are processed int emails 0-inf 10 /automation-engine/common/repair-tickets-monitor/new-closed-tickets-feedback-job-interval Defines how often new closed tickets fetched from Bruin are sent to the KRE(konstellation) to train the AI model int seconds 0-inf 86400 /automation-engine/common/repair-tickets-monitor/new-created-tickets-feedback-job-interval Defines how often new created tickets fetched from Bruin are sent to the KRE(konstellation) to train the AI model int seconds 0-inf 10 /automation-engine/common/repair-tickets-monitor/rta-monitor-job-interval Defines how often new emails tagged by the E-mail Tagger are processed int seconds 0-inf 10 /automation-engine/common/repair-tickets-monitor/tag-ids-mapping Mapping of tag names and their corresponding numeric ID, as defined in the AI model dictionary {tag name: corresponding number id} {\"Repair\": 1, \"New Order\": 2, \"Change\": 3, \"Billing\": 4, \"Other\": 5} /automation-engine/common/service-affecting/daily-bandwidth-report/enabled-customers-per-host Mapping of VeloCloud hosts and Bruin customer IDs for whom this report will trigger periodically dictionary {host: [client id,...]} { \"metvco02.mettel.net\": [86937] } /automation-engine/common/service-affecting/daily-bandwidth-report/execution-cron-expression Cron expression that determines when to build and deliver this report string 0 4 * * * /automation-engine/common/service-affecting/daily-bandwidth-report/lookup-interval Defines how much time back to look for bandwidth metrics and Bruin tickets int seconds 0-inf 86400 /automation-engine/common/service-affecting/daily-bandwidth-report/recipients List of recipients that will get these reports list emails [email address, ...] [ \"bsullivan@mettel.net\",\"efox@mettel.net\",\"mettel.automation@intelygenz.com\" ] /automation-engine/common/service-affecting/monitor/autoresolve-lookup-interval Defines how much time back to look for all kinds of metrics while running auto-resolves int seconds 0-inf 1200 /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-lookup-interval Defines how much time back to look for Bandwidth metrics in Bandwidth Over Utilization checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/bandwidth-over-utilization-monitoring-threshold Threshold for Bandwidth Over Utilization troubles int 0-inf 80 /automation-engine/common/service-affecting/monitor/circuit-instability-autoresolve-threshold Max DOWN events allowed in Circuit Instability checks while auto-resolving tickets int events 0-inf 4 /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-lookup-interval Defines how much time back to look for DOWN events in Circuit Instability checks int seconds 0-inf 3600 /automation-engine/common/service-affecting/monitor/circuit-instability-monitoring-threshold Threshold for Circuit Instability troubles int 0-inf 10 /automation-engine/common/service-affecting/monitor/customers-to-always-use-default-contact-info [Monitoring] List Bruin customers that should always use the default contact info list client ids [client id, ...] [83109, \"ALL_FIS_CLIENTS\"] /automation-engine/common/service-affecting/monitor/customers-with-bandwidth-over-utilization-monitoring List of client IDs for which Bandwidth Over Utilization checks are enabled list client ids [client id, ...] [83109, 85940, 86937] /automation-engine/common/service-affecting/monitor/default-contact-info-per-customer Mapping of VeloCloud hosts, Bruin customers and default contact info dictionary {host:{client id:[default client info]}} {\"mettel.velocloud.net\":{\"72959\":[{\"email\":\"DL_Tenet_Telecom@nttdata.com\",\"name\":\"TenetTelecom\",\"type\":\"ticket\"},{\"email\":\"DL_Tenet_Telecom@nttdata.com\",\"name\":\"TenetTelecom\",\"type\":\"site\"}],\"83959\":[{\"email\":\"MetTelTicket@oreillyauto.com\",\"phone\":\"4178622647EXT8305\",\"name\":\"O'ReillyHelpDesk\",\"type\":\"ticket\"},{\"email\":\"MetTelTicket@oreillyauto.com\",\"phone\":\"4178622647EXT8305\",\"name\":\"O'ReillyHelpDesk\",\"type\":\"site\"}],\"85940\":[{\"email\":\"mettel_alerts@titanamerica.com\",\"phone\":\"757-858-6600\",\"name\":\"TitanAmericaServiceDesk\",\"type\":\"ticket\"},{\"email\":\"mettel_alerts@titanamerica.com\",\"phone\":\"757-858-6600\",\"name\":\"TitanAmericaServiceDesk\",\"type\":\"site\"}],\"85134\":[{\"email\":\"service@carouselindustries.com\",\"phone\":\"800-401-0760\",\"name\":\"CarouselNOC\",\"type\":\"ticket\"},{\"email\":\"service@carouselindustries.com\",\"phone\":\"800-401-0760\",\"name\":\"CarouselNOC\",\"type\":\"site\"}]},\"metvco02.mettel.net\":{\"86937\":[{\"email\":\"itstslevel2_3@signetjewelers.com\",\"phone\":\"8008778825\",\"name\":\"ItstsLevel2_3\",\"type\":\"ticket\"},{\"email\":\"itstslevel2_3@signetjewelers.com\",\"phone\":\"8008778825\",\"name\":\"ItstsLevel2_3\",\"type\":\"site\"}]},\"metvco03.mettel.net\":{\"87671\":[{\"email\":\"Bruin@confie.com\",\"phone\":\"7142522752\",\"name\":\"ITSupportHelpdesk\",\"type\":\"ticket\"},{\"email\":\"Bruin@confie.com\",\"phone\":\"7142522752\",\"name\":\"ITSupportHelpdesk\",\"type\":\"site\"}],\"85719\":[{\"email\":\"Telecom@rotech.com\",\"name\":\"HelpDesk\",\"type\":\"ticket\"},{\"email\":\"Telecom@rotech.com\",\"name\":\"HelpDesk\",\"type\":\"site\"}],\"88480\":[{\"email\":\"helpdesk@royalbrassandhose.com\",\"phone\":\"865-251-9161\",\"name\":\"RoyalBrassHelpDesk\",\"type\":\"ticket\"},{\"email\":\"helpdesk@royalbrassandhose.com\",\"phone\":\"865-251-9161\",\"name\":\"RoyalBrassHelpDesk\",\"type\":\"site\"}],\"87827\":[{\"email\":\"tixfix@blackfinsquare.com\",\"phone\":\"770-992-9199\",\"name\":\"BlackfinSquareEscalationsSupportTeam\",\"type\":\"ticket\"},{\"email\":\"tixfix@blackfinsquare.com\",\"phone\":\"770-992-9199\",\"name\":\"BlackfinSquareEscalationsSupportTeam\",\"type\":\"site\"}],\"83109\":[{\"email\":\"DLITCoreNetwork@republicservices.com\",\"name\":\"DlitCoreNetwork\",\"type\":\"ticket\"},{\"email\":\"DLITCoreNetwork@republicservices.com\",\"name\":\"DlitCoreNetwork\",\"type\":\"site\"}],\"84782\":[{\"email\":\"plant_netops_dl@nrg.com\",\"phone\":\"7134885660\",\"name\":\"PlantnetopsPlantsd-wansites\",\"type\":\"ticket\"},{\"email\":\"plant_netops_dl@nrg.com\",\"phone\":\"7134885660\",\"name\":\"PlantnetopsPlantsd-wansites\",\"type\":\"site\"}]},\"metvco04.mettel.net\":{\"ALL_FIS_CLIENTS\":[{\"email\":\"az_phx_team-sdwan-support@fisglobal.com\",\"phone\":\"6023875757\",\"name\":\"PronetspocSupportteam\",\"type\":\"ticket\"},{\"email\":\"az_phx_team-sdwan-support@fisglobal.com\",\"phone\":\"6023875757\",\"name\":\"PronetspocSupportteam\",\"type\":\"site\"}]}} /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day int seconds 0-inf 5400 /automation-engine/common/service-affecting/monitor/grace-period-to-autoresolve-after-last-documented-trouble-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night int seconds 0-inf 10800 /automation-engine/common/service-affecting/monitor/jitter-monitoring-lookup-interval Defines how much time back to look for Jitter metrics in Jitter checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/jitter-monitoring-threshold Threshold for Jitter troubles int 0-inf 50 /automation-engine/common/service-affecting/monitor/latency-monitoring-lookup-interval Defines how much time back to look for Latency metrics in Latency checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/latency-monitoring-threshold Threshold for Latency troubles int 0-inf 140 /automation-engine/common/service-affecting/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved int 0-inf 3 /automation-engine/common/service-affecting/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int seconds 0-inf 600 /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-lookup-interval Defines how much time back to look for Packet Loss metrics in Packet Loss checks int seconds 0-inf 1800 /automation-engine/common/service-affecting/monitor/packet-loss-monitoring-threshold Threshold for Packet Loss troubles int 0-inf 8 /automation-engine/common/service-affecting/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent int seconds 0-inf 86400 /automation-engine/common/service-affecting/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/service-affecting/reoccurring-trouble-report/recipients-per-host-and-customer Mapping of VeloCloud hosts, Bruin customer IDs and recipients of these reports dictionary {host:{client id:[list of customer emails]}} {\"mettel.velocloud.net\":{\"83959\":[\"clmillsap@oreillyauto.com\",\"mgallion2@oreillyauto.com\",\"rbodenhamer@oreillyauto.com\",\"tkaufmann@oreillyauto.com\",\"mgoldstein@mettel.net\",\"dshim@mettel.net\"],\"85940\":[\"ta-infrastructure@titanamerica.com\"]},\"metvco02.mettel.net\":{\"86937\":[\"networkservices@signetjewelers.com\",\"pallen@mettel.net\"]},\"metvco03.mettel.net\":{\"72959\":[\"DL_Tenet_Telecom@nttdata.com\",\"Jake.Salas@tenethealth.com\",\"dshim@mettel.net\",\"mgoldstein@mettel.net\"],\"83109\":[\"JIngwersen@republicservices.com\",\"LRozendal@republicservices.com\",\"bsherman@mettel.net\"],\"89267\":[\"_DL_IT_Admin@benevis.com\"]}} /automation-engine/common/service-affecting/reoccurring-trouble-report/reported-troubles Troubles that will be reported list troubles [trouble, ...] [\"Jitter\", \"Latency\", \"Packet Loss\", \"Bandwidth Over Utilization\"] /automation-engine/common/service-affecting/reoccurring-trouble-report/default-contacts List of default contacts to whom this report will always be delivered to list emails [email, ...] [\"bsullivan@mettel.net\",\"jtaylor@mettel.net\",\"HNOCleaderteam@mettel.net\",\"mettel.automation@intelygenz.com\"] /automation-engine/common/service-affecting/reoccurring-trouble-report/execution-cron-expression Cron expression that determines when to build and deliver this report int 0 3 * * 0 /automation-engine/common/service-affecting/reoccurring-trouble-report/reoccurring-trouble-tickets-threshold Number of different tickets a trouble must appear in for a particular edge and interface to include it in the report int tickets 0-inf 3 /automation-engine/common/service-affecting/reoccurring-trouble-report/tickets-lookup-interval Defines how much time back to look for Bruin tickets int seconds 0-inf 1209600 /automation-engine/common/service-outage/monitor/business-grade-link-labels List of labels that define a link as business grade list labels [label, ...] [\"DIA\"] /automation-engine/common/service-outage/monitor/grace-period-before-attempting-new-digi-reboots Defines for how long the monitor will wait before attempting a new DiGi Reboot on an edge int seconds 0-inf 1800 /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-day-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the day int seconds 0-inf 5400 /automation-engine/common/service-outage/monitor/grace-period-to-autoresolve-after-last-documented-outage-night-time Defines for how long a ticket can be auto-resolved after the last documented trouble during the night int seconds 0-inf 10800 /automation-engine/common/service-outage/monitor/max-autoresolves-per-ticket Defines how many times a ticket can be auto-resolved int 0-inf 3 /automation-engine/common/service-outage/monitor/missing-edges-from-cache-report-recipient E-mail address that will receive a tiny report showing which edges from VeloCloud responses are not in the cache of customers string email address mettel@intelygenz.com /automation-engine/common/service-outage/monitor/monitoring-job-interval Defines how often devices are checked to find and report issues int 0-inf 600 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down (HA) state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down (HA) state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-ha-soft-down-outage Defines how much time to wait before re-checking an edge currently in Soft Down (HA) state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-hard-down-outage Defines how much time to wait before re-checking an edge currently in Hard Down state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/quarantine-for-edges-in-link-down-outage Defines how much time to wait before re-checking an edge currently in Link Down state int seconds 0-inf 180 /automation-engine/common/service-outage/monitor/severity-for-edge-down-outages Severity level for Edge Down outages int [1 High, 2 MediumHigh , 3 MediumLow , 4Low] 2 /automation-engine/common/service-outage/monitor/severity-for-link-down-outages Severity level for Link Down outages int [1 High, 2 MediumHigh , 3 MediumLow , 4Low] 3 /automation-engine/common/service-outage/monitor/wait-time-before-sending-new-milestone-reminder How long we need to wait for the milestone reminder email to be sent int seconds 0-inf 86400 /automation-engine/common/service-outage/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/service-outage/triage/last-note-interval Defines how long the last note on a ticket is considered recent int seconds 0-inf 3600 /automation-engine/common/service-outage/triage/max-events-per-event-note Defines how many events will be included in events notes int events 0-inf 15 /automation-engine/common/service-outage/triage/monitoring-job-interval Defines how often tickets are checked to see if it needs an initial triage or events note int seconds 0-inf 600 /automation-engine/common/sites-monitor/monitoring-job-interval Defines how often to look for links and edges, and write their data to the metrics server int seconds 0-inf 1200 /automation-engine/common/tnba-feedback/feedback-job-interval Defines how often tickets are pulled from Bruin and sent to the KRE(konstellation) to train the predictive model int seconds 0-inf 3600 /automation-engine/common/tnba-feedback/grace-period-before-resending-tickets Defines for how long a ticket needs to wait before being re-sent to the KRE(konstellation) int seconds 0-inf 604800 /automation-engine/common/tnba-feedback/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/tnba-monitor/grace-period-before-appending-new-tnba-notes Defines for how long a ticket needs to wait since it was opened before appending a new TNBA note int seconds 0-inf 1800 /automation-engine/common/tnba-monitor/grace-period-before-monitoring-tickets-based-on-last-documented-outage Defines for how long a Service Outage ticket needs to wait after the last documented outage to get a new TNBA note appended int seconds 0-inf 3600 /automation-engine/common/tnba-monitor/min-required-confidence-for-request-and-repair-completed-predictions Defines the minimum confidence level required to consider a Request Completed / Repair Completed prediction accurate in TNBA auto-resolves int 0-inf 75 /automation-engine/common/tnba-monitor/monitored-product-category Bruin's product category under monitoring string product category SD-WAN /automation-engine/common/tnba-monitor/monitoring-job-interval Defines how often tickets are checked to see if they need a new TNBA note int seconds 0-inf 300","title":"/automation-engine/common/.."},{"location":"parameters/parameters/#automation-enginepro","text":"Name Description Format Units Range Default /automation-engine/pro/bruin-bridge/base-url Base URL for Bruin's production API string https://api.bruin.com /automation-engine/pro/bruin-bridge/client-id Client ID credential to authenticate against Bruin's production API string /automation-engine/pro/bruin-bridge/client-secret Client secret credential to authenticate against Bruin's production API string /automation-engine/pro/bruin-bridge/login-url Login URL for Bruin's production API string https://id.bruin.com /automation-engine/pro/bruin-bridge/test-base-url Base URL for Bruin's TEST API string https://bruinapi.mettel.net /automation-engine/pro/bruin-bridge/test-client-id Client ID credential to authenticate against Bruin's TEST API string /automation-engine/pro/bruin-bridge/test-client-secret Client secret credential to authenticate against Bruin's TEST API string /automation-engine/pro/bruin-bridge/test-login-url Login URL for Bruin's TEST API string https://id.bruin.com /automation-engine/pro/customer-cache/blacklisted-clients-with-pending-status Client IDs whose edges have Pending management status that should be ignored in the caching process list clients [83109, 87671, 88377, 87915, 88854, 87903, 88012, 89242, 89044, 88912, 89180, 89317, 88748, 89401, 88792, 87916, 89309, 89544, 89268, 88434, 88873, 89332, 89416, 89235, 88550, 89160, 89162, 88345, 88803, 89336, 83763, 89024, 88883, 88848, 89322, 89261, 89191, 89190, 88286, 88272, 88509, 88859, 88110, 88926, 89164, 89233, 88417, 88270, 88698, 89134, 88839, 87957, 89279, 87978, 89342, 88987, 89441, 88989, 89195, 82368, 89353, 89305, 89548, 89080, 88271, 89023, 87955, 88715, 89139, 89077, 89341, 88015, 89521, 89665, 88293, 89321, 89501, 89072, 89107, 88187, 89556, 89323, 88416, 89326, 79939] /automation-engine/pro/customer-cache/blacklisted-edges VeloCloud edges that should be ignored in the caching process list [{\"host\": ..., \"enterprise_id\": ..., \"edge_id\": ...}, ...] [{\"host\": \"mettel.velocloud.net\", \"enterprise_id\": 170, \"edge_id\": 3195}] /automation-engine/pro/customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories string mettel.alerts@intelygenz.com /automation-engine/pro/customer-cache/velocloud-hosts VeloCloud hosts whose edges will be stored to the cache list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/digi-bridge/digi-reboot-api-base-url Base URL for Digi API string https://luci.mettel.net /automation-engine/pro/digi-bridge/digi-reboot-api-client-id Client ID credentials for Digi API string /automation-engine/pro/digi-bridge/digi-reboot-api-client-secret Client Secret credentials for Digi API string /automation-engine/pro/digi-reboot-report/report-recipient Email address to send the report to string mettel.reports@intelygenz.com /automation-engine/pro/email-tagger-kre-bridge/kre-base-url Base URL for E-mail Tagger's KRE(konstellation) string entrypoint.kre-email-tagger.mettel-automation.net:443 /automation-engine/pro/email-tagger-monitor/api-request-key API request key for incoming requests string /automation-engine/pro/email-tagger-monitor/api-request-signature-secret-key API signature secret key for incoming requests string /automation-engine/pro/external-secrets/iam-role-arn The ARN(Amazon Resource Name) of the AWS(Amazon Web Services) IAM(Identity and Access Management) role necessary to manage parameter store and secret manager string /automation-engine/pro/fraud-monitor/observed-inbox-email-address E-mail account that receives Fraud e-mails for later analysis string mettel.automation@intelygenz.com /automation-engine/pro/gateway-monitor/monitored-velocloud-hosts VeloCloud hosts whose gateways will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/hawkeye-customer-cache/duplicate-inventories-recipient E-mail address that will get e-mails with a relation of service numbers that have multiple Bruin inventories string mettel.alerts@intelygenz.com /automation-engine/pro/intermapper-outage-monitor/observed-inbox-email-address E-mail account that receives InterMapper events for later analysis string mettel.automation@intelygenz.com /automation-engine/pro/last-contact-report/monitored-velocloud-hosts VeloCloud hosts whose edges will be used to build the report list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/last-contact-report/report-recipient Email address to send the report to string mettel.reports@intelygenz.com /automation-engine/pro/notifier/monitorable-email-accounts Mapping of e-mail addresses and passwords whose inboxes can be read for later analysis dictionary /automation-engine/pro/notifier/slack-webhook-url Slack webhook to send messages string /automation-engine/pro/papertrail/enabled Enable/Disable Papertrail logs boolean True/False True /automation-engine/pro/redis/customer-cache-hostname Customer Cache Redis hostname string redis-mettel-automation-customer-cache.pro.mettel-automation.net /automation-engine/pro/redis/email-tagger-hostname Email Tagger Redis Hostname string redis-mettel-automation-email-tagger.pro.mettel-automation.net /automation-engine/pro/redis/main-hostname Main Redis server for Automation-Engine string redis-mettel-automation.pro.mettel-automation.net /automation-engine/pro/redis/tnba-feedback-hostname TNBA Feedback hostname string redis-mettel-automation-tnba-feedback.pro.mettel-automation.net /automation-engine/pro/repair-tickets-kre-bridge/kre-base-url Base URL for RTA's KRE(konstellation) dns entrypoint.kre-rta.mettel-automation.net:443 /automation-engine/pro/service-affecting/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list host [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/service-outage/monitor/blacklisted-edges List of edges that are excluded from Service Outage monitoring list edges [{\"host\": ..., \"enterprise_id\": ..., \"edge_id\": ...}, ...] [{\"host\": \"mettel.velocloud.net\", \"enterprise_id\": 170, \"edge_id\": 3195}] /automation-engine/pro/service-outage/monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/service-outage/triage/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/sites-monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/t7-bridge/kre-base-url Base URL for TNBA's KRE(konstellation) dns entrypoint.kre-tnba.mettel-automation.net:443 /automation-engine/pro/tnba-feedback/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/tnba-monitor/blacklisted-edges List of edges that are excluded from TNBA monitoring list edges [{\"host\": ..., \"enterprise_id\": ..., \"edge_id\": ...}, ...] [ {\"host\": \"mettel.velocloud.net\", \"enterprise_id\": 170, \"edge_id\": 3195} ] /automation-engine/pro/tnba-monitor/monitored-velocloud-hosts VeloCloud hosts whose edges will be monitored list hosts [host, ...] [\"mettel.velocloud.net\", \"metvco02.mettel.net\", \"metvco03.mettel.net\", \"metvco04.mettel.net\"] /automation-engine/pro/velocloud-bridge/velocloud-credentials Velocloud credentials string","title":"/automation-engine/pro/.."},{"location":"parameters/parameters/#data-highwaydevelop","text":"Name Description Format Units Range Default /data-highway/develop/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins string https://develop.mettel-data.net /data-highway/develop/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries string SERVICE_ACCOUNTS /data-highway/develop/shared/SECRET_JWT Velocloud hosts to fetch data from string /data-highway/develop/velocloud-fetcher/BLACKLISTED_VENDORS Blacklisted vendors string invented:0 /data-highway/develop/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers string /data-highway/develop/velocloud-fetcher/KAFKA_PASSWORD Kafka develop password to publish velocloud data on Kafka string /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential string /data-highway/develop/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL string /data-highway/develop/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka develop ssl ca path of Kafka string /data-highway/develop/velocloud-fetcher/KAFKA_USERNAME Kafka develop username to publish velocloud data on kafka string /data-highway/develop/velocloud-fetcher/METRICS_LIST Metrics list string metrics;... bytesRx;bytesTx;totalBytes;totalPackets;p1BytesRx;p1BytesTx;p1PacketsRx;p1PacketsTx;p2BytesRx;p2BytesTx;p2PacketsRx;p2PacketsTx;p3BytesRx;p3BytesTx;p3PacketsRx;p3PacketsTx;packetsRx;packetsTx;controlBytesRx;controlBytesTx;controlPacketsRx;controlPacketsTx;bestJitterMsRx;bestJitterMsTx;bestLatencyMsRx;bestLatencyMsTx;bestLossPctRx;bestLossPctTx;bpsOfBestPathRx;bpsOfBestPathTx;signalStrength;scoreTx;scoreRx /data-highway/develop/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials string /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas boolean True/False True /data-highway/develop/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list boolean True/False True /data-highway/develop/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from string host;... mettel.velocloud.net,metvco02.mettel.net,metvco03.mettel.net,metvco04.mettel.net /data-highway/develop/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl string yes/no yes","title":"/data-highway/develop/.."},{"location":"parameters/parameters/#data-highwaymaster","text":"Name Description Format Units Range Default /data-highway/master/metrics-velocloud-api/ALLOW_ORIGINS Metrics allow origins string https://mettel-data.net /data-highway/master/metrics-velocloud-api/WAREHOUSE Metrics velocloud api snowflake warehouse to execute the queries string SERVICE_ACCOUNTS /data-highway/master/shared/SECRET_JWT Velocloud hosts to fetch data from string /data-highway/master/velocloud-fetcher/BOOTSTRAP_SERVERS Bootstrap servers string /data-highway/master/velocloud-fetcher/KAFKA_PASSWORD Kafka master password to publish velocloud data on Kafka string /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_CREDENTIALS Kafka registry credential string /data-highway/master/velocloud-fetcher/KAFKA_REGISTRY_SCHEMA_URL Kafka registry URL string /data-highway/master/velocloud-fetcher/KAFKA_SSL_CA_FILE Kafka master ssl ca path of Kafka string /data-highway/master/velocloud-fetcher/KAFKA_USERNAME Kafka master username to publish velocloud data on kafka string /data-highway/master/velocloud-fetcher/VELOCLOUD_CREDENTIALS Velocloud hosts credentials string /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_AVRO_ENFORCEMENT_FOR_SCHEMAS Bypass velocloud pruner for schemas boolean True/False True /data-highway/master/velocloud-fetcher/VELOCLOUD_FETCHER_BYPASS_PRUNER_FOR_SCHEMAS Metrics list boolean True/False True /data-highway/master/velocloud-fetcher/VELOCLOUD_HOSTS Velocloud hosts to fetch data from string host;... mettel.velocloud.net,metvco02.mettel.net,metvco03.mettel.net,metvco04.mettel.net /data-highway/master/velocloud-fetcher/VELOCLOUD_VERIFY_SSL Velocloud verify ssl string yes/no yes","title":"/data-highway/master/.."},{"location":"pipeline/BASIC_CI_CONFIGURATION/","text":"1. CI/CD PROJECT CONFIGURATION FROM 0 To release this project an make it work we need to configure next stuff: - semantic release - AIVEN - AWS 1.1 Semantic Release Semantic release is depending of a base image in this repository to be faster on this project CI/CD. The only two configurations we need to make it work here is: - Have prepared the base image repository and point to that image in the .gitlab-ci.yml(variable SEMANTIC_RELEASE_IMAGE) - Get an access token on the project (Settings/Access tokens) and get all permissions to interacts with the API. Create a variable called GITLAB_TOKEN and put the token you crete there. 1.2 AIVEN TODO 1.3 AWS It's recommended to create an account for terraform and a group of permissions with admin access, also it's necessary to create an S3 bucket that the user can access to. To accomplish aws connection in some steps of the CI/CD we need to add next variables (on the settings section, not in the YAML) in the CI: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY It's necessary to connect to the S3 from amazon, where we store the tfstate files to maintain the state of our infrastructure deployments. 1.4 Snowflake","title":"Basic configurations"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#1-cicd-project-configuration-from-0","text":"To release this project an make it work we need to configure next stuff: - semantic release - AIVEN - AWS","title":"1. CI/CD PROJECT CONFIGURATION FROM 0"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#11-semantic-release","text":"Semantic release is depending of a base image in this repository to be faster on this project CI/CD. The only two configurations we need to make it work here is: - Have prepared the base image repository and point to that image in the .gitlab-ci.yml(variable SEMANTIC_RELEASE_IMAGE) - Get an access token on the project (Settings/Access tokens) and get all permissions to interacts with the API. Create a variable called GITLAB_TOKEN and put the token you crete there.","title":"1.1 Semantic Release"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#12-aiven","text":"TODO","title":"1.2 AIVEN"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#13-aws","text":"It's recommended to create an account for terraform and a group of permissions with admin access, also it's necessary to create an S3 bucket that the user can access to. To accomplish aws connection in some steps of the CI/CD we need to add next variables (on the settings section, not in the YAML) in the CI: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY It's necessary to connect to the S3 from amazon, where we store the tfstate files to maintain the state of our infrastructure deployments.","title":"1.3 AWS"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#14-snowflake","text":"","title":"1.4 Snowflake"},{"location":"pipeline/PIPELINE_RULES/","text":"PIPELINES RULES Add new section To add new section of jobs in the pipeline, we must include the section adding an included in the general .gitlab-ci.yml: include : - local : microservices/.gitlab-ci.yml Could be possible that an included .gitlab-ci.yml of another section include more sub-sections(Example microservices): include : - local : microservices/fetchers/velocloud/.gitlab-ci.yml We should follow this organization technique to isolate code to their respective area and avoid big files with spaghetti code. Add new template To add a new template we should take a look in the folder structure: # templates (folder where project templating is going to be saved) ## gitlab (section of the templating) Inside gitlab folder we should create template YAML files that fits with a section, for example, for microservices we created a microservice-ci.yml to store the templates of that section. Same in infrastructure. Also, we should include new templates in the index.yml in templates/index.yml include : # CI templates - local : templates/gitlab/microservice-ci.yml - local : templates/gitlab/infrastructure-ci.yml","title":"Pipeline rules"},{"location":"pipeline/PIPELINE_RULES/#pipelines-rules","text":"","title":"PIPELINES RULES"},{"location":"pipeline/PIPELINE_RULES/#add-new-section","text":"To add new section of jobs in the pipeline, we must include the section adding an included in the general .gitlab-ci.yml: include : - local : microservices/.gitlab-ci.yml Could be possible that an included .gitlab-ci.yml of another section include more sub-sections(Example microservices): include : - local : microservices/fetchers/velocloud/.gitlab-ci.yml We should follow this organization technique to isolate code to their respective area and avoid big files with spaghetti code.","title":"Add new section"},{"location":"pipeline/PIPELINE_RULES/#add-new-template","text":"To add a new template we should take a look in the folder structure: # templates (folder where project templating is going to be saved) ## gitlab (section of the templating) Inside gitlab folder we should create template YAML files that fits with a section, for example, for microservices we created a microservice-ci.yml to store the templates of that section. Same in infrastructure. Also, we should include new templates in the index.yml in templates/index.yml include : # CI templates - local : templates/gitlab/microservice-ci.yml - local : templates/gitlab/infrastructure-ci.yml","title":"Add new template"},{"location":"recovery_processes/GITLAB_RECOVERY/","text":"1. Summary Thes process requires one workday and doesn't affect the functionality of the Automation-Engine production environment. Gitlab is deployed with the Automation-Engine fedramp repository , is under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . The deployment, maintenance, and updates are managed locally by the infrastructure team as explained here . Gitlab has most of its configuration files saved in external storage, we use AWS S3 for all objects and AWS RDS (PostgreSQL) for the database; all of this is in US-EAST-1 region. Gitlab has its own process to make backups every day at 06:00 (UTC+02:00) which also includes a Postgres backup and is also saved in S3. All S3 buckets are replicated in US-WEST-1 region so we have backups of GitLab backups on the opposite coast of the united states in case of disaster failure in the main region. Aditionally to this, we require the secrets.yml config file that have the certificates that gitlab creates on his first deploy to use internally. This file is exported and saved in 1password , with the name \u201cGITLAB FedRAMP rails secret backup\u201d and the file name is gitlab-secrets.yaml . This file is required if we want to redeploy gitlab and restore the backup. The backups contain: * db (database) * uploads (attachments) * builds (CI job output logs) * artifacts (CI job artifacts) * lfs (LFS objects) * terraform_state (Terraform states) * registry (Container Registry images) * pages (Pages content) * repositories (Git repositories data) * packages (Packages) 2. Recovery Deploy required infrastructure. Use the infra-as-code/basic-infra folder, only need: 1-parameters , 2-network and 4-services . The terraform workspace is builder . Set the required parameters, check de data.tf file in parameters section. Change the name of the buckets to avoid conflicts. (files 4-services/s3.tf and 4-services/s3-cross-replica ) For deploying the required infra in a new region only need to change Terraform Local variable called region (file locals.tf ) with the new region in the 3 folders. Define a secundary region (file 4-services/terraform_config.tf ) to set the new S3 bucket replicas in another region if main is not available to be the replica. Or if dont want to create the replicas only change the name of the terrafform file (example s3-cross-replica.back ) Install a clean gitlab helm chart with the same version of the backup. Restore the gitlab-rails-secret.yaml as a secret in the cluster: Find the object name for the rails secrets: kubectl -n gitlab get secrets | grep rails-secret Delete the existing secret: kubectl delete secret <rails-secret-name> (gitlab-rails-secret) Create the new secret using the same name as the old, and passing in your local YAML file: kubectl create secret generic <rails-secret-name> --from-file=secrets.yml=gitlab-secrets.yaml In order to use the new secrets, the Webservice, Sidekiq and Toolbox pods need to be restarted. The safest way to restart those pods is to run: kubectl delete -n gitlab pods -lapp=sidekiq,release=<helm release name> kubectl delete -n gitlab pods -lapp=webservice,release=<helm release name> kubectl delete -n gitlab pods -lapp=toolbox,release=<helm release name> Restore the backup file: Ensure the Toolbox pod is enabled and running by executing the following command kubectl get pods -lrelease=RELEASE_NAME,app=toolbox Get the tarball ready in S3 bucket. Make sure it is named in the [timestamp]_[version]_gitlab_backup.tar format. Run the backup utility to restore the tarball kubectl exec <Toolbox pod name> -it -- backup-utility --restore -t <timestamp>_<version> Here, _ is from the name of the tarball stored in gitlab-backups bucket. In case you want to provide a public URL, use the following command kubectl exec <Toolbox pod name> -it -- backup-utility --restore -f <URL> NOTE : You can provide a local path as a URL as long as it's in the format: file:// This process will take time depending on the size of the tarball. The restoration process will erase the existing contents of database, move existing repositories to temporary locations and extract the contents of the tarball. Repositories will be moved to their corresponding locations on the disk and other data, like artifacts, uploads, LFS etc. will be uploaded to corresponding buckets in Object Storage. During restoration, the backup tarball needs to be extracted to disk. This means the Toolbox pod should have disk of necessary size available. For more details and configuration please see the Toolbox documentation. Restore the runner registration token: after restoring, the included runner will not be able to register to the instance because it no longer has the correct registration token (token has been changed with the new GitLab chart installation). Find the new shared runner token located on the admin/runners webpage of your GitLab installation. kubectl get secrets | grep gitlab-runner-secret Find the name of existing runner token Secret stored in Kubernetes kubectl delete secret <runner-secret-name> Delete the existing secret kubectl delete secret <runner-secret-name> Create the new secret with two keys, (runner-registration-token with your shared token, and an empty runner-token) kubectl create secret generic <runner-secret-name> (gitlab-gitlab-runner-secret) --from-literal=runner-registration-token=<new-shared-runner-token> (gitlab-gitlab-runner-token-xxxxx) --from-literal=runner-token=\"\" (optional) Reset the root user\u2019s password: The restoration process does not update the gitlab-initial-root-password secret with the value from backup. For logging in as root, use the original password included in the backup. In the case that the password is no longer accessible, follow the steps below to reset it. Attach to the Webservice pod by executing the command kubectl exec <Webservice pod name> (gitlab-webservice-default-xxxxx-xxxxx) -it -- bash Run the following command to reset the password of root user. Replace #{password} with a password of your choice /srv/gitlab/bin/rails runner \"user = User.first; user.password='#{password}'; user.password_confirmation='#{password}'; user.save!\"","title":"GITLAB RECOVERY"},{"location":"recovery_processes/GITLAB_RECOVERY/#1-summary","text":"Thes process requires one workday and doesn't affect the functionality of the Automation-Engine production environment. Gitlab is deployed with the Automation-Engine fedramp repository , is under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . The deployment, maintenance, and updates are managed locally by the infrastructure team as explained here . Gitlab has most of its configuration files saved in external storage, we use AWS S3 for all objects and AWS RDS (PostgreSQL) for the database; all of this is in US-EAST-1 region. Gitlab has its own process to make backups every day at 06:00 (UTC+02:00) which also includes a Postgres backup and is also saved in S3. All S3 buckets are replicated in US-WEST-1 region so we have backups of GitLab backups on the opposite coast of the united states in case of disaster failure in the main region. Aditionally to this, we require the secrets.yml config file that have the certificates that gitlab creates on his first deploy to use internally. This file is exported and saved in 1password , with the name \u201cGITLAB FedRAMP rails secret backup\u201d and the file name is gitlab-secrets.yaml . This file is required if we want to redeploy gitlab and restore the backup. The backups contain: * db (database) * uploads (attachments) * builds (CI job output logs) * artifacts (CI job artifacts) * lfs (LFS objects) * terraform_state (Terraform states) * registry (Container Registry images) * pages (Pages content) * repositories (Git repositories data) * packages (Packages)","title":"1. Summary"},{"location":"recovery_processes/GITLAB_RECOVERY/#2-recovery","text":"Deploy required infrastructure. Use the infra-as-code/basic-infra folder, only need: 1-parameters , 2-network and 4-services . The terraform workspace is builder . Set the required parameters, check de data.tf file in parameters section. Change the name of the buckets to avoid conflicts. (files 4-services/s3.tf and 4-services/s3-cross-replica ) For deploying the required infra in a new region only need to change Terraform Local variable called region (file locals.tf ) with the new region in the 3 folders. Define a secundary region (file 4-services/terraform_config.tf ) to set the new S3 bucket replicas in another region if main is not available to be the replica. Or if dont want to create the replicas only change the name of the terrafform file (example s3-cross-replica.back ) Install a clean gitlab helm chart with the same version of the backup. Restore the gitlab-rails-secret.yaml as a secret in the cluster: Find the object name for the rails secrets: kubectl -n gitlab get secrets | grep rails-secret Delete the existing secret: kubectl delete secret <rails-secret-name> (gitlab-rails-secret) Create the new secret using the same name as the old, and passing in your local YAML file: kubectl create secret generic <rails-secret-name> --from-file=secrets.yml=gitlab-secrets.yaml In order to use the new secrets, the Webservice, Sidekiq and Toolbox pods need to be restarted. The safest way to restart those pods is to run: kubectl delete -n gitlab pods -lapp=sidekiq,release=<helm release name> kubectl delete -n gitlab pods -lapp=webservice,release=<helm release name> kubectl delete -n gitlab pods -lapp=toolbox,release=<helm release name> Restore the backup file: Ensure the Toolbox pod is enabled and running by executing the following command kubectl get pods -lrelease=RELEASE_NAME,app=toolbox Get the tarball ready in S3 bucket. Make sure it is named in the [timestamp]_[version]_gitlab_backup.tar format. Run the backup utility to restore the tarball kubectl exec <Toolbox pod name> -it -- backup-utility --restore -t <timestamp>_<version> Here, _ is from the name of the tarball stored in gitlab-backups bucket. In case you want to provide a public URL, use the following command kubectl exec <Toolbox pod name> -it -- backup-utility --restore -f <URL> NOTE : You can provide a local path as a URL as long as it's in the format: file:// This process will take time depending on the size of the tarball. The restoration process will erase the existing contents of database, move existing repositories to temporary locations and extract the contents of the tarball. Repositories will be moved to their corresponding locations on the disk and other data, like artifacts, uploads, LFS etc. will be uploaded to corresponding buckets in Object Storage. During restoration, the backup tarball needs to be extracted to disk. This means the Toolbox pod should have disk of necessary size available. For more details and configuration please see the Toolbox documentation. Restore the runner registration token: after restoring, the included runner will not be able to register to the instance because it no longer has the correct registration token (token has been changed with the new GitLab chart installation). Find the new shared runner token located on the admin/runners webpage of your GitLab installation. kubectl get secrets | grep gitlab-runner-secret Find the name of existing runner token Secret stored in Kubernetes kubectl delete secret <runner-secret-name> Delete the existing secret kubectl delete secret <runner-secret-name> Create the new secret with two keys, (runner-registration-token with your shared token, and an empty runner-token) kubectl create secret generic <runner-secret-name> (gitlab-gitlab-runner-secret) --from-literal=runner-registration-token=<new-shared-runner-token> (gitlab-gitlab-runner-token-xxxxx) --from-literal=runner-token=\"\" (optional) Reset the root user\u2019s password: The restoration process does not update the gitlab-initial-root-password secret with the value from backup. For logging in as root, use the original password included in the backup. In the case that the password is no longer accessible, follow the steps below to reset it. Attach to the Webservice pod by executing the command kubectl exec <Webservice pod name> (gitlab-webservice-default-xxxxx-xxxxx) -it -- bash Run the following command to reset the password of root user. Replace #{password} with a password of your choice /srv/gitlab/bin/rails runner \"user = User.first; user.password='#{password}'; user.password_confirmation='#{password}'; user.save!\"","title":"2. Recovery"},{"location":"snowflake/","text":"1. Create a private key for a user/service To create a new user follow the link from snowflake about key creation It's only allowed to create users with keys to automate connection between services. If we need a key for testing purposes should be a temporary user or in a development infrastructure. Important to know that only a SECURITYADMIN user can modify users to have a key pair access. 2. Key rotation policy Key rotation is a must to have a high security standard, at this moment is important to know is a manual process, each first week of the month should be a calendar task in the devops team to change it a redeployment the infrastructure with these new keys. Right now is not automated because no make sense to do it, you have to have a static user that can modify these stuff manually in the web, if we discover a way to do it automatically without a static user or a bastion one we could think about it. 3. Add a New provider 4. Rules Each private key must have a passphrase. Each new provider must have their own key Devops team must renew keys each month and redeploy the infra.","title":"Datalake"},{"location":"snowflake/#1-create-a-private-key-for-a-userservice","text":"To create a new user follow the link from snowflake about key creation It's only allowed to create users with keys to automate connection between services. If we need a key for testing purposes should be a temporary user or in a development infrastructure. Important to know that only a SECURITYADMIN user can modify users to have a key pair access.","title":"1. Create a private key for a user/service"},{"location":"snowflake/#2-key-rotation-policy","text":"Key rotation is a must to have a high security standard, at this moment is important to know is a manual process, each first week of the month should be a calendar task in the devops team to change it a redeployment the infrastructure with these new keys. Right now is not automated because no make sense to do it, you have to have a static user that can modify these stuff manually in the web, if we discover a way to do it automatically without a static user or a bastion one we could think about it.","title":"2. Key rotation policy"},{"location":"snowflake/#3-add-a-new-provider","text":"","title":"3. Add a New provider"},{"location":"snowflake/#4-rules","text":"Each private key must have a passphrase. Each new provider must have their own key Devops team must renew keys each month and redeploy the infra.","title":"4. Rules"}]}