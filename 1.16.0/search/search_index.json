{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CONCEPTS Monorepo Semantic Release Infrastructure as code Datalake Kafka PIPELINES BASIC CONFIGURATIONS Semantic release AIVEN AWS Snowflake PIPELINES RULES Add new section Add new template DOCUMENTATION Organization MetTel Decisions Metrics definitions DEVELOPMENT RULES Branch name convention Semantic release WORK IN A LOCAL ENVIRONMENT Launch docker compose DATALAKE Create private key for a user/service Key rotation policy Add a new provider Rules MANUAL PROCESSES ... MANUAL CONFIGURATIONS AWS SSO Okta identity provider Revoke Session token Gitlab maintenance RECOVERY PROCESSES Gitlab recovery AUTOMATION ENGINE Lambda Parameter-Replicator EXAMPLES PRODUCER PYTHON COMMAND LINE CONSUMER PYTHON COMMAND LINE With passion from the Intelygenz Team @ 2022","title":"Manueal processes"},{"location":"CONCEPTS/","text":"With passion from the Intelygenz Team @ 2021","title":"Concepts"},{"location":"CREATE_NEW_MICROSERVICE/","text":"CREATE NEW MICROSERVICE This process describes step by step how to create a new microservice, from the ECR repository to the helm chart templates that define the microservice. All of this is created from the Gitlab repository in the pipeline; no need for more tools or actions by the developer. Introduction The process requires that the steps be carried out in order. Basically, it is necessary to create the ECR repository first so that we can then start developing and testing our new microservice in ephemeral environments or in production. 1. Create ECR repository We can't create the ECR repository in the branch where we are developing because the creation or update of the ECR repositories is only in the Master branch. This means that the first thing that we need to do is make a little merge to master to create our new microservice repo, by that way we can deploy our microservice later in dev branches. create a new branch from master create a new terraform ECR repo file in the folder: infra-as-code/ecr-repositories you can copy any of the other repos to have an example. this is an example: bash resource \"aws_ecr_repository\" \"new-bridge-repository\" { name = \"new-bridge\" tags = { Project = var.common_info.project Provisioning = var.common_info.provisioning Module = \"new-bridge\" } } merge the new repository to Master branch. The pipeline will run and create the new ECR repo new-bridge 2. Create our new microservice folder We can start working on our new microservice based on an existing one. It depends if is a capability (bridges) or a use case . Select one or other depends on what are you developing. For example let's copy a capability \"bruing-bridge\" and paste in the root of the repo to change his name to \"new-bridge\". from this moment you can start to develop and do your tests locally. Every microservice must have the following directory structure: new-bridge \u251c\u2500\u2500 .gitlab-ci.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 package.json \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u251c\u2500\u2500 app.py \u251c\u2500\u2500 application \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests \u2514\u2500\u2500 ... 3. Update CI-CD gitlab files with the proper values It's important to have the .gitlab-ci.yaml files correctly defined to enable pipelines: * new-bridge/.gitlab-ci.yml Change any reference to de template microservice to the new one.. example find and replace \"bruin-bridge\" for \"new-bridge\" * .gitlab-ci.yml (the file in the root of the repository) Here we need to specify to gitlab-ci that we define other jobs in a different directories (the .gitlab-ci.yml of our new repo). So locate the root gitlab file and add a new line with the path of the new micro jobs (do it respecting the alphabetical order) ... - local: 'services/links-metrics-api/.gitlab-ci.yml' - local: 'services/links-metrics-collector/.gitlab-ci.yml' - local: 'services/lumin-billing-report/.gitlab-ci.yml' - local: 'services/new-bridge/.gitlab-ci.yml' <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! - local: 'services/notifier/.gitlab-ci.yml' - local: 'services/service-affecting-monitor/.gitlab-ci.yml' - local: 'services/service-outage-monitor/.gitlab-ci.yml' ... Now in the same file, let's define the \"desired_tast\" variable for this new micro (do it respecting the alphabetical order): ... NATS_SERVER_DESIRED_TASKS: \"1\" NATS_SERVER_1_DESIRED_TASKS: \"1\" NATS_SERVER_2_DESIRED_TASKS: \"1\" NEW_BRIDGE_DESIRED_TASKS: \"1\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! NOTIFIER_DESIRED_TASKS: \"1\" SERVICE_AFFECTING_MONITOR_DESIRED_TASKS: \"1\" SERVICE_OUTAGE_MONITOR_1_DESIRED_TASKS: \"1\" ... 3. Configure semantic-release We need to edit two files, one in the new micro path and other in the root of the repo: * new-bridge/package.json update the name of the micro with our new working name (do it respecting the alphabetical order): { \"name\": \"new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"version\": \"0.0.1\", \"dependencies\": {}, \"devDependencies\": {} } * package.json (the file in the root of the repository) add to the semantic-release global config our new path to analize version changes (do it respecting the alphabetical order): ... \"./services/links-metrics-api\", \"./services/links-metrics-collector\", \"./services/lumin-billing-report\", \"./services/new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"./services/notifier\", \"./services/service-affecting-monitor\", \"./services/service-outage-monitor\", ... 3. Configure logs We use 2 systems to storage logs, papertrail for 3 days and cloudwath for 1 month. Let's add those config in: * ci-utils/papertrail-provisioning/config.py just copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... { \"query\": f\"lumin-billing-report AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[lumin-billing-report] - logs\", \"repository\": \"lumin-billing-report\", }, { \u00af\u2502 \"query\": f\"new-bridge AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \u2502 \"search_name\": f\"[new-bridge] - logs\", \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! \"repository\": \"new-bridge\", \u2502 }, _\u2502 { \"query\": f\"notifier AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[notifier] - logs\", \"repository\": \"notifier\", }, ... helm/charts/fluent-bit-custom/templates/configmap.yaml The same; copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... [OUTPUT] Name cloudwatch Match kube.var.log.containers.lumin-billing-report* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name lumin-billing-report auto_create_group true [OUTPUT] \u00af\u2502 Name cloudwatch \u2502 Match kube.var.log.containers.new-bridge* \u2502 region {{ .Values.config.region }} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! log_group_name {{ .Values.config.logGroupName }} \u2502 log_stream_name new-bridge \u2502 auto_create_group true _\u2502 [OUTPUT] Name cloudwatch Match kube.var.log.containers.notifier* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name notifier auto_create_group true ... 3. Update docker-compose to enable local deployments docker-compose.yml here we define our container along with the rest of the microservices. Just add the definition of our container respecting the alphabetical order: ```` ... new-bridge: \u00af\u2502 build: \u2502 # Context must be the root of the monorepo \u2502 context: . \u2502 dockerfile: new-bridge/Dockerfile \u2502 args: \u2502 REPOSITORY_URL: 374050862540.dkr.ecr.us-east-1.amazonaws.com \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! DOCKER_BASE_IMAGE_VERSION: 2.2.0 \u2502 env_file: \u2502 new-bridge/src/config/env \u2502 depends_on: \u2502 \"nats-server\" \u2502 redis \u2502 ports: \u2502 5006:5000 _\u2502 notifier: build: ... ```` 4. Add option to enable or disable our microservice helm/charts/automation-engine/Chart.yaml in this file we define our Automation-engine chart version and his dependencies. Let's add a condition for our microservice to have the possibility of disable or enable in our future deployments. ```` ... name: lumin-billing-report version: ' . .*' condition: lumin-billing-report.enabled name: new-bridge \u00af\u2502 version: ' . .*' \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! condition: new-bridge.enabled _\u2502 name: notifier version: ' . .*' condition: notifier.enabled ... ```` 5. Helm templates and variables Here we will define the infraestructure part of our microservice with a helm chart. Is very important to know that this \"how to\" is only to copy an existing microservice, therefore, we take the following statements for granted: 1. The microservice will not have a public endpoint (except email-tagger-monitor) 2. The microservice port will always be 5000 3. Depends on the base chart you use to copy & paste, you will have more or less kubernetes resources. although most microservices have: configmap, deployment, secret and service. Perfect, now let's copy and paste another chart to use as template, if we will develop a use-case, we must copy the most similar use-case. For this example we are creating a \"new-bridge\", so let's copy a \"bruin-bridge\" as a template: * BASE-FOLDER: copy this folder helm/charts/automation-engine/charts/bruin-bridge and paste here helm/charts/automation-engine/charts/ we will have something like bruin-bridge copy change the name to new-bridge . PREPARE BASE_FOLDER: now let's do a find and replace in our new folder new-bridge . find bruin-bridge and replace for new-bridge . many substitutions should appear (at the moment of write this, i can see 46 sustitutions in 10 files but over time, this can change). Just remember to do this in the new-bridge folder context to evoid modify other resources. DEPENDENCIES and CHECKS: Now we have to customize our new microservice, first we must ask ourselves, what dependency does my new microservice have on other services? for example, bruin-bridge have a dependency with Nats and Redis, so it have a few checks to see if those services are available and if they are, it can be deployed. We can find this checks in the deployment.yaml file. Specifically in this part: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} ... We can add or remove all the initcontainers we want. Even, it is very possible that all the dependencies that we need already have the microservice that we use as a base or some other microservice already developed. So we can navigate through the folders of the rest of the microservices and copy any other dependency check and use it in ours. I will add a new dependency for notifier copied from other microservice, and my file will look like the following: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} {{- if .Values.config.capabilities_enabled.notifier }} \u00af\u2502 - name: wait-notifier \u2502 image: busybox:1.28 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! command: ['sh', '-c', 'until wget --spider -S http://notifier:5000/_health; do echo waiting for notifier; sleep 2; done'] \u2502 {{- end }} _\u2502 ... Note that we have a if condition. You will see this in some check, we use this because if we deploy only some microservices, we must contemplate this. If the notifier not exist, the check will not be created. Nats and Redis are always required, thats wy don't have the conditional. VARIABLES: time to update the variables that will use our microservice, this involves various files: helm/charts/automation-engine/charts/new-bridge/templates/configmap.yaml this file always will be part of the deployment, it contains the variables base and the variables with no sensitive information. let's add a new variable NEW_VAR: ... CURRENT_ENVIRONMENT: {{ .Values.global.current_environment }} ENVIRONMENT_NAME: \"{{ .Values.global.environment }}\" NATS_SERVER1: {{ .Values.global.nats_server }} NEW_VAR: \"{{ .Values.config.new_var }}\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! REDIS_HOSTNAME: {{ .Values.global.redis_hostname }} PAPERTRAIL_ACTIVE: \"{{ .Values.global.papertrail_active }}\" PAPERTRAIL_HOST: {{ .Values.global.papertrail_host }} PAPERTRAIL_PORT: \"{{ .Values.global.papertrail_port }}\" PAPERTRAIL_PREFIX: \"{{ .Values.config.papertrail_prefix }}\" ... You can see here two important things, 1. there are variables with quotes and without quotes: this depends on your needs, if you don't put quotes, YAML will interpret the best case for you.. example, if you put a number like 5 as a value, YAML will interpret this as an integer, but careful, this could be a danger if your application expects a string variable; if this is the case, use quotes to define your var. 2. Additionally, we have variables of two types: \"global\" and \"config\". The global ones are common for all microservices, and the \"config\" is specific for this microservice. All the additional variables that we add will be of the type \"config\" helm/charts/automation-engine/charts/new-bridge/templates/secret.yaml this file may or may not exist in the chart and contains variables that have sensitive information. This info will be encoded with base64 to no show in clear text. let's add a new variable NEW_SENSITIVE_VAR: apiVersion: v1 kind: Secret metadata: name: {{ include \"new-bridge.secretName\" . }} labels: {{- include \"new-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" data: NEW_SENSITIVE_VAR: {{ .Values.config.new_sensitive_var | b64enc }} <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! helm/charts/automation-engine/charts/new-bridge/values.yaml Now that we add our new variable in the configmap.yaml, we have to define it in our values file in order to use it. As you can see above, the definition of our variable points to the values file of our microservice; \"Values.config.new_var\" so let's go update it: ... config: <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 in config section!!! papertrail_prefix: \"\" # -- New usefull variable with no sensitive iformation \u00af\u2502______________ here the configmap variable! new_var: \"\" _\u2502 # -- New usefull variable with sensitive iformation \u00af\u2502______________ and here the secret variable! new_sensitive_var: \"\" _\u2502 ... Check that we only define the variable but no put any value, although we can also set a default value if we want. helm/charts/automation-engine/values.yaml This is the values template off the entire automation-engine application. This only have the structure of the values and no contain any real value. For this part we will copy the content of the values file that we just created and paste in the plase that correspond (respecting the alphabetical order). It's important to note that we are pasting the values inside another Yaml, so we must adapt the indentation for the destiny file: ```` ... -- lumin-billing-report subchart specific configuration lumin-billing-report: # -- Field to indicate if the lumin-billing-report module is going to be deployed enabled: true # -- Number of replicas of lumin-billing-report module replicaCount: 1 config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"\" # -- URI of Lumin API lumin_uri: \"\" # -- Token credentials for Lumin API lumin_token: \"\" # -- Name of customer to generate lumin-billing-report customer_name: \"\" # -- Email address to send lumin-billing-report billing_recipient: \"\" image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: \"\" service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: 1 \u2502 enabled: true \u2502 config: \u2502 papertrail_prefix: \"\" \u2502 # -- New usefull variable with no sensitive iformation \u2502 new_var: \"\" \u2502 # -- New usefull variable with sensitive iformation \u2502 new_sensitive_var: \"\" \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: \"\" \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 -- notifier subchart specific configuration notifier: # -- Field to indicate if the notifier module is going to be deployed enabled: true # -- Number of replicas of notifier module replicaCount: 1 # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: \"\" # -- notifier Service Configuration ... ```` Things to check: first the indentation!!.. second, the \"global\" config is not set here; it is define at the beginning of the values file and is common for all microservices. and finnaly, we remove blanck and default configurations to get a sorter file (things removed: autoscaling, the default is false so we can ommit. nodeSelector. tolerations and affinity). PD: You can mantein autoscaling if you will enable it. helm/charts/automation-engine/values.yaml.tpl This is the most important file, it contains the values that will be parsed and used to deploy the Automation-Engine application. Bassicaly it's the same file of values.yaml, but with the variables that will be replaced in the pipeline to deploy a production or develop environment. Let's add our new micro with the variables: ```` ... -- lumin-billing-report subchart specific configuration lumin-billing-report: enabled: ${LUMIN_BILLING_REPORT_ENABLED} replicaCount: ${LUMIN_BILLING_REPORT_DESIRED_TASKS} config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"lumin-billing-report-${LUMIN_BILLING_REPORT_BUILD_NUMBER}\" # -- URI of Lumin API lumin_uri: ${LUMIN_URI} # -- Token credentials for Lumin API lumin_token: ${LUMIN_TOKEN} # -- Name of customer to generate lumin-billing-report customer_name: ${CUSTOMER_NAME_BILLING_REPORT} # -- Email address to send lumin-billing-report billing_recipient: ${BILLING_RECIPIENT} image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: ${LUMIN_BILLING_REPORT_BUILD_NUMBER} service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi -- new-bridge subchart specific configuration \u00af\u2502 new-bridge: \u2502 replicaCount: ${NEW_BRIDGE_DESIRED_TASKS} \u2502 enabled: ${NEW_BRIDGE_ENABLED} \u2502 config: \u2502 papertrail_prefix: \"new-bridge-${NEW_BRIDGE_BUILD_NUMBER}\" \u2502 # -- New usefull variable with no sensitive iformation \u2502 new_var: ${NEW_BRIDGE_NEW_VAR} \u2502 # -- New usefull variable with sensitive iformation \u2502 new_sensitive_var: ${NEW_BRIDGE_NEW_SENSITIVE_VAR} \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: ${NEW_BRIDGE_BUILD_NUMBER} \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502 -- notifier subchart specific configuration notifier: enabled: ${NOTIFIER_ENABLED} replicaCount: ${NOTIFIER_DESIRED_TASKS} # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: ${NOTIFIER_BUILD_NUMBER} # -- notifier Service Configuration service: type: ClusterIP port: 5000 ... ```` With this, we have the entire template of our new microservice. Now we need to set in the pipeline the variables that we just created. ci-utils/environments/deploy_environment_vars.sh In this file, we define the variables that will be used in the values file. Most of the cases are variables that we create in GitLab with the value of dev and production environments. This file is a bash script that has multiple functions to define the variables, each function is for the microservice that requires those variables. If we are adding a new micro that requires variables, we need to define the function and in the bottom of the file execute that function. PD: no all microservices needs specific variables, so in some cases, we wouldn't need to touch this file or even create a secret.yaml. Rebember to respect the alphabetical order: ```` ... function lumin_billing_report_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # lumin-billing-report environment variables for ephemeral environments export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_DEV} else # lumin-billing-report environment variables for production environment export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_PROD} fi } function new_bridge_variables() { \u00af\u2502 if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then \u2502 # new-bridge environment variables for ephemeral environments \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_DEV} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_DEV} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! else \u2502 # new-bridge environment variables for production environment \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_PRO} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_PRO} \u2502 fi \u2502 } _\u2502 function notifier_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # notifier environment variables for ephemeral environments export NOTIFIER_SLACK_URL=${SLACK_URL_DEV} else # notifier environment variables for production environment export NOTIFIER_SLACK_URL=${SLACK_URL_PRO} fi } _\u2502 ... function environments_assign() { # assign enabled variable for each subchart create_enabled_var_for_each_subchart # assign common environment variables for each environment common_variables_by_environment # assign specific environment variables for each subchart bruin_bridge_variables cts_bridge_variables digi_bridge_variables digi_reboot_report_variables email_tagger_monitor_variables hawkeye_bridge_variables links_metrics_api_variables lit_bridge_variables lumin_billing_report_variables new_bridge_variables <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 and here! notifier_variables t7_bridge_variables ticket_collector_variables velocloud_bridge_variables } ... ```` add the variables in gitlab-ci finaly, we have all the path until the real value in gitlab.. let's go to the repository settings/ci-cd section and create the new varaibles: That's all, with this and the proper commit message the pipeline will run and deploy an ephimeral environment.","title":"CREATE NEW MICROSERVICE"},{"location":"CREATE_NEW_MICROSERVICE/#create-new-microservice","text":"This process describes step by step how to create a new microservice, from the ECR repository to the helm chart templates that define the microservice. All of this is created from the Gitlab repository in the pipeline; no need for more tools or actions by the developer.","title":"CREATE NEW MICROSERVICE"},{"location":"CREATE_NEW_MICROSERVICE/#introduction","text":"The process requires that the steps be carried out in order. Basically, it is necessary to create the ECR repository first so that we can then start developing and testing our new microservice in ephemeral environments or in production.","title":"Introduction"},{"location":"CREATE_NEW_MICROSERVICE/#1-create-ecr-repository","text":"We can't create the ECR repository in the branch where we are developing because the creation or update of the ECR repositories is only in the Master branch. This means that the first thing that we need to do is make a little merge to master to create our new microservice repo, by that way we can deploy our microservice later in dev branches. create a new branch from master create a new terraform ECR repo file in the folder: infra-as-code/ecr-repositories you can copy any of the other repos to have an example. this is an example: bash resource \"aws_ecr_repository\" \"new-bridge-repository\" { name = \"new-bridge\" tags = { Project = var.common_info.project Provisioning = var.common_info.provisioning Module = \"new-bridge\" } } merge the new repository to Master branch. The pipeline will run and create the new ECR repo new-bridge","title":"1. Create ECR repository"},{"location":"CREATE_NEW_MICROSERVICE/#2-create-our-new-microservice-folder","text":"We can start working on our new microservice based on an existing one. It depends if is a capability (bridges) or a use case . Select one or other depends on what are you developing. For example let's copy a capability \"bruing-bridge\" and paste in the root of the repo to change his name to \"new-bridge\". from this moment you can start to develop and do your tests locally. Every microservice must have the following directory structure: new-bridge \u251c\u2500\u2500 .gitlab-ci.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 package.json \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u251c\u2500\u2500 app.py \u251c\u2500\u2500 application \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests \u2514\u2500\u2500 ...","title":"2. Create our new microservice folder"},{"location":"CREATE_NEW_MICROSERVICE/#3-update-ci-cd-gitlab-files-with-the-proper-values","text":"It's important to have the .gitlab-ci.yaml files correctly defined to enable pipelines: * new-bridge/.gitlab-ci.yml Change any reference to de template microservice to the new one.. example find and replace \"bruin-bridge\" for \"new-bridge\" * .gitlab-ci.yml (the file in the root of the repository) Here we need to specify to gitlab-ci that we define other jobs in a different directories (the .gitlab-ci.yml of our new repo). So locate the root gitlab file and add a new line with the path of the new micro jobs (do it respecting the alphabetical order) ... - local: 'services/links-metrics-api/.gitlab-ci.yml' - local: 'services/links-metrics-collector/.gitlab-ci.yml' - local: 'services/lumin-billing-report/.gitlab-ci.yml' - local: 'services/new-bridge/.gitlab-ci.yml' <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! - local: 'services/notifier/.gitlab-ci.yml' - local: 'services/service-affecting-monitor/.gitlab-ci.yml' - local: 'services/service-outage-monitor/.gitlab-ci.yml' ... Now in the same file, let's define the \"desired_tast\" variable for this new micro (do it respecting the alphabetical order): ... NATS_SERVER_DESIRED_TASKS: \"1\" NATS_SERVER_1_DESIRED_TASKS: \"1\" NATS_SERVER_2_DESIRED_TASKS: \"1\" NEW_BRIDGE_DESIRED_TASKS: \"1\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! NOTIFIER_DESIRED_TASKS: \"1\" SERVICE_AFFECTING_MONITOR_DESIRED_TASKS: \"1\" SERVICE_OUTAGE_MONITOR_1_DESIRED_TASKS: \"1\" ...","title":"3. Update CI-CD gitlab files with the proper values"},{"location":"CREATE_NEW_MICROSERVICE/#3-configure-semantic-release","text":"We need to edit two files, one in the new micro path and other in the root of the repo: * new-bridge/package.json update the name of the micro with our new working name (do it respecting the alphabetical order): { \"name\": \"new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"version\": \"0.0.1\", \"dependencies\": {}, \"devDependencies\": {} } * package.json (the file in the root of the repository) add to the semantic-release global config our new path to analize version changes (do it respecting the alphabetical order): ... \"./services/links-metrics-api\", \"./services/links-metrics-collector\", \"./services/lumin-billing-report\", \"./services/new-bridge\", <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! \"./services/notifier\", \"./services/service-affecting-monitor\", \"./services/service-outage-monitor\", ...","title":"3. Configure semantic-release"},{"location":"CREATE_NEW_MICROSERVICE/#3-configure-logs","text":"We use 2 systems to storage logs, papertrail for 3 days and cloudwath for 1 month. Let's add those config in: * ci-utils/papertrail-provisioning/config.py just copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... { \"query\": f\"lumin-billing-report AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[lumin-billing-report] - logs\", \"repository\": \"lumin-billing-report\", }, { \u00af\u2502 \"query\": f\"new-bridge AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \u2502 \"search_name\": f\"[new-bridge] - logs\", \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! \"repository\": \"new-bridge\", \u2502 }, _\u2502 { \"query\": f\"notifier AND {ENVIRONMENT_NAME} AND <BUILD_NUMBER>\", \"search_name\": f\"[notifier] - logs\", \"repository\": \"notifier\", }, ... helm/charts/fluent-bit-custom/templates/configmap.yaml The same; copy one block form other microservice and paste with the name of our new micro (do it respecting the alphabetical order): ... [OUTPUT] Name cloudwatch Match kube.var.log.containers.lumin-billing-report* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name lumin-billing-report auto_create_group true [OUTPUT] \u00af\u2502 Name cloudwatch \u2502 Match kube.var.log.containers.new-bridge* \u2502 region {{ .Values.config.region }} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! log_group_name {{ .Values.config.logGroupName }} \u2502 log_stream_name new-bridge \u2502 auto_create_group true _\u2502 [OUTPUT] Name cloudwatch Match kube.var.log.containers.notifier* region {{ .Values.config.region }} log_group_name {{ .Values.config.logGroupName }} log_stream_name notifier auto_create_group true ...","title":"3. Configure logs"},{"location":"CREATE_NEW_MICROSERVICE/#3-update-docker-compose-to-enable-local-deployments","text":"docker-compose.yml here we define our container along with the rest of the microservices. Just add the definition of our container respecting the alphabetical order: ```` ... new-bridge: \u00af\u2502 build: \u2502 # Context must be the root of the monorepo \u2502 context: . \u2502 dockerfile: new-bridge/Dockerfile \u2502 args: \u2502 REPOSITORY_URL: 374050862540.dkr.ecr.us-east-1.amazonaws.com \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! DOCKER_BASE_IMAGE_VERSION: 2.2.0 \u2502 env_file: \u2502 new-bridge/src/config/env \u2502 depends_on: \u2502 \"nats-server\" \u2502 redis \u2502 ports: \u2502 5006:5000 _\u2502 notifier: build: ... ````","title":"3. Update docker-compose to enable local deployments"},{"location":"CREATE_NEW_MICROSERVICE/#4-add-option-to-enable-or-disable-our-microservice","text":"helm/charts/automation-engine/Chart.yaml in this file we define our Automation-engine chart version and his dependencies. Let's add a condition for our microservice to have the possibility of disable or enable in our future deployments. ```` ... name: lumin-billing-report version: ' . .*' condition: lumin-billing-report.enabled name: new-bridge \u00af\u2502 version: ' . .*' \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! condition: new-bridge.enabled _\u2502 name: notifier version: ' . .*' condition: notifier.enabled ... ````","title":"4. Add option to enable or disable our microservice"},{"location":"CREATE_NEW_MICROSERVICE/#5-helm-templates-and-variables","text":"Here we will define the infraestructure part of our microservice with a helm chart. Is very important to know that this \"how to\" is only to copy an existing microservice, therefore, we take the following statements for granted: 1. The microservice will not have a public endpoint (except email-tagger-monitor) 2. The microservice port will always be 5000 3. Depends on the base chart you use to copy & paste, you will have more or less kubernetes resources. although most microservices have: configmap, deployment, secret and service. Perfect, now let's copy and paste another chart to use as template, if we will develop a use-case, we must copy the most similar use-case. For this example we are creating a \"new-bridge\", so let's copy a \"bruin-bridge\" as a template: * BASE-FOLDER: copy this folder helm/charts/automation-engine/charts/bruin-bridge and paste here helm/charts/automation-engine/charts/ we will have something like bruin-bridge copy change the name to new-bridge . PREPARE BASE_FOLDER: now let's do a find and replace in our new folder new-bridge . find bruin-bridge and replace for new-bridge . many substitutions should appear (at the moment of write this, i can see 46 sustitutions in 10 files but over time, this can change). Just remember to do this in the new-bridge folder context to evoid modify other resources. DEPENDENCIES and CHECKS: Now we have to customize our new microservice, first we must ask ourselves, what dependency does my new microservice have on other services? for example, bruin-bridge have a dependency with Nats and Redis, so it have a few checks to see if those services are available and if they are, it can be deployed. We can find this checks in the deployment.yaml file. Specifically in this part: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} ... We can add or remove all the initcontainers we want. Even, it is very possible that all the dependencies that we need already have the microservice that we use as a base or some other microservice already developed. So we can navigate through the folders of the rest of the microservices and copy any other dependency check and use it in ours. I will add a new dependency for notifier copied from other microservice, and my file will look like the following: ... initContainers: - name: wait-nats-server image: busybox:1.28 command: ['sh', '-c', 'until wget --spider -S http://automation-engine-nats:8222/varz; do echo waiting for nats-server; sleep 2; done'] - name: wait-redis image: busybox:1.28 command: ['sh', '-c', 'until printf \"PING\\r\\n\" | nc ${REDIS_HOSTNAME} 6379; do echo waiting for redis; sleep 2; done'] envFrom: - configMapRef: name: {{ include \"new-bridge.configmapName\" . }} {{- if .Values.config.capabilities_enabled.notifier }} \u00af\u2502 - name: wait-notifier \u2502 image: busybox:1.28 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! command: ['sh', '-c', 'until wget --spider -S http://notifier:5000/_health; do echo waiting for notifier; sleep 2; done'] \u2502 {{- end }} _\u2502 ... Note that we have a if condition. You will see this in some check, we use this because if we deploy only some microservices, we must contemplate this. If the notifier not exist, the check will not be created. Nats and Redis are always required, thats wy don't have the conditional. VARIABLES: time to update the variables that will use our microservice, this involves various files: helm/charts/automation-engine/charts/new-bridge/templates/configmap.yaml this file always will be part of the deployment, it contains the variables base and the variables with no sensitive information. let's add a new variable NEW_VAR: ... CURRENT_ENVIRONMENT: {{ .Values.global.current_environment }} ENVIRONMENT_NAME: \"{{ .Values.global.environment }}\" NATS_SERVER1: {{ .Values.global.nats_server }} NEW_VAR: \"{{ .Values.config.new_var }}\" <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! REDIS_HOSTNAME: {{ .Values.global.redis_hostname }} PAPERTRAIL_ACTIVE: \"{{ .Values.global.papertrail_active }}\" PAPERTRAIL_HOST: {{ .Values.global.papertrail_host }} PAPERTRAIL_PORT: \"{{ .Values.global.papertrail_port }}\" PAPERTRAIL_PREFIX: \"{{ .Values.config.papertrail_prefix }}\" ... You can see here two important things, 1. there are variables with quotes and without quotes: this depends on your needs, if you don't put quotes, YAML will interpret the best case for you.. example, if you put a number like 5 as a value, YAML will interpret this as an integer, but careful, this could be a danger if your application expects a string variable; if this is the case, use quotes to define your var. 2. Additionally, we have variables of two types: \"global\" and \"config\". The global ones are common for all microservices, and the \"config\" is specific for this microservice. All the additional variables that we add will be of the type \"config\" helm/charts/automation-engine/charts/new-bridge/templates/secret.yaml this file may or may not exist in the chart and contains variables that have sensitive information. This info will be encoded with base64 to no show in clear text. let's add a new variable NEW_SENSITIVE_VAR: apiVersion: v1 kind: Secret metadata: name: {{ include \"new-bridge.secretName\" . }} labels: {{- include \"new-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" data: NEW_SENSITIVE_VAR: {{ .Values.config.new_sensitive_var | b64enc }} <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 here! helm/charts/automation-engine/charts/new-bridge/values.yaml Now that we add our new variable in the configmap.yaml, we have to define it in our values file in order to use it. As you can see above, the definition of our variable points to the values file of our microservice; \"Values.config.new_var\" so let's go update it: ... config: <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 in config section!!! papertrail_prefix: \"\" # -- New usefull variable with no sensitive iformation \u00af\u2502______________ here the configmap variable! new_var: \"\" _\u2502 # -- New usefull variable with sensitive iformation \u00af\u2502______________ and here the secret variable! new_sensitive_var: \"\" _\u2502 ... Check that we only define the variable but no put any value, although we can also set a default value if we want. helm/charts/automation-engine/values.yaml This is the values template off the entire automation-engine application. This only have the structure of the values and no contain any real value. For this part we will copy the content of the values file that we just created and paste in the plase that correspond (respecting the alphabetical order). It's important to note that we are pasting the values inside another Yaml, so we must adapt the indentation for the destiny file: ```` ...","title":"5. Helm templates and variables"},{"location":"CREATE_NEW_MICROSERVICE/#-lumin-billing-report-subchart-specific-configuration","text":"lumin-billing-report: # -- Field to indicate if the lumin-billing-report module is going to be deployed enabled: true # -- Number of replicas of lumin-billing-report module replicaCount: 1 config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"\" # -- URI of Lumin API lumin_uri: \"\" # -- Token credentials for Lumin API lumin_token: \"\" # -- Name of customer to generate lumin-billing-report customer_name: \"\" # -- Email address to send lumin-billing-report billing_recipient: \"\" image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: \"\" service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi","title":"-- lumin-billing-report subchart specific configuration"},{"location":"CREATE_NEW_MICROSERVICE/#-new-bridge-subchart-specific-configuration","text":"new-bridge: \u2502 replicaCount: 1 \u2502 enabled: true \u2502 config: \u2502 papertrail_prefix: \"\" \u2502 # -- New usefull variable with no sensitive iformation \u2502 new_var: \"\" \u2502 # -- New usefull variable with sensitive iformation \u2502 new_sensitive_var: \"\" \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: \"\" \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502","title":"-- new-bridge subchart specific configuration                           \u00af\u2502"},{"location":"CREATE_NEW_MICROSERVICE/#-notifier-subchart-specific-configuration","text":"notifier: # -- Field to indicate if the notifier module is going to be deployed enabled: true # -- Number of replicas of notifier module replicaCount: 1 # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: \"\" # -- notifier Service Configuration ... ```` Things to check: first the indentation!!.. second, the \"global\" config is not set here; it is define at the beginning of the values file and is common for all microservices. and finnaly, we remove blanck and default configurations to get a sorter file (things removed: autoscaling, the default is false so we can ommit. nodeSelector. tolerations and affinity). PD: You can mantein autoscaling if you will enable it. helm/charts/automation-engine/values.yaml.tpl This is the most important file, it contains the values that will be parsed and used to deploy the Automation-Engine application. Bassicaly it's the same file of values.yaml, but with the variables that will be replaced in the pipeline to deploy a production or develop environment. Let's add our new micro with the variables: ```` ...","title":"-- notifier subchart specific configuration"},{"location":"CREATE_NEW_MICROSERVICE/#-lumin-billing-report-subchart-specific-configuration_1","text":"lumin-billing-report: enabled: ${LUMIN_BILLING_REPORT_ENABLED} replicaCount: ${LUMIN_BILLING_REPORT_DESIRED_TASKS} config: # -- Papertrail prefix for create logs definition papertrail_prefix: \"lumin-billing-report-${LUMIN_BILLING_REPORT_BUILD_NUMBER}\" # -- URI of Lumin API lumin_uri: ${LUMIN_URI} # -- Token credentials for Lumin API lumin_token: ${LUMIN_TOKEN} # -- Name of customer to generate lumin-billing-report customer_name: ${CUSTOMER_NAME_BILLING_REPORT} # -- Email address to send lumin-billing-report billing_recipient: ${BILLING_RECIPIENT} image: repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/lumin-billing-report pullPolicy: Always # Overrides the image tag whose default is the chart appVersion. tag: ${LUMIN_BILLING_REPORT_BUILD_NUMBER} service: type: ClusterIP port: 5000 resources: limits: cpu: 200m memory: 256Mi requests: cpu: 100m memory: 128Mi","title":"-- lumin-billing-report subchart specific configuration"},{"location":"CREATE_NEW_MICROSERVICE/#-new-bridge-subchart-specific-configuration_1","text":"new-bridge: \u2502 replicaCount: ${NEW_BRIDGE_DESIRED_TASKS} \u2502 enabled: ${NEW_BRIDGE_ENABLED} \u2502 config: \u2502 papertrail_prefix: \"new-bridge-${NEW_BRIDGE_BUILD_NUMBER}\" \u2502 # -- New usefull variable with no sensitive iformation \u2502 new_var: ${NEW_BRIDGE_NEW_VAR} \u2502 # -- New usefull variable with sensitive iformation \u2502 new_sensitive_var: ${NEW_BRIDGE_NEW_SENSITIVE_VAR} \u2502 image: \u2502 repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/new-bridge \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! pullPolicy: Always \u2502 # Overrides the image tag whose default is the chart appVersion. \u2502 tag: ${NEW_BRIDGE_BUILD_NUMBER} \u2502 service: \u2502 type: ClusterIP \u2502 port: 5000 \u2502 resources: \u2502 limits: \u2502 cpu: 200m \u2502 memory: 256Mi \u2502 requests: \u2502 cpu: 100m \u2502 memory: 128Mi _\u2502","title":"-- new-bridge subchart specific configuration                           \u00af\u2502"},{"location":"CREATE_NEW_MICROSERVICE/#-notifier-subchart-specific-configuration_1","text":"notifier: enabled: ${NOTIFIER_ENABLED} replicaCount: ${NOTIFIER_DESIRED_TASKS} # -- notifier image details image: # -- notifier repository for docker images repository: 374050862540.dkr.ecr.us-east-1.amazonaws.com/notifier pullPolicy: Always # -- notifier tag of docker image tag: ${NOTIFIER_BUILD_NUMBER} # -- notifier Service Configuration service: type: ClusterIP port: 5000 ... ```` With this, we have the entire template of our new microservice. Now we need to set in the pipeline the variables that we just created. ci-utils/environments/deploy_environment_vars.sh In this file, we define the variables that will be used in the values file. Most of the cases are variables that we create in GitLab with the value of dev and production environments. This file is a bash script that has multiple functions to define the variables, each function is for the microservice that requires those variables. If we are adding a new micro that requires variables, we need to define the function and in the bottom of the file execute that function. PD: no all microservices needs specific variables, so in some cases, we wouldn't need to touch this file or even create a secret.yaml. Rebember to respect the alphabetical order: ```` ... function lumin_billing_report_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # lumin-billing-report environment variables for ephemeral environments export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_DEV} else # lumin-billing-report environment variables for production environment export BILLING_RECIPIENT=${BILLING_RECIPIENT_REPORT_PROD} fi } function new_bridge_variables() { \u00af\u2502 if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then \u2502 # new-bridge environment variables for ephemeral environments \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_DEV} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_DEV} \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500> here! else \u2502 # new-bridge environment variables for production environment \u2502 export NEW_BRIDGE_NEW_VARIABLE=${NEW_BRIDGE_NEW_VARIABLE_PRO} \u2502 export NEW_BRIDGE_NEW_SENSITIVE_VARIABLE=${NEW_BRIDGE_NEW_SENSITIVE_VARIABLE_PRO} \u2502 fi \u2502 } _\u2502 function notifier_variables() { if [[ \"${CI_COMMIT_REF_SLUG}\" != \"master\" ]]; then # notifier environment variables for ephemeral environments export NOTIFIER_SLACK_URL=${SLACK_URL_DEV} else # notifier environment variables for production environment export NOTIFIER_SLACK_URL=${SLACK_URL_PRO} fi } _\u2502 ... function environments_assign() { # assign enabled variable for each subchart create_enabled_var_for_each_subchart # assign common environment variables for each environment common_variables_by_environment # assign specific environment variables for each subchart bruin_bridge_variables cts_bridge_variables digi_bridge_variables digi_reboot_report_variables email_tagger_monitor_variables hawkeye_bridge_variables links_metrics_api_variables lit_bridge_variables lumin_billing_report_variables new_bridge_variables <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 and here! notifier_variables t7_bridge_variables ticket_collector_variables velocloud_bridge_variables } ... ```` add the variables in gitlab-ci finaly, we have all the path until the real value in gitlab.. let's go to the repository settings/ci-cd section and create the new varaibles: That's all, with this and the proper commit message the pipeline will run and deploy an ephimeral environment.","title":"-- notifier subchart specific configuration"},{"location":"DOCUMENTATION/","text":"1. Organization With passion from the Intelygenz Team @ 2021","title":"Organization"},{"location":"DOCUMENTATION/#1-organization","text":"With passion from the Intelygenz Team @ 2021","title":"1. Organization"},{"location":"INFRASTRUCTURE_AS_CODE/","text":"INFRASTRUCTURE AS CODE Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. IaC is a key DevOps practice and is used in conjunction with continuous delivery. Azure Introduction Infrastructure as Code enables DevOps to test the deployment of environments before use it in production. IaC can deliver stable environments rapidly and at scale. Avoiding manual configuration of environments and enforce consistency by representing the desired state of their environments via code. This technique improves the automatic deployments in automation-engine, each time the pipelines launch the Continuous delivery will create, update or destroy the infrastructure if it's necessary. IaC in MetTel Automation Automation-engine runs IaC with terraform , this task is/will be included in the automation pipelines . Terraform save the state of the infrastructure in a storage, these files have the extension .tfstate . In MetTel Automation we saves these files in a protected Cloud storage to centralize the states and be accessible each time the pipeline needs to deploy/update the infrastructure. Folder structure infra-as-code/ \u251c\u2500\u2500 basic-infra # basic infrastructure in AWS \u2514\u2500\u2500 data-collector # data-collector infrastructre in AWS \u2514\u2500\u2500 dev # AWS resources for each environment (ECS Cluster, ElastiCache Cluster, etc.) \u2514\u2500\u2500 kre # kre infrastructure \u2514\u2500\u2500 0-create-bucket # bucket to store EKS information \u2514\u2500\u2500 1-eks-roles # IAM roles infrastructure for EKS cluster \u2514\u2500\u2500 2-smtp # SES infrastructure folder \u2514\u2500\u2500 kre-runtimes # kre runtimes infrastructure \u2514\u2500\u2500 modules # custom terraform modules folders used for create KRE infrastructure \u2514\u2500\u2500 runtimes # KRE runtimes folders \u2514\u2500\u2500 network-resources # network resources infrastructure in AWS Al terraform files are located inside ./infra-as-code , in this folder there are four additional folders, basic-infra , dev , ecs-services and network-resources . basic-infra : there are the necessary terraform files to create the Docker images repositories in ECS, and the roles and policies necessary for use these. data-collector : there are the necessary terraform files to create a Lambda, a DocumentDB Cluster, as well as an API Gateway to call the necessary and all the necessary resources to perform the conexion between these elements. These resources will only be created for production environment dev : there are the necessary terraform files for create the resources used for each environment in AWS, these are as follows An ECS Cluster , the ECS Services and its Task Definition for all the microservices present in the project Three ElastiCache Redis Clusters An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A set of Security Groups for all the resources created by the terraform files present in this folder A set of null_resource Terraform type resources to execute the python script in charge of health checking the task instances created in the deployment of capabilities microservices. kre : there are the necessary terraform files for create the infrastructure for the kre component of konstellation , as well as all the components it needs at AWS. There is a series of folders with terraform code that have a number in their names, these will be used to deploy the components in a certain order and are detailed below: 0-create-bucket : In this folder the terraform code is available to create a bucket for each environment and save information about the cluster, such as the SSH key to connect to the worker nodes of the EKS cluster that is going to be created. 1-eks-roles : In this folder the terraform code is available to create different IAM roles to map with EKS users and assign specific permissions for each one, for this purpose, a cli will be used later. 2-create-eks-cluster : In this folder the terraform code is available to create the following resources An EKS cluster to be able to deploy the different kre components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A hosted zone on Route53 for the corresponding kre environment A SSH key to connect to any worker node of EKS 3-smtp : In this folder the terraform code to create a SMTP service through Amazon SES and all the necessary componentes of it. kre-runtimes : there are the necessary terraform files for create the infrastructure needed by a KRE runtime: modules : Contains the terraform code for custom modules created for provision a KRE runtimes. It will create the following for each KRE runtime: A Route53 Hosted Zone in mettel-automation.net domain. runtimes : Contains the terraform code files for deploy KRE runtimes used the custom module located in modules folder. network-resources : there are the necessary terraform files for create the VPC and all related resources in the environment used for deployment, these being the following: Internet Gateway Elastic IP Addresses NAT Gateways Subnets Route tables for the created subnets A set of Security Groups for all the resources created by the terraform files present in this folder","title":"INFRASTRUCTURE AS CODE"},{"location":"INFRASTRUCTURE_AS_CODE/#infrastructure-as-code","text":"Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. IaC is a key DevOps practice and is used in conjunction with continuous delivery. Azure","title":"INFRASTRUCTURE AS CODE"},{"location":"INFRASTRUCTURE_AS_CODE/#introduction","text":"Infrastructure as Code enables DevOps to test the deployment of environments before use it in production. IaC can deliver stable environments rapidly and at scale. Avoiding manual configuration of environments and enforce consistency by representing the desired state of their environments via code. This technique improves the automatic deployments in automation-engine, each time the pipelines launch the Continuous delivery will create, update or destroy the infrastructure if it's necessary.","title":"Introduction"},{"location":"INFRASTRUCTURE_AS_CODE/#iac-in-mettel-automation","text":"Automation-engine runs IaC with terraform , this task is/will be included in the automation pipelines . Terraform save the state of the infrastructure in a storage, these files have the extension .tfstate . In MetTel Automation we saves these files in a protected Cloud storage to centralize the states and be accessible each time the pipeline needs to deploy/update the infrastructure.","title":"IaC in MetTel Automation"},{"location":"INFRASTRUCTURE_AS_CODE/#folder-structure","text":"infra-as-code/ \u251c\u2500\u2500 basic-infra # basic infrastructure in AWS \u2514\u2500\u2500 data-collector # data-collector infrastructre in AWS \u2514\u2500\u2500 dev # AWS resources for each environment (ECS Cluster, ElastiCache Cluster, etc.) \u2514\u2500\u2500 kre # kre infrastructure \u2514\u2500\u2500 0-create-bucket # bucket to store EKS information \u2514\u2500\u2500 1-eks-roles # IAM roles infrastructure for EKS cluster \u2514\u2500\u2500 2-smtp # SES infrastructure folder \u2514\u2500\u2500 kre-runtimes # kre runtimes infrastructure \u2514\u2500\u2500 modules # custom terraform modules folders used for create KRE infrastructure \u2514\u2500\u2500 runtimes # KRE runtimes folders \u2514\u2500\u2500 network-resources # network resources infrastructure in AWS Al terraform files are located inside ./infra-as-code , in this folder there are four additional folders, basic-infra , dev , ecs-services and network-resources . basic-infra : there are the necessary terraform files to create the Docker images repositories in ECS, and the roles and policies necessary for use these. data-collector : there are the necessary terraform files to create a Lambda, a DocumentDB Cluster, as well as an API Gateway to call the necessary and all the necessary resources to perform the conexion between these elements. These resources will only be created for production environment dev : there are the necessary terraform files for create the resources used for each environment in AWS, these are as follows An ECS Cluster , the ECS Services and its Task Definition for all the microservices present in the project Three ElastiCache Redis Clusters An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A set of Security Groups for all the resources created by the terraform files present in this folder A set of null_resource Terraform type resources to execute the python script in charge of health checking the task instances created in the deployment of capabilities microservices. kre : there are the necessary terraform files for create the infrastructure for the kre component of konstellation , as well as all the components it needs at AWS. There is a series of folders with terraform code that have a number in their names, these will be used to deploy the components in a certain order and are detailed below: 0-create-bucket : In this folder the terraform code is available to create a bucket for each environment and save information about the cluster, such as the SSH key to connect to the worker nodes of the EKS cluster that is going to be created. 1-eks-roles : In this folder the terraform code is available to create different IAM roles to map with EKS users and assign specific permissions for each one, for this purpose, a cli will be used later. 2-create-eks-cluster : In this folder the terraform code is available to create the following resources An EKS cluster to be able to deploy the different kre components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A hosted zone on Route53 for the corresponding kre environment A SSH key to connect to any worker node of EKS 3-smtp : In this folder the terraform code to create a SMTP service through Amazon SES and all the necessary componentes of it. kre-runtimes : there are the necessary terraform files for create the infrastructure needed by a KRE runtime: modules : Contains the terraform code for custom modules created for provision a KRE runtimes. It will create the following for each KRE runtime: A Route53 Hosted Zone in mettel-automation.net domain. runtimes : Contains the terraform code files for deploy KRE runtimes used the custom module located in modules folder. network-resources : there are the necessary terraform files for create the VPC and all related resources in the environment used for deployment, these being the following: Internet Gateway Elastic IP Addresses NAT Gateways Subnets Route tables for the created subnets A set of Security Groups for all the resources created by the terraform files present in this folder","title":"Folder structure"},{"location":"LOGGING_AND_MONITORING/","text":"Logging and Monitoring Cloudwatch Cloudwatch Log Groups A log group is created in Cloudwatch for the different environments deployed in AWS: Production environment : A log group will be created with the name automation-master in which the different logStreams will be created to store the logs of the different ECS services for the production environment. Ephemeral environments : A log group will be created with the name automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment, the logStreams required for the different ECS services deployed in that environment will be created using the mentioned log group . Cloudwatch Log Streams As mentioned in the previous section, the different logStreams of the deployed services will be stored in a specific logGroup for each environment. A logStream will be created for each of the ECS cluster services tasks created in each environment, which will follow the following scheme <environment_name>-<microservice_image_build_number>/<microservice_name>/<ecs_tasks_id> environment_name : The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. microservice_image_build_number : The pipeline number is used as build number to build the image of those microservices that need to build a new docker image. microservice_name : The name of the microservice deployed in ECS, e.g. bruin-bridge . ecs_task_id> : For each ECS service, one or several tasks are created, depending on the desired number in the service, these tasks are identified with an identifier formed by number and letters, e.g. 961ef51b61834a2e9dd804db564a9fe0 . Cloudwatch Retention Period All environments deployed on AWS have been configured to use Cloudwatch to record the logs of the microservices present in them, although it is important to note the following differences: Production environment : The retention period of the log group created for such an environment is 90 days. Ephemeral environments : The retention period of the log group created for such an environment is 14 days. This retention period is configured in the infra-as-code/dev/logs.tf file. Cloudwatch logs retrieval tool It is possible to obtain the events in logs of a logGroup through a tool designed for this purpose available in Github called log-stream-filter . Download and install This tool is available for Linux, MacOS and Windows, it is possible to download the latest binary for each of these OSs: Linux : It's possible download and install as a deb package sh curl -o log-stream-filter.deb -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_amd64.deb\")) | .browser_download_url') sudo dpkg -i log-stream-filter.deb It's also possible download and install as a simple binary sh curl -o log-stream-filter.tgz -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_64-bit.tar.gz\")) | .browser_download_url') tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin MacOS : sh curl -o log-stream-filter.tgz -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"macOS\")) | .browser_download_url') tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin Windows : sh curl -o log-stream-filter.zip -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"windows\")) | .browser_download_url') unzip log-stream-filter.zip Example of usage Example of search text Outage monitoring process finished between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 in log group with name automation-master for log streams that match with name service-outage-monitor-1 using AWS profile with name mettel-automation : ```sh $ log-stream-filter -n \"automation-master\" -l \"service-outage-monitor-1\" -a \"mettel-automation\" -s \"04/06/2021 12:00:00\" -e \"04/07/2021 12:00:00\" -T \"Outage monitoring process finished\" -t true Filtering logs for logGroup automation-master params: [aws-profile mettel-automation] [log-stream-filter: service-outage-monitor-1] [search-term-search: true] [search-term: Outage monitoring process finished] [path: /tmp] [start-date: 04/06/2021 12:00:00] [end-date: 04/07/2021 12:00:00] Getting logStreams for logGroup automation-master applying filter service-outage-monitor-1 Getting the logEvents for those logStreams whose last event was inserted between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 * * * * * * * * * * * * * * ** LogStreamName: automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 CreationTime: 04/05/2021 23:08:29 LastEventTime: 04/06/2021 12:43:49 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 from time 1617710400000 Event messages for stream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 in log group automation-master are going to be saved in the following files /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished LogStreamName: automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 CreationTime: 04/06/2021 12:44:40 LastEventTime: 04/07/2021 10:39:16 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 from time 1617710400000 Event messages for stream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished LogStreamName: automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 CreationTime: 04/07/2021 10:41:22 LastEventTime: 04/07/2021 11:04:33 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 from time 1617710400000 Event messages for stream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished 3 files generated for logs of logStreams filtered for logGroup automation-master Location of files where logs of logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 were stored are the following - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 were stored are the following - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 were stored are the following - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished ``` To know all the options it is recommended to read the README of log-stream-filter in github. Papertrail There is a papertrail account used in the project for sending logs, but it will only be used for the production environment , this is because it is a high cost service and it is not feasible to have an account for all ephemeral environments due to its cost and the high volume of logs generated in these environments. Papertrail dashboards The system logs of the production environment are stored in papertrail, the account credentials are in the project's onepassword vault to which all people in the project have access. Below is a screenshot of the main screen of the system. In this system it's possible see three useful dashboards: [production] - master alarms : alarms are defined on different modules with notifications sent to slack through the mettel-alarms-papertrail-production channel, for example when the number of error messages in a module exceeds 100 times in an hour. Below is a screenshot where these alarms have been marked in a red rectangle. [production] - master logs : searches are defined to gather the logs of the replicas of each deployed microservice. Below is a screenshot where these searches have been marked in a red rectangle. [production] - master notifications : Searches on different modules are defined with their notification to slack through the mettel-notifications-papertrail-production channel. Below is a screenshot where these searches have been marked in a red rectangle. Papertrail logging configuration A certain configuration must be made for the microservices to use papertrail for sending logs in production, this configuration is made in the LOG_CONFIG section of the config.py file present in the src/config folder of each microservice, this configuration is shown below: LOG_CONFIG = { 'name': '<microservice_name>', 'level': logging.DEBUG, 'stream_handler': logging.StreamHandler(sys.stdout), 'format': f'%(asctime)s: {ENVIRONMENT_NAME}: %(hostname)s: %(module)s::%(lineno)d %(levelname)s: %(message)s', 'papertrail': { 'active': True if os.getenv('PAPERTRAIL_ACTIVE') == \"true\" else False, 'prefix': os.getenv('PAPERTRAIL_PREFIX', f'{ENVIRONMENT_NAME}-<microservice_name>'), 'host': os.getenv('PAPERTRAIL_HOST'), 'port': int(os.getenv('PAPERTRAIL_PORT')) }, } It would be necessary to put the microservice name instead of microservice_name for each microservice in the project Papertrail searches configuration The papertrail-provisioning tool is available in the repository to configure the different groups of searches in papertrail, you must follow the procedure explained in the README of the same for its use. The above mentioned guide should be followed to add log searches for a specific microservice, it is also possible to configure alarms that will send notifications to slack.","title":"LOGGING AND MONITORING"},{"location":"LOGGING_AND_MONITORING/#logging-and-monitoring","text":"","title":"Logging and Monitoring"},{"location":"LOGGING_AND_MONITORING/#cloudwatch","text":"","title":"Cloudwatch"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-log-groups","text":"A log group is created in Cloudwatch for the different environments deployed in AWS: Production environment : A log group will be created with the name automation-master in which the different logStreams will be created to store the logs of the different ECS services for the production environment. Ephemeral environments : A log group will be created with the name automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment, the logStreams required for the different ECS services deployed in that environment will be created using the mentioned log group .","title":"Cloudwatch Log Groups"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-log-streams","text":"As mentioned in the previous section, the different logStreams of the deployed services will be stored in a specific logGroup for each environment. A logStream will be created for each of the ECS cluster services tasks created in each environment, which will follow the following scheme <environment_name>-<microservice_image_build_number>/<microservice_name>/<ecs_tasks_id> environment_name : The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. microservice_image_build_number : The pipeline number is used as build number to build the image of those microservices that need to build a new docker image. microservice_name : The name of the microservice deployed in ECS, e.g. bruin-bridge . ecs_task_id> : For each ECS service, one or several tasks are created, depending on the desired number in the service, these tasks are identified with an identifier formed by number and letters, e.g. 961ef51b61834a2e9dd804db564a9fe0 .","title":"Cloudwatch Log Streams"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-retention-period","text":"All environments deployed on AWS have been configured to use Cloudwatch to record the logs of the microservices present in them, although it is important to note the following differences: Production environment : The retention period of the log group created for such an environment is 90 days. Ephemeral environments : The retention period of the log group created for such an environment is 14 days. This retention period is configured in the infra-as-code/dev/logs.tf file.","title":"Cloudwatch Retention Period"},{"location":"LOGGING_AND_MONITORING/#cloudwatch-logs-retrieval-tool","text":"It is possible to obtain the events in logs of a logGroup through a tool designed for this purpose available in Github called log-stream-filter .","title":"Cloudwatch logs retrieval tool"},{"location":"LOGGING_AND_MONITORING/#download-and-install","text":"This tool is available for Linux, MacOS and Windows, it is possible to download the latest binary for each of these OSs: Linux : It's possible download and install as a deb package sh curl -o log-stream-filter.deb -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_amd64.deb\")) | .browser_download_url') sudo dpkg -i log-stream-filter.deb It's also possible download and install as a simple binary sh curl -o log-stream-filter.tgz -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"linux_64-bit.tar.gz\")) | .browser_download_url') tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin MacOS : sh curl -o log-stream-filter.tgz -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"macOS\")) | .browser_download_url') tar -zxvf log-stream-filter.tgz chmod +x ./log-stream-filter && sudo mv ./log-stream-filter /usr/local/bin Windows : sh curl -o log-stream-filter.zip -L $(curl -s https://api.github.com/repos/xoanmm/log-stream-filter/releases/latest | jq -r '.assets[] | select(.name | contains(\"windows\")) | .browser_download_url') unzip log-stream-filter.zip","title":"Download and install"},{"location":"LOGGING_AND_MONITORING/#example-of-usage","text":"Example of search text Outage monitoring process finished between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 in log group with name automation-master for log streams that match with name service-outage-monitor-1 using AWS profile with name mettel-automation : ```sh $ log-stream-filter -n \"automation-master\" -l \"service-outage-monitor-1\" -a \"mettel-automation\" -s \"04/06/2021 12:00:00\" -e \"04/07/2021 12:00:00\" -T \"Outage monitoring process finished\" -t true Filtering logs for logGroup automation-master params: [aws-profile mettel-automation] [log-stream-filter: service-outage-monitor-1] [search-term-search: true] [search-term: Outage monitoring process finished] [path: /tmp] [start-date: 04/06/2021 12:00:00] [end-date: 04/07/2021 12:00:00] Getting logStreams for logGroup automation-master applying filter service-outage-monitor-1 Getting the logEvents for those logStreams whose last event was inserted between 04/06/2021 12:00:00 and 04/07/2021 12:00:00 * * * * * * * * * * * * * * ** LogStreamName: automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 CreationTime: 04/05/2021 23:08:29 LastEventTime: 04/06/2021 12:43:49 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 from time 1617710400000 Event messages for stream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 in log group automation-master are going to be saved in the following files /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished LogStreamName: automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 CreationTime: 04/06/2021 12:44:40 LastEventTime: 04/07/2021 10:39:16 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 from time 1617710400000 Event messages for stream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished LogStreamName: automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 CreationTime: 04/07/2021 10:41:22 LastEventTime: 04/07/2021 11:04:33 All log events are going to be retrieved in logGroup automation-master for logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 from time 1617710400000 Event messages for stream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 in log group automation-master are going to be saved in the following files - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished 3 files generated for logs of logStreams filtered for logGroup automation-master Location of files where logs of logStream automation-master-75993/service-outage-monitor-1/75d4e2cbb3f64e84a97063d34ffe6177 were stored are the following - /tmp/automation-master-75993_service-outage-monitor-1_75d4e2cbb3f64e84a97063d34ffe6177-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76009/service-outage-monitor-1/aab10e3e029c44fcb08dddeb51431fa2 were stored are the following - /tmp/automation-master-76009_service-outage-monitor-1_aab10e3e029c44fcb08dddeb51431fa2-Outage_monitoring_process_finished Location of files where logs of logStream automation-master-76051/service-outage-monitor-1/c921aa5c9bf04d71b0fab6d8673946a3 were stored are the following - /tmp/automation-master-76051_service-outage-monitor-1_c921aa5c9bf04d71b0fab6d8673946a3-Outage_monitoring_process_finished ``` To know all the options it is recommended to read the README of log-stream-filter in github.","title":"Example of usage"},{"location":"LOGGING_AND_MONITORING/#papertrail","text":"There is a papertrail account used in the project for sending logs, but it will only be used for the production environment , this is because it is a high cost service and it is not feasible to have an account for all ephemeral environments due to its cost and the high volume of logs generated in these environments.","title":"Papertrail"},{"location":"LOGGING_AND_MONITORING/#papertrail-dashboards","text":"The system logs of the production environment are stored in papertrail, the account credentials are in the project's onepassword vault to which all people in the project have access. Below is a screenshot of the main screen of the system. In this system it's possible see three useful dashboards: [production] - master alarms : alarms are defined on different modules with notifications sent to slack through the mettel-alarms-papertrail-production channel, for example when the number of error messages in a module exceeds 100 times in an hour. Below is a screenshot where these alarms have been marked in a red rectangle. [production] - master logs : searches are defined to gather the logs of the replicas of each deployed microservice. Below is a screenshot where these searches have been marked in a red rectangle. [production] - master notifications : Searches on different modules are defined with their notification to slack through the mettel-notifications-papertrail-production channel. Below is a screenshot where these searches have been marked in a red rectangle.","title":"Papertrail dashboards"},{"location":"LOGGING_AND_MONITORING/#papertrail-logging-configuration","text":"A certain configuration must be made for the microservices to use papertrail for sending logs in production, this configuration is made in the LOG_CONFIG section of the config.py file present in the src/config folder of each microservice, this configuration is shown below: LOG_CONFIG = { 'name': '<microservice_name>', 'level': logging.DEBUG, 'stream_handler': logging.StreamHandler(sys.stdout), 'format': f'%(asctime)s: {ENVIRONMENT_NAME}: %(hostname)s: %(module)s::%(lineno)d %(levelname)s: %(message)s', 'papertrail': { 'active': True if os.getenv('PAPERTRAIL_ACTIVE') == \"true\" else False, 'prefix': os.getenv('PAPERTRAIL_PREFIX', f'{ENVIRONMENT_NAME}-<microservice_name>'), 'host': os.getenv('PAPERTRAIL_HOST'), 'port': int(os.getenv('PAPERTRAIL_PORT')) }, } It would be necessary to put the microservice name instead of microservice_name for each microservice in the project","title":"Papertrail logging configuration"},{"location":"LOGGING_AND_MONITORING/#papertrail-searches-configuration","text":"The papertrail-provisioning tool is available in the repository to configure the different groups of searches in papertrail, you must follow the procedure explained in the README of the same for its use. The above mentioned guide should be followed to add log searches for a specific microservice, it is also possible to configure alarms that will send notifications to slack.","title":"Papertrail searches configuration"},{"location":"MONOREPO/","text":"Monorepo In revision control systems, a monorepo (syllabic abbreviation of monolithic repository) is a software development strategy where code for many projects are stored in the same repository. Wikipedia Advantages Simplified organization : The organization is simplified separating all the projects(called modules) in different folders that are stored in the root folder of the repository. Simplified automation : The automation gets easy with this approach, each time the repo has a commit in develop or master the automation will deploy all the necessary parts of the app to make it work correctly. Refactoring changes : When a project has a dependency with another in the monorepo, the changes are easier to be made. Atomic commits : When projects that work together are contained in separate repositories, releases need to determine which versions of one project are related to the other and then syncing them. Collaboration across team : The integration between projects will be easier thanks to the branches strategy Single source of truth : Like a developer you'll find all the available code, automation and documentation in the same place. What is not a monorepo about Monorepo is not the same that Monolith. Monolith is huge amount of coupled code of one application that is hell to maintain. This is not only a repository, it's a sum of good practices between automation, code and documentation. With passion from the Intelygenz Team @ 2020","title":"MONOREPO"},{"location":"MONOREPO/#monorepo","text":"In revision control systems, a monorepo (syllabic abbreviation of monolithic repository) is a software development strategy where code for many projects are stored in the same repository. Wikipedia","title":"Monorepo"},{"location":"MONOREPO/#advantages","text":"Simplified organization : The organization is simplified separating all the projects(called modules) in different folders that are stored in the root folder of the repository. Simplified automation : The automation gets easy with this approach, each time the repo has a commit in develop or master the automation will deploy all the necessary parts of the app to make it work correctly. Refactoring changes : When a project has a dependency with another in the monorepo, the changes are easier to be made. Atomic commits : When projects that work together are contained in separate repositories, releases need to determine which versions of one project are related to the other and then syncing them. Collaboration across team : The integration between projects will be easier thanks to the branches strategy Single source of truth : Like a developer you'll find all the available code, automation and documentation in the same place.","title":"Advantages"},{"location":"MONOREPO/#what-is-not-a-monorepo-about","text":"Monorepo is not the same that Monolith. Monolith is huge amount of coupled code of one application that is hell to maintain. This is not only a repository, it's a sum of good practices between automation, code and documentation. With passion from the Intelygenz Team @ 2020","title":"What is not a monorepo about"},{"location":"PIPELINES/","text":"Pipelines In this project is implemented Software delivery with total automation, thus avoiding manual intervention and therefore human errors in our product. Human error can and does occur when carrying out these boring and repetitive tasks manually and ultimately does affect the ability to meet deliverables. All of the automation is made with Gitlab CI technology, taking advantage of all the tools that Gitlab has. We separate the automatation in two parts, continuous integration and continuous delivery , that are explained in the next sections. To improve the speed and optimization of the pipelines, only the jobs and stages will be executed on those modules that change in each commit . Launch all jobs in a pipeline Exceptionally, it is possible to launch a pipeline with all the jobs and stages on a branch using the web interface, as shown in the following image . To do so, the following steps must be followed: From the project repository select the CI/CD option in the left sidebar and this Pipelines , as shown in the following image where these options are marked in red. Choose the Run Pipeline option, as shown in the image below in red. Indicate the branch where you want to run the pipeline in the Run for box and then click on Run pipeline . It's possible see an example in the following image, where the box Run for is shown in green and Run pipeline is shown in red. It is important to note that due to the extra time added by the tests of the dispacth-portal-frontend microservice, the tests of this one will only be executed when any of the files within it change or a pipeline with the Gitlab variable TEST_DISPATCH_PORTAL_FRONTEND with value true is executed. Environments Microservices Environments For the microservices there are the following environments Production : The environment is related to everything currently running in AWS related to the latest version of the master branch of the repository. Ephemerals : These environments are created from branches that start with name dev/feature or dev/fix . The name of any environment, regardless of the type, will identify all the resources created in the deployment process. The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. These names will identify all the resources created in AWS during the continuous delivery process, explained in the following sections. KRE Environments For KRE component there are the following environments: dev : This will be used for the various tests and calls made from the project's microservices in any ephemeral environment , ie from the microservices deployed in the ECS cluster with name automation-<environment_id> . production : This will be used for the different calls made from the project's microservices in the production environment , that is, from the microservices deployed in the ECS cluster with the name automation-master . Continuous integration (CI) Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day. Each integration can then be verified by an automated build and automated tests. While automated testing is not strictly part of CI it is typically implied. Codeship Validation steps This stage checks the following: All python microservices comply with the rules of PEP8 Terraform files used to configure the infrastructure are valid from a syntactic point of view. The frontend modules comply with the linter configured for them Unit tests steps All the available unit tests for each service should be run in this stage of the CI process. If the coverage obtained from these tests for a service is not greater than or equal to 80%, it will cause this phase to fail, this will mean that the steps of the next stage will not be executed and the process will fail. In cases in which a module does not reach the minimum coverage mentioned above, a message like the following will be seen in the step executed for that module. Continuous delivery (CD) Continuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements. Puppet.com Basic_infra steps This area covers the checking and creation, if necessary, of all the basic resources for the subsequent deployment, these being the specific image repositories in ECR Docker Container Registry , as well as the roles necessary in AWS to be able to display these images in ECS Container Orchestrator . In this stage there is also a job that must be executed manually if necessary, this is responsible for checking and creating if necessary network resources for the production environment or ephemeral environments. In this stage is also checked whether there are enough free resources in ECS to carry out the deployment with success or not. It's necessary run the basic-infra job the first time a new microservice is created in the project This has been done because ECR repositories are global resources and are stored in the same tfstate file, thus avoiding that when a microservice that creates a repository is created, it is not deleted by other branches that do not have it added. Basic_infra_kre steps In this stage will be the following jobs: * deploy-basic-infra-kre-dev for ephemeral environments and deploy-basic-infra-kre-production for the production environment, these are executed optionally manually . This job is reponsible of checking and creation, if necessary, of the EKS cluster used by KRE in each environment and all the necessary resources related (RBAC configuration, helm charts needed for the KRE runtimes, etc) The process followed in this job is as follows: The necessary infrastructure is created in AWS for KRE, creating for them the following components: An S3 bucket for each environment and save information about the cluster, such as the SSH key to connect to the nodes. An EKS cluster to be able to deploy the different KRE components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A SMTP service through Amazon SES and all the necessary componentes of it A set of IAM roles, one for each user with access to the project. These will be used to assign subsequent permissions in the Kubernetes cluster according to the role they belong to. These are stored as terraform output values , saving the list of user roles belonging to each role in their corresponding variable. Below is an example of a pipeline execution where it's possible see the IAM roles of users generated for each role in the project: sh Outputs: eks_developer_ops_privileged_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xisco.capllonch\", \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xoan.mallon.developer\", ] eks_developer_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-brandon.samudio\", \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-daniel.fernandez\", \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-joseluis.vega\", \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-marc.vivancos\", ] eks_devops_roles = [ \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-alberto.iglesias\", \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.costales\", \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.luis.piquero\", ] A set of helm charts necessary for any KRE runtime: external-dns , using the helm chart from bitnami repository external-dns is a Kubernetes addon that configures public DNS servers with information about exposed Kubernetes services to make them discoverable. It allows in a simple way that through the creation of an ingress in AWS you can create an entry in Route53 of type alias so that the calls to that ingress redirect to the value configured for the alias, being the most typical the DNS of the balancer created by the ingress. cert-mananger , using the helm chart from jetstack repository . This component automate the management lifecycle of all required certificates used by the KRE component in each environment. nginx ingress controller , using the helm chart from ingress-nginx repository . A series of configurations are provided so that the IP of clients in Kubernetes services can be known, since by default it will always use the internal IP in EKS of the load balancer for requests made from the Internet. A list of allowed IPs is also provided in the chart configuration through a specific configuration key, thus restricting access to the cluster's microservices. This component will create a Classic Load Balancer in AWS to expose nginx ingress component. hostpath provisioner , using the helm chart from rimusz repository Using a Python cli , permissions are assigned in Kubernetes Cluster created for KRE for each of the IAM roles created in the previous step. Deploy_kre_runtimes steps In this stage the KRE runtimes will be deployed in the corresponding environment, creating the necessary infrastructure and resources: A Hosted Zone in Route53 for the runtime in the specified environment using the mettel-automation.net domain name The kre helm chart with the necessary values for the environment creating a specific namespace in the EKS cluster for deploy the helm chart Build steps This area will cover all build steps of all necessary modules to deploy the app to the selected environment. It's typical to build the docker images and push to the repository in this step. Deploy steps In this stage there are one job: deploy-branches for ephemeral environments and deploy-master for the production environment, these are executed automatically . In which MetTel Automation modules in the monorepo will be deployed to the selected environment, as well as all the resources associated to that environment in AWS. The deploy steps will deploy the following in AWS: An ECS Cluster will be created for the environment with a set of resources An ECS Service that will use the new Docker image uploaded for each service of the project, being these services the specified below: bruin-bridge cts-bridge customer-cache dispatch-portal-backend dispatch-portal-frontend last-contact-report lit-bridge lumin-billing-report metrics-prometheus nats-server, nats-server-1, nats-server-2 notifier service-affecting-monitor service-dispatch-monitor service-outage-monitor-1, service-outage-monitor-2, service-outage-monitor-3, service-outage-monitor-4, service-outage-monitor-triage sites-monitor t7-bridge tnba-feedback tnba-monitor velocloud-bridge A Task Definition for each of the above ECS Services In this process, a series of resources will also be created in AWS for the selected environment, as follows: Three ElastiCache Redis Clusters , which are detailed below: <environment> : used to save some data about dispatches, as well as to skip the limitation of messages of more than 1MB when passing them to NATS. <environment>-customer-cache-redis : used to save the mappings between Bruin clients and Velocloud edges, being able to use them between restarts if any occurs. <environment>-tnba-feedback-cache-redis : used to save ticket metrics sent to T7, so tnba-feedback can avoid sending them again afterwards. An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A S3 bucket to store the content of the metrics obtained by Thanos and displayed through Grafana . Also, resources of type null_resource are created to execute some Python scripts: The creation of ECS Services starts only if a Python script launched as a null_resource finishes with success. This script checks that the last ECS service created for NATS is running in HEALTHY state. If the previous step succeeded then ECS services related to capabilities microservices are created, with these being the following: bruin-bridge cts-bridge lit-bridge notifier prometheus t7-bridge velocloud-bridge hawkeye-bridge Once created, the script used for NATS is launched through null_resource to check that the task instances for each of these ECS services were created successfully and are in RUNNING and HEALTHY status. Once all the scripts for the capabilities microservices have finished successfully, ECS services for the use-cases microservices are all created, with these being the following: customer-cache dispatch-portal-backend hawkeye-customer-cache hawkeye-outage-monitor last-contact-report lumin-billing-report service-affecting-monitor service-dispatch-monitor service-outage-monitor-1 service-outage-monitor-2 service-outage-monitor-3 service-outage-monitor-4 service-outage-monitor-triage sites-monitor tnba-feedback tnba-monitor This is achieved by defining explicit dependencies between the ECS services for the capabilities microservices and the set of null resources that perform the healthcheck of the capabilities microservices.\u200b The following is an example of a definition for the use-case microservice service-affecting-monitor using Terraform . Here, the dependency between the corresponding null_resource type resources in charge of performing the health check of the different capabilities microservices in Terraform code for this microservice is established. ```terraform resource \"aws_ecs_service\" \"automation-service-affecting-monitor\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck, null_resource.cts-bridge-healthcheck, null_resource.lit-bridge-healthcheck, null_resource.velocloud-bridge-healthcheck, null_resource.hawkeye-bridge-healthcheck, null_resource.t7-bridge-healthcheck, null_resource.notifier-healthcheck, null_resource.metrics-prometheus-healthcheck ] . . . } ``` This procedure has been done to ensure that use case microservices are not created in ECS until new versions of the capability-type microservices are properly deployed, as use case microservices need to use capability-type microservices. Following the same procedure as in the previous step, a dependency is established between the microservice dispatch-portal-frontend and dispatch-portal-backend . The reason for this is that the dispatch-portal-frontend microservice needs to know the corresponding IP with the DNS entry in Route53 for the dispatch-portal-backend microservice, since if the previous deployment is saved, the new IP corresponding to the DNS entry is not updated. The following is the configuration in the terraform code of the service in ECS for the dispatch-portal-frontend microservice, where the necessary configuration to comply with this restriction can be seen. ```terraform resource \"aws_ecs_service\" \"automation-dispatch-portal-frontend\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck, null_resource.cts-bridge-healthcheck, null_resource.lit-bridge-healthcheck, null_resource.velocloud-bridge-healthcheck, null_resource.hawkeye-bridge-healthcheck, null_resource.t7-bridge-healthcheck, null_resource.notifier-healthcheck, null_resource.metrics-prometheus-healthcheck, null_resource.dispatch-portal-backend-healthcheck, aws_lb.automation-alb ] } ``` The provisioning of the different groups and the searches included in each one of them is done through a python utility , this makes calls to the util go-papertrail-cli who is in charge of the provisioning of the elements mentioned in Papertrail . Destroy steps In this stage a series of manual jobs are available to destroy what was created in the previous stage, both for KRE and for the microservices of the repository in AWS. These are detailed below: destroy-branches for ephemeral environments or destroy-master for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-branches or deploy-master depending on the environment. destroy-branches-aws-nuke : This job is only available for ephemeral environments, it generates a yml file using a specific script to be used by aws-nuke to destroy all the infrastructure created for an ephemeral environment in AWS. This job should only be used when the `destroy-branches' job fails. - destroy-basic-infra-kre-dev for ephemeral environments or destroy-basic-infra-kre-production for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-kre-dev or deploy-kre-production depending on the environment. With passion from the Intelygenz Team @ 2020","title":"PIPELINES"},{"location":"PIPELINES/#pipelines","text":"In this project is implemented Software delivery with total automation, thus avoiding manual intervention and therefore human errors in our product. Human error can and does occur when carrying out these boring and repetitive tasks manually and ultimately does affect the ability to meet deliverables. All of the automation is made with Gitlab CI technology, taking advantage of all the tools that Gitlab has. We separate the automatation in two parts, continuous integration and continuous delivery , that are explained in the next sections. To improve the speed and optimization of the pipelines, only the jobs and stages will be executed on those modules that change in each commit .","title":"Pipelines"},{"location":"PIPELINES/#launch-all-jobs-in-a-pipeline","text":"Exceptionally, it is possible to launch a pipeline with all the jobs and stages on a branch using the web interface, as shown in the following image . To do so, the following steps must be followed: From the project repository select the CI/CD option in the left sidebar and this Pipelines , as shown in the following image where these options are marked in red. Choose the Run Pipeline option, as shown in the image below in red. Indicate the branch where you want to run the pipeline in the Run for box and then click on Run pipeline . It's possible see an example in the following image, where the box Run for is shown in green and Run pipeline is shown in red. It is important to note that due to the extra time added by the tests of the dispacth-portal-frontend microservice, the tests of this one will only be executed when any of the files within it change or a pipeline with the Gitlab variable TEST_DISPATCH_PORTAL_FRONTEND with value true is executed.","title":"Launch all jobs in a pipeline"},{"location":"PIPELINES/#environments","text":"","title":"Environments"},{"location":"PIPELINES/#microservices-environments","text":"For the microservices there are the following environments Production : The environment is related to everything currently running in AWS related to the latest version of the master branch of the repository. Ephemerals : These environments are created from branches that start with name dev/feature or dev/fix . The name of any environment, regardless of the type, will identify all the resources created in the deployment process. The names for environments are automation-master for production, as well as automation-<branch_identifier> for ephemeral environments, being branch_identifier the result of applying echo -n \"<branch_name>\" | sha256sum | cut -c1-8 on the branch name related to the ephemeral environment. These names will identify all the resources created in AWS during the continuous delivery process, explained in the following sections.","title":"Microservices Environments"},{"location":"PIPELINES/#kre-environments","text":"For KRE component there are the following environments: dev : This will be used for the various tests and calls made from the project's microservices in any ephemeral environment , ie from the microservices deployed in the ECS cluster with name automation-<environment_id> . production : This will be used for the different calls made from the project's microservices in the production environment , that is, from the microservices deployed in the ECS cluster with the name automation-master .","title":"KRE Environments"},{"location":"PIPELINES/#continuous-integration-ci","text":"Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day. Each integration can then be verified by an automated build and automated tests. While automated testing is not strictly part of CI it is typically implied. Codeship","title":"Continuous integration (CI)"},{"location":"PIPELINES/#validation-steps","text":"This stage checks the following: All python microservices comply with the rules of PEP8 Terraform files used to configure the infrastructure are valid from a syntactic point of view. The frontend modules comply with the linter configured for them","title":"Validation steps"},{"location":"PIPELINES/#unit-tests-steps","text":"All the available unit tests for each service should be run in this stage of the CI process. If the coverage obtained from these tests for a service is not greater than or equal to 80%, it will cause this phase to fail, this will mean that the steps of the next stage will not be executed and the process will fail. In cases in which a module does not reach the minimum coverage mentioned above, a message like the following will be seen in the step executed for that module.","title":"Unit tests steps"},{"location":"PIPELINES/#continuous-delivery-cd","text":"Continuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements. Puppet.com","title":"Continuous delivery (CD)"},{"location":"PIPELINES/#basic_infra-steps","text":"This area covers the checking and creation, if necessary, of all the basic resources for the subsequent deployment, these being the specific image repositories in ECR Docker Container Registry , as well as the roles necessary in AWS to be able to display these images in ECS Container Orchestrator . In this stage there is also a job that must be executed manually if necessary, this is responsible for checking and creating if necessary network resources for the production environment or ephemeral environments. In this stage is also checked whether there are enough free resources in ECS to carry out the deployment with success or not. It's necessary run the basic-infra job the first time a new microservice is created in the project This has been done because ECR repositories are global resources and are stored in the same tfstate file, thus avoiding that when a microservice that creates a repository is created, it is not deleted by other branches that do not have it added.","title":"Basic_infra steps"},{"location":"PIPELINES/#basic_infra_kre-steps","text":"In this stage will be the following jobs: * deploy-basic-infra-kre-dev for ephemeral environments and deploy-basic-infra-kre-production for the production environment, these are executed optionally manually . This job is reponsible of checking and creation, if necessary, of the EKS cluster used by KRE in each environment and all the necessary resources related (RBAC configuration, helm charts needed for the KRE runtimes, etc) The process followed in this job is as follows: The necessary infrastructure is created in AWS for KRE, creating for them the following components: An S3 bucket for each environment and save information about the cluster, such as the SSH key to connect to the nodes. An EKS cluster to be able to deploy the different KRE components designed for Kubernetes An AutoScaling Group to have the desired number of Kubernetes worker nodes A SMTP service through Amazon SES and all the necessary componentes of it A set of IAM roles, one for each user with access to the project. These will be used to assign subsequent permissions in the Kubernetes cluster according to the role they belong to. These are stored as terraform output values , saving the list of user roles belonging to each role in their corresponding variable. Below is an example of a pipeline execution where it's possible see the IAM roles of users generated for each role in the project: sh Outputs: eks_developer_ops_privileged_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xisco.capllonch\", \"arn:aws:iam::374050862540:role/eks-developer-ops-mettel-automation-kre-xoan.mallon.developer\", ] eks_developer_roles = [ \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-brandon.samudio\", \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-daniel.fernandez\", \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-joseluis.vega\", \"arn:aws:iam::374050862540:role/eks-developer-mettel-automation-kre-marc.vivancos\", ] eks_devops_roles = [ \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-alberto.iglesias\", \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.costales\", \"arn:aws:iam::374050862540:role/eks-devops-mettel-automation-kre-angel.luis.piquero\", ] A set of helm charts necessary for any KRE runtime: external-dns , using the helm chart from bitnami repository external-dns is a Kubernetes addon that configures public DNS servers with information about exposed Kubernetes services to make them discoverable. It allows in a simple way that through the creation of an ingress in AWS you can create an entry in Route53 of type alias so that the calls to that ingress redirect to the value configured for the alias, being the most typical the DNS of the balancer created by the ingress. cert-mananger , using the helm chart from jetstack repository . This component automate the management lifecycle of all required certificates used by the KRE component in each environment. nginx ingress controller , using the helm chart from ingress-nginx repository . A series of configurations are provided so that the IP of clients in Kubernetes services can be known, since by default it will always use the internal IP in EKS of the load balancer for requests made from the Internet. A list of allowed IPs is also provided in the chart configuration through a specific configuration key, thus restricting access to the cluster's microservices. This component will create a Classic Load Balancer in AWS to expose nginx ingress component. hostpath provisioner , using the helm chart from rimusz repository Using a Python cli , permissions are assigned in Kubernetes Cluster created for KRE for each of the IAM roles created in the previous step.","title":"Basic_infra_kre steps"},{"location":"PIPELINES/#deploy_kre_runtimes-steps","text":"In this stage the KRE runtimes will be deployed in the corresponding environment, creating the necessary infrastructure and resources: A Hosted Zone in Route53 for the runtime in the specified environment using the mettel-automation.net domain name The kre helm chart with the necessary values for the environment creating a specific namespace in the EKS cluster for deploy the helm chart","title":"Deploy_kre_runtimes steps"},{"location":"PIPELINES/#build-steps","text":"This area will cover all build steps of all necessary modules to deploy the app to the selected environment. It's typical to build the docker images and push to the repository in this step.","title":"Build steps"},{"location":"PIPELINES/#deploy-steps","text":"In this stage there are one job: deploy-branches for ephemeral environments and deploy-master for the production environment, these are executed automatically . In which MetTel Automation modules in the monorepo will be deployed to the selected environment, as well as all the resources associated to that environment in AWS. The deploy steps will deploy the following in AWS: An ECS Cluster will be created for the environment with a set of resources An ECS Service that will use the new Docker image uploaded for each service of the project, being these services the specified below: bruin-bridge cts-bridge customer-cache dispatch-portal-backend dispatch-portal-frontend last-contact-report lit-bridge lumin-billing-report metrics-prometheus nats-server, nats-server-1, nats-server-2 notifier service-affecting-monitor service-dispatch-monitor service-outage-monitor-1, service-outage-monitor-2, service-outage-monitor-3, service-outage-monitor-4, service-outage-monitor-triage sites-monitor t7-bridge tnba-feedback tnba-monitor velocloud-bridge A Task Definition for each of the above ECS Services In this process, a series of resources will also be created in AWS for the selected environment, as follows: Three ElastiCache Redis Clusters , which are detailed below: <environment> : used to save some data about dispatches, as well as to skip the limitation of messages of more than 1MB when passing them to NATS. <environment>-customer-cache-redis : used to save the mappings between Bruin clients and Velocloud edges, being able to use them between restarts if any occurs. <environment>-tnba-feedback-cache-redis : used to save ticket metrics sent to T7, so tnba-feedback can avoid sending them again afterwards. An ALB A record in Route53 A CloudWatch Log Group An Service Discovery Service for each ECS Service of the ECS Cluster created for this environment and a Service Discovery Namespace to logically group these Service Discovery Services . A set of resources related to the metrics of the environment: CloudWatch Alarms CloudWatch Dashboard CloudWatch Log Filters A CloudFormation Stack for create the SNS topic that will be used by CloudWatch Alarms notifications of this environment A S3 bucket to store the content of the metrics obtained by Thanos and displayed through Grafana . Also, resources of type null_resource are created to execute some Python scripts: The creation of ECS Services starts only if a Python script launched as a null_resource finishes with success. This script checks that the last ECS service created for NATS is running in HEALTHY state. If the previous step succeeded then ECS services related to capabilities microservices are created, with these being the following: bruin-bridge cts-bridge lit-bridge notifier prometheus t7-bridge velocloud-bridge hawkeye-bridge Once created, the script used for NATS is launched through null_resource to check that the task instances for each of these ECS services were created successfully and are in RUNNING and HEALTHY status. Once all the scripts for the capabilities microservices have finished successfully, ECS services for the use-cases microservices are all created, with these being the following: customer-cache dispatch-portal-backend hawkeye-customer-cache hawkeye-outage-monitor last-contact-report lumin-billing-report service-affecting-monitor service-dispatch-monitor service-outage-monitor-1 service-outage-monitor-2 service-outage-monitor-3 service-outage-monitor-4 service-outage-monitor-triage sites-monitor tnba-feedback tnba-monitor This is achieved by defining explicit dependencies between the ECS services for the capabilities microservices and the set of null resources that perform the healthcheck of the capabilities microservices.\u200b The following is an example of a definition for the use-case microservice service-affecting-monitor using Terraform . Here, the dependency between the corresponding null_resource type resources in charge of performing the health check of the different capabilities microservices in Terraform code for this microservice is established. ```terraform resource \"aws_ecs_service\" \"automation-service-affecting-monitor\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck, null_resource.cts-bridge-healthcheck, null_resource.lit-bridge-healthcheck, null_resource.velocloud-bridge-healthcheck, null_resource.hawkeye-bridge-healthcheck, null_resource.t7-bridge-healthcheck, null_resource.notifier-healthcheck, null_resource.metrics-prometheus-healthcheck ] . . . } ``` This procedure has been done to ensure that use case microservices are not created in ECS until new versions of the capability-type microservices are properly deployed, as use case microservices need to use capability-type microservices. Following the same procedure as in the previous step, a dependency is established between the microservice dispatch-portal-frontend and dispatch-portal-backend . The reason for this is that the dispatch-portal-frontend microservice needs to know the corresponding IP with the DNS entry in Route53 for the dispatch-portal-backend microservice, since if the previous deployment is saved, the new IP corresponding to the DNS entry is not updated. The following is the configuration in the terraform code of the service in ECS for the dispatch-portal-frontend microservice, where the necessary configuration to comply with this restriction can be seen. ```terraform resource \"aws_ecs_service\" \"automation-dispatch-portal-frontend\" { . . . depends_on = [ null_resource.bruin-bridge-healthcheck, null_resource.cts-bridge-healthcheck, null_resource.lit-bridge-healthcheck, null_resource.velocloud-bridge-healthcheck, null_resource.hawkeye-bridge-healthcheck, null_resource.t7-bridge-healthcheck, null_resource.notifier-healthcheck, null_resource.metrics-prometheus-healthcheck, null_resource.dispatch-portal-backend-healthcheck, aws_lb.automation-alb ] } ``` The provisioning of the different groups and the searches included in each one of them is done through a python utility , this makes calls to the util go-papertrail-cli who is in charge of the provisioning of the elements mentioned in Papertrail .","title":"Deploy steps"},{"location":"PIPELINES/#destroy-steps","text":"In this stage a series of manual jobs are available to destroy what was created in the previous stage, both for KRE and for the microservices of the repository in AWS. These are detailed below: destroy-branches for ephemeral environments or destroy-master for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-branches or deploy-master depending on the environment. destroy-branches-aws-nuke : This job is only available for ephemeral environments, it generates a yml file using a specific script to be used by aws-nuke to destroy all the infrastructure created for an ephemeral environment in AWS. This job should only be used when the `destroy-branches' job fails.","title":"Destroy steps"},{"location":"PIPELINES/#-destroy-basic-infra-kre-dev-for-ephemeral-environments-or-destroy-basic-infra-kre-production-for-the-production-environment-this-job-will-destroy-everything-created-by-terraform-in-the-previous-stage-by-the-job-deploy-kre-dev-or-deploy-kre-production-depending-on-the-environment","text":"With passion from the Intelygenz Team @ 2020","title":"- destroy-basic-infra-kre-dev for ephemeral environments or destroy-basic-infra-kre-production for the production environment: This job will destroy everything created by Terraform in the previous stage by the job deploy-kre-dev or deploy-kre-production depending on the environment."},{"location":"README_AUTOMATION/","text":"Documentation directory System overview System architecture Monorepo Pipelines Launch all jobs in a pipeline Environments CI CD Infrastructure as code Infrastructure Environment infrastructure Network infrastructure Logging and monitoring Cloudwatch Cloudwatch Log Groups Cloudwatch Log Streams Cloudwatch Retention Periord Cloudwatch logs retrieval tool Papertrail Papertrail Dashboards Papertrail Logging Configuration Papertrail Searches Configuration With passion from the Intelygenz Team @ 2021","title":"README AUTOMATION"},{"location":"README_AUTOMATION/#documentation-directory","text":"System overview System architecture Monorepo Pipelines Launch all jobs in a pipeline Environments CI CD Infrastructure as code Infrastructure Environment infrastructure Network infrastructure Logging and monitoring Cloudwatch Cloudwatch Log Groups Cloudwatch Log Streams Cloudwatch Retention Periord Cloudwatch logs retrieval tool Papertrail Papertrail Dashboards Papertrail Logging Configuration Papertrail Searches Configuration With passion from the Intelygenz Team @ 2021","title":"Documentation directory"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/","text":"Context Configurations are stored in AWS Systems Manager Parameter Store . Parameter Store is a capability of AWS Systems Manager that allows storing configuration values in hierarchical structures. It also provides the ability to encrypt these values through AWS KMS keys, and allows keeping track of configuration changes / versions for auditing purposes. From a pricing perspective, parameters can be defined either as simple or advanced : - Simple . This kind of parameters come with no additional charges, and can hold up to 4KB of data. Parameter Store allows defining up to 10,000 simple parameters per account and region. - Advanced . AWS charges $0.05 per advanced parameter per month. These can hold up to 8KB of data, and Parameter Store allows defining up to 100,000 of them per account and region. Attending to their type, parameters can be defined as: * String . These parameters consist of a block of text. Nothing else. * StringList . These parameters hold comma-separated values, but bear in mind that this is just a convention. AWS will not turn the value into an array before returning it through Parameter Store API; that is, it will be kept as a string. * SecureString . Essentially, these are the same as String parameters, but empowered with encryption features thanks to KMS keys. Conventions Please, take some time to go through the following conventions before publishing new parameters to Parameter Store. Parameter names Any new configuration published to Parameter Store must match the following pattern: /automation-engine/<environment>/<service-name>/<parameter> where: * environment refers to the Kubernetes context whose namespaces will take this parameter. Allowed values are dev , pro and common . * service-name refers to the name of the service this parameter will be loaded to. Examples of allowed values would be bruin-bridge , tnba-monitor , repair-tickets-monitor , and so on. * parameter refers to the name of the parameter that will be loaded to the service through an environment variable. Most configurations will follow the previous pattern, but if the business logic of a service carries out several tasks related to the same domain concept, an alternate form can be used: /automation-engine/<environment>/<domain>/<task>/<parameter> where: * domain refers to a domain concept that can represent multiple tasks accurately. An example would be service-affecting , which is a domain area that represents strongly related tasks (in this case, tasks related to Service Affecting troubles). * task refers to the underlying piece of logic that is independent of other tasks, but is still suitable to have under the subpath defined by domain . Considering that domain is set to service-affecting , acceptable values for task would be monitor , daily-bandwidth-report , or reoccurring-trouble-report . As a rule of thumb, choose the first pattern to name parameters if possible. However, if there is a strong reason to choose the second pattern over the first one, that is totally fine. If there are doubts about choosing one or the other, it can be brought up to discussion with the rest of the team. Values Before getting into how to define new configurations, bear these considerations in mind to keep certain consistency and avoid confusion / potential errors: * Values representing a time duration must always be expressed in seconds. * 24 should not be used to represent the hours in a period of 1 day. * 86400 should be used to represent the hours in a period of 1 day. Values representing percentages must always be expressed in their integer form. 0.75 should not be used to represent the value 75% . 75 should be used to represent the value 75% . JSON-like values must adhere to the JSON specification . The most relevant considerations are: All keys in an object-like JSON must be strings, even if they represent numbers. Example: ```json // NOT valid { 1: \"foo\", 2: \"bar\" } // Valid { \"1\": \"foo\", \"2\": \"bar\" } // Valid { \"foo\": \"bar\", \"baz\": \"hey\" } ``` 2. An array-like JSON can hold a combination of values with different types, be them integers, floats, boolean, arrays, objects, and so on. Example: json [ 1, \"foo\", 0.75, \"2\", \"bar\", true, [ \"hello\", \"world\" ], { \"hello\": \"world\" } ] Consider prettifying JSON-like configurations before publishing to Parameter Store. This enhances readability when dealing with huge JSONs. Developers are responsible for making any necessary transformations related to data types, time conversions, etc. in the config files of a particular service . Encryption A parameter must be encrypted if it holds: * Personally Identifiable Information. This includes e-mail addresses, names, surnames, phone numbers, and so on. * Authentication credentials of any kind. * URLs from third party services. * Information related to application domains, such as: * IDs (even incremental ones) * Organization names * Domain-related values that potentially expose internal details about third party systems Publishing configurations to Parameter Store Adding new configurations to Parameter Store is pretty straightforward. The process should be as easy as: 1. Access the AWS Management Console. 2. Go to the AWS Systems Manager Parameter Store dashboard. 3. Hit Create parameter . 4. Give the parameter a Name that complies with any of the patterns under the Parameter names section. 5. Make sure to add a meaningful Description to the parameter. This is extremely important to give context to anyone in need of making changes to parameters, so take some time to think about a good, meaningful description. 6. Choose the tier that fits better for this parameter: 1. If the parameter is small and is not expected to grow much as time passes, choose Standard . 2. On the other hand, if the parameter is large enough and is expected to grow, choose Advanced . 7. Choose the type that fits better for this parameter: 1. If the parameter is safe to stay unencrypted in AWS: 1. Choose String . 2. Set the Data Type field to text . 2. On the other hand, if the parameter needs to be encrypted (see the Encryption section): 1. Choose SecureString . 2. Set the KMS Key Source field to My current account to pick a KMS key registered in the current account. 3. Set the KMS Key ID field to alias/aws/ssm to pick the default KMS key for Parameter Store. In general, avoid using StringList parameters. These are special cases of the String type, which essentially means that String can be used to create parameters based on comma-separated values as well. 8. Give the parameter a value that adheres to the conventions specified in the Values section. 9. Finally, hit Create parameter to save it. About the different environments When creating a new parameter in the Parameter Store, please decide if it will be different on dev and pro or if it will be same in both environments. If they're different, you should create a parameter for each environment. Otherwise, just create one under common . In general, most parameters will share the same value, unless there is a strong reason to keep them different. For example, parameters used to point to third party systems may differ if their app has not only a Production system, but also a Development / Test one. In that case, the pro version of the parameter should aim at the third party's Production system, and the dev version should aim at their Development / Test system. Hooking AWS Parameter Store with Kubernetes clusters Pre-requisites Before moving on, make sure to install k9s in your system. k9s is a powerful terminal UI that lets users manage Kubernetes clusters from their own machines, and even edit Kubernetes objects in place to reflect those changes immediately. After installing k9s , follow these steps to install a plugin that allows editing decoded Secret objects (more in the next section): 1. Install krew , following the instructions for the desired OS. 2. Install kubectl 's plugin kubectl-modify-secret , following the instructions for the desired OS. 3. Integrate kubectl-modify-secret into k9s 's plugin system: 1. In a terminal, run k9s info to check which folder holds k9s configurations. 2. Create file plugin.yml under that folder, if it does not exist yet. 3. Add the following snippet: yaml plugin: edit-secret: shortCut: Ctrl-X confirm: false description: \"Edit Decoded Secret\" scopes: - secrets command: kubectl background: false args: - modify-secret - --namespace - $NAMESPACE - --context - $CONTEXT - $NAME > By default, the shortcut to edit decoded secrets is set to Ctrl + X . If needed, set a custom one instead. Kubernetes: Secrets and ConfigMaps Secret and ConfigMap objects are the Kubernetes objects used to inject environment variables to one or multiple pods. Like other Kubernetes objects, they are defined through .yaml files. These objects hold mappings between environment variables and their values, along with some metadata. Here is an example of a ConfigMap definition: apiVersion: v1 data: CURRENT_ENVIRONMENT: production ENVIRONMENT_NAME: production REDIS_HOSTNAME: redis.pro.somewhere.com kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: automation-engine meta.helm.sh/release-namespace: automation-engine reloader.stakater.com/match: \"true\" creationTimestamp: \"2021-08-30T15:08:12Z\" labels: app.kubernetes.io/instance: automation-engine app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: some-fancy-bridge app.kubernetes.io/version: 1.16.0 component: some-fancy-bridge current-environment: production environment-name: production helm.sh/chart: some-fancy-bridge-0.1.0 microservice-type: case-of-use project: mettel-automation name: some-fancy-bridge-configmap namespace: automation-engine resourceVersion: \"83171937\" selfLink: /api/v1/namespaces/automation-engine/configmaps/some-fancy-bridge-configmap uid: ba625451-8ba4-4ac7-a8da-593cc938eae7 And here is an example of a Secret definition: apiVersion: v1 data: THIRD_PARTY_API_USERNAME: aGVsbG8K THIRD_PARTY_API_PASSWORD: d29ybGQK kind: Secret metadata: annotations: meta.helm.sh/release-name: automation-engine meta.helm.sh/release-namespace: automation-engine reconcile.external-secrets.io/data-hash: 3162fd065a6777587e9a3a604e0c56e2 reloader.stakater.com/match: \"true\" creationTimestamp: \"2022-03-08T18:31:36Z\" labels: app.kubernetes.io/instance: automation-engine app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: some-fancy-bridge app.kubernetes.io/version: 1.16.0 component: some-fancy-bridge current-environment: production environment-name: production helm.sh/chart: some-fancy-bridge-0.1.0 microservice-type: capability project: mettel-automation name: some-fancy-bridge-secret namespace: automation-engine ownerReferences: - apiVersion: external-secrets.io/v1alpha1 blockOwnerDeletion: true controller: true kind: ExternalSecret name: some-fancy-bridge-secret uid: 42bdad1c-37c4-4890-b86b-d9623333df18 resourceVersion: \"82588568\" selfLink: /api/v1/namespaces/automation-engine/secrets/some-fancy-bridge-secret uid: 81ad3356-8696-406e-bba0-c4ec389676ee type: Opaque Both objects have a data field where the different configurations are stored. The main difference between both objects is that all fields under data remain clear in ConfigMap objects, but in Secret ones, these fields are base64-encoded. These objects lack of mechanisms to pull configurations from external sources, so an additional tool is needed to hook them with AWS Parameter Store. External secrets The (External Secrets Operator)[https://github.com/external-secrets/external-secrets] is a tool that allows setting up Secret objects based on external references through a new kind of object called ExternalSecret . ExternalSecret objects define the external source to pull configurations from, and the references that should be resolved. After these references have been resolved, the ExternalSecret will create a regular Secret object with a data section whose key-value pairs are based on the environment variables the pod expects to see, and the values gotten after resolving the references from the external source. Aside from that, ExternalSecret objects pull configuration values from the external source periodically to keep secrets up to date, also known as reconciling secrets . An example of ExternalSecret object would be this one: apiVersion: external-secrets.io/v1alpha1 kind: ExternalSecret metadata: annotations: meta.helm.sh/release-name: automation-engine meta.helm.sh/release-namespace: automation-engine reloader.stakater.com/match: \"true\" creationTimestamp: \"2022-03-08T18:11:02Z\" generation: 1 labels: app.kubernetes.io/instance: automation-engine app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: some-fancy-bridge app.kubernetes.io/version: 1.16.0 component: some-fancy-bridge current-environment: production environment-name: production helm.sh/chart: some-fancy-bridge-0.1.0 microservice-type: capability project: mettel-automation name: some-fancy-bridge-secret namespace: automation-engine resourceVersion: \"83201847\" selfLink: /apis/external-secrets.io/v1alpha1/namespaces/automation-engine/externalsecrets/some-fancy-bridge-secret uid: ca5c1faf-1b76-4f59-b57f-e10e43154ace spec: data: - remoteRef: key: /automation-engine/pro/some-fancy-bridge/some-common-value secretKey: SOME_COMMON_VALUE - remoteRef: key: /automation-engine/pro/some-fancy-bridge/some-env-specific-value secretKey: SOME_ENV_SPECIFIC_VALUE status: conditions: - lastTransitionTime: \"2022-03-08T18:11:10Z\" message: Secret was synced reason: SecretSynced status: \"True\" type: Ready refreshTime: \"2022-03-10T13:29:12Z\" syncedResourceVersion: 1-3efb62db37d8b935be922ecc6f7ed99f In this example, there are two items under the data section. Each item refers to an external / remote reference (field remoteRef::key ), and the environment variable that the value behind that remote reference should be loaded to (field secretKey ). Manipulating external secrets in the Automation system To add, remove, or update external secrets for a particular service in the Automation system, head to helm/charts/automation-engine/<service>/templates/external-secret.yaml and make the appropriate changes under the data section. For example, consider the following external-secret.yaml : {{- if and .Values.global.externalSecrets.enabled -}} apiVersion: external-secrets.io/v1alpha1 kind: ExternalSecret metadata: name: {{ include \"some-fancy-bridge.secretName\" . }} labels: {{- include \"some-fancy-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" spec: secretStoreRef: name: {{ .Values.global.environment }}-parameter-store kind: SecretStore target: creationPolicy: 'Owner' Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration) May be set to zero to fetch and create it {{- if eq .Values.global.current_environment \"dev\" }} refreshInterval: \"0\" {{ else }} refreshInterval: \"5m\" {{- end }} data: {{- with .Values.global.externalSecrets.envPath }} - remoteRef: key: {{ .commonPath }}/some-fancy-bridge/some-common-value secretKey: SOME_COMMON_VALUE - remoteRef: key: {{ .envPath }}/some-fancy-bridge/some-env-specific-value secretKey: SOME_ENV_SPECIFIC_VALUE {{- end }} {{- end }} If a new configuration called THIRD_PARTY_API_URL must be added to the underlying Secret created by this ExternalSecret , a new item should be placed under the data section, and it should look like this: - remoteRef: key: {{ . }}/some-fancy-bridge/third-party-api-url secretKey: THIRD_PARTY_API_URL The change can then be deployed to the Kubernetes cluster. A word of caution about ephemeral environments Although external-secrets is part of the EKS cluster for production and the cluster for ephemeral environments, the truth is it behaves differently across contexts. In the production cluster, ExternalSecret objects are configured to pull configurations from AWS Parameter Store every 5 minutes . That way, if a developer adds, removes, or updated a parameter in AWS, the cluster will realize about that event to update the regular Secret object created by the ExternalSecret with the most recent value. This will trigger another piece in the cluster called reloader , which ultimately will kill any pod that relies on the updated secret to spin up a new one with the configurations in the updated Secret . In ephemeral environments however, this is completely different. The ExternalSecret object will create a Secret object only when the ephemeral environment is deployed for the first time. After it has been deployed, control and management of Secret objects is delegated to developers; that is, external-secrets will never pull parameters from AWS again. The reasoning behind this behavior is that these parameters are shared by all ephemeral environments, so if one of them were to be updated in AWS and the polling rate was set to 5 minutes , all ephemeral environments would be updated because they all share the same reference. So essentially, the set of configurations for ephemeral environments that are stored in AWS are used as a template to populate ExternalSecret and Secret objects in ephemeral environments. If a secret needs to be updated, k9s should be used to edit the Secret in place. Using an AWS param key in our services Add the new variable to automation-engine/installation-utils/environment_files_generator.py python SERVICE__DOMAIN__PARAM_KEY_NAME = parameters['environment']['service']['domain']['param-key-name'] And inside the env dictionary in the same file python f'DOMAIN__PARAM_KEY_NAME={SERVICE__DOMAIN__PARAM_KEY_NAME}', Add the new variable as a template to services/<service>/src/config/.template.env python DOMAIN__PARAM_KEY_NAME= Add the new variable retrieving the value from the environment to services/<service>/src/config/config.py python 'param_key_name': ( os.environ['DOMAIN__PARAM_KEY_NAME'] ), Any parameter that should be used with a primitive type other than str , should be cast here. We MUST use primitive types. DO NOT convert values to complex types like timedelta or datetime . Consumers of the service configuration should be responsible for doing that. Add the new variable to services/<service>/src/config/testconfig.py for being available when testing python 'param_key_name': 86400 After that you can use it in the service python self._config.MONITOR_CONFIG['param_key_name'] And while testing python param_key_name = action._config.MONITOR_CONFIG['param_key_name']","title":"SYSTEM CONFIGURATIONS THROUGH AWS PARAM KEYS"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#context","text":"Configurations are stored in AWS Systems Manager Parameter Store . Parameter Store is a capability of AWS Systems Manager that allows storing configuration values in hierarchical structures. It also provides the ability to encrypt these values through AWS KMS keys, and allows keeping track of configuration changes / versions for auditing purposes. From a pricing perspective, parameters can be defined either as simple or advanced : - Simple . This kind of parameters come with no additional charges, and can hold up to 4KB of data. Parameter Store allows defining up to 10,000 simple parameters per account and region. - Advanced . AWS charges $0.05 per advanced parameter per month. These can hold up to 8KB of data, and Parameter Store allows defining up to 100,000 of them per account and region. Attending to their type, parameters can be defined as: * String . These parameters consist of a block of text. Nothing else. * StringList . These parameters hold comma-separated values, but bear in mind that this is just a convention. AWS will not turn the value into an array before returning it through Parameter Store API; that is, it will be kept as a string. * SecureString . Essentially, these are the same as String parameters, but empowered with encryption features thanks to KMS keys.","title":"Context"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#conventions","text":"Please, take some time to go through the following conventions before publishing new parameters to Parameter Store.","title":"Conventions"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#parameter-names","text":"Any new configuration published to Parameter Store must match the following pattern: /automation-engine/<environment>/<service-name>/<parameter> where: * environment refers to the Kubernetes context whose namespaces will take this parameter. Allowed values are dev , pro and common . * service-name refers to the name of the service this parameter will be loaded to. Examples of allowed values would be bruin-bridge , tnba-monitor , repair-tickets-monitor , and so on. * parameter refers to the name of the parameter that will be loaded to the service through an environment variable. Most configurations will follow the previous pattern, but if the business logic of a service carries out several tasks related to the same domain concept, an alternate form can be used: /automation-engine/<environment>/<domain>/<task>/<parameter> where: * domain refers to a domain concept that can represent multiple tasks accurately. An example would be service-affecting , which is a domain area that represents strongly related tasks (in this case, tasks related to Service Affecting troubles). * task refers to the underlying piece of logic that is independent of other tasks, but is still suitable to have under the subpath defined by domain . Considering that domain is set to service-affecting , acceptable values for task would be monitor , daily-bandwidth-report , or reoccurring-trouble-report . As a rule of thumb, choose the first pattern to name parameters if possible. However, if there is a strong reason to choose the second pattern over the first one, that is totally fine. If there are doubts about choosing one or the other, it can be brought up to discussion with the rest of the team.","title":"Parameter names"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#values","text":"Before getting into how to define new configurations, bear these considerations in mind to keep certain consistency and avoid confusion / potential errors: * Values representing a time duration must always be expressed in seconds. * 24 should not be used to represent the hours in a period of 1 day. * 86400 should be used to represent the hours in a period of 1 day. Values representing percentages must always be expressed in their integer form. 0.75 should not be used to represent the value 75% . 75 should be used to represent the value 75% . JSON-like values must adhere to the JSON specification . The most relevant considerations are: All keys in an object-like JSON must be strings, even if they represent numbers. Example: ```json // NOT valid { 1: \"foo\", 2: \"bar\" } // Valid { \"1\": \"foo\", \"2\": \"bar\" } // Valid { \"foo\": \"bar\", \"baz\": \"hey\" } ``` 2. An array-like JSON can hold a combination of values with different types, be them integers, floats, boolean, arrays, objects, and so on. Example: json [ 1, \"foo\", 0.75, \"2\", \"bar\", true, [ \"hello\", \"world\" ], { \"hello\": \"world\" } ] Consider prettifying JSON-like configurations before publishing to Parameter Store. This enhances readability when dealing with huge JSONs. Developers are responsible for making any necessary transformations related to data types, time conversions, etc. in the config files of a particular service .","title":"Values"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#encryption","text":"A parameter must be encrypted if it holds: * Personally Identifiable Information. This includes e-mail addresses, names, surnames, phone numbers, and so on. * Authentication credentials of any kind. * URLs from third party services. * Information related to application domains, such as: * IDs (even incremental ones) * Organization names * Domain-related values that potentially expose internal details about third party systems","title":"Encryption"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#publishing-configurations-to-parameter-store","text":"Adding new configurations to Parameter Store is pretty straightforward. The process should be as easy as: 1. Access the AWS Management Console. 2. Go to the AWS Systems Manager Parameter Store dashboard. 3. Hit Create parameter . 4. Give the parameter a Name that complies with any of the patterns under the Parameter names section. 5. Make sure to add a meaningful Description to the parameter. This is extremely important to give context to anyone in need of making changes to parameters, so take some time to think about a good, meaningful description. 6. Choose the tier that fits better for this parameter: 1. If the parameter is small and is not expected to grow much as time passes, choose Standard . 2. On the other hand, if the parameter is large enough and is expected to grow, choose Advanced . 7. Choose the type that fits better for this parameter: 1. If the parameter is safe to stay unencrypted in AWS: 1. Choose String . 2. Set the Data Type field to text . 2. On the other hand, if the parameter needs to be encrypted (see the Encryption section): 1. Choose SecureString . 2. Set the KMS Key Source field to My current account to pick a KMS key registered in the current account. 3. Set the KMS Key ID field to alias/aws/ssm to pick the default KMS key for Parameter Store. In general, avoid using StringList parameters. These are special cases of the String type, which essentially means that String can be used to create parameters based on comma-separated values as well. 8. Give the parameter a value that adheres to the conventions specified in the Values section. 9. Finally, hit Create parameter to save it.","title":"Publishing configurations to Parameter Store"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#about-the-different-environments","text":"When creating a new parameter in the Parameter Store, please decide if it will be different on dev and pro or if it will be same in both environments. If they're different, you should create a parameter for each environment. Otherwise, just create one under common . In general, most parameters will share the same value, unless there is a strong reason to keep them different. For example, parameters used to point to third party systems may differ if their app has not only a Production system, but also a Development / Test one. In that case, the pro version of the parameter should aim at the third party's Production system, and the dev version should aim at their Development / Test system.","title":"About the different environments"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#hooking-aws-parameter-store-with-kubernetes-clusters","text":"","title":"Hooking AWS Parameter Store with Kubernetes clusters"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#pre-requisites","text":"Before moving on, make sure to install k9s in your system. k9s is a powerful terminal UI that lets users manage Kubernetes clusters from their own machines, and even edit Kubernetes objects in place to reflect those changes immediately. After installing k9s , follow these steps to install a plugin that allows editing decoded Secret objects (more in the next section): 1. Install krew , following the instructions for the desired OS. 2. Install kubectl 's plugin kubectl-modify-secret , following the instructions for the desired OS. 3. Integrate kubectl-modify-secret into k9s 's plugin system: 1. In a terminal, run k9s info to check which folder holds k9s configurations. 2. Create file plugin.yml under that folder, if it does not exist yet. 3. Add the following snippet: yaml plugin: edit-secret: shortCut: Ctrl-X confirm: false description: \"Edit Decoded Secret\" scopes: - secrets command: kubectl background: false args: - modify-secret - --namespace - $NAMESPACE - --context - $CONTEXT - $NAME > By default, the shortcut to edit decoded secrets is set to Ctrl + X . If needed, set a custom one instead.","title":"Pre-requisites"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#kubernetes-secrets-and-configmaps","text":"Secret and ConfigMap objects are the Kubernetes objects used to inject environment variables to one or multiple pods. Like other Kubernetes objects, they are defined through .yaml files. These objects hold mappings between environment variables and their values, along with some metadata. Here is an example of a ConfigMap definition: apiVersion: v1 data: CURRENT_ENVIRONMENT: production ENVIRONMENT_NAME: production REDIS_HOSTNAME: redis.pro.somewhere.com kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: automation-engine meta.helm.sh/release-namespace: automation-engine reloader.stakater.com/match: \"true\" creationTimestamp: \"2021-08-30T15:08:12Z\" labels: app.kubernetes.io/instance: automation-engine app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: some-fancy-bridge app.kubernetes.io/version: 1.16.0 component: some-fancy-bridge current-environment: production environment-name: production helm.sh/chart: some-fancy-bridge-0.1.0 microservice-type: case-of-use project: mettel-automation name: some-fancy-bridge-configmap namespace: automation-engine resourceVersion: \"83171937\" selfLink: /api/v1/namespaces/automation-engine/configmaps/some-fancy-bridge-configmap uid: ba625451-8ba4-4ac7-a8da-593cc938eae7 And here is an example of a Secret definition: apiVersion: v1 data: THIRD_PARTY_API_USERNAME: aGVsbG8K THIRD_PARTY_API_PASSWORD: d29ybGQK kind: Secret metadata: annotations: meta.helm.sh/release-name: automation-engine meta.helm.sh/release-namespace: automation-engine reconcile.external-secrets.io/data-hash: 3162fd065a6777587e9a3a604e0c56e2 reloader.stakater.com/match: \"true\" creationTimestamp: \"2022-03-08T18:31:36Z\" labels: app.kubernetes.io/instance: automation-engine app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: some-fancy-bridge app.kubernetes.io/version: 1.16.0 component: some-fancy-bridge current-environment: production environment-name: production helm.sh/chart: some-fancy-bridge-0.1.0 microservice-type: capability project: mettel-automation name: some-fancy-bridge-secret namespace: automation-engine ownerReferences: - apiVersion: external-secrets.io/v1alpha1 blockOwnerDeletion: true controller: true kind: ExternalSecret name: some-fancy-bridge-secret uid: 42bdad1c-37c4-4890-b86b-d9623333df18 resourceVersion: \"82588568\" selfLink: /api/v1/namespaces/automation-engine/secrets/some-fancy-bridge-secret uid: 81ad3356-8696-406e-bba0-c4ec389676ee type: Opaque Both objects have a data field where the different configurations are stored. The main difference between both objects is that all fields under data remain clear in ConfigMap objects, but in Secret ones, these fields are base64-encoded. These objects lack of mechanisms to pull configurations from external sources, so an additional tool is needed to hook them with AWS Parameter Store.","title":"Kubernetes: Secrets and ConfigMaps"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#external-secrets","text":"The (External Secrets Operator)[https://github.com/external-secrets/external-secrets] is a tool that allows setting up Secret objects based on external references through a new kind of object called ExternalSecret . ExternalSecret objects define the external source to pull configurations from, and the references that should be resolved. After these references have been resolved, the ExternalSecret will create a regular Secret object with a data section whose key-value pairs are based on the environment variables the pod expects to see, and the values gotten after resolving the references from the external source. Aside from that, ExternalSecret objects pull configuration values from the external source periodically to keep secrets up to date, also known as reconciling secrets . An example of ExternalSecret object would be this one: apiVersion: external-secrets.io/v1alpha1 kind: ExternalSecret metadata: annotations: meta.helm.sh/release-name: automation-engine meta.helm.sh/release-namespace: automation-engine reloader.stakater.com/match: \"true\" creationTimestamp: \"2022-03-08T18:11:02Z\" generation: 1 labels: app.kubernetes.io/instance: automation-engine app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: some-fancy-bridge app.kubernetes.io/version: 1.16.0 component: some-fancy-bridge current-environment: production environment-name: production helm.sh/chart: some-fancy-bridge-0.1.0 microservice-type: capability project: mettel-automation name: some-fancy-bridge-secret namespace: automation-engine resourceVersion: \"83201847\" selfLink: /apis/external-secrets.io/v1alpha1/namespaces/automation-engine/externalsecrets/some-fancy-bridge-secret uid: ca5c1faf-1b76-4f59-b57f-e10e43154ace spec: data: - remoteRef: key: /automation-engine/pro/some-fancy-bridge/some-common-value secretKey: SOME_COMMON_VALUE - remoteRef: key: /automation-engine/pro/some-fancy-bridge/some-env-specific-value secretKey: SOME_ENV_SPECIFIC_VALUE status: conditions: - lastTransitionTime: \"2022-03-08T18:11:10Z\" message: Secret was synced reason: SecretSynced status: \"True\" type: Ready refreshTime: \"2022-03-10T13:29:12Z\" syncedResourceVersion: 1-3efb62db37d8b935be922ecc6f7ed99f In this example, there are two items under the data section. Each item refers to an external / remote reference (field remoteRef::key ), and the environment variable that the value behind that remote reference should be loaded to (field secretKey ).","title":"External secrets"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#manipulating-external-secrets-in-the-automation-system","text":"To add, remove, or update external secrets for a particular service in the Automation system, head to helm/charts/automation-engine/<service>/templates/external-secret.yaml and make the appropriate changes under the data section. For example, consider the following external-secret.yaml : {{- if and .Values.global.externalSecrets.enabled -}} apiVersion: external-secrets.io/v1alpha1 kind: ExternalSecret metadata: name: {{ include \"some-fancy-bridge.secretName\" . }} labels: {{- include \"some-fancy-bridge.labels\" . | nindent 4 }} annotations: reloader.stakater.com/match: \"true\" spec: secretStoreRef: name: {{ .Values.global.environment }}-parameter-store kind: SecretStore target: creationPolicy: 'Owner' Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\" (from time.ParseDuration) May be set to zero to fetch and create it {{- if eq .Values.global.current_environment \"dev\" }} refreshInterval: \"0\" {{ else }} refreshInterval: \"5m\" {{- end }} data: {{- with .Values.global.externalSecrets.envPath }} - remoteRef: key: {{ .commonPath }}/some-fancy-bridge/some-common-value secretKey: SOME_COMMON_VALUE - remoteRef: key: {{ .envPath }}/some-fancy-bridge/some-env-specific-value secretKey: SOME_ENV_SPECIFIC_VALUE {{- end }} {{- end }} If a new configuration called THIRD_PARTY_API_URL must be added to the underlying Secret created by this ExternalSecret , a new item should be placed under the data section, and it should look like this: - remoteRef: key: {{ . }}/some-fancy-bridge/third-party-api-url secretKey: THIRD_PARTY_API_URL The change can then be deployed to the Kubernetes cluster.","title":"Manipulating external secrets in the Automation system"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#a-word-of-caution-about-ephemeral-environments","text":"Although external-secrets is part of the EKS cluster for production and the cluster for ephemeral environments, the truth is it behaves differently across contexts. In the production cluster, ExternalSecret objects are configured to pull configurations from AWS Parameter Store every 5 minutes . That way, if a developer adds, removes, or updated a parameter in AWS, the cluster will realize about that event to update the regular Secret object created by the ExternalSecret with the most recent value. This will trigger another piece in the cluster called reloader , which ultimately will kill any pod that relies on the updated secret to spin up a new one with the configurations in the updated Secret . In ephemeral environments however, this is completely different. The ExternalSecret object will create a Secret object only when the ephemeral environment is deployed for the first time. After it has been deployed, control and management of Secret objects is delegated to developers; that is, external-secrets will never pull parameters from AWS again. The reasoning behind this behavior is that these parameters are shared by all ephemeral environments, so if one of them were to be updated in AWS and the polling rate was set to 5 minutes , all ephemeral environments would be updated because they all share the same reference. So essentially, the set of configurations for ephemeral environments that are stored in AWS are used as a template to populate ExternalSecret and Secret objects in ephemeral environments. If a secret needs to be updated, k9s should be used to edit the Secret in place.","title":"A word of caution about ephemeral environments"},{"location":"SYSTEM_CONFIGURATIONS_THROUGH_AWS_PARAM_KEYS/#using-an-aws-param-key-in-our-services","text":"Add the new variable to automation-engine/installation-utils/environment_files_generator.py python SERVICE__DOMAIN__PARAM_KEY_NAME = parameters['environment']['service']['domain']['param-key-name'] And inside the env dictionary in the same file python f'DOMAIN__PARAM_KEY_NAME={SERVICE__DOMAIN__PARAM_KEY_NAME}', Add the new variable as a template to services/<service>/src/config/.template.env python DOMAIN__PARAM_KEY_NAME= Add the new variable retrieving the value from the environment to services/<service>/src/config/config.py python 'param_key_name': ( os.environ['DOMAIN__PARAM_KEY_NAME'] ), Any parameter that should be used with a primitive type other than str , should be cast here. We MUST use primitive types. DO NOT convert values to complex types like timedelta or datetime . Consumers of the service configuration should be responsible for doing that. Add the new variable to services/<service>/src/config/testconfig.py for being available when testing python 'param_key_name': 86400 After that you can use it in the service python self._config.MONITOR_CONFIG['param_key_name'] And while testing python param_key_name = action._config.MONITOR_CONFIG['param_key_name']","title":"Using an AWS param key in our services"},{"location":"SYSTEM_OVERVIEW/","text":"System overview Architecture Basic concepts It's a project based on microservices, in which two types are distinguished: Capabilites : They are in charge of carrying out certain common actions for the business logic. I.e.: Collect information from SD-WAN routers For example: Collect information from SD-WAN routers. Use cases : They use capabilities as a base to make specific use cases. I.e.: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for the subsequent storage in the corresponding tickets. For example: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for subsequent storage in the corresponding tickets. It is important to emphasize on the architecture of the system the use of NATS in it. Messaging system It's important to emphasize in the architecture of the system the use of NATS in it. NATS is a simple, secure and performant communications system for digital system, services and devices NATS is used in the microservices system as a communication center for all of them. NATS it's used in cluster mode to safistify more work to be done by it due to the high number of events in the system to be processed Microservices communications There are two types of microservices depending on the connection between them and NATS: Microservices that communicates with NATS , divided into three types: Those that take the role of replier in the context of NATS, usually microservices that contains capabilities : bruin-bridge cts-bridge hawkeye-bridge lit-bridge notifier t7-bridge Those that take the role of requester in the context of NATS, usually microservices that contains use cases : dispatch-portal-backend grafana component, from metrics-prometheus microservice hawkeye-affecting-monitor hawkeye-outage-monitor last-contact-report service-affecting-monitor service-dispatch-monitor service-outage-monitor sites-monitor tnba-feedback tnba-monitor Those that take the role of both requester and replier in the context of NATS. These microservices can be considered a mixture between use use cases and capabilities : customer-cache hawkeye-customer-cache It's important take into account that all microservices that communicate with NATS can also communicate with the Redis Cluster. This is needed to bypass the limit size that NATS enforces for all messages it receives (1MB). Microservices that doesn't communicate with NATS : dispatch-portal-frontend lumin-billing-report prometheus and thanos components, from metrics-prometheus microservice redis cluster (Docker container in local environment or an Elasticache Redis Cluster in AWS environments) NATS is used in the microservice system as a communication center for all of them. It is used in cluster mode to satisfy more work to be done by it. In the following diagram it's possible see a graph with the relationships between the microservices explained previously in this section Relationships between microservices The services that are part of the previously explained architecture are related to each other, in the following diagram it's possible see the relationships between them. Capabilities microservices Bruin-bridge microservice This microservice is in charge of making requests to the bruin API, taking the role of replier in the context of NATS. When another microservice requests bruin data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. Bruin is a third-party system that allows creating and managing support tickets to deal with issues that appear in network devices, among other types of devices. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Cts-bridge microservice This microservice is in charge of making requests to the CTS API, taking the role of replier in the context of NATS. When another microservice requests CTS data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Digi-bridge microservice This microservice is in charge of making requests to the Digi Reboot API, taking the role of replier in the context of NATS. When another microservice asks to reboot a SD-WAN device, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Hawkeye-bridge microservice This microservice is in charge of making requests to the Hawkeye API, taking the role of replier in the context of NATS. When another microservice requests Hawkeye data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Lit-bridge microservice This microservice is in charge of making requests to the LIT API, taking the role of replier in the context of NATS. When another microservice requests LIT data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Notifier microservice This microservice is in charge of sending emails, Slack notifications and SMS. It is important to point out that it is not in charge of the composition of the messages to be sent, that is to say, of their content, but only of sending them. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. T7-bridge microservice The function of this microservice is to embed in the notes of a ticket the prediction calculated by T7, this prediction will store information on the recommendations actions for the ticket. In order to carry out the mentioned actions, it communicates with the API of T7 to obtain the information about the prediction, as it can be seen in the following diagram . Velocloud-bridge microservice This microservice is in charge of making requests to the velocloud API, taking the role of replier in the context of NATS. When another microservice requests velocloud data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above. Use cases microservices Dispatch-portal-backend microservice In conjunction with dispatch-portal-frontend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It also updates Bruin tickets to keep support people posted about the changes in the dispatch requests. It acts as an intermediary between dispatch-portal-frontend and CTS & LIT APIs by providing a REST API with multiple endpoints that, once they receive a payload from the frontent side, it modifies its fields with the help of some mappers to match the formats expected by CTS and LIT and then forward those customized payloads to their APIs. The following diagram shows the dependencies or interactions of this microservice with the others. Grafana microservice Although Grafana is a visualization tool for metrics, it needs to fetch some data from VeloCloud API to build dashboards for customer Titan America. The following diagram shows the dependencies or interactions of this microservice with the others. Hawkeye-outage-monitor microservice This service is responsible for resolving/unresolving outage tickets depending on the state of a Hawkeye device. It is triggered every 3 minutes. If a device is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the device is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the device was detected to be healthy, the system looks for an open outage ticket for this device and resolves it in case it exists. In the following diagram it's possible see the relationship of this microservice with the others. Last-contact-report microservice The function to be performed by this microservice is to send a monthly report with information about routers that were last contacted more than 30 days ago. The following flow is used to make this report: The last-contact-report microservice communicates with the velocloud-bridge microservice to obtain events from an edge. Once the events are obtained from an edge, it communicates with the notifier microservice to send an email with this information. It is possible to see the relations between the mentioned services for the flow in the following diagram . Service-affecting-monitor microservice In this microservice are defined a series of scales and thresholds, the function of this will be to check if there is loss of packages, latencies or jitter measurements that exceed the thresholds defined. In case the thresholds are exceeded, it will communicate with the notifier service to send a notification by email and slack, by means of which it will warn of the problems detected on a specific edge. This microservice also communicates with the bruin-bridge microservice to create tickets or add notes to an existing one, including in this information about the routers for which a problem is detected. In the following diagram it's possible see the relationships between this microservice and the others. Service-dispatch-monitor microservice This microservice monitor dispatches statuses for different vendors, at the time of writting this document LIT and CTS. Both processes are pretty much the same in concept but with differences in the implementation. A dispatch is general terms can have the following statuses: Requested Confirmed Tech on site Canceled Completed The main use is to monitor: Dispatch status changed Updates in the dispatch like the technician Send sms prior 2 and 12 hours before Send sms tech on site Cancel dispatch The basic algorithm behaves like this: Get all dispatches for a vendor Filter dispatches that are created through the dispatch-portal Discard invalid ticket ids or dispatches with not proper fields Split the dispatches by status and then send them to the function to proccess them, there are 3 general functions Confirmed dispatch: Send sms and append note to bruin when a dispatch is confirmed Send sms and append note to bruin 12 or 2 hours prior the dispatch Send sms and append note to bruin when a tech has changed Tech on site dispatch: Send sms and append note to bruin when tech on site Canceled dispatch: Append note to bruin when a dispatch is canceled Each vendor has it's own details like how to retrieve some fields or how we identify the tickets with the dispatches, all explained in the service-dispatch-monitor . In the following diagram it's possible see the relationships between this microservice and the others. Service-outage-monitor microservice This microservice orchestrates the execution of two different processes: Outage monitoring. This process is responsible for resolving/unresolving outage tickets depending on the state of an edge. It is triggered every 3 minutes. If an edge is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the edge is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the edge was detected to be healthy, the system looks for an open outage ticket for this edge and resolves it in case it exists. Triage. This process is aimed at updating Bruin tickets with information related to recent edge events. It is triggered every 10 minutes. At the beginning, the process gathers all the open tickets related with the companies that are under triage monitoring. Tickets not related with edges belonging to these companies are discarded before going on. The process starts dealing with every ticket in the set collected in the previous step: * If the outage ticket does not have any triage note from a previous execution of the triage process then a triage note is appended with information of the events related to the edge corresponding to this ticket. Events correspond to the period between 7 days ago and the current moment. If the current environment is DEV instead of PRODUCTION then no note is appended to the ticket; instead, a notification with a summary of the triage results is delivered to a Slack channel. If the outage ticket already has a triage note from a previous execution then the process attempts to append new triage notes to the ticket but only if the last triage note was not appended recently (30 minutes or less ago). In case there's no recent triage note, edge events from the period between the creation date of the last triage note and the current moment are claimed to Velocloud and then they are included in the triage notes, which are finally appended to the ticket. Note that due to Bruin limitations it is not feasible to have a triage note with 1500 characters or more; that is the reason why several triage notes are appended to the ticket (instead of just appending one). In the following diagram it's possible see the relationship of this microservice with the others. Sites-monitor microservice This microservice requests data from the velocloud API via the velocloud-bridge microservice, using this information to enrich Prometheus. The prometheus data serves as a feed for Grafana. The following diagram shows the relationship between this microservice and the others. TNBA-feedback microservice This microservice is in charge of collecting closed tickets that had a TNBA note appended by tnba-monitor at some point. After collecting them, they are sent to t7-bridge to retrain predictive models and hence improve the accuracy of predictions claimed by tnba-monitor . The following diagram shows the relationship between this microservice and the others. TNBA-monitor microservice This microservice is in charge of appending notes to Bruin tickets indicating what is T he N ext B est A ction a member of the support team of Bruin can take to move forward on the resolution of the ticket. It mostly communicates with bruin-bridge and t7-bridge to embed predictions into tickets, but it also communicates with other capabilities as shown in the following diagram . The following diagram shows the relationship between this microservice and the others. Special microservices (NATS Requester and Replier) Customer-cache microservice This microservice is in charge of crossing Bruin and Velocloud data. More specifically, it focus on associating Bruin customers with Velocloud edges. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of customer-cache , it plays the role of a requester as it asks for data to Velocloud and Bruin to cross it. Hawkeye-customer-cache microservice This microservice is in charge of crossing Bruin and Hawkeye data. More specifically, it focus on associating Bruin customers with Hawkeye devices. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of hawkeye-customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of hawkeye-customer-cache , it plays the role of a requester as it asks for data to Hawkeye and Bruin to cross it. Microservices that don't communicate with NATS dispatch-portal-frontend In conjunction with dispatch-portal-backend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It exposes a UI that communicates directly with a REST API in dispatch-portal-backend to handle the visualization, creation and update of dispatch requests. The following diagram shows the relationship between this microservice and dispatch-portal-backend . lumin-billing-report This service automates requesting billing information for a given customer from the Lumin.AI service provider, generating a summary HTML email and attaching a csv with all data for the current billing period. This service is self-contained, i.e., it does not require access to NATS or Redis, or any other microservice within the Automation Engine. The following diagram shows the relationship between this service and the third-party services it uses. Prometheus & Thanos The purpose of Prometheus is to scrape metrics from HTTP servers placed in those services with the ability to write metrics, nothing else. Thanos is just another component that adds a layer of persistence to Prometheus, thus allowing to save metrics before they are lost when a service is re-deployed. These metrics can be restored after the deployment completes. Metrics are usually displayed in a Grafana instance with a few custom dashboards. The following diagram shows the relationship between Prometheus, the metrics servers it scrapes metrics, and Grafana. Redis Redis is an in-memory key-value store that, in this system, is used mostly for caching purposes, and also as a temporary storage for messages larger than 1 MB, which NATS cannot handle by itself. There are three Redis instances: * redis . Used to store NATS messages larger than 1 MB temporarily. All microservices that communicate with NATS in some way have the ability to store and retrieve messages from this Redis instance. redis-customer-cache . Used to turn customer-cache and hawkeye-customer-cache into fault-tolerant services, so if any of them fail caches will still be available to serve as soon as they come back. redis-tnba-feedback . Used to collect huge amounts of Bruin tickets' task histories before they are sent to T7 by the tnba-feedback service. Technologies and tools Code repository Intelygenz's Gitlab is used to store the project's code Gitlab CI is used as the CI/CD tool for the project Containerization The following containerization tools are used: Docker is used to create o container of this type by microservice > In the folder of each microservice there is a Dockerfile that allows to execute that microservice as a container Docker-compose is used for defining and running project microservices as a multi-container Docker application: > At the root of the repository there is a docker-compose.yml file that allows to run one or more microservices as docker containers Infrastructure Microservices Infrastructure For the microservices ECS is used to deploy a container for each microservice for all environments deployed, as each one has its own repository in the ECR registry used in the project. In the following diagram it's possible see how the microservices of the project are deployed, using the different images available in the registry created for the project in ECR. KRE Infrastructure In this project KRE is used, it has been deployed in an Kubernetes cluster using EKS for each of the necessary environments , as well as all the parts needed for this in AWS. In the following diagram it's possible see how is configured the KRE infrastructure in the project. Network infrastructure For the infrastructure of the network resources there is a distinction according to the microservice environments and also the kre-environmetns to deploy belongs to dev or production . In the following diagram it's possible see the infrastructure relative to the existing network resources in AWS created for the two type of environments. When deploying an environment it will use the resources belonging to the environment type. This approach has been implemented so that regardless of the number of ECS clusters being used, the same public IPs are always used to make requests outward from the different environments. KRE's clusters will also use the VPCs corresponding to each environment, i.e., dev or production . With passion from the Intelygenz Team @ 2020","title":"System overview"},{"location":"SYSTEM_OVERVIEW/#system-overview","text":"","title":"System overview"},{"location":"SYSTEM_OVERVIEW/#architecture","text":"","title":"Architecture"},{"location":"SYSTEM_OVERVIEW/#basic-concepts","text":"It's a project based on microservices, in which two types are distinguished: Capabilites : They are in charge of carrying out certain common actions for the business logic. I.e.: Collect information from SD-WAN routers For example: Collect information from SD-WAN routers. Use cases : They use capabilities as a base to make specific use cases. I.e.: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for the subsequent storage in the corresponding tickets. For example: Obtain certain tickets from SD-WAN routers of a company, obtaining the information from the routers for subsequent storage in the corresponding tickets. It is important to emphasize on the architecture of the system the use of NATS in it.","title":"Basic concepts"},{"location":"SYSTEM_OVERVIEW/#messaging-system","text":"It's important to emphasize in the architecture of the system the use of NATS in it. NATS is a simple, secure and performant communications system for digital system, services and devices NATS is used in the microservices system as a communication center for all of them. NATS it's used in cluster mode to safistify more work to be done by it due to the high number of events in the system to be processed","title":"Messaging system"},{"location":"SYSTEM_OVERVIEW/#microservices-communications","text":"There are two types of microservices depending on the connection between them and NATS: Microservices that communicates with NATS , divided into three types: Those that take the role of replier in the context of NATS, usually microservices that contains capabilities : bruin-bridge cts-bridge hawkeye-bridge lit-bridge notifier t7-bridge Those that take the role of requester in the context of NATS, usually microservices that contains use cases : dispatch-portal-backend grafana component, from metrics-prometheus microservice hawkeye-affecting-monitor hawkeye-outage-monitor last-contact-report service-affecting-monitor service-dispatch-monitor service-outage-monitor sites-monitor tnba-feedback tnba-monitor Those that take the role of both requester and replier in the context of NATS. These microservices can be considered a mixture between use use cases and capabilities : customer-cache hawkeye-customer-cache It's important take into account that all microservices that communicate with NATS can also communicate with the Redis Cluster. This is needed to bypass the limit size that NATS enforces for all messages it receives (1MB). Microservices that doesn't communicate with NATS : dispatch-portal-frontend lumin-billing-report prometheus and thanos components, from metrics-prometheus microservice redis cluster (Docker container in local environment or an Elasticache Redis Cluster in AWS environments) NATS is used in the microservice system as a communication center for all of them. It is used in cluster mode to satisfy more work to be done by it. In the following diagram it's possible see a graph with the relationships between the microservices explained previously in this section","title":"Microservices communications"},{"location":"SYSTEM_OVERVIEW/#relationships-between-microservices","text":"The services that are part of the previously explained architecture are related to each other, in the following diagram it's possible see the relationships between them.","title":"Relationships between microservices"},{"location":"SYSTEM_OVERVIEW/#capabilities-microservices","text":"","title":"Capabilities microservices"},{"location":"SYSTEM_OVERVIEW/#bruin-bridge-microservice","text":"This microservice is in charge of making requests to the bruin API, taking the role of replier in the context of NATS. When another microservice requests bruin data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. Bruin is a third-party system that allows creating and managing support tickets to deal with issues that appear in network devices, among other types of devices. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Bruin-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#cts-bridge-microservice","text":"This microservice is in charge of making requests to the CTS API, taking the role of replier in the context of NATS. When another microservice requests CTS data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Cts-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#digi-bridge-microservice","text":"This microservice is in charge of making requests to the Digi Reboot API, taking the role of replier in the context of NATS. When another microservice asks to reboot a SD-WAN device, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Digi-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-bridge-microservice","text":"This microservice is in charge of making requests to the Hawkeye API, taking the role of replier in the context of NATS. When another microservice requests Hawkeye data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Hawkeye-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#lit-bridge-microservice","text":"This microservice is in charge of making requests to the LIT API, taking the role of replier in the context of NATS. When another microservice requests LIT data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Lit-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#notifier-microservice","text":"This microservice is in charge of sending emails, Slack notifications and SMS. It is important to point out that it is not in charge of the composition of the messages to be sent, that is to say, of their content, but only of sending them. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Notifier microservice"},{"location":"SYSTEM_OVERVIEW/#t7-bridge-microservice","text":"The function of this microservice is to embed in the notes of a ticket the prediction calculated by T7, this prediction will store information on the recommendations actions for the ticket. In order to carry out the mentioned actions, it communicates with the API of T7 to obtain the information about the prediction, as it can be seen in the following diagram .","title":"T7-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#velocloud-bridge-microservice","text":"This microservice is in charge of making requests to the velocloud API, taking the role of replier in the context of NATS. When another microservice requests velocloud data, it will be in charge of making response messages to the same and never of request, that is to say, it will always be a producer within a NATS topic and never a consumer. The following diagram shows the dependencies or interactions of this microservice with the others, being in this case none, since it is in charge of one of the isolated microservices as explained above.","title":"Velocloud-bridge microservice"},{"location":"SYSTEM_OVERVIEW/#use-cases-microservices","text":"","title":"Use cases microservices"},{"location":"SYSTEM_OVERVIEW/#dispatch-portal-backend-microservice","text":"In conjunction with dispatch-portal-frontend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It also updates Bruin tickets to keep support people posted about the changes in the dispatch requests. It acts as an intermediary between dispatch-portal-frontend and CTS & LIT APIs by providing a REST API with multiple endpoints that, once they receive a payload from the frontent side, it modifies its fields with the help of some mappers to match the formats expected by CTS and LIT and then forward those customized payloads to their APIs. The following diagram shows the dependencies or interactions of this microservice with the others.","title":"Dispatch-portal-backend microservice"},{"location":"SYSTEM_OVERVIEW/#grafana-microservice","text":"Although Grafana is a visualization tool for metrics, it needs to fetch some data from VeloCloud API to build dashboards for customer Titan America. The following diagram shows the dependencies or interactions of this microservice with the others.","title":"Grafana microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-outage-monitor-microservice","text":"This service is responsible for resolving/unresolving outage tickets depending on the state of a Hawkeye device. It is triggered every 3 minutes. If a device is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the device is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the device was detected to be healthy, the system looks for an open outage ticket for this device and resolves it in case it exists. In the following diagram it's possible see the relationship of this microservice with the others.","title":"Hawkeye-outage-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#last-contact-report-microservice","text":"The function to be performed by this microservice is to send a monthly report with information about routers that were last contacted more than 30 days ago. The following flow is used to make this report: The last-contact-report microservice communicates with the velocloud-bridge microservice to obtain events from an edge. Once the events are obtained from an edge, it communicates with the notifier microservice to send an email with this information. It is possible to see the relations between the mentioned services for the flow in the following diagram .","title":"Last-contact-report microservice"},{"location":"SYSTEM_OVERVIEW/#service-affecting-monitor-microservice","text":"In this microservice are defined a series of scales and thresholds, the function of this will be to check if there is loss of packages, latencies or jitter measurements that exceed the thresholds defined. In case the thresholds are exceeded, it will communicate with the notifier service to send a notification by email and slack, by means of which it will warn of the problems detected on a specific edge. This microservice also communicates with the bruin-bridge microservice to create tickets or add notes to an existing one, including in this information about the routers for which a problem is detected. In the following diagram it's possible see the relationships between this microservice and the others.","title":"Service-affecting-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#service-dispatch-monitor-microservice","text":"This microservice monitor dispatches statuses for different vendors, at the time of writting this document LIT and CTS. Both processes are pretty much the same in concept but with differences in the implementation. A dispatch is general terms can have the following statuses: Requested Confirmed Tech on site Canceled Completed The main use is to monitor: Dispatch status changed Updates in the dispatch like the technician Send sms prior 2 and 12 hours before Send sms tech on site Cancel dispatch The basic algorithm behaves like this: Get all dispatches for a vendor Filter dispatches that are created through the dispatch-portal Discard invalid ticket ids or dispatches with not proper fields Split the dispatches by status and then send them to the function to proccess them, there are 3 general functions Confirmed dispatch: Send sms and append note to bruin when a dispatch is confirmed Send sms and append note to bruin 12 or 2 hours prior the dispatch Send sms and append note to bruin when a tech has changed Tech on site dispatch: Send sms and append note to bruin when tech on site Canceled dispatch: Append note to bruin when a dispatch is canceled Each vendor has it's own details like how to retrieve some fields or how we identify the tickets with the dispatches, all explained in the service-dispatch-monitor . In the following diagram it's possible see the relationships between this microservice and the others.","title":"Service-dispatch-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#service-outage-monitor-microservice","text":"This microservice orchestrates the execution of two different processes: Outage monitoring. This process is responsible for resolving/unresolving outage tickets depending on the state of an edge. It is triggered every 3 minutes. If an edge is detected to be in outage state then it is scheduled for a recheck in the next 5 seconds. If the edge is still in outage state, the system will try creating a new outage ticket. If Bruin reports back that an outage ticket with Resolved status exists already then it is unresolved; if not, a new outage ticket may have been created or an outage ticket with In Progress status may exist already, so no additional action will be taken. In case the edge was detected to be healthy, the system looks for an open outage ticket for this edge and resolves it in case it exists. Triage. This process is aimed at updating Bruin tickets with information related to recent edge events. It is triggered every 10 minutes. At the beginning, the process gathers all the open tickets related with the companies that are under triage monitoring. Tickets not related with edges belonging to these companies are discarded before going on. The process starts dealing with every ticket in the set collected in the previous step: * If the outage ticket does not have any triage note from a previous execution of the triage process then a triage note is appended with information of the events related to the edge corresponding to this ticket. Events correspond to the period between 7 days ago and the current moment. If the current environment is DEV instead of PRODUCTION then no note is appended to the ticket; instead, a notification with a summary of the triage results is delivered to a Slack channel. If the outage ticket already has a triage note from a previous execution then the process attempts to append new triage notes to the ticket but only if the last triage note was not appended recently (30 minutes or less ago). In case there's no recent triage note, edge events from the period between the creation date of the last triage note and the current moment are claimed to Velocloud and then they are included in the triage notes, which are finally appended to the ticket. Note that due to Bruin limitations it is not feasible to have a triage note with 1500 characters or more; that is the reason why several triage notes are appended to the ticket (instead of just appending one). In the following diagram it's possible see the relationship of this microservice with the others.","title":"Service-outage-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#sites-monitor-microservice","text":"This microservice requests data from the velocloud API via the velocloud-bridge microservice, using this information to enrich Prometheus. The prometheus data serves as a feed for Grafana. The following diagram shows the relationship between this microservice and the others.","title":"Sites-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#tnba-feedback-microservice","text":"This microservice is in charge of collecting closed tickets that had a TNBA note appended by tnba-monitor at some point. After collecting them, they are sent to t7-bridge to retrain predictive models and hence improve the accuracy of predictions claimed by tnba-monitor . The following diagram shows the relationship between this microservice and the others.","title":"TNBA-feedback microservice"},{"location":"SYSTEM_OVERVIEW/#tnba-monitor-microservice","text":"This microservice is in charge of appending notes to Bruin tickets indicating what is T he N ext B est A ction a member of the support team of Bruin can take to move forward on the resolution of the ticket. It mostly communicates with bruin-bridge and t7-bridge to embed predictions into tickets, but it also communicates with other capabilities as shown in the following diagram . The following diagram shows the relationship between this microservice and the others.","title":"TNBA-monitor microservice"},{"location":"SYSTEM_OVERVIEW/#special-microservices-nats-requester-and-replier","text":"","title":"Special microservices (NATS Requester and Replier)"},{"location":"SYSTEM_OVERVIEW/#customer-cache-microservice","text":"This microservice is in charge of crossing Bruin and Velocloud data. More specifically, it focus on associating Bruin customers with Velocloud edges. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of customer-cache , it plays the role of a requester as it asks for data to Velocloud and Bruin to cross it.","title":"Customer-cache microservice"},{"location":"SYSTEM_OVERVIEW/#hawkeye-customer-cache-microservice","text":"This microservice is in charge of crossing Bruin and Hawkeye data. More specifically, it focus on associating Bruin customers with Hawkeye devices. On the other hand, it also serves this information to the rest of services. This service is a special one, since it acts as a requester (to build and store caches) but also as a replier (to serve caches to services requesting them). The following diagram shows the dependencies or interactions of this microservice with the rest. From the point of view of services to the left of hawkeye-customer-cache , it plays the role of a replier as it answers to requests sent by them. From the point of view of services to the right of hawkeye-customer-cache , it plays the role of a requester as it asks for data to Hawkeye and Bruin to cross it.","title":"Hawkeye-customer-cache microservice"},{"location":"SYSTEM_OVERVIEW/#microservices-that-dont-communicate-with-nats","text":"","title":"Microservices that don't communicate with NATS"},{"location":"SYSTEM_OVERVIEW/#dispatch-portal-frontend","text":"In conjunction with dispatch-portal-backend , this service provides MetTel the ability to track the status of dispatch requests, as well as create and update them, so their technicians can assist customers when they report any issue related to a device. It exposes a UI that communicates directly with a REST API in dispatch-portal-backend to handle the visualization, creation and update of dispatch requests. The following diagram shows the relationship between this microservice and dispatch-portal-backend .","title":"dispatch-portal-frontend"},{"location":"SYSTEM_OVERVIEW/#lumin-billing-report","text":"This service automates requesting billing information for a given customer from the Lumin.AI service provider, generating a summary HTML email and attaching a csv with all data for the current billing period. This service is self-contained, i.e., it does not require access to NATS or Redis, or any other microservice within the Automation Engine. The following diagram shows the relationship between this service and the third-party services it uses.","title":"lumin-billing-report"},{"location":"SYSTEM_OVERVIEW/#prometheus-thanos","text":"The purpose of Prometheus is to scrape metrics from HTTP servers placed in those services with the ability to write metrics, nothing else. Thanos is just another component that adds a layer of persistence to Prometheus, thus allowing to save metrics before they are lost when a service is re-deployed. These metrics can be restored after the deployment completes. Metrics are usually displayed in a Grafana instance with a few custom dashboards. The following diagram shows the relationship between Prometheus, the metrics servers it scrapes metrics, and Grafana.","title":"Prometheus &amp; Thanos"},{"location":"SYSTEM_OVERVIEW/#redis","text":"Redis is an in-memory key-value store that, in this system, is used mostly for caching purposes, and also as a temporary storage for messages larger than 1 MB, which NATS cannot handle by itself. There are three Redis instances: * redis . Used to store NATS messages larger than 1 MB temporarily. All microservices that communicate with NATS in some way have the ability to store and retrieve messages from this Redis instance. redis-customer-cache . Used to turn customer-cache and hawkeye-customer-cache into fault-tolerant services, so if any of them fail caches will still be available to serve as soon as they come back. redis-tnba-feedback . Used to collect huge amounts of Bruin tickets' task histories before they are sent to T7 by the tnba-feedback service.","title":"Redis"},{"location":"SYSTEM_OVERVIEW/#technologies-and-tools","text":"","title":"Technologies and tools"},{"location":"SYSTEM_OVERVIEW/#code-repository","text":"Intelygenz's Gitlab is used to store the project's code Gitlab CI is used as the CI/CD tool for the project","title":"Code repository"},{"location":"SYSTEM_OVERVIEW/#containerization","text":"The following containerization tools are used: Docker is used to create o container of this type by microservice > In the folder of each microservice there is a Dockerfile that allows to execute that microservice as a container Docker-compose is used for defining and running project microservices as a multi-container Docker application: > At the root of the repository there is a docker-compose.yml file that allows to run one or more microservices as docker containers","title":"Containerization"},{"location":"SYSTEM_OVERVIEW/#infrastructure","text":"","title":"Infrastructure"},{"location":"SYSTEM_OVERVIEW/#microservices-infrastructure","text":"For the microservices ECS is used to deploy a container for each microservice for all environments deployed, as each one has its own repository in the ECR registry used in the project. In the following diagram it's possible see how the microservices of the project are deployed, using the different images available in the registry created for the project in ECR.","title":"Microservices Infrastructure"},{"location":"SYSTEM_OVERVIEW/#kre-infrastructure","text":"In this project KRE is used, it has been deployed in an Kubernetes cluster using EKS for each of the necessary environments , as well as all the parts needed for this in AWS. In the following diagram it's possible see how is configured the KRE infrastructure in the project.","title":"KRE Infrastructure"},{"location":"SYSTEM_OVERVIEW/#network-infrastructure","text":"For the infrastructure of the network resources there is a distinction according to the microservice environments and also the kre-environmetns to deploy belongs to dev or production . In the following diagram it's possible see the infrastructure relative to the existing network resources in AWS created for the two type of environments. When deploying an environment it will use the resources belonging to the environment type. This approach has been implemented so that regardless of the number of ECS clusters being used, the same public IPs are always used to make requests outward from the different environments. KRE's clusters will also use the VPCs corresponding to each environment, i.e., dev or production . With passion from the Intelygenz Team @ 2020","title":"Network infrastructure"},{"location":"WELCOME_PACK/","text":"Welcome Pack Team Composition Julia - Manager Dani - Tech Lead \u00c1ngel - Devops Brandon - Developer David - Developer Sergio - Developer Javier - Developer First steps Please request access to all the things listed below: Project repository https://gitlab.intelygenz.com/mettel/automation-engine/ , docker repository https://gitlab.intelygenz.com/mettel/docker_images/-/tree/master and One password to itcrowd@intelygenz.com throught their ticketing system https://docs.google.com/document/d/1YLYdI9Dyq8tNlNy2iJ29InquKDz8r_Dw4XBsxI7pPiM/edit and CC your manager to allow the request Configure vpn https://docs.google.com/document/d/16_LFpkiBWN0mbfjAoqR4BaEB5kPNsuNHrUS7PtrWnEA/edit#heading=h.to49i8wu1vn3 AWS account creation to our devops Jira board to our manager Mettel's Slack channels to our tech lead Project overview Resources Check docs folder inside the mettel gitlab project and read carefully the readme for installing all the required programs and configure the project. After RRHH's on-boarding our team lead will give an overview of the project https://docs.google.com/presentation/d/1Y18vXn6lsSp-6pJVsB5swWg7UcTDWYrIYtXq8_brRMk/edit#slide=id.g6183830b53_0_5 Project guidelines This project uses Black and isort. You just need to install pip install pre-commit and then just run pre-commit run --all-files on the root folder. Please check pre-commit-config.yaml for more info about it. Another option (after adding the project poetry env as interpreter in pycharm) would be running poetry run black . and poetry run isort . on the root folder, the config options will be taken automatically from pyproject.toml . For adding this as autosave option please refer to https://black.readthedocs.io/en/stable/integrations/editors.html When updating a git branch please use rebase instead of merge. Tools k9s https://k9scli.io/ bash-completion","title":"Welcome Pack"},{"location":"WELCOME_PACK/#welcome-pack","text":"","title":"Welcome Pack"},{"location":"WELCOME_PACK/#team-composition","text":"Julia - Manager Dani - Tech Lead \u00c1ngel - Devops Brandon - Developer David - Developer Sergio - Developer Javier - Developer","title":"Team Composition"},{"location":"WELCOME_PACK/#first-steps","text":"Please request access to all the things listed below: Project repository https://gitlab.intelygenz.com/mettel/automation-engine/ , docker repository https://gitlab.intelygenz.com/mettel/docker_images/-/tree/master and One password to itcrowd@intelygenz.com throught their ticketing system https://docs.google.com/document/d/1YLYdI9Dyq8tNlNy2iJ29InquKDz8r_Dw4XBsxI7pPiM/edit and CC your manager to allow the request Configure vpn https://docs.google.com/document/d/16_LFpkiBWN0mbfjAoqR4BaEB5kPNsuNHrUS7PtrWnEA/edit#heading=h.to49i8wu1vn3 AWS account creation to our devops Jira board to our manager Mettel's Slack channels to our tech lead","title":"First steps"},{"location":"WELCOME_PACK/#project-overview","text":"","title":"Project overview"},{"location":"WELCOME_PACK/#resources","text":"Check docs folder inside the mettel gitlab project and read carefully the readme for installing all the required programs and configure the project. After RRHH's on-boarding our team lead will give an overview of the project https://docs.google.com/presentation/d/1Y18vXn6lsSp-6pJVsB5swWg7UcTDWYrIYtXq8_brRMk/edit#slide=id.g6183830b53_0_5","title":"Resources"},{"location":"WELCOME_PACK/#project-guidelines","text":"This project uses Black and isort. You just need to install pip install pre-commit and then just run pre-commit run --all-files on the root folder. Please check pre-commit-config.yaml for more info about it. Another option (after adding the project poetry env as interpreter in pycharm) would be running poetry run black . and poetry run isort . on the root folder, the config options will be taken automatically from pyproject.toml . For adding this as autosave option please refer to https://black.readthedocs.io/en/stable/integrations/editors.html When updating a git branch please use rebase instead of merge.","title":"Project guidelines"},{"location":"WELCOME_PACK/#tools","text":"k9s https://k9scli.io/ bash-completion","title":"Tools"},{"location":"decisions/","text":"GLOBAL DECISIONS Dashboard infrastructure and architecture With passion from the Intelygenz Team @ 2022","title":"MetTel decisions"},{"location":"decisions/01-dashboards-infrastructure-and-architecture/","text":"1: Design of an isolated an unique dashboard user experience Status: Approved Decission: Future is to have an external Grafana that connects to an external Prometheus & InfluxDB 2 Implementation plan as following: - Prometheus will be deployed as an AWS Managed Prometheus. - Grafana will be deployed independently in AWS pointing to AWS Prometheus. - Konstellation KRE will be merged to a single KRE instance as soon as KRE allows it. - KRE will use a dedicated InfluxDB2 in AWS. - Grafana will connect to InfluxDB2 as additional data source. - Chronograf dashboards will be migrated to the dedicated Grafana in AWS. Miro diagram of affected systems: https://miro.com/app/board/uXjVO6bg-zY=/ Alternatives considered: Justification: Current dashboarding solution has several different interfaces, this is confusing for all kind of users. Current stability of dashboarding is dependent in the stability of our K8s deployments, losing all visibility if anything goes wrong on K8s. License limitations on InfluxDB OSS1 could become a problem. Consequences:","title":"**1: Design of an isolated an unique dashboard user experience**"},{"location":"decisions/01-dashboards-infrastructure-and-architecture/#1-design-of-an-isolated-an-unique-dashboard-user-experience","text":"Status: Approved Decission: Future is to have an external Grafana that connects to an external Prometheus & InfluxDB 2 Implementation plan as following: - Prometheus will be deployed as an AWS Managed Prometheus. - Grafana will be deployed independently in AWS pointing to AWS Prometheus. - Konstellation KRE will be merged to a single KRE instance as soon as KRE allows it. - KRE will use a dedicated InfluxDB2 in AWS. - Grafana will connect to InfluxDB2 as additional data source. - Chronograf dashboards will be migrated to the dedicated Grafana in AWS. Miro diagram of affected systems: https://miro.com/app/board/uXjVO6bg-zY=/ Alternatives considered: Justification: Current dashboarding solution has several different interfaces, this is confusing for all kind of users. Current stability of dashboarding is dependent in the stability of our K8s deployments, losing all visibility if anything goes wrong on K8s. License limitations on InfluxDB OSS1 could become a problem. Consequences:","title":"1: Design of an isolated an unique dashboard user experience"},{"location":"diagrams/TOOLS/","text":"https://github.com/mingrammer/diagrams","title":"TOOLS"},{"location":"kafka/LAUNCH_DOCKER_COMPOSE/","text":"1. How to run In this section we explain all the useful information that we can use when working in our local environment. In order to raise our system we will have to go to the root folder of our system and launch the following command docker-compose up This will launch our system by creating a local docker image and deploying our fetcher. It is important to keep in mind that this process launches a multitude of requests against the production environment and it is important to limit them all so as not to overload the system. To facilitate the work in local we also have several local configurations that allow us to test step by step our application avoiding possible problems of launching too many requests. To launch this configuration in Intellij you will need to verify that the following steps were done: - Go to Settings >> Build, Execution, Deployment >> Docker - Select \"TCP socket\" - Enter 'unix:///var/run/docker.sock' under \"Engine API URL\"","title":"Launch docker compose"},{"location":"kafka/LAUNCH_DOCKER_COMPOSE/#1-how-to-run","text":"In this section we explain all the useful information that we can use when working in our local environment. In order to raise our system we will have to go to the root folder of our system and launch the following command docker-compose up This will launch our system by creating a local docker image and deploying our fetcher. It is important to keep in mind that this process launches a multitude of requests against the production environment and it is important to limit them all so as not to overload the system. To facilitate the work in local we also have several local configurations that allow us to test step by step our application avoiding possible problems of launching too many requests. To launch this configuration in Intellij you will need to verify that the following steps were done: - Go to Settings >> Build, Execution, Deployment >> Docker - Select \"TCP socket\" - Enter 'unix:///var/run/docker.sock' under \"Engine API URL\"","title":"1. How to run"},{"location":"lambda/PARAMETER_REPLICATOR/","text":"1. Summary Parameter Store is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. This service is only available in the region where is deployed. We use it to set configuration of Automation-Engine app. But if a desaster occurs in the main region we need to have the parameters replicated en the stand-by region to redeploy the app. Parameter-replicator is a phython lambda that replicate parameters from one region to another. If any parameter change, that change will be replicated in the other region, by this way we have a configuration ready to run the application in the mirror region. This lambda also run ones a day to create a parameter backup and store in S3. 1. Diagram parameter-replicator.drawio.svg","title":"PARAMETER REPLICATOR"},{"location":"lambda/PARAMETER_REPLICATOR/#1-summary","text":"Parameter Store is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. This service is only available in the region where is deployed. We use it to set configuration of Automation-Engine app. But if a desaster occurs in the main region we need to have the parameters replicated en the stand-by region to redeploy the app. Parameter-replicator is a phython lambda that replicate parameters from one region to another. If any parameter change, that change will be replicated in the other region, by this way we have a configuration ready to run the application in the mirror region. This lambda also run ones a day to create a parameter backup and store in S3.","title":"1. Summary"},{"location":"lambda/PARAMETER_REPLICATOR/#1-diagram","text":"parameter-replicator.drawio.svg","title":"1. Diagram"},{"location":"manual_configurations/GITLAB_MAINTENANCE/","text":"1. Summary Gitlab is the tool selected to manage CI-CD process of Automation-Engine APP. The source code is in the project repository , under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . This nevironment only require the modules 1-parameters , 2-network and 4-services . Gitlab is deployed in a separated environemt (workspace) of automation-engine application, so the changes applied in this workspace not afect the main app. Gitlab is the basis for CI-CD, so it cannot manage its own lifecycle (it could be the snake biting its own tail). So the integration and maintenance of this environment is managed locally with Terraform by the infrastructure administrators, keeping the code versions in the repository of the project itself. 2. Modules 1-parameters , define the parameter required by the workspace \"builder\", like password of the database or gitlab root account. The parameter creation are managed by terraform, but the value is updated by operators in AWS parameter store. 2-network , manage the networking of the environemt, this terraform configuration have all the VPC configuration required to deploy gitlab. 4-services , deploy AWS services required like RDS, EKS, or S3 among others. Also deploy the Chart of the gitlab application. 3. Deploy All terraform modules have a common Makefile that contain the required configurati\u00f3n to apply terraform in a specifig workspace. For example, to deploy the complete gitlab environment you need to: cd infra-as-code/basic-infra/1-parameters make terraform_apply env=builder // note: after parameter creation, you need to update the value of that paramters in AWS web console. cd ../2-network make terraform_apply env=builder cd ../4-services make terraform_apply env=builder 4. Updates Gitlab receives constant security patches and updates, it is recommended to keep gitlab up to date. To do this, we go to the gitlab-ci.yml root file of the repo and update the version, for example: ... TF_VAR_GITLAB_CHART_VERSION: \"6.0.3\" ... Then just execute in 4-services folder the command: make terraform apply env=builder verify the changes, and accepting them by write \"yes\" to applied it. 5. References https://docs.gitlab.com/ee/raketasks/backup_restore.html https://gitlab.com/gitlab-org/charts/gitlab/-/blob/master/doc/backup-restore/index.md https://gitlab.com/gitlab-org/charts/gitlab/blob/master/doc/backup-restore/restore.md","title":"GITLAB MAINTENANCE"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#1-summary","text":"Gitlab is the tool selected to manage CI-CD process of Automation-Engine APP. The source code is in the project repository , under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . This nevironment only require the modules 1-parameters , 2-network and 4-services . Gitlab is deployed in a separated environemt (workspace) of automation-engine application, so the changes applied in this workspace not afect the main app. Gitlab is the basis for CI-CD, so it cannot manage its own lifecycle (it could be the snake biting its own tail). So the integration and maintenance of this environment is managed locally with Terraform by the infrastructure administrators, keeping the code versions in the repository of the project itself.","title":"1. Summary"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#2-modules","text":"1-parameters , define the parameter required by the workspace \"builder\", like password of the database or gitlab root account. The parameter creation are managed by terraform, but the value is updated by operators in AWS parameter store. 2-network , manage the networking of the environemt, this terraform configuration have all the VPC configuration required to deploy gitlab. 4-services , deploy AWS services required like RDS, EKS, or S3 among others. Also deploy the Chart of the gitlab application.","title":"2. Modules"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#3-deploy","text":"All terraform modules have a common Makefile that contain the required configurati\u00f3n to apply terraform in a specifig workspace. For example, to deploy the complete gitlab environment you need to: cd infra-as-code/basic-infra/1-parameters make terraform_apply env=builder // note: after parameter creation, you need to update the value of that paramters in AWS web console. cd ../2-network make terraform_apply env=builder cd ../4-services make terraform_apply env=builder","title":"3. Deploy"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#4-updates","text":"Gitlab receives constant security patches and updates, it is recommended to keep gitlab up to date. To do this, we go to the gitlab-ci.yml root file of the repo and update the version, for example: ... TF_VAR_GITLAB_CHART_VERSION: \"6.0.3\" ... Then just execute in 4-services folder the command: make terraform apply env=builder verify the changes, and accepting them by write \"yes\" to applied it.","title":"4. Updates"},{"location":"manual_configurations/GITLAB_MAINTENANCE/#5-references","text":"https://docs.gitlab.com/ee/raketasks/backup_restore.html https://gitlab.com/gitlab-org/charts/gitlab/-/blob/master/doc/backup-restore/index.md https://gitlab.com/gitlab-org/charts/gitlab/blob/master/doc/backup-restore/restore.md","title":"5. References"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/","text":"Pre requisites Okta Account AWS Account Description Because of FEDRAMP we need to implement as a team a IdP that control all users and permissions related to MetTel projects that are made by Intelygenz. For automatic synchronization we are going to create SCIM sync between OKTA and AWS SSO, because of that, groups and users are going to be synced if someone deletes/create a group/user in Okta. Considerations Remove a user/group don't revoke the session tokens in AWS, the minimum duration of these tokens are of 1h. Info here Using the same Okta group for both assignments and group push is not currently supported. To maintain consistent group memberships between Okta and AWS SSO, you need to create a separate group and configure it to push groups to AWS SSO. Info here If you update a user\u2019s address you must have streetAddress, city, state, zipCode and the countryCode value specified. If any of these values are not specified for the Okta user at the time of synchronization, the user or changes to the user will not be provisioned. Info here Entitlements and role attributes are not supported and cannot be synced to AWS SSO. Info here Steps Configure IdP with Okta, this is the guide . Create the following groups: IPA-FED-INT-PRIVILEGED: Internal users Federated privileged group on the federal account. Administration accounts. IPA-FED-INT-NON-PRIVILEGED: Internal users Federated non privileged group on the federal account. IPA-COM-INT-PRIVILEGED: Internal users Commercial privileged group on the commercial account. Administration accounts. IPA-COM-INT-NON-PRIVILEGED: Internal users Federated privileged group on the commercial account. Administration accounts. IPA-FED-EXT-PRIVILEGED: External users Federated privileged group on the federal account. Administration accounts. IPA-FED-EXT-NON-PRIVILEGED: External users Federated non privileged group on the federal account. IPA-COM-EXT-PRIVILEGED: External users Commercial privileged group on the commercial account. Administration accounts. IPA-COM-EXT-NON-PRIVILEGED: External users Federated privileged group on the commercial account. Administration accounts. Associate permissions to groups. Guide Privileged accounts will have general administrator permissions Non privileged accounts will only have access to logs on cloud watch and grafana Revoke permissions Because of the problem of token duration of 1h that can not be revoked from okta, there is a manual procedure to delete the access from AWS SSO. For revoking access follow this guide With passion from the Intelygenz Team @ 2022","title":"OKTA CONFIGURATIONS"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#pre-requisites","text":"Okta Account AWS Account","title":"Pre requisites"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#description","text":"Because of FEDRAMP we need to implement as a team a IdP that control all users and permissions related to MetTel projects that are made by Intelygenz. For automatic synchronization we are going to create SCIM sync between OKTA and AWS SSO, because of that, groups and users are going to be synced if someone deletes/create a group/user in Okta.","title":"Description"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#considerations","text":"Remove a user/group don't revoke the session tokens in AWS, the minimum duration of these tokens are of 1h. Info here Using the same Okta group for both assignments and group push is not currently supported. To maintain consistent group memberships between Okta and AWS SSO, you need to create a separate group and configure it to push groups to AWS SSO. Info here If you update a user\u2019s address you must have streetAddress, city, state, zipCode and the countryCode value specified. If any of these values are not specified for the Okta user at the time of synchronization, the user or changes to the user will not be provisioned. Info here Entitlements and role attributes are not supported and cannot be synced to AWS SSO. Info here","title":"Considerations"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#steps","text":"Configure IdP with Okta, this is the guide . Create the following groups: IPA-FED-INT-PRIVILEGED: Internal users Federated privileged group on the federal account. Administration accounts. IPA-FED-INT-NON-PRIVILEGED: Internal users Federated non privileged group on the federal account. IPA-COM-INT-PRIVILEGED: Internal users Commercial privileged group on the commercial account. Administration accounts. IPA-COM-INT-NON-PRIVILEGED: Internal users Federated privileged group on the commercial account. Administration accounts. IPA-FED-EXT-PRIVILEGED: External users Federated privileged group on the federal account. Administration accounts. IPA-FED-EXT-NON-PRIVILEGED: External users Federated non privileged group on the federal account. IPA-COM-EXT-PRIVILEGED: External users Commercial privileged group on the commercial account. Administration accounts. IPA-COM-EXT-NON-PRIVILEGED: External users Federated privileged group on the commercial account. Administration accounts. Associate permissions to groups. Guide Privileged accounts will have general administrator permissions Non privileged accounts will only have access to logs on cloud watch and grafana","title":"Steps"},{"location":"manual_configurations/OKTA_CONFIGURATIONS/#revoke-permissions","text":"Because of the problem of token duration of 1h that can not be revoked from okta, there is a manual procedure to delete the access from AWS SSO. For revoking access follow this guide With passion from the Intelygenz Team @ 2022","title":"Revoke permissions"},{"location":"metrics-definitions/","text":"Metrics Definitons This folder will contain all the metrics created to track functional and business values that improve the overall observability of the system There will be one markdown file in this folder per metric, the filename will be the metric name and it will contain all their descriptions and all their possible variations. Naming conventions must follow the Prometheus Best Practices for naming and units: https://prometheus.io/docs/practices/naming/ List of metrics Metric Name Description File tasks_created Task Creations tasks_created.md tasks_reopened Task Re-opens tasks_reopened.md tasks_forwarded Task Forwards tasks_forwarded.md tasks_autoresolved Task Auto-resolves tasks_autoresolved.md velocloud_fetcher_to_kafka_messages_attempts Velocloud fetcher attempts to kafka velocloud_fetcher_to_kafka_messages_attempts.md velocloud_fetcher_to_kafka_messages_status Velocloud fetcher errors when to kafka velocloud_fetcher_to_kafka_messages_status.md","title":"Metrics definitions"},{"location":"metrics-definitions/#metrics-definitons","text":"This folder will contain all the metrics created to track functional and business values that improve the overall observability of the system There will be one markdown file in this folder per metric, the filename will be the metric name and it will contain all their descriptions and all their possible variations. Naming conventions must follow the Prometheus Best Practices for naming and units: https://prometheus.io/docs/practices/naming/","title":"Metrics Definitons"},{"location":"metrics-definitions/#list-of-metrics","text":"Metric Name Description File tasks_created Task Creations tasks_created.md tasks_reopened Task Re-opens tasks_reopened.md tasks_forwarded Task Forwards tasks_forwarded.md tasks_autoresolved Task Auto-resolves tasks_autoresolved.md velocloud_fetcher_to_kafka_messages_attempts Velocloud fetcher attempts to kafka velocloud_fetcher_to_kafka_messages_attempts.md velocloud_fetcher_to_kafka_messages_status Velocloud fetcher errors when to kafka velocloud_fetcher_to_kafka_messages_status.md","title":"List of metrics"},{"location":"metrics-definitions/tasks_autoresolved/","text":"Task Auto-resolves Metric name: tasks_autoresolved Type of metric: Counter Data store: Prometheus VeloCloud - Service Outage tasks auto-resolved Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges have been auto-resolved since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: [2 | 3] - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both] Ixia - Service Outage tasks auto-resolved Description: This metric counts how many Service Outage tasks in progress related to Ixia probes have been auto-resolved since the hawkeye-outage-monitor started until now. Labels: - feature: Hawkeye Outage Monitor - system: Ixia - topic: VOO - client: <one of the relevant clients defined in AWS> - severity: 2 InterMapper - Service Outage tasks auto-resolved Description: This metric counts how many Service Outage tasks in progress related to InterMapper devices have been auto-resolved since the intermapper-outage-monitor started until now. Labels: - feature: InterMapper Outage Monitor - system: InterMapper - topic: VOO - severity: 2 - event: [Up | OK | Down | Critical | Alarm | Warning | Link Warning] - is_piab: [True | False] VeloCloud - Service Affecting tasks auto-resolved Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges have been auto-resolved since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 AI-powered - Service Outage tasks auto-resolved Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges have been auto-resolved since the tnba-monitor started until now. Labels: - feature: TNBA Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: 2 - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both] AI-powered - Service Affecting tasks auto-resolved Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges have been auto-resolved since the tnba-monitor started until now. Labels: - feature: TNBA Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3","title":"Task Auto-resolves #"},{"location":"metrics-definitions/tasks_autoresolved/#task-auto-resolves","text":"Metric name: tasks_autoresolved Type of metric: Counter Data store: Prometheus","title":"Task Auto-resolves"},{"location":"metrics-definitions/tasks_autoresolved/#velocloud-service-outage-tasks-auto-resolved","text":"Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges have been auto-resolved since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: [2 | 3] - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both]","title":"VeloCloud  - Service Outage tasks auto-resolved"},{"location":"metrics-definitions/tasks_autoresolved/#ixia-service-outage-tasks-auto-resolved","text":"Description: This metric counts how many Service Outage tasks in progress related to Ixia probes have been auto-resolved since the hawkeye-outage-monitor started until now. Labels: - feature: Hawkeye Outage Monitor - system: Ixia - topic: VOO - client: <one of the relevant clients defined in AWS> - severity: 2","title":"Ixia - Service Outage tasks auto-resolved"},{"location":"metrics-definitions/tasks_autoresolved/#intermapper-service-outage-tasks-auto-resolved","text":"Description: This metric counts how many Service Outage tasks in progress related to InterMapper devices have been auto-resolved since the intermapper-outage-monitor started until now. Labels: - feature: InterMapper Outage Monitor - system: InterMapper - topic: VOO - severity: 2 - event: [Up | OK | Down | Critical | Alarm | Warning | Link Warning] - is_piab: [True | False]","title":"InterMapper - Service Outage tasks auto-resolved"},{"location":"metrics-definitions/tasks_autoresolved/#velocloud-service-affecting-tasks-auto-resolved","text":"Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges have been auto-resolved since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3","title":"VeloCloud - Service Affecting tasks auto-resolved"},{"location":"metrics-definitions/tasks_autoresolved/#ai-powered-service-outage-tasks-auto-resolved","text":"Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges have been auto-resolved since the tnba-monitor started until now. Labels: - feature: TNBA Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: 2 - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both]","title":"AI-powered - Service Outage tasks auto-resolved"},{"location":"metrics-definitions/tasks_autoresolved/#ai-powered-service-affecting-tasks-auto-resolved","text":"Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges have been auto-resolved since the tnba-monitor started until now. Labels: - feature: TNBA Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3","title":"AI-powered - Service Affecting tasks auto-resolved"},{"location":"metrics-definitions/tasks_created/","text":"Task Creations Metric name: tasks_created Type of metric: Counter Data store: Prometheus VeloCloud - Service Outage tasks created Description: This metric counts how many Service Outage tasks related to VeloCloud edges have been created since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: [2 | 3] - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both] Ixia - Service Outage tasks created Description: This metric counts how many Service Outage tasks related to Ixia probes have been created since the hawkeye-outage-monitor instances started until now. Labels: - feature: Hawkeye Outage Monitor - system: Ixia - topic: VOO - client: <one of the relevant clients defined in AWS> - outage_type: [Node To Node | Real Service | Both] - severity: 2 InterMapper - Service Outage tasks created Description: This metric counts how many Service Outage tasks related to InterMapper devices have been created since the intermapper-outage-monitor started until now. Labels: - feature: InterMapper Outage Monitor - system: InterMapper - topic: VOO - severity: 2 - event: [Up | OK | Down | Critical | Alarm | Warning | Link Warning] - is_piab: [True | False] VeloCloud - Service Affecting tasks created Description: This metric counts how many Service Affecting tasks related to VeloCloud edges have been created since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: [Jitter | Latency | Packet Loss | Bandwidth Over Utilization | Circuit Instability] Fraud - Service Affecting tasks created Description: This metric counts how many Service Affecting tasks related to Fraud alerts have been created since the fraud-monitor started until now. Labels: - feature: Fraud Monitor - system: MetTel Fraud Alerts - topic: VAS - severity: 3 - trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Task Creations #"},{"location":"metrics-definitions/tasks_created/#task-creations","text":"Metric name: tasks_created Type of metric: Counter Data store: Prometheus","title":"Task Creations"},{"location":"metrics-definitions/tasks_created/#velocloud-service-outage-tasks-created","text":"Description: This metric counts how many Service Outage tasks related to VeloCloud edges have been created since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: [2 | 3] - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both]","title":"VeloCloud - Service Outage tasks created"},{"location":"metrics-definitions/tasks_created/#ixia-service-outage-tasks-created","text":"Description: This metric counts how many Service Outage tasks related to Ixia probes have been created since the hawkeye-outage-monitor instances started until now. Labels: - feature: Hawkeye Outage Monitor - system: Ixia - topic: VOO - client: <one of the relevant clients defined in AWS> - outage_type: [Node To Node | Real Service | Both] - severity: 2","title":"Ixia - Service Outage tasks created"},{"location":"metrics-definitions/tasks_created/#intermapper-service-outage-tasks-created","text":"Description: This metric counts how many Service Outage tasks related to InterMapper devices have been created since the intermapper-outage-monitor started until now. Labels: - feature: InterMapper Outage Monitor - system: InterMapper - topic: VOO - severity: 2 - event: [Up | OK | Down | Critical | Alarm | Warning | Link Warning] - is_piab: [True | False]","title":"InterMapper - Service Outage tasks created"},{"location":"metrics-definitions/tasks_created/#velocloud-service-affecting-tasks-created","text":"Description: This metric counts how many Service Affecting tasks related to VeloCloud edges have been created since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: [Jitter | Latency | Packet Loss | Bandwidth Over Utilization | Circuit Instability]","title":"VeloCloud - Service Affecting tasks created"},{"location":"metrics-definitions/tasks_created/#fraud-service-affecting-tasks-created","text":"Description: This metric counts how many Service Affecting tasks related to Fraud alerts have been created since the fraud-monitor started until now. Labels: - feature: Fraud Monitor - system: MetTel Fraud Alerts - topic: VAS - severity: 3 - trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Fraud - Service Affecting tasks created"},{"location":"metrics-definitions/tasks_forwarded/","text":"Task Forwards Metric name: tasks_forwarded Type of metric: Counter Data store: Prometheus VeloCloud - Service Outage tasks forwarded to HNOC Investigate queue Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges have been forwarded to the HNOC Investigate queue since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down Outage (no HA) | Hard Down Outage (HA) | Soft Down Outage (HA) | Link Down Outage (no HA) | Link Down Outage (HA)] - severity: [2 | 3] - target_queue: HNOC Investigate VeloCloud - Service Outage tasks forwarded to Wireless Repair Intervention Needed queue Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges with at least one disconnected DiGi link have been forwarded to the Wireless Repair Intervention Needed queue after a failed DiGi reboot since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Link Down Outage (no HA) | Link Down Outage (HA)] - severity: 3 - has_digi: True - has_byob: [True | False] - link_types: Wireless - target_queue: Wireless Repair Intervention Needed VeloCloud - Service Outage tasks forwarded to ASR Investigate queue Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges with at least one wired disconnected link that is not BYOB, Customer Owned or a PIAB have been forwarded to the ASR Investigate queue since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Link Down Outage (no HA) | Link Down Outage (HA)] - severity: 3 - has_digi: [True | False] - has_byob: False - link_types: Wired - target_queue: ASR Investigate VeloCloud - Service Affecting tasks forwarded to HNOC Investigate queue Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges with a jitter, latency, packet loss or bandwidth over utilization issue have been forwarded to the HNOC Investigate queue since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: [Jitter | Latency | Packet Loss | Bandwidth Over Utilization] - target_queue: HNOC Investigate VeloCloud - Service Affecting tasks forwarded to ASR Investigate queue Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges related to a non BYOB or Customer Owned wired link with a circuit instability issue have been forwarded to the ASR Investigate queue since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: Circuit Instability - target_queue: ASR Investigate Fraud - Service Affecting tasks forwarded to HNOC Investigate queue Description: This metric counts how many Service Affecting tasks in progress related to Fraud alerts have been forwarded to the HNOC Investigate queue since the fraud-monitor started until now. Labels: - feature: Fraud Monitor - system: MetTel Fraud Alerts - topic: VAS - severity: 3 - trouble: [Possible Fraud | Request Rate Monitor Violation] - target_queue: HNOC Investigate","title":"Task Forwards #"},{"location":"metrics-definitions/tasks_forwarded/#task-forwards","text":"Metric name: tasks_forwarded Type of metric: Counter Data store: Prometheus","title":"Task Forwards"},{"location":"metrics-definitions/tasks_forwarded/#velocloud-service-outage-tasks-forwarded-to-hnoc-investigate-queue","text":"Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges have been forwarded to the HNOC Investigate queue since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down Outage (no HA) | Hard Down Outage (HA) | Soft Down Outage (HA) | Link Down Outage (no HA) | Link Down Outage (HA)] - severity: [2 | 3] - target_queue: HNOC Investigate","title":"VeloCloud - Service Outage tasks forwarded to HNOC Investigate queue"},{"location":"metrics-definitions/tasks_forwarded/#velocloud-service-outage-tasks-forwarded-to-wireless-repair-intervention-needed-queue","text":"Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges with at least one disconnected DiGi link have been forwarded to the Wireless Repair Intervention Needed queue after a failed DiGi reboot since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Link Down Outage (no HA) | Link Down Outage (HA)] - severity: 3 - has_digi: True - has_byob: [True | False] - link_types: Wireless - target_queue: Wireless Repair Intervention Needed","title":"VeloCloud  - Service Outage tasks forwarded to Wireless Repair Intervention Needed queue"},{"location":"metrics-definitions/tasks_forwarded/#velocloud-service-outage-tasks-forwarded-to-asr-investigate-queue","text":"Description: This metric counts how many Service Outage tasks in progress related to VeloCloud edges with at least one wired disconnected link that is not BYOB, Customer Owned or a PIAB have been forwarded to the ASR Investigate queue since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Link Down Outage (no HA) | Link Down Outage (HA)] - severity: 3 - has_digi: [True | False] - has_byob: False - link_types: Wired - target_queue: ASR Investigate","title":"VeloCloud - Service Outage tasks forwarded to ASR Investigate queue"},{"location":"metrics-definitions/tasks_forwarded/#velocloud-service-affecting-tasks-forwarded-to-hnoc-investigate-queue","text":"Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges with a jitter, latency, packet loss or bandwidth over utilization issue have been forwarded to the HNOC Investigate queue since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: [Jitter | Latency | Packet Loss | Bandwidth Over Utilization] - target_queue: HNOC Investigate","title":"VeloCloud - Service Affecting tasks forwarded to HNOC Investigate queue"},{"location":"metrics-definitions/tasks_forwarded/#velocloud-service-affecting-tasks-forwarded-to-asr-investigate-queue","text":"Description: This metric counts how many Service Affecting tasks in progress related to VeloCloud edges related to a non BYOB or Customer Owned wired link with a circuit instability issue have been forwarded to the ASR Investigate queue since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: Circuit Instability - target_queue: ASR Investigate","title":"VeloCloud - Service Affecting tasks forwarded to ASR Investigate queue"},{"location":"metrics-definitions/tasks_forwarded/#fraud-service-affecting-tasks-forwarded-to-hnoc-investigate-queue","text":"Description: This metric counts how many Service Affecting tasks in progress related to Fraud alerts have been forwarded to the HNOC Investigate queue since the fraud-monitor started until now. Labels: - feature: Fraud Monitor - system: MetTel Fraud Alerts - topic: VAS - severity: 3 - trouble: [Possible Fraud | Request Rate Monitor Violation] - target_queue: HNOC Investigate","title":"Fraud - Service Affecting tasks forwarded to HNOC Investigate queue"},{"location":"metrics-definitions/tasks_reopened/","text":"Task Creations Metric name: tasks_reopened Type of metric: Counter Data store: Prometheus VeloCloud - Service Outage tasks re-opened Description: This metric counts how many Service Outage tasks related to VeloCloud edges have been re-opened since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: [2 | 3] - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both] Ixia - Service Outage tasks re-opened Description: This metric counts how many Service Outage tasks related to Ixia probes have been re-opened since the hawkeye-outage-monitor instances started until now. Labels: - feature: Hawkeye Outage Monitor - system: Ixia - topic: VOO - client: <one of the relevant clients defined in AWS> - outage_type: [Node To Node | Real Service | Both] - severity: 2 InterMapper - Service Outage tasks re-opened Description: This metric counts how many Service Outage tasks related to InterMapper devices have been re-opened since the intermapper-outage-monitor started until now. Labels: - feature: InterMapper Outage Monitor - system: InterMapper - topic: VOO - severity: 2 - event: [Up | OK | Down | Critical | Alarm | Warning | Link Warning] - is_piab: [True | False] VeloCloud - Service Affecting tasks re-opened Description: This metric counts how many Service Affecting tasks related to VeloCloud edges have been re-opened since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: [Jitter | Latency | Packet Loss | Bandwidth Over Utilization | Circuit Instability] Fraud - Service Affecting tasks re-opened Description: This metric counts how many Service Affecting tasks related to Fraud alerts have been re-opened since the fraud-monitor started until now. Labels: - feature: Fraud Monitor - system: MetTel Fraud Alerts - topic: VAS - severity: 3 - trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Task Creations #"},{"location":"metrics-definitions/tasks_reopened/#task-creations","text":"Metric name: tasks_reopened Type of metric: Counter Data store: Prometheus","title":"Task Creations"},{"location":"metrics-definitions/tasks_reopened/#velocloud-service-outage-tasks-re-opened","text":"Description: This metric counts how many Service Outage tasks related to VeloCloud edges have been re-opened since the service-outage-monitor instances started until now. Labels: - feature: Service Outage Monitor - system: VeloCloud - topic: VOO - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - outage_type: [Hard Down (no HA) | Hard Down (HA) | Soft Down (HA) | Link Down (no HA) | Link Down (HA)] - severity: [2 | 3] - has_digi: [True | False] - has_byob: [True | False] - link_types: [Wired | Wireless | Both]","title":"VeloCloud - Service Outage tasks re-opened"},{"location":"metrics-definitions/tasks_reopened/#ixia-service-outage-tasks-re-opened","text":"Description: This metric counts how many Service Outage tasks related to Ixia probes have been re-opened since the hawkeye-outage-monitor instances started until now. Labels: - feature: Hawkeye Outage Monitor - system: Ixia - topic: VOO - client: <one of the relevant clients defined in AWS> - outage_type: [Node To Node | Real Service | Both] - severity: 2","title":"Ixia - Service Outage tasks re-opened"},{"location":"metrics-definitions/tasks_reopened/#intermapper-service-outage-tasks-re-opened","text":"Description: This metric counts how many Service Outage tasks related to InterMapper devices have been re-opened since the intermapper-outage-monitor started until now. Labels: - feature: InterMapper Outage Monitor - system: InterMapper - topic: VOO - severity: 2 - event: [Up | OK | Down | Critical | Alarm | Warning | Link Warning] - is_piab: [True | False]","title":"InterMapper - Service Outage tasks re-opened"},{"location":"metrics-definitions/tasks_reopened/#velocloud-service-affecting-tasks-re-opened","text":"Description: This metric counts how many Service Affecting tasks related to VeloCloud edges have been re-opened since the service-affecting-monitor instances started until now. Labels: - feature: Service Affecting Monitor - system: VeloCloud - topic: VAS - client: <one of the relevant clients defined in AWS> - host: [mettel.velocloud.net | metvco02.mettel.net | metvco03.mettel.net | metvco04.mettel.net] - severity: 3 - trouble: [Jitter | Latency | Packet Loss | Bandwidth Over Utilization | Circuit Instability]","title":"VeloCloud - Service Affecting tasks re-opened"},{"location":"metrics-definitions/tasks_reopened/#fraud-service-affecting-tasks-re-opened","text":"Description: This metric counts how many Service Affecting tasks related to Fraud alerts have been re-opened since the fraud-monitor started until now. Labels: - feature: Fraud Monitor - system: MetTel Fraud Alerts - topic: VAS - severity: 3 - trouble: [Possible Fraud | Request Rate Monitor Violation]","title":"Fraud - Service Affecting tasks re-opened"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/","text":"Task Velocloud fetcher attempts to kafka Metric name: velocloud_fetcher_to_kafka_messages_attempts Type of metric: Counter Data store: Prometheus VeloCloud - Attempts total messages to Kafka Description: This metrics counts the number of attempts sending messages to kafka. Labels: - schema_name: Name of the schema - environment: [develop, master]","title":"Task Velocloud fetcher attempts to kafka #"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/#task-velocloud-fetcher-attempts-to-kafka","text":"Metric name: velocloud_fetcher_to_kafka_messages_attempts Type of metric: Counter Data store: Prometheus","title":"Task Velocloud fetcher attempts to kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_attempts/#velocloud-attempts-total-messages-to-kafka","text":"Description: This metrics counts the number of attempts sending messages to kafka. Labels: - schema_name: Name of the schema - environment: [develop, master]","title":"VeloCloud  - Attempts total messages to Kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/","text":"Task Velocloud fetcher errors when to kafka Metric name: velocloud_fetcher_to_kafka_messages_status Type of metric: Counter Data store: Prometheus VeloCloud - Status of Messages sent to Kafka Description: This metrics counts the number of OK calls or Errors when push data to the kafka server. Labels: - schema_name: Name of the schema - status: [OK, ERROR] - environment: [develop, master]","title":"Task Velocloud fetcher errors when to kafka #"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/#task-velocloud-fetcher-errors-when-to-kafka","text":"Metric name: velocloud_fetcher_to_kafka_messages_status Type of metric: Counter Data store: Prometheus","title":"Task Velocloud fetcher errors when to kafka"},{"location":"metrics-definitions/velocloud_fetcher_to_kafka_messages_status/#velocloud-status-of-messages-sent-to-kafka","text":"Description: This metrics counts the number of OK calls or Errors when push data to the kafka server. Labels: - schema_name: Name of the schema - status: [OK, ERROR] - environment: [develop, master]","title":"VeloCloud  - Status of Messages sent to Kafka"},{"location":"pipeline/BASIC_CI_CONFIGURATION/","text":"1. CI/CD PROJECT CONFIGURATION FROM 0 To release this project an make it work we need to configure next stuff: - semantic release - AIVEN - AWS 1.1 Semantic Release Semantic release is depending of a base image in this repository to be faster on this project CI/CD. The only two configurations we need to make it work here is: - Have prepared the base image repository and point to that image in the .gitlab-ci.yml(variable SEMANTIC_RELEASE_IMAGE) - Get an access token on the project (Settings/Access tokens) and get all permissions to interacts with the API. Create a variable called GITLAB_TOKEN and put the token you crete there. 1.2 AIVEN TODO 1.3 AWS It's recommended to create an account for terraform and a group of permissions with admin access, also it's necessary to create an S3 bucket that the user can access to. To accomplish aws connection in some steps of the CI/CD we need to add next variables (on the settings section, not in the YAML) in the CI: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY It's necessary to connect to the S3 from amazon, where we store the tfstate files to maintain the state of our infrastructure deployments. 1.4 Snowflake With passion from the Intelygenz Team @ 2021","title":"Basic configurations"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#1-cicd-project-configuration-from-0","text":"To release this project an make it work we need to configure next stuff: - semantic release - AIVEN - AWS","title":"1. CI/CD PROJECT CONFIGURATION FROM 0"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#11-semantic-release","text":"Semantic release is depending of a base image in this repository to be faster on this project CI/CD. The only two configurations we need to make it work here is: - Have prepared the base image repository and point to that image in the .gitlab-ci.yml(variable SEMANTIC_RELEASE_IMAGE) - Get an access token on the project (Settings/Access tokens) and get all permissions to interacts with the API. Create a variable called GITLAB_TOKEN and put the token you crete there.","title":"1.1 Semantic Release"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#12-aiven","text":"TODO","title":"1.2 AIVEN"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#13-aws","text":"It's recommended to create an account for terraform and a group of permissions with admin access, also it's necessary to create an S3 bucket that the user can access to. To accomplish aws connection in some steps of the CI/CD we need to add next variables (on the settings section, not in the YAML) in the CI: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY It's necessary to connect to the S3 from amazon, where we store the tfstate files to maintain the state of our infrastructure deployments.","title":"1.3 AWS"},{"location":"pipeline/BASIC_CI_CONFIGURATION/#14-snowflake","text":"With passion from the Intelygenz Team @ 2021","title":"1.4 Snowflake"},{"location":"pipeline/PIPELINE_RULES/","text":"PIPELINES RULES Add new section To add new section of jobs in the pipeline, we must include the section adding an included in the general .gitlab-ci.yml: include: - local: microservices/.gitlab-ci.yml Could be possible that an included .gitlab-ci.yml of another section include more sub-sections(Example microservices): include: - local: microservices/fetchers/velocloud/.gitlab-ci.yml We should follow this organization technique to isolate code to their respective area and avoid big files with spaghetti code. Add new template To add a new template we should take a look in the folder structure: # templates (folder where project templating is going to be saved) ## gitlab (section of the templating) Inside gitlab folder we should create template YAML files that fits with a section, for example, for microservices we created a microservice-ci.yml to store the templates of that section. Same in infrastructure. Also, we should include new templates in the index.yml in templates/index.yml include: # CI templates - local: templates/gitlab/microservice-ci.yml - local: templates/gitlab/infrastructure-ci.yml With passion from the Intelygenz Team @ 2021","title":"Pipeline rules"},{"location":"pipeline/PIPELINE_RULES/#pipelines-rules","text":"","title":"PIPELINES RULES"},{"location":"pipeline/PIPELINE_RULES/#add-new-section","text":"To add new section of jobs in the pipeline, we must include the section adding an included in the general .gitlab-ci.yml: include: - local: microservices/.gitlab-ci.yml Could be possible that an included .gitlab-ci.yml of another section include more sub-sections(Example microservices): include: - local: microservices/fetchers/velocloud/.gitlab-ci.yml We should follow this organization technique to isolate code to their respective area and avoid big files with spaghetti code.","title":"Add new section"},{"location":"pipeline/PIPELINE_RULES/#add-new-template","text":"To add a new template we should take a look in the folder structure: # templates (folder where project templating is going to be saved) ## gitlab (section of the templating) Inside gitlab folder we should create template YAML files that fits with a section, for example, for microservices we created a microservice-ci.yml to store the templates of that section. Same in infrastructure. Also, we should include new templates in the index.yml in templates/index.yml include: # CI templates - local: templates/gitlab/microservice-ci.yml - local: templates/gitlab/infrastructure-ci.yml With passion from the Intelygenz Team @ 2021","title":"Add new template"},{"location":"recovery_processes/GITLAB_RECOVERY/","text":"1. Summary Thes process requires one workday and doesn't affect the functionality of the Automation-Engine production environment. Gitlab is deployed with the Automation-Engine fedramp repository , is under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . The deployment, maintenance, and updates are managed locally by the infrastructure team as explained here . Gitlab has most of its configuration files saved in external storage, we use AWS S3 for all objects and AWS RDS (PostgreSQL) for the database; all of this is in US-EAST-1 region. Gitlab has its own process to make backups every day at 06:00 (UTC+02:00) which also includes a Postgres backup and is also saved in S3. All S3 buckets are replicated in US-WEST-1 region so we have backups of GitLab backups on the opposite coast of the united states in case of disaster failure in the main region. Aditionally to this, we require the secrets.yml config file that have the certificates that gitlab creates on his first deploy to use internally. This file is exported and saved in 1password , with the name \u201cGITLAB FedRAMP rails secret backup\u201d and the file name is gitlab-secrets.yaml . This file is required if we want to redeploy gitlab and restore the backup. The backups contain: * db (database) * uploads (attachments) * builds (CI job output logs) * artifacts (CI job artifacts) * lfs (LFS objects) * terraform_state (Terraform states) * registry (Container Registry images) * pages (Pages content) * repositories (Git repositories data) * packages (Packages) 2. Recovery Deploy required infrastructure. Use the infra-as-code/basic-infra folder, only need: 1-parameters , 2-network and 4-services . The terraform workspace is builder . Set the required parameters, check de data.tf file in parameters section. Change the name of the buckets to avoid conflicts. (files 4-services/s3.tf and 4-services/s3-cross-replica ) For deploying the required infra in a new region only need to change Terraform Local variable called region (file locals.tf ) with the new region in the 3 folders. Define a secundary region (file 4-services/terraform_config.tf ) to set the new S3 bucket replicas in another region if main is not available to be the replica. Or if dont want to create the replicas only change the name of the terrafform file (example s3-cross-replica.back ) Install a clean gitlab helm chart with the same version of the backup. Restore the gitlab-rails-secret.yaml as a secret in the cluster: Find the object name for the rails secrets: kubectl -n gitlab get secrets | grep rails-secret Delete the existing secret: kubectl delete secret <rails-secret-name> (gitlab-rails-secret) Create the new secret using the same name as the old, and passing in your local YAML file: kubectl create secret generic <rails-secret-name> --from-file=secrets.yml=gitlab-secrets.yaml In order to use the new secrets, the Webservice, Sidekiq and Toolbox pods need to be restarted. The safest way to restart those pods is to run: kubectl delete -n gitlab pods -lapp=sidekiq,release=<helm release name> kubectl delete -n gitlab pods -lapp=webservice,release=<helm release name> kubectl delete -n gitlab pods -lapp=toolbox,release=<helm release name> Restore the backup file: Ensure the Toolbox pod is enabled and running by executing the following command kubectl get pods -lrelease=RELEASE_NAME,app=toolbox Get the tarball ready in S3 bucket. Make sure it is named in the [timestamp]_[version]_gitlab_backup.tar format. Run the backup utility to restore the tarball kubectl exec <Toolbox pod name> -it -- backup-utility --restore -t <timestamp>_<version> Here, _ is from the name of the tarball stored in gitlab-backups bucket. In case you want to provide a public URL, use the following command kubectl exec <Toolbox pod name> -it -- backup-utility --restore -f <URL> NOTE : You can provide a local path as a URL as long as it's in the format: file:// This process will take time depending on the size of the tarball. The restoration process will erase the existing contents of database, move existing repositories to temporary locations and extract the contents of the tarball. Repositories will be moved to their corresponding locations on the disk and other data, like artifacts, uploads, LFS etc. will be uploaded to corresponding buckets in Object Storage. During restoration, the backup tarball needs to be extracted to disk. This means the Toolbox pod should have disk of necessary size available. For more details and configuration please see the Toolbox documentation. Restore the runner registration token: after restoring, the included runner will not be able to register to the instance because it no longer has the correct registration token (token has been changed with the new GitLab chart installation). Find the new shared runner token located on the admin/runners webpage of your GitLab installation. kubectl get secrets | grep gitlab-runner-secret Find the name of existing runner token Secret stored in Kubernetes kubectl delete secret <runner-secret-name> Delete the existing secret kubectl delete secret <runner-secret-name> Create the new secret with two keys, (runner-registration-token with your shared token, and an empty runner-token) kubectl create secret generic <runner-secret-name> (gitlab-gitlab-runner-secret) --from-literal=runner-registration-token=<new-shared-runner-token> (gitlab-gitlab-runner-token-xxxxx) --from-literal=runner-token=\"\" (optional) Reset the root user\u2019s password: The restoration process does not update the gitlab-initial-root-password secret with the value from backup. For logging in as root, use the original password included in the backup. In the case that the password is no longer accessible, follow the steps below to reset it. Attach to the Webservice pod by executing the command kubectl exec <Webservice pod name> (gitlab-webservice-default-xxxxx-xxxxx) -it -- bash Run the following command to reset the password of root user. Replace #{password} with a password of your choice /srv/gitlab/bin/rails runner \"user = User.first; user.password='#{password}'; user.password_confirmation='#{password}'; user.save!\"","title":"GITLAB RECOVERY"},{"location":"recovery_processes/GITLAB_RECOVERY/#1-summary","text":"Thes process requires one workday and doesn't affect the functionality of the Automation-Engine production environment. Gitlab is deployed with the Automation-Engine fedramp repository , is under \"builder\" workspace of Terraform definition located in infra-as-code/basic-infra . The deployment, maintenance, and updates are managed locally by the infrastructure team as explained here . Gitlab has most of its configuration files saved in external storage, we use AWS S3 for all objects and AWS RDS (PostgreSQL) for the database; all of this is in US-EAST-1 region. Gitlab has its own process to make backups every day at 06:00 (UTC+02:00) which also includes a Postgres backup and is also saved in S3. All S3 buckets are replicated in US-WEST-1 region so we have backups of GitLab backups on the opposite coast of the united states in case of disaster failure in the main region. Aditionally to this, we require the secrets.yml config file that have the certificates that gitlab creates on his first deploy to use internally. This file is exported and saved in 1password , with the name \u201cGITLAB FedRAMP rails secret backup\u201d and the file name is gitlab-secrets.yaml . This file is required if we want to redeploy gitlab and restore the backup. The backups contain: * db (database) * uploads (attachments) * builds (CI job output logs) * artifacts (CI job artifacts) * lfs (LFS objects) * terraform_state (Terraform states) * registry (Container Registry images) * pages (Pages content) * repositories (Git repositories data) * packages (Packages)","title":"1. Summary"},{"location":"recovery_processes/GITLAB_RECOVERY/#2-recovery","text":"Deploy required infrastructure. Use the infra-as-code/basic-infra folder, only need: 1-parameters , 2-network and 4-services . The terraform workspace is builder . Set the required parameters, check de data.tf file in parameters section. Change the name of the buckets to avoid conflicts. (files 4-services/s3.tf and 4-services/s3-cross-replica ) For deploying the required infra in a new region only need to change Terraform Local variable called region (file locals.tf ) with the new region in the 3 folders. Define a secundary region (file 4-services/terraform_config.tf ) to set the new S3 bucket replicas in another region if main is not available to be the replica. Or if dont want to create the replicas only change the name of the terrafform file (example s3-cross-replica.back ) Install a clean gitlab helm chart with the same version of the backup. Restore the gitlab-rails-secret.yaml as a secret in the cluster: Find the object name for the rails secrets: kubectl -n gitlab get secrets | grep rails-secret Delete the existing secret: kubectl delete secret <rails-secret-name> (gitlab-rails-secret) Create the new secret using the same name as the old, and passing in your local YAML file: kubectl create secret generic <rails-secret-name> --from-file=secrets.yml=gitlab-secrets.yaml In order to use the new secrets, the Webservice, Sidekiq and Toolbox pods need to be restarted. The safest way to restart those pods is to run: kubectl delete -n gitlab pods -lapp=sidekiq,release=<helm release name> kubectl delete -n gitlab pods -lapp=webservice,release=<helm release name> kubectl delete -n gitlab pods -lapp=toolbox,release=<helm release name> Restore the backup file: Ensure the Toolbox pod is enabled and running by executing the following command kubectl get pods -lrelease=RELEASE_NAME,app=toolbox Get the tarball ready in S3 bucket. Make sure it is named in the [timestamp]_[version]_gitlab_backup.tar format. Run the backup utility to restore the tarball kubectl exec <Toolbox pod name> -it -- backup-utility --restore -t <timestamp>_<version> Here, _ is from the name of the tarball stored in gitlab-backups bucket. In case you want to provide a public URL, use the following command kubectl exec <Toolbox pod name> -it -- backup-utility --restore -f <URL> NOTE : You can provide a local path as a URL as long as it's in the format: file:// This process will take time depending on the size of the tarball. The restoration process will erase the existing contents of database, move existing repositories to temporary locations and extract the contents of the tarball. Repositories will be moved to their corresponding locations on the disk and other data, like artifacts, uploads, LFS etc. will be uploaded to corresponding buckets in Object Storage. During restoration, the backup tarball needs to be extracted to disk. This means the Toolbox pod should have disk of necessary size available. For more details and configuration please see the Toolbox documentation. Restore the runner registration token: after restoring, the included runner will not be able to register to the instance because it no longer has the correct registration token (token has been changed with the new GitLab chart installation). Find the new shared runner token located on the admin/runners webpage of your GitLab installation. kubectl get secrets | grep gitlab-runner-secret Find the name of existing runner token Secret stored in Kubernetes kubectl delete secret <runner-secret-name> Delete the existing secret kubectl delete secret <runner-secret-name> Create the new secret with two keys, (runner-registration-token with your shared token, and an empty runner-token) kubectl create secret generic <runner-secret-name> (gitlab-gitlab-runner-secret) --from-literal=runner-registration-token=<new-shared-runner-token> (gitlab-gitlab-runner-token-xxxxx) --from-literal=runner-token=\"\" (optional) Reset the root user\u2019s password: The restoration process does not update the gitlab-initial-root-password secret with the value from backup. For logging in as root, use the original password included in the backup. In the case that the password is no longer accessible, follow the steps below to reset it. Attach to the Webservice pod by executing the command kubectl exec <Webservice pod name> (gitlab-webservice-default-xxxxx-xxxxx) -it -- bash Run the following command to reset the password of root user. Replace #{password} with a password of your choice /srv/gitlab/bin/rails runner \"user = User.first; user.password='#{password}'; user.password_confirmation='#{password}'; user.save!\"","title":"2. Recovery"},{"location":"snowflake/","text":"1. Create a private key for a user/service To create a new user follow the link from snowflake about key creation It's only allowed to create users with keys to automate connection between services. If we need a key for testing purposes should be a temporary user or in a development infrastructure. Important to know that only a SECURITYADMIN user can modify users to have a key pair access. 2. Key rotation policy Key rotation is a must to have a high security standard, at this moment is important to know is a manual process, each first week of the month should be a calendar task in the devops team to change it a redeployment the infrastructure with these new keys. Right now is not automated because no make sense to do it, you have to have a static user that can modify these stuff manually in the web, if we discover a way to do it automatically without a static user or a bastion one we could think about it. 3. Add a New provider 4. Rules Each private key must have a passphrase. Each new provider must have their own key Devops team must renew keys each month and redeploy the infra. With passion from the Intelygenz Team @ 2021","title":"Datalake"},{"location":"snowflake/#1-create-a-private-key-for-a-userservice","text":"To create a new user follow the link from snowflake about key creation It's only allowed to create users with keys to automate connection between services. If we need a key for testing purposes should be a temporary user or in a development infrastructure. Important to know that only a SECURITYADMIN user can modify users to have a key pair access.","title":"1. Create a private key for a user/service"},{"location":"snowflake/#2-key-rotation-policy","text":"Key rotation is a must to have a high security standard, at this moment is important to know is a manual process, each first week of the month should be a calendar task in the devops team to change it a redeployment the infrastructure with these new keys. Right now is not automated because no make sense to do it, you have to have a static user that can modify these stuff manually in the web, if we discover a way to do it automatically without a static user or a bastion one we could think about it.","title":"2. Key rotation policy"},{"location":"snowflake/#3-add-a-new-provider","text":"","title":"3. Add a New provider"},{"location":"snowflake/#4-rules","text":"Each private key must have a passphrase. Each new provider must have their own key Devops team must renew keys each month and redeploy the infra. With passion from the Intelygenz Team @ 2021","title":"4. Rules"}]}