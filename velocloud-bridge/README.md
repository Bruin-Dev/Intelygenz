# Table of contents
- [Velocloud SDK integration](#velocloud-sdk-integration)
  * [Connecting to several clusters](#connecting-to-several-clusters)
  * [Service logic](#service-logic)
  * [Parallel bridges](#parallel-bridges)

# Velocloud SDK integration

## Connecting to several clusters
Velocloud provided us with an SDK. The SDK is currently located in the `custompackages` folder, so we use a locked
and customized (for large amounts of requests) version of it.

The service's Velocloud client will create a Velocloud's SDK client for each cluster the service must connect to.

Credentials are put inside an enviroment variable with the next schema:
`some.host.name+hostusername+hostpassword;other.host.name+otherusername+otherpassword`

In the `config.py`script, there's a way to split this into an array of dictionaries like this one:

````
        {'url': "some.host.name",
         'username': "hostusername",
         'password': "hostpassword"
         }
````

## Service logic
The bridge will subscribe to both `edge.list.request` and `edge.status.request`.

When a message is received from `edge.list.request` the bridge will call upon velocloud and publish a list of edge info
to `edge.list.response`. 

If a filter is given in the `edge.list.request` message then only the edges in the filter will be published.

When a message is received from `edge.status.request` the bridge will get the specific edge status and link status and
send to `edge.status.response`

A bridge instance can perform any task generated by an orchestrator as long as they both have the same credentials array
for velocloud clusters, since the bridge will check which cluster it should go when getting the task from 
`edge.list.request` and `edge.status.request` queue.


## Parallel bridges
Is possible to have more than one replica of the bridge working. They are in the same `durable group`(durable_name + queue in code) in NATS, so they share
the same offset as long as they belong to the same `durable group`.
