<div align="center">
<img src="https://media.licdn.com/dms/image/C4E0BAQHrME9aCW6ulg/company-logo_200_200/0?e=2159024400&v=beta&t=6xMNS1zK1F8asBlM16EzbJ4Im7SlQ8L7a7sgcaNzZQE"  width="200" height="200">
</div>

|           Module           |                                                                                                              Coverage                                                                                                              |
|:--------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|        bruin-bridge        |               [![bruin-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=bruin-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)               |
|       customer-cache       |             [![customer-cache-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=customer-cache-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)             |
|        digi-bridge         |                [![digi-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=digi-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)                |
|     digi-reboot-report     |         [![digi-reboot-report-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=digi-reboot-report-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)         |
|         dri-bridge         |                 [![dri-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=dri-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)                 |
|  email-tagger-kre-bridge   |    [![email-tagger-kre-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=email-tagger-kre-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)    |
|    email-tagger-monitor    |       [![email-tagger-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=email-tagger-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)       |
|       fraud-monitor        |              [![fraud-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=fraud-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)              |
| hawkeye-affecting-monitor  |  [![hawkeye-affecting-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=hawkeye-affecting-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)  |
|       hawkeye-bridge       |             [![hawkeye-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=hawkeye-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)             |
|   hawkeye-customer-cache   |     [![hawkeye-customer-cache-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=hawkeye-customer-cache-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)     |
|   hawkeye-outage-monitor   |     [![hawkeye-outage-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=hawkeye-outage-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)     |
| intermapper-outage-monitor | [![intermapper-outage-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=intermapper-outage-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master) |
|    last-contact-report     |        [![last-contact-report-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=last-contact-report-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)        |
|    lumin-billing-report    |         [![lumin-billing-report](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=lumin-billing-report-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)          |
|          notifier          |                   [![notifier-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=notifier-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)                   |
|    notifications-bridge    |       [![notifications-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=notifications-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)       |
| repair-tickets-kre-bridge  |  [![repair-tickets-kre-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=repair-tickets-kre-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)  |
|   repair-tickets-monitor   |     [![repair-tickets-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=repair-tickets-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)     |
| service-affecting-monitor  |  [![service-affecting-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=service-affecting-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)  |
|   service-outage-monitor   |     [![service-outage-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=service-outage-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)     |
|         t7-bridge          |                  [![t7-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=t7-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)                  |
|       tnba-feedback        |              [![tnba-feedback-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=tnba-feedback-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)              |
|        tnba-monitor        |               [![tnba-monitor-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=tnba-monitor-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)               |
|      velocloud-bridge      |           [![velocloud-bridge-test](https://gitlab.intelygenz.com/mettel/automation-engine/badges/master/coverage.svg?job=velocloud-bridge-test)](https://gitlab.intelygenz.com/mettel/automation-engine/commits/master)           |

# Table of Contents

- [Table of Contents](#table-of-contents)
- [Project structure](#project-structure)
  - [Naming conventions](#naming-conventions)
  - [Commit Message Format](#commit-message-format)
    - [Commit Message Header](#commit-message-header)
      - [Type](#type)
      - [Types that trigger jobs in pipelines](#types-that-trigger-jobs-in-pipelines)
      - [Scope](#scope)
      - [Short summary](#short-summary)
    - [Commit Message Body](#commit-message-body)
    - [Commit Message Footer](#commit-message-footer)
    - [Revert commits](#revert-commits)
    - [Example](#example)
  - [Adding new microservices to the system](#adding-new-microservices-to-the-system)
    - [automation-engine](#automation-engine)
    - [infra-as-code](#infra-as-code)
    - [helm](#helm)
- [Technologies used](#technologies-used)
- [Developing flow](#developing-flow)
  - [Deploying just a subset of microservices](#deploying-just-a-subset-of-microservices)
  - [DOD(Definition of Done)](#doddefinition-of-done)
- [Running the project](#running-the-project)
  - [Python 3.6](#python-36)
  - [Docker and Docker Compose](#docker-and-docker-compose)
  - [Docker ECR private repository](#docker-ecr-private-repository)
  - [Docker custom images and Python Libraries](#docker-custom-images-and-python-libraries)
  - [Env files](#env-files)
  - [Finish up](#finish-up)
- [EKS and KRE](#eks-and-kre)
  - [Access Control](#access-control)
    - [Roles](#roles)
    - [Roles assigned to Users](#roles-assigned-to-users)
  - [Access Configuration](#access-configuration)
    - [Prerequisites](#prerequisites)
    - [Setup](#setup)
- [Lists of projects READMEs](#lists-of-projects-readmes)
  - [Microservices](#microservices)
  - [Acceptance Tests](#acceptance-tests)
- [Good Practices](#good-practices)
- [Setting up logs with Papertrail](#setting-up-logs-with-papertrail)
- [Testing device](#testing-device)
- [METRICS](#metrics)

# Project structure

## Naming conventions

- For folders containing services: kebab-case
- For a python package(directory): all lowercase, without underscores
- For a python module(file): all lowercase, with underscores only if improves readability.
- For a python class: should use CapsWord convention.
- Virtual env folders should be `project-name-env`. In case it is in a custom package as a user test environment 
it should be `package-example-env`

From [PEP-0008](https://www.python.org/dev/peps/pep-0008/#package-and-module-names)
Also check this, more synthesized [Python naming conventions](https://visualgit.readthedocs.io/en/latest/pages/naming_convention.html) 

## <a name="commit"></a> Commit Message Format
*This specification is inspired by and supersedes the AngularJS commit message format.*

We have very precise rules over how our Git commit messages must be formatted. This format leads to easier to read commit history.
**If the rules not match, the jobs of the pipeline will not be triggered.**

Each commit message consists of a *header*, a *body*, and a *footer*.

```
<header>
<BLANK LINE>
<body>
<BLANK LINE>
<footer>
```

The `header` is mandatory and must conform to the [Commit Message Header](#commit-header) format.

The `body` is optional for all commits except for those of type `perf` although the recommendation is to always use it. For `perf` case is required that `BREAKING CHANGE: `part exists for 
semantic-release be able to create a new ~~Major~~ Breaking Release.
When the body is present it must be at least 20 characters long and must conform to the [Commit Message Body](#commit-body) format.

The `footer` is optional. The [Commit Message Footer](#commit-footer) format describes what the footer is used for and the structure it must have.

Any line of the commit message cannot be longer than 50 characters.

### <a name="commit-header"></a>Commit Message Header
```
<type>(<scope>): <short summary>
  │       │             │
  │       │             └─⫸ Summary in present tense. Not capitalized. No period at the end.
  │       │
  │       └─⫸ Commit Scope: infra-as-code|ci-cd|helm|bruin-bridge|ci-utils|customer-cache|digi-bridge|
  │                          digi-reboot-report|email-tagger-kre-bridge|email-tagger-kre-monitor|
  │                          hawkeye-affecting-monitor|hawkeye-bridge|hawkeye-customer-cache|
  │                          hawkeye-outage-monitor|intermapper-outage-monitor|last-contact-report|links-metrics-api|
  │                          links-metrics-collector|lumin-billing-report|notifier|service-affecting-monitor|
  │                          service-outage-monitor|t7-bridge|tnba-feedback|tnba-monitor|velocloud-bridge
  │
  └─⫸ Commit Type: build|ci|docs|feat|fix|perf|refactor|test
```
The `<type>` and `<short summary>` fields are mandatory, the `(<scope>)` field is optional.
**NOTE**: beware with space between `(scope):` and `short summary`, it's necessary for semantic-release functionality.

#### Type
This part is very important, depend of the type we use, the pipeline will trigger a job or not.
Must be one of the following:

* **build**: Changes that affect the build system or external dependencies (example scopes: terraform, npm)
* **ci**: Changes to our CI configuration files and scripts (example scopes: gitlab-ci.yml files)
* **docs**: Documentation only changes
* **feat**: A new feature
* **fix**: A bug fix
* **perf**: A code change that improves performance. (also needs the text `BREAKING CHANGE:` in the commit description to create performance release)
* **refactor**: A code change that neither fixes a bug nor adds a feature
* **test**: Adding missing tests or correcting existing tests
* **destroy**: Destroy resources. Used to trigger pipelines jobs for destroy sensitive resources like basic_infra (manual jobs).

#### Types that trigger jobs in pipelines
* *feat|fix|perf|refactor* will trigger jobs: `validation`(linters), `unit_tests`, `basic_infra`, `build`, `helm` and `destroy_helm` for *ephemeral environments*
* *feat|fix|perf|refactor* will trigger jobs: `validation`(linters), `unit_tests`, `basic_infra`, `build`, `semantic-release`, and `helm` for *production environment*
* *feat|fix|perf|refactor|build(kre)* will trigger jobs: `kre_basic_infra`, `deploy_kre_runtime` and `destroy_kre_runtime` for *development environment* (manual jobs)
* *feat|fix|perf|refactor|build(kre)* will trigger jobs: `kre_basic_infra` and `deploy_kre_runtime` for *production environment* (manual jobs)
* *destroy* will trigger jobs: `destroy_basic_infra`, `destroy_helm` and `destroy_kre_basic_infra` and `destroy_kre_runtime` for all environments (manual jobs)

#### Scope
The scope should be the name of the npm package affected (as perceived by the person reading the changelog generated from commit messages).

#### Short summary
Use the summary field to provide a succinct description of the change:

use the imperative, present tense: "change" not "changed" nor "changes"
don't capitalize the first letter
no dot (.) at the end

### <a name="commit-body"></a>Commit Message Body
Just as in the summary, use the imperative, present tense: "fix" not "fixed" nor "fixes".

Explain the motivation for the change in the commit message body. This commit message should explain why you are making the change. You can include a comparison of the previous behavior with the new behavior in order to illustrate the impact of the change.

### <a name="commit-footer"></a>Commit Message Footer
The footer can contain information about breaking changes and is also the place to reference GitHub issues, Jira tickets, and other PRs that this commit closes or is related to.

```
BREAKING CHANGE: <breaking change summary>
<BLANK LINE>
<breaking change description + migration instructions>
<BLANK LINE>
<BLANK LINE>
Fixes #<issue number>
```

Breaking Change section should start with the phrase "BREAKING CHANGE: " followed by a summary of the breaking change, a blank line, and a detailed description of the breaking change that also includes migration instructions.

### Revert commits
If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit.

The content of the commit message body should contain:

- information about the SHA of the commit being reverted in the following format: `This reverts commit <SHA>`,
- a clear description of the reason for reverting the commit message.

### Example

The table below shows which commit message gets you which release type when semantic-release runs (using the default configuration):

| Commit message                                                                                                                                                                                | Release type               | Tag       |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------- | --------- |
| `fix(bruin-bridge): stop 'something' breaking when too much pressure applied`                                                                                                                 | Patch Release              | 1.0.**1** |
| `feat(bruin-bridge): add 'something' option to 20`                                                                                                                                            | ~~Minor~~ Feature Release  | 1.**1**.0 |
| `perf(bruin-bridge): remove something option`<br><br>`BREAKING CHANGE: The something option has been removed.`<br>`The default something value of 10 is always used for performance reasons.` | ~~Major~~ Breaking Release | **2**.0.0 |

## Adding new microservices to the system
In addition to creating the microservice folder and all the standards files inside that folder, you must add this
new microservice and any new env variables to the system's files. This is done in order to add 
it to both the infrastructure and the pipeline.
You will need to add/modify files in the folders of the 
- [automation-engine](#automation-engine)  
- [infra-as-code](#infra-as-code) 
- [helm](#helm) 

Any new env variables should be added to the gitlab. And if there are two different var for PRO and DEV
specify it by appending `_PRO` or `_DEV` to the variable name on the gitlab.

[FOLLOW THIS HOWTO](docs/CREATE_NEW_MICROSERVICE.md). explains step by step the whole process to create a new microservice that can be deployed in any environment with pipelines.

# Technologies used

- [Python 3.6](https://www.python.org/downloads/release/python-360/)
- [Python asyncio](https://docs.python.org/3/library/asyncio.html)
- [Quart](http://pgjones.gitlab.io/quart/)
- [Quart OpenAPI](https://github.com/factset/quart-openapi)
- [Pytest](https://docs.pytest.org/en/latest/)
- [Behave](https://pypi.org/project/behave/)
- [Pip](https://pypi.org/project/pip/)
- [Virtualenv](https://virtualenv.pypa.io/en/latest/)
- [Hypercorn to deploy Quart server](https://pgjones.gitlab.io/hypercorn/)
- [Requests for python HTTP requests](http://docs.python-requests.org/en/master/)
- [NATS (as event bus)](https://github.com/nats-io/nats-server)
- [Docker](https://www.docker.com/)
- [Docker-compose](https://docs.docker.com/compose/)
- [markdown-toc (for Table of Contents generation in READMEs)](https://github.com/jonschlinkert/markdown-toc)
- [PEP8 pre-commit hook](https://github.com/cbrueffer/pep8-git-hook) **MANDATORY**
- [AWS Fargate](https://aws.amazon.com/fargate/)

# Developing flow

- Create a branch from development
  
  - "feature" branches starts with `feature/<feature-name>` or `dev/feature/<feature-name>`
  - "fix" branches starts with `fix/<issue-name>` or `dev/fix/<issue-name>`

  Branches whose name begins with `dev/feature/<feature-name>` or `dev/fix/issue-name` will perform both [CI](docs/PIPELINES.md#continuous-integration-ci) and [CD](docs/PIPELINES.md#continuous-delivery-cd) processing, while those whose name begins with `feature-<feature-name>` or `fix-<issue-name>` will perform only [CI](docs/PIPELINES.md#continuous-integration-ci) processing.

  >It is strongly recommended to always start with a `feature/<feature-name>` or `fix/<issue-name>` branch, and once the development is ready, rename it to `dev/feature/<feature-name>` or `dev/fix/<issue-name>` and push this changes to the repository.

- When taking a fix or a feature, create a new branch. After first push to the remote repository, start a Merge Request with the title like the following: "WIP: your title goes here". That way, Maintainer and Approvers can read your changes while you develop them.
- It is possible to skip the entire pipeline when uploading a new branch to the repository. To do so, the following command must be executed.
  ```bash
  $ git push origin <branch_name> -o ci.skip
  ```
  >If this command is executed in the following commits the CI/CD pipeline will be executed directly on those modules that have changed in each commit.
- **Remember that all code must have automated tests(unit and integration and must be part of an acceptance test) in it's pipeline.** 
- Assign that merge request to a any developer of the repository. Also add any affected developer as Approver. I.E: if you are developing a microservice which is part of a process, you should add as Approvers both the developers of the first microservice ahead and the first behind in the process chain. Those microservices will be the more affected by your changes. 
- When a branch is merged into master, it will be deployed in production environment.
- When a new branch is created, it will be deployed in a new Fargate cluster. When a branch is deleted that cluster is deleted. **So every merge request should have "delete branch after merge"**
- You can also check in gitlab's project view, inside Operations>Environments, to see current running environments

## Deploying just a subset of microservices

Due to the limited number of task instances available per account in AWS (50 instances at the time of this writing), it is highly recommended that developers configure just the tasks they need to use for their deployments so ephemeral environments do not consume more AWS resources (task instances) than needed. To do so, they must perform the following steps:

1. There are two jobs defined in the `infra-as-code/.gitlab-ci.yml` file: `deploy-branches` and `check-ecs-resources-branches`. A set of variables are used within them to configure the number of tasks per microservice. The naming of these variables follows the convention `<service_name>_desired_tasks`, where `service_name` is the name of the microservice declared in AWS. These variables are declared in the global variables section of the `.gitlab-ci.yml` file in the repository root.
    > If the variable is set to 0, no tasks instances will be created for that microservice. The corresponding service won't be created at ECS either.

2. If the developer has doubts about what microservices should be taken into account for an ephemeral environment, they should take a look at the README file of that microservice. After guessing that, the corresponding `<service_name>_desired_tasks` must be set with a minimal value of 1 in order to get the task instances created when the deployment finishes.

3. Once the development has finished, the value of `<service_name>_desired_tasks` variables that were modified must be reverted to the value they hold in the `master` branch.

The following example shows how to configure `<service_name>_desired_tasks` variables strictly needed to have the `service-affecting-monitor` microservice working in an ephemeral environment, including microservices it depends on.

```sh
variables:
  . . .
  BRUIN_BRIDGE_DESIRED_TASKS: 0
  CTS_BRIDGE_DESIRED_TASKS: 0
  CUSTOMER_CACHE_DESIRED_TASKS: 1
  DISPATCH_PORTAL_BACKEND_DESIRED_TASKS: 1
  HAWKEYE_AFFECTING_MONITOR_DESIRED_TASKS: 0
  HAWKEYE_BRIDGE_DESIRED_TASKS: 0
  HAWKEYE_CUSTOMER_CACHE_DESIRED_TASKS: 0
  HAWKEYE_OUTAGE_MONITOR_DESIRED_TASKS: 0
  LAST_CONTACT_REPORT_DESIRED_TASKS: 1
  LIT_BRIDGE_DESIRED_TASKS: 1
  METRICS_PROMETHEUS_DESIRED_TASKS: 0
  NATS_SERVER_DESIRED_TASKS: 0
  NATS_SERVER_1_DESIRED_TASKS: 1
  NATS_SERVER_2_DESIRED_TASKS: 1
  NOTIFIER_DESIRED_TASKS: 1
  NOTIFICATIONS_BRIDGE_DESIRED_TASKS: 1
  SERVICE_AFFECTING_MONITOR_DESIRED_TASKS: 1
  SERVICE_OUTAGE_MONITOR_1_DESIRED_TASKS: 0
  SERVICE_OUTAGE_MONITOR_2_DESIRED_TASKS: 0
  SERVICE_OUTAGE_MONITOR_3_DESIRED_TASKS: 0
  SERVICE_OUTAGE_MONITOR_4_DESIRED_TASKS: 0
  SERVICE_OUTAGE_MONITOR_TRIAGE_DESIRED_TASKS: 0
  T7_BRIDGE_DESIRED_TASKS: 0
  VELOCLOUD_BRIDGE_DESIRED_TASKS: 5
  . . .
```

## DOD(Definition of Done)

If any of the next requirements is not fulfilled in a merge request, merge request can't be merged. 

- Each service must have unit tests with a coverage percent of the 80% or more.
- Each service must have it's dockerfile and must be referenced in the docker-compose.
- Each service must have a linter job and a unit tests job in the gitlab.ci pipeline.
- If it is a new service, all the terraform code to deploy it should be present.
- Developers should take care of notify the devops/tech lead of putting in the pipeline env any environment variable needed in the pipeline's execution.

# Running the project

This tutorial assumes Ubuntu 18.04 - it might work in other versions of Ubuntu though, but hasn't been tested.

## Python 3.6

Open a terminal and run:

`$ python3 -V`

Python 3.6 is pre-installed in Ubuntu 18.04 so the output should be something like:

`Python 3.6.8`

## Docker and Docker Compose

Remove any old versions and install the latest one:

```bash
$ apt update -y
$ apt -y upgrade
$ apt remove docker docker-engine docker.io
$ apt install docker.io
```

For Docker Compose there's a good tutorial in [the official docs](https://docs.docker.com/compose/install/). Summarizing it says:

```bash
$ curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
$ chmod +x /usr/local/bin/docker-compose
$ docker-compose --version
```

Last command should output something like:

`docker-compose version 1.24.1, build 4667896b`

## Docker ECR private repository

For the images of the Python microservices, one of the custom images uploaded to the ECR repository used in the project is used as a base, these are generated from a [specific repository](https://gitlab.intelygenz.com/mettel/docker_images) for this purpose.

In order to use these images it will be necessary to have configured the authentication with this private repository. To do this it is necessary to perform the following steps:

1. Install awscli, executing from a terminal the following command

    ```bash
    $ sudo pip3 install awscli -U
    ```

2. Set up a specific profile with the user's credentials in the AWS account used in the project. To do this, if the `~/.aws/credentials` file does not exist, it will be necessary to modify or create it and add the following

   ```bash
   [mettel-automation]
   aws_access_key_id = <user_aws_access_key_id>
   aws_secret_access_key = <user_secret_access_key>
   ```

3. Configure the profile created in the previous step, to do this it is necessary to modify or create in case there is no `~/.aws/config` and add the following:

   ```bash
   [profile mettel-automation]
   region=us-east-1
   output=json
   ```

4. Use the awscli tool to generate a new entry in `~/.docker/config.json` with the necessary credentials for the ECR repository used in the repository. To do this you need to run the following command:

   ```bash
   $(aws ecr get-login --no-include-email --profile mettel-automation)
   ```

   The above command creates a temporary token in the `~/.docker/config.json` file, so it is possible that after a while an error like the one below will occur when trying to access the ECR repository:

    ```bash
    ERROR: Service 'last-contact-report' failed to build: pull access denied for 374050862540.dkr.ecr.us-east-1.amazonaws.com/automation-python-3.6, repository does not exist or may require 'docker login': denied: Your Authorization Token has expired. Please run 'aws ecr get-login --no-include-email' to fetch a new one.
    ```

   To solve this error, simply execute the command mentioned in this step for the generation of a new token.

## Env files

Clone the mettel repo and run:

```bash
$ cd installation-utils
$ python3 -m pip install -r requirements.txt
$ python3 environment_files_generator.py --aws-profile=<profile>
```

The `aws-profile` argument is optional, `ops-mettel` is used by default.

## Finish up

Run:

`$ docker-compose up --build`

# EKS and KRE

- Automation-engine is deployed in [Kubernetes](https://kubernetes.io/docs/home/) cluster using [EKS](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html).

- Additionally we use [KRE](https://konstellation-io.github.io/website/) project for IA, it has been deployed as well in a [Kubernetes](https://kubernetes.io/docs/home/) cluster using [EKS](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html) for each of the necessary environments.

**NOTE:** *to edit KRE environments we need to use specific commits messages to trigger pipeline jobs, see the [Commit Message Format](#commit) for more info.*
## Access Control

In the creation and possible updates in the EKS clusters an association is made between IAM roles created for each of the project users and `ClusterRole` and `ClusterRoleBinding` created in these clusters. In this way, each user will have access to certain resources of both clusters.

### Roles

IAM roles are created for each of the users, although these are distinguished into three categories according to the tag `Project-Role` of them, this tag will also be used to associate them to a `ClusterRole` of the EKS cluster and allow access to certain resources of the same. The mentioned tags are the following:

- **developer**: This tag identify users that will only have access to the [pods](https://kubernetes.io/docs/concepts/workloads/pods/) of any namespace to perform `get`, `list`, `watch`, and `delete` on them.

- **devops**: This tag identify users that will access to any resource in any namespace.

- **ops**: This tag identify users that will have the same access of the developer, but with additionally options to manage [configmaps](https://kubernetes.io/docs/concepts/configuration/configmap/), [secrets](https://kubernetes.io/docs/concepts/configuration/secret/) and [deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). This role is util for a developer can manage ephemeral environments.

### Roles assigned to Users

Below are the roles created:

| Environments   | Roles in Project       | IAM role per project                                                                                     |
| -------------- | ---------------------- | -------------------------------------------------------------------------------------------------------- |
| dev, pro       | developer, devops, ops | `automation-engine:` arn:aws:iam::[aws_account_id]:role/[env]-[role]-mettel-automation-[aws-username]    |
|                |                        | `konstellation:` arn:aws:iam::[aws_account_id]:role/[env]-[role]-mettel-automation-kre-[aws-username]    |

> The number of `aws_account_id` is available through the `.csv` file with the AWS credentials for each user. If the user does not have one value, contact the *DevOps* of the project to get the data.

## Access Configuration

### Prerequisites

The following tools are required to access to the EKS clusters created for each environment:

- **pip3**: It is necessary to have `pip3` for install python packages required to access the KRE clusters. It can be installed with the following command show below:

  ```sh
  $ sudo apt-get install python3-pip
  ```

- **kubectl**: It is necessary to have the command-line tool for Kubernetes in version `1.21.9` to interact with the cluster. It can be installed by following the commands shown below:

  ```sh
  $ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.21.9/bin/linux/amd64/kubectl
  $ chmod +x ./kubectl
  $ sudo mv ./kubectl /usr/local/bin/kubectl
  ```

- **k9s**: utility to manage and inspect a kubernetes cluster. It can be installed with the following the commands show below:

  1. Get the latest tag name:

      ```sh
      LATEST_RELEASE_TAG=$(curl --silent "https://api.github.com/repos/derailed/k9s/releases/latest" | jq -r .tag_name)
      ```

  2. Download the binary according to the OS used:

     - *MacOS*:

       ```sh
       $ curl -L "https://github.com/derailed/k9s/releases/download/${LATEST_RELEASE_TAG}/k9s_darwin_x86_64.tar.gz" -o k9s.tar.gz
       ```

     - *Linux*:

       ```sh
       $ curl -L "https://github.com/derailed/k9s/releases/download/${LATEST_RELEASE_TAG}/k9s_linux_x86_64.tar.gz" -o k9s.tar.gz
       ```

  3. Unzip the files downloaded in the previous step and configure:

     ```sh
     $ tar -zxvf k9s.tar.gz
     $ chmod +x ./k9s
     $ sudo mv ./k9s /usr/local/bin/k9s
     ```

- **robo3t** (optional): It is necessary install this command line tool to interact with [MongoDB](https://www.mongodb.com/) DBs used in KRE. It can be installed with the following commands:

    - *MacOS*:
      
      ```sh
      $ LATEST_RELEASE_URL=$(curl --silent "https://api.github.com/repos/Studio3T/robomongo/releases/latest" | jq -r '.assets[] | select(.name | contains("dmg")) | .browser_download_url')
      $ curl -L "${LATEST_RELEASE_URL}" -o robo3t
      ```
      > Once downloaded, to install it just double click on the file and drag the robo3t icon to 'Applications'.

  - *Linux*:

      ```sh
      $ LATEST_RELEASE_URL=$(curl --silent "https://api.github.com/repos/Studio3T/robomongo/releases/latest" | jq -r '.assets[] | select(.name | contains("linux")) | .browser_download_url')
      $ LATEST_RELEASE_NAME=$(curl --silent "https://api.github.com/repos/Studio3T/robomongo/releases/latest" | jq -r '.assets[] | select(.name | contains("linux")) | .name')
      $ curl -L "${LATEST_RELEASE_URL}" -o robo3t.tar.gz
      $ tar -zxvf robo3t.tar.gz
      $ sudo mkdir /opt/robomongo
      $ sudo mv ${LATEST_RELEASE_NAME%%.tar.gz}/* /opt/robomongo
      $ sudo chmod +x /opt/robomongo/bin/robo3t
      $ sudo ln -s /opt/robomongo/bin/robo3t /usr/bin/robo3t
      $ cat <<EOF | tee /tmp/robomongo.txt
      [Desktop Entry]
      Encoding=UTF-8
      Name=Robomongo
      Comment=Launch Robomongo
      Icon=/opt/robomongo/robomongo.png
      Exec=/usr/bin/robo3t
      Terminal=false
      Type=Application
      Categories=Developer;
      StartupNotify=true
    EOF
      $ mv /tmp/robomongo.txt ~/.local/share/applications/robomongo.desktop
      ``` 


- **awscli**: It is necessary to have the command line tool for AWS in version `2.x.x` or higher. It can be installed with `pip`, `brew`, or with an installer. Go to [aws documentation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) reference to install it.

NOTE: is very important to have `awscli` properly working because is used to create the Kubernetes configuration file (~/.kube/config) to interact with the EKS cluster (this feature is only available in the most recent versions). Check that you have installed the correct compatible version with:

     ```sh
     aws --version
     ```

### Setup

The following steps must be followed to set up the configuration to access any of the EKS clusters:

1. Setting up a profile to use `awscli` in the project, this will require two steps:

   - Configure the credentials for the profile, adding the following to file `~/.aws/credentials`:
   
     ```sh
     [mettel-automation]
     aws_access_key_id     = <aws_access_key_id>
     aws_secret_access_key = <aws_secret_access_key>
     ```
  
   - Create the settings for the profile created in the previous step, adding the following to file `~/.aws/config`:
    
     ```sh
     [profile mettel-automation]
     region=us-east-1
     output=json
     ```

2. Add the configuration of the IAM role created for the user in the awscli config file. The role IAM arn created for each user uses the following format `arn:aws:iam::<aws_account_id>:role/<env>-<role>-mettel-automation-kre-<iam_user_name>`, where the `role_tag` value will be one of those explained in the [previous section](#roles).

   The following is an example of the configuration for a a Ops role in dev, and Developer role in pro through the modification of the `~/.aws/config` file for Automation-Engine cluster:

   ```
   [profile ops-role-automation-dev]
   role_arn = arn:aws:iam::<aws_account_id>:role/dev-ops-mettel-automation-your.username
   source_profile = mettel-automation
   region=us-east-1
   output=json

   [profile developer-role-automation-pro]
   role_arn = arn:aws:iam::<aws_account_id>:role/pro-developer-mettel-automation-your.username
   source_profile = mettel-automation
   region=us-east-1
   output=json
   ```

   The following is an example of the configuration of the Devops role in pro through the modification of the `~/.aws/config` file for konstellation cluster:

   ```
   [profile devops-role-kre-pro]
   role_arn = arn:aws:iam::<aws_account_id>:role/pro-devops-mettel-automation-kre-your.username
   source_profile = mettel-automation
   region=us-east-1
   output=json
   ```

Remember: you have to know the role (developer, devops or ops) assigned per environment (dev or pro), so contact with the project *devops* to get this specific access info for you. If you will have access to both envs, you have to configure two AWS Profiles, one for `dev` and other for `pro`.

3. Add to the Kubernetes kubeconfig file the EKS cluster to which the user wants to connect. This can be done through the following `awscli` command:

    ```sh
    $ aws eks update-kubeconfig --name <eks_cluster_name> --profile <aws_config_file_profile_name>

    # example Automation-Enginer DEV cluster with profile Ops:
    $ aws eks update-kubeconfig --name mettel-automation-dev --profile ops-role-automation-dev

    # example Automation-Enginer PRO cluster with profile Developer:
    $ aws eks update-kubeconfig --name mettel-automation --profile developer-role-automation-pro

    # example Konstellation PRO cluster with profile Devops:
    $ aws eks update-kubeconfig --name mettel-automation-kre --profile devops-role-kre-pro
    ```

   > The name of the EKS cluster's are, production automation engine: `mettel-automation`, production konstelation: `mettel-automation-kre`, development automation engine: `mettel-automation-dev` and development konstellation: `mettel-automation-kre-dev`.

4. Open k9s to connect to the cluster, if you have more clusters in you kubeconfig file you can chose you cluster with `context` command inside k9s:

    ```sh
    $ k9s
    
    # after k9s open, select you cluster by put this command in k9s:
    :context
    ```

NOTE: k9s works like text editor `vi` so you can use the most of shortcuts, like ':/' to find or ':q' to exit. You can find more information about k9s in his [github page](https://github.com/derailed/k9s).

# Lists of projects READMEs

## Microservices

- [Base microservice](base-microservice/README.md)
- [Bruin bridge](bruin-bridge/README.md)
- [Customer cache](customer-cache/README.md)
- [Hawkeye affecting monitor](hawkeye-affecting-monitor/README.md)
- [Hawkeye bridge](hawkeye-bridge/README.md)
- [Hawkeye customer cache](hawkeye-customer-cache/README.md)
- [Hawkeye outage monitor](hawkeye-outage-monitor/README.md)
- [Last contact report](last-contact-report/README.md)
- [Lumin billing report](lumin-billing-report/README.md)
- [Notifier](notifier/README.md)
- [Notifications bridge](notifications-bridge/README.md)
- [Service affecting monitor](service-affecting-monitor/README.md)
- [Service outage monitor](service-outage-monitor/README.md)
- [T7 bridge](t7-bridge/README.md)
- [TNBA feedback](tnba-feedback/README.md)
- [TNBA monitor](tnba-monitor/README.md)
- [Velocloud bridge](velocloud-bridge/README.md)

## Acceptance Tests

- [Acceptance tests](acceptance-tests/README.md)

# Good Practices

- Documentation **must** be updated as frequently as possible. It's recommended to annotate every action taken in the development phase, and afterwards, add to the documentation the actions or information considered relevant.
- Pair programming is strongly recommended when doing difficult or delicate tasks. It is **mandatory** when a new teammate arrives.
- Solutions of hard problems should be put in common in order to use all the knowledge and points of view of the team.

# Setting up logs with Papertrail

[Papertrail](https://papertrailapp.com/) is used as a centralized system of logs, for each microservice a series of predefined searches are used, with the possibility of creating new ones and/or modifying the current ones. It is necessary to use the [papertrail-provisioning](ci-utils/papertrail-provisioning) tool for this purpose.


# Testing device
In order to create a new test ticket for a VCE the following data should be used:
- Serial: VC05200011984
- Client ID: 30000

Even if the client ID is Mettel's edge is completely ours, IGZ is physically in possession of the device.
# METRICS

- [Prometheus](http://localhost:9090) 
  - Github [link](https://github.com/prometheus/client_python) to documentation of Prometheus
  
  - Prometheus allows us to create counters/gauges in the velocloud-bridge to keep track of the metrics
    about edges processed in the bridge, amount of certain edges states found in the bridge, and amount of certain link states found in the bridge.
  
  - Using prometheus `start_http_server` we can host our metrics on a server. By using the file `prometheus.yml`located at
`/metrics-dashboard/prometheus/` and using the format below, you can add your server to the prometheus app. All the servers connected
to the prometheus app can be found at `http://localhost:9090/targets`.

    ```bash
    - job_name: <the microservice that's hosting you server>
      scrape_interval: 5s
      static_configs:
      - targets: [' <the microservice that's hosting you server>:<that server's port>']
    ```

- [Grafana](http://localhost:3000) admin/password

  - Grafana allows us to create graphs, tables, charts, and etc with the metrics created using Prometheus and other 
    default metrics.
  - [Link](https://prometheus.io/docs/prometheus/latest/querying/functions/) to functions you can use with your 
    Prometheus metrics in Grafana. 
  - You can run the Grafana server at `http://localhost:3000` using the credentials above.
  - In Grafana you can export a dashboard as a json file. By going to `metrics-dashboard/grafana/dashboard-definitions`
    you can add that json file to that folder and whenever the Grafana app is loaded up you can choose to make a new
    dashboard or use the dashboard that you created.  
  - [Link](https://grafana.com/docs/reference/dashboard/) to the documentation to the dashboard's json file.
  - The docker_compose should include the credentials above,specifically the password, in the`GF_SECURITY_ADMIN_PASSWORD` 
    area for the [local docker-compose](docker-compose.yml). Also the `GF_INSTALL_PLUGINS` field can be used to add any plugins you want to add to the
    grafana dashboard.
